{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Using Labeled Data to Train a Network, Use The Network to Label New Videos, and Create Trajectories </h1> \n",
    "<p> Use another notebook create a project, extract frames, and label training/testing data </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RK255E7YoEIt"
   },
   "source": [
    "# DLC with docker\n",
    "This notebook illustrates how to use the Docker container to:\n",
    "- train a network\n",
    "- evaluate a network\n",
    "- analyze a novel video\n",
    "\n",
    "many of the functions have additional parameters / complexity, see the DLC docs for more inf on each.\n",
    "\n",
    "This assumes you already have a project folder with labeled data! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "txoddlM8hLKm"
   },
   "source": [
    "## info about the Docker Environment:\n",
    "Docker is essentially a better alternative to virtual machines. It is able to containerize applications in a way that keeps them seperate the OS and other software. \n",
    "\n",
    "For tensorflow / deeplab cut, docker is critical for two reasons. \n",
    "- It 'claims' the GPU, making it closed for use by other processes.\n",
    "- It queries the GPU properly, in some cases tf (and the python kernal itself) does not know how to properly communincate with the GPU on windows systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4C5WRoS9g5Od"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2017 NVIDIA Corporation\n",
      "Built on Fri_Sep__1_21:08:32_Central_Daylight_Time_2017\n",
      "Cuda compilation tools, release 9.0, V9.0.176\n"
     ]
    }
   ],
   "source": [
    "# make sure you graphic driver is accessable\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HxVNyimFp-PJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the tensorflow version\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> The following is very important as it allow the current GPU process to grow dynamically </h1>\n",
    "Without this option tf will likely run out of VRAM when trying to update the weight tensor. In theory, these options could cause the GPU to run out of memory entirely, but there is no other way to allow training to complete successfully. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#allow video memory growth as network expands to avoid convolutional network errors\n",
    "TF_FORCE_GPU_ALLOW_GROWTH = True\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pm_PC1Q8lRrH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 16720441354705742554,\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 6586089472\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 12626349863795168373\n",
       " physical_device_desc: \"device: 0, name: GeForce RTX 2080, pci bus id: 0000:41:00.0, compute capability: 7.5\",\n",
       " name: \"/device:GPU:1\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 6586089472\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 15045792244527553363\n",
       " physical_device_desc: \"device: 1, name: GeForce RTX 2080, pci bus id: 0000:81:00.0, compute capability: 7.5\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's make sure we see a GPU:\n",
    "#tf.test.gpu_device_name()\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start here for training DeepLabCut and analyzing new videos!\n",
    "<p><br>If the first imports fail, there is again - sadly - an issue with you enviroment. Make sure all packages beside DLC are installed via conda. </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sXufoX6INe6w"
   },
   "outputs": [],
   "source": [
    "#GUIs don't work on in Docker (or the cloud), so label your data locally on your computer! \n",
    "#This notebook is for you to train and run video analysis!\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3K9Ndy1beyfG",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.10.1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we are ready to train!\n",
    "#should see version 2.0.8\n",
    "import deeplabcut\n",
    "deeplabcut.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> change to your path: </h1>\n",
    "<p> this should be the same path as the one in the createDLCproject notebook. The path is the path to the config.yaml file, not the project directory itself </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z7ZlDr3wV4D1"
   },
   "outputs": [],
   "source": [
    "path_config_file = r'F:\\MysoreData\\nbk\\mouseVideoAnalysis\\Box1\\cam1\\box1_cam1-spencerloggia-2021-02-07\\config.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xNi9s1dboEJN"
   },
   "source": [
    "## Create a training dataset\n",
    "This function generates the training data information for DeepCut (which requires a mat file) based on the pandas dataframes that hold label information. The user can set the fraction of the training set size (from all labeled image in the hd5 file) in the config.yaml file. While creating the dataset, the user can create multiple shuffles. \n",
    "\n",
    "After running this script the training dataset is created and saved in the project directory under the subdirectory **'training-datasets'**\n",
    "\n",
    "This function also creates new subdirectories under **dlc-models** and appends the project config.yaml file with the correct path to the training and testing pose configuration file. These files hold the parameters for training the network. Such an example file is provided with the toolbox and named as **pose_cfg.yaml**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed on F:\\MysoreData\\nbk\\mouseVideoAnalysis\\Box1\\cam1\\box1_cam1-spencerloggia-2021-02-07\\labeled-data\\camera-1_clip-36\\CollectedData_spencerloggia.h5\n",
      "failed on F:\\MysoreData\\nbk\\mouseVideoAnalysis\\Box1\\cam1\\box1_cam1-spencerloggia-2021-02-07\\labeled-data\\camera-1_clip-75\\CollectedData_spencerloggia.h5\n",
      "Downloading a ImageNet-pretrained model from http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz....\n",
      "The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.95,\n",
       "  1,\n",
       "  (array([16, 43, 30, 42, 19, 45, 48, 29, 17,  5, 22, 18,  3,  8, 14, 38, 46,\n",
       "          34, 25,  0, 15, 51, 44,  6, 36, 24,  9,  7, 13, 28, 27, 32, 31, 39,\n",
       "          21, 47,  2, 49, 10, 41, 23, 11,  1, 50, 35, 20, 40, 33, 26]),\n",
       "   array([ 4, 12, 37])))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deeplabcut.create_training_dataset(path_config_file, net_type='resnet_50', augmenter_type='imgaug')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now go edit the pose_cfg.yaml to make display_iters: low (i.e. 10), and save_iters: 500\n",
    "\n",
    "Now it is the time to start training the network!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c4FczXGDoEJU"
   },
   "source": [
    "## Start training\n",
    "This function trains the network for a specific shuffle of the training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_pOvDq_2oEJW",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "{'all_joints': [[0], [1], [2]],\n",
      " 'all_joints_names': ['bodypart1', 'bodypart2', 'bodypart3'],\n",
      " 'alpha_r': 0.02,\n",
      " 'batch_size': 1,\n",
      " 'crop_pad': 0,\n",
      " 'cropratio': 0.4,\n",
      " 'dataset': 'training-datasets\\\\iteration-0\\\\UnaugmentedDataSet_box1_cam1Feb7\\\\box1_cam1_spencerloggia95shuffle1.mat',\n",
      " 'dataset_type': 'imgaug',\n",
      " 'decay_steps': 30000,\n",
      " 'deterministic': False,\n",
      " 'display_iters': 1000,\n",
      " 'fg_fraction': 0.25,\n",
      " 'global_scale': 0.8,\n",
      " 'init_weights': 'f:\\\\mysoredata\\\\nbk\\\\mousevideoanalysis\\\\dlc_env_conda\\\\lib\\\\site-packages\\\\deeplabcut\\\\pose_estimation_tensorflow\\\\models\\\\pretrained\\\\resnet_v1_50.ckpt',\n",
      " 'intermediate_supervision': False,\n",
      " 'intermediate_supervision_layer': 12,\n",
      " 'location_refinement': True,\n",
      " 'locref_huber_loss': True,\n",
      " 'locref_loss_weight': 0.05,\n",
      " 'locref_stdev': 7.2801,\n",
      " 'log_dir': 'log',\n",
      " 'lr_init': 0.0005,\n",
      " 'max_input_size': 1500,\n",
      " 'mean_pixel': [123.68, 116.779, 103.939],\n",
      " 'metadataset': 'training-datasets\\\\iteration-0\\\\UnaugmentedDataSet_box1_cam1Feb7\\\\Documentation_data-box1_cam1_95shuffle1.pickle',\n",
      " 'min_input_size': 64,\n",
      " 'mirror': False,\n",
      " 'multi_step': [[0.005, 10000],\n",
      "                [0.02, 430000],\n",
      "                [0.002, 730000],\n",
      "                [0.001, 1030000]],\n",
      " 'net_type': 'resnet_50',\n",
      " 'num_joints': 3,\n",
      " 'optimizer': 'sgd',\n",
      " 'pairwise_huber_loss': False,\n",
      " 'pairwise_predict': False,\n",
      " 'partaffinityfield_predict': False,\n",
      " 'pos_dist_thresh': 17,\n",
      " 'project_path': 'F:\\\\MysoreData\\\\nbk\\\\mouseVideoAnalysis\\\\Box1\\\\cam1\\\\box1_cam1-spencerloggia-2021-02-07',\n",
      " 'regularize': False,\n",
      " 'rotation': 25,\n",
      " 'rotratio': 0.4,\n",
      " 'save_iters': 50000,\n",
      " 'scale_jitter_lo': 0.5,\n",
      " 'scale_jitter_up': 1.25,\n",
      " 'scoremap_dir': 'test',\n",
      " 'shuffle': True,\n",
      " 'snapshot_prefix': 'F:\\\\MysoreData\\\\nbk\\\\mouseVideoAnalysis\\\\Box1\\\\cam1\\\\box1_cam1-spencerloggia-2021-02-07\\\\dlc-models\\\\iteration-0\\\\box1_cam1Feb7-trainset95shuffle1\\\\train\\\\snapshot',\n",
      " 'stride': 8.0,\n",
      " 'weigh_negatives': False,\n",
      " 'weigh_only_present_joints': False,\n",
      " 'weigh_part_predictions': False,\n",
      " 'weight_decay': 0.0001}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting single-animal trainer\n",
      "Starting with imgaug pose-dataset loader (=default).\n",
      "Batch Size is 1\n",
      "Initializing ResNet\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C902C7F0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C902C7F0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002581BD58EF0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002581BD58EF0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x00000258C902CBE0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x00000258C902CBE0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C9104FD0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C9104FD0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C90556D8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C90556D8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C9104550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C9104550>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9055240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9055240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C9104278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C9104278>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C90559B0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C90559B0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C91040B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C91040B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9055978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9055978>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C9104F28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C9104F28>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9055FD0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9055FD0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C9243FD0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C9243FD0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9055CF8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9055CF8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C9104E80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C9104E80>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9055C88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9055C88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x00000258C9200A58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x00000258C9200A58>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C90A6208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C90A6208>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C93B64A8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C93B64A8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C9104E80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C9104E80>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002581BD58EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002581BD58EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C93B6D30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C93B6D30>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9102CF8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9102CF8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C906BD68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C906BD68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C90F34A8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C90F34A8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C906BD68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C906BD68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C93B6B38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C93B6B38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C94C8F28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C94C8F28>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C90A6DD8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C90A6DD8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C906BD68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C906BD68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9451240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9451240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C9102EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C9102EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C94515C0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C94515C0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C960EB38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C960EB38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9451940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9451940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C94C86D8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C94C86D8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C94515C0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C94515C0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C96AD470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C96AD470>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9141C50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9141C50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C94513C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C94513C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9518CC0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9518CC0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C9141048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C9141048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C95185C0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C95185C0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x00000258C968CE10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x00000258C968CE10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C95BEA90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C95BEA90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C95186D8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C95186D8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C97295F8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C97295F8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C97D2C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C97D2C18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C997DDD8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C997DDD8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9657BA8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9657BA8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C99BD2B0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C99BD2B0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9729160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9729160>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C90D5EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C90D5EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9A8F6A0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9A8F6A0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C95BE780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C95BE780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C906B860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C906B860>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C919BBA8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C919BBA8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C97D2668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C97D2668>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C9102B00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C9102B00>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9861EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9861EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C9729160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C9729160>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C98ED668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C98ED668>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C97296A0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C97296A0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9AF5C88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9AF5C88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C97EEB70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C97EEB70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9AF59B0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9AF59B0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C9C943C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C9C943C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9BE5828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9BE5828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C9C47E80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C9C47E80>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9A77F98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9A77F98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C9B440F0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C9B440F0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9A77D68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9A77D68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C9E7BEF0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C9E7BEF0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9BC9240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9BC9240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C9E29D30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C9E29D30>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9E4DA90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9E4DA90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C9B440F0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C9B440F0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9BE59B0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9BE59B0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C9FACEF0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C9FACEF0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9BE5E10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9BE5E10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C9BC9240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C9BC9240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258CA103BE0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258CA103BE0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C97EEB70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258C97EEB70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258CA003F60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258CA003F60>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258CA003F60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258CA003F60>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9E4D588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9E4D588>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258CA057240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258CA057240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258CA2013C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258CA2013C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258CA003080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258CA003080>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9E4D3C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9E4D3C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258CA057FD0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258CA057FD0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258CA1AD400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258CA1AD400>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258CA1DC0B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258CA1DC0B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9EB27B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258C9EB27B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258CA1DC390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258CA1DC390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258CA18D4E0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258CA18D4E0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258CA1DC358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258CA1DC358>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258CB3FAA20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258CB3FAA20>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258CB37B358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258CB37B358>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258CA18DEF0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258CA18DEF0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258CB219358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258CB219358>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258CB613E10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258CB613E10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258CB2D47F0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258CB2D47F0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258CB723DD8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258CB723DD8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258CB5F90F0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258CB5F90F0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258CB6827F0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258CB6827F0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258CB37B358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000258CB37B358>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258CB27A438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000258CB27A438>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x00000258C98C4D68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x00000258C98C4D68>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x00000258C902C128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x00000258C902C128>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "Loading ImageNet-pretrained resnet_50\n",
      "Display_iters overwritten as 10\n",
      "Save_iters overwritten as 1000\n",
      "Training parameter:\n",
      "{'stride': 8.0, 'weigh_part_predictions': False, 'weigh_negatives': False, 'fg_fraction': 0.25, 'mean_pixel': [123.68, 116.779, 103.939], 'shuffle': True, 'snapshot_prefix': 'F:\\\\MysoreData\\\\nbk\\\\mouseVideoAnalysis\\\\Box1\\\\cam1\\\\box1_cam1-spencerloggia-2021-02-07\\\\dlc-models\\\\iteration-0\\\\box1_cam1Feb7-trainset95shuffle1\\\\train\\\\snapshot', 'log_dir': 'log', 'global_scale': 0.8, 'location_refinement': True, 'locref_stdev': 7.2801, 'locref_loss_weight': 0.05, 'locref_huber_loss': True, 'optimizer': 'sgd', 'intermediate_supervision': False, 'intermediate_supervision_layer': 12, 'regularize': False, 'weight_decay': 0.0001, 'crop_pad': 0, 'scoremap_dir': 'test', 'batch_size': 1, 'dataset_type': 'imgaug', 'deterministic': False, 'mirror': False, 'pairwise_huber_loss': False, 'weigh_only_present_joints': False, 'partaffinityfield_predict': False, 'pairwise_predict': False, 'all_joints': [[0], [1], [2]], 'all_joints_names': ['bodypart1', 'bodypart2', 'bodypart3'], 'alpha_r': 0.02, 'cropratio': 0.4, 'dataset': 'training-datasets\\\\iteration-0\\\\UnaugmentedDataSet_box1_cam1Feb7\\\\box1_cam1_spencerloggia95shuffle1.mat', 'decay_steps': 30000, 'display_iters': 1000, 'init_weights': 'f:\\\\mysoredata\\\\nbk\\\\mousevideoanalysis\\\\dlc_env_conda\\\\lib\\\\site-packages\\\\deeplabcut\\\\pose_estimation_tensorflow\\\\models\\\\pretrained\\\\resnet_v1_50.ckpt', 'lr_init': 0.0005, 'max_input_size': 1500, 'metadataset': 'training-datasets\\\\iteration-0\\\\UnaugmentedDataSet_box1_cam1Feb7\\\\Documentation_data-box1_cam1_95shuffle1.pickle', 'min_input_size': 64, 'multi_step': [[0.005, 10000], [0.02, 430000], [0.002, 730000], [0.001, 1030000]], 'net_type': 'resnet_50', 'num_joints': 3, 'pos_dist_thresh': 17, 'project_path': 'F:\\\\MysoreData\\\\nbk\\\\mouseVideoAnalysis\\\\Box1\\\\cam1\\\\box1_cam1-spencerloggia-2021-02-07', 'rotation': 25, 'rotratio': 0.4, 'save_iters': 50000, 'scale_jitter_lo': 0.5, 'scale_jitter_up': 1.25, 'covering': True, 'elastic_transform': True, 'motion_blur': True, 'motion_blur_params': {'k': 7, 'angle': [-90, 90]}}\n",
      "Starting training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 10 loss: 0.2887 lr: 0.005\n",
      "iteration: 20 loss: 0.0631 lr: 0.005\n",
      "iteration: 30 loss: 0.0463 lr: 0.005\n",
      "iteration: 40 loss: 0.0443 lr: 0.005\n",
      "iteration: 50 loss: 0.0387 lr: 0.005\n",
      "iteration: 60 loss: 0.0348 lr: 0.005\n",
      "iteration: 70 loss: 0.0407 lr: 0.005\n",
      "iteration: 80 loss: 0.0356 lr: 0.005\n",
      "iteration: 90 loss: 0.0404 lr: 0.005\n",
      "iteration: 100 loss: 0.0415 lr: 0.005\n",
      "iteration: 110 loss: 0.0288 lr: 0.005\n",
      "iteration: 120 loss: 0.0351 lr: 0.005\n",
      "iteration: 130 loss: 0.0363 lr: 0.005\n",
      "iteration: 140 loss: 0.0390 lr: 0.005\n",
      "iteration: 150 loss: 0.0354 lr: 0.005\n",
      "iteration: 160 loss: 0.0339 lr: 0.005\n",
      "iteration: 170 loss: 0.0389 lr: 0.005\n",
      "iteration: 180 loss: 0.0362 lr: 0.005\n",
      "iteration: 190 loss: 0.0328 lr: 0.005\n",
      "iteration: 200 loss: 0.0342 lr: 0.005\n",
      "iteration: 210 loss: 0.0266 lr: 0.005\n",
      "iteration: 220 loss: 0.0300 lr: 0.005\n",
      "iteration: 230 loss: 0.0355 lr: 0.005\n",
      "iteration: 240 loss: 0.0369 lr: 0.005\n",
      "iteration: 250 loss: 0.0309 lr: 0.005\n",
      "iteration: 260 loss: 0.0339 lr: 0.005\n",
      "iteration: 270 loss: 0.0308 lr: 0.005\n",
      "iteration: 280 loss: 0.0304 lr: 0.005\n",
      "iteration: 290 loss: 0.0317 lr: 0.005\n",
      "iteration: 300 loss: 0.0345 lr: 0.005\n",
      "iteration: 310 loss: 0.0307 lr: 0.005\n",
      "iteration: 320 loss: 0.0296 lr: 0.005\n",
      "iteration: 330 loss: 0.0274 lr: 0.005\n",
      "iteration: 340 loss: 0.0274 lr: 0.005\n",
      "iteration: 350 loss: 0.0344 lr: 0.005\n",
      "iteration: 360 loss: 0.0277 lr: 0.005\n",
      "iteration: 370 loss: 0.0217 lr: 0.005\n",
      "iteration: 380 loss: 0.0287 lr: 0.005\n",
      "iteration: 390 loss: 0.0288 lr: 0.005\n",
      "iteration: 400 loss: 0.0220 lr: 0.005\n",
      "iteration: 410 loss: 0.0276 lr: 0.005\n",
      "iteration: 420 loss: 0.0238 lr: 0.005\n",
      "iteration: 430 loss: 0.0258 lr: 0.005\n",
      "iteration: 440 loss: 0.0280 lr: 0.005\n",
      "iteration: 450 loss: 0.0288 lr: 0.005\n",
      "iteration: 460 loss: 0.0209 lr: 0.005\n",
      "iteration: 470 loss: 0.0277 lr: 0.005\n",
      "iteration: 480 loss: 0.0286 lr: 0.005\n",
      "iteration: 490 loss: 0.0265 lr: 0.005\n",
      "iteration: 500 loss: 0.0272 lr: 0.005\n",
      "iteration: 510 loss: 0.0215 lr: 0.005\n",
      "iteration: 520 loss: 0.0284 lr: 0.005\n",
      "iteration: 530 loss: 0.0241 lr: 0.005\n",
      "iteration: 540 loss: 0.0230 lr: 0.005\n",
      "iteration: 550 loss: 0.0244 lr: 0.005\n",
      "iteration: 560 loss: 0.0213 lr: 0.005\n",
      "iteration: 570 loss: 0.0254 lr: 0.005\n",
      "iteration: 580 loss: 0.0224 lr: 0.005\n",
      "iteration: 590 loss: 0.0233 lr: 0.005\n",
      "iteration: 600 loss: 0.0300 lr: 0.005\n",
      "iteration: 610 loss: 0.0298 lr: 0.005\n",
      "iteration: 620 loss: 0.0285 lr: 0.005\n",
      "iteration: 630 loss: 0.0208 lr: 0.005\n",
      "iteration: 640 loss: 0.0235 lr: 0.005\n",
      "iteration: 650 loss: 0.0259 lr: 0.005\n",
      "iteration: 660 loss: 0.0267 lr: 0.005\n",
      "iteration: 670 loss: 0.0218 lr: 0.005\n",
      "iteration: 680 loss: 0.0230 lr: 0.005\n",
      "iteration: 690 loss: 0.0268 lr: 0.005\n",
      "iteration: 700 loss: 0.0295 lr: 0.005\n",
      "iteration: 710 loss: 0.0320 lr: 0.005\n",
      "iteration: 720 loss: 0.0229 lr: 0.005\n",
      "iteration: 730 loss: 0.0241 lr: 0.005\n",
      "iteration: 740 loss: 0.0218 lr: 0.005\n",
      "iteration: 750 loss: 0.0227 lr: 0.005\n",
      "iteration: 760 loss: 0.0261 lr: 0.005\n",
      "iteration: 770 loss: 0.0185 lr: 0.005\n",
      "iteration: 780 loss: 0.0252 lr: 0.005\n",
      "iteration: 790 loss: 0.0200 lr: 0.005\n",
      "iteration: 800 loss: 0.0222 lr: 0.005\n",
      "iteration: 810 loss: 0.0246 lr: 0.005\n",
      "iteration: 820 loss: 0.0198 lr: 0.005\n",
      "iteration: 830 loss: 0.0208 lr: 0.005\n",
      "iteration: 840 loss: 0.0236 lr: 0.005\n",
      "iteration: 850 loss: 0.0169 lr: 0.005\n",
      "iteration: 860 loss: 0.0251 lr: 0.005\n",
      "iteration: 870 loss: 0.0223 lr: 0.005\n",
      "iteration: 880 loss: 0.0256 lr: 0.005\n",
      "iteration: 890 loss: 0.0261 lr: 0.005\n",
      "iteration: 900 loss: 0.0221 lr: 0.005\n",
      "iteration: 910 loss: 0.0200 lr: 0.005\n",
      "iteration: 920 loss: 0.0240 lr: 0.005\n",
      "iteration: 930 loss: 0.0236 lr: 0.005\n",
      "iteration: 940 loss: 0.0180 lr: 0.005\n",
      "iteration: 950 loss: 0.0295 lr: 0.005\n",
      "iteration: 960 loss: 0.0207 lr: 0.005\n",
      "iteration: 970 loss: 0.0210 lr: 0.005\n",
      "iteration: 980 loss: 0.0246 lr: 0.005\n",
      "iteration: 990 loss: 0.0216 lr: 0.005\n",
      "iteration: 1000 loss: 0.0193 lr: 0.005\n",
      "iteration: 1010 loss: 0.0294 lr: 0.005\n",
      "iteration: 1020 loss: 0.0261 lr: 0.005\n",
      "iteration: 1030 loss: 0.0238 lr: 0.005\n",
      "iteration: 1040 loss: 0.0192 lr: 0.005\n",
      "iteration: 1050 loss: 0.0223 lr: 0.005\n",
      "iteration: 1060 loss: 0.0242 lr: 0.005\n",
      "iteration: 1070 loss: 0.0263 lr: 0.005\n",
      "iteration: 1080 loss: 0.0156 lr: 0.005\n",
      "iteration: 1090 loss: 0.0180 lr: 0.005\n",
      "iteration: 1100 loss: 0.0227 lr: 0.005\n",
      "iteration: 1110 loss: 0.0219 lr: 0.005\n",
      "iteration: 1120 loss: 0.0198 lr: 0.005\n",
      "iteration: 1130 loss: 0.0225 lr: 0.005\n",
      "iteration: 1140 loss: 0.0230 lr: 0.005\n",
      "iteration: 1150 loss: 0.0234 lr: 0.005\n",
      "iteration: 1160 loss: 0.0211 lr: 0.005\n",
      "iteration: 1170 loss: 0.0250 lr: 0.005\n",
      "iteration: 1180 loss: 0.0259 lr: 0.005\n",
      "iteration: 1190 loss: 0.0214 lr: 0.005\n",
      "iteration: 1200 loss: 0.0213 lr: 0.005\n",
      "iteration: 1210 loss: 0.0212 lr: 0.005\n",
      "iteration: 1220 loss: 0.0185 lr: 0.005\n",
      "iteration: 1230 loss: 0.0198 lr: 0.005\n",
      "iteration: 1240 loss: 0.0226 lr: 0.005\n",
      "iteration: 1250 loss: 0.0229 lr: 0.005\n",
      "iteration: 1260 loss: 0.0200 lr: 0.005\n",
      "iteration: 1270 loss: 0.0234 lr: 0.005\n",
      "iteration: 1280 loss: 0.0189 lr: 0.005\n",
      "iteration: 1290 loss: 0.0213 lr: 0.005\n",
      "iteration: 1300 loss: 0.0185 lr: 0.005\n",
      "iteration: 1310 loss: 0.0205 lr: 0.005\n",
      "iteration: 1320 loss: 0.0225 lr: 0.005\n",
      "iteration: 1330 loss: 0.0202 lr: 0.005\n",
      "iteration: 1340 loss: 0.0205 lr: 0.005\n",
      "iteration: 1350 loss: 0.0162 lr: 0.005\n",
      "iteration: 1360 loss: 0.0176 lr: 0.005\n",
      "iteration: 1370 loss: 0.0202 lr: 0.005\n",
      "iteration: 1380 loss: 0.0207 lr: 0.005\n",
      "iteration: 1390 loss: 0.0245 lr: 0.005\n",
      "iteration: 1400 loss: 0.0170 lr: 0.005\n",
      "iteration: 1410 loss: 0.0183 lr: 0.005\n",
      "iteration: 1420 loss: 0.0157 lr: 0.005\n",
      "iteration: 1430 loss: 0.0255 lr: 0.005\n",
      "iteration: 1440 loss: 0.0215 lr: 0.005\n",
      "iteration: 1450 loss: 0.0214 lr: 0.005\n",
      "iteration: 1460 loss: 0.0258 lr: 0.005\n",
      "iteration: 1470 loss: 0.0203 lr: 0.005\n",
      "iteration: 1480 loss: 0.0216 lr: 0.005\n",
      "iteration: 1490 loss: 0.0249 lr: 0.005\n",
      "iteration: 1500 loss: 0.0233 lr: 0.005\n",
      "iteration: 1510 loss: 0.0207 lr: 0.005\n",
      "iteration: 1520 loss: 0.0215 lr: 0.005\n",
      "iteration: 1530 loss: 0.0188 lr: 0.005\n",
      "iteration: 1540 loss: 0.0221 lr: 0.005\n",
      "iteration: 1550 loss: 0.0249 lr: 0.005\n",
      "iteration: 1560 loss: 0.0192 lr: 0.005\n",
      "iteration: 1570 loss: 0.0170 lr: 0.005\n",
      "iteration: 1580 loss: 0.0217 lr: 0.005\n",
      "iteration: 1590 loss: 0.0164 lr: 0.005\n",
      "iteration: 1600 loss: 0.0244 lr: 0.005\n",
      "iteration: 1610 loss: 0.0207 lr: 0.005\n",
      "iteration: 1620 loss: 0.0189 lr: 0.005\n",
      "iteration: 1630 loss: 0.0223 lr: 0.005\n",
      "iteration: 1640 loss: 0.0194 lr: 0.005\n",
      "iteration: 1650 loss: 0.0194 lr: 0.005\n",
      "iteration: 1660 loss: 0.0178 lr: 0.005\n",
      "iteration: 1670 loss: 0.0185 lr: 0.005\n",
      "iteration: 1680 loss: 0.0248 lr: 0.005\n",
      "iteration: 1690 loss: 0.0205 lr: 0.005\n",
      "iteration: 1700 loss: 0.0199 lr: 0.005\n",
      "iteration: 1710 loss: 0.0210 lr: 0.005\n",
      "iteration: 1720 loss: 0.0181 lr: 0.005\n",
      "iteration: 1730 loss: 0.0186 lr: 0.005\n",
      "iteration: 1740 loss: 0.0179 lr: 0.005\n",
      "iteration: 1750 loss: 0.0179 lr: 0.005\n",
      "iteration: 1760 loss: 0.0187 lr: 0.005\n",
      "iteration: 1770 loss: 0.0163 lr: 0.005\n",
      "iteration: 1780 loss: 0.0168 lr: 0.005\n",
      "iteration: 1790 loss: 0.0173 lr: 0.005\n",
      "iteration: 1800 loss: 0.0193 lr: 0.005\n",
      "iteration: 1810 loss: 0.0184 lr: 0.005\n",
      "iteration: 1820 loss: 0.0186 lr: 0.005\n",
      "iteration: 1830 loss: 0.0163 lr: 0.005\n",
      "iteration: 1840 loss: 0.0207 lr: 0.005\n",
      "iteration: 1850 loss: 0.0198 lr: 0.005\n",
      "iteration: 1860 loss: 0.0160 lr: 0.005\n",
      "iteration: 1870 loss: 0.0163 lr: 0.005\n",
      "iteration: 1880 loss: 0.0239 lr: 0.005\n",
      "iteration: 1890 loss: 0.0220 lr: 0.005\n",
      "iteration: 1900 loss: 0.0175 lr: 0.005\n",
      "iteration: 1910 loss: 0.0177 lr: 0.005\n",
      "iteration: 1920 loss: 0.0145 lr: 0.005\n",
      "iteration: 1930 loss: 0.0164 lr: 0.005\n",
      "iteration: 1940 loss: 0.0222 lr: 0.005\n",
      "iteration: 1950 loss: 0.0187 lr: 0.005\n",
      "iteration: 1960 loss: 0.0194 lr: 0.005\n",
      "iteration: 1970 loss: 0.0159 lr: 0.005\n",
      "iteration: 1980 loss: 0.0228 lr: 0.005\n",
      "iteration: 1990 loss: 0.0180 lr: 0.005\n",
      "iteration: 2000 loss: 0.0203 lr: 0.005\n",
      "iteration: 2010 loss: 0.0219 lr: 0.005\n",
      "iteration: 2020 loss: 0.0162 lr: 0.005\n",
      "iteration: 2030 loss: 0.0142 lr: 0.005\n",
      "iteration: 2040 loss: 0.0155 lr: 0.005\n",
      "iteration: 2050 loss: 0.0177 lr: 0.005\n",
      "iteration: 2060 loss: 0.0205 lr: 0.005\n",
      "iteration: 2070 loss: 0.0213 lr: 0.005\n",
      "iteration: 2080 loss: 0.0199 lr: 0.005\n",
      "iteration: 2090 loss: 0.0158 lr: 0.005\n",
      "iteration: 2100 loss: 0.0192 lr: 0.005\n",
      "iteration: 2110 loss: 0.0220 lr: 0.005\n",
      "iteration: 2120 loss: 0.0231 lr: 0.005\n",
      "iteration: 2130 loss: 0.0174 lr: 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 2140 loss: 0.0207 lr: 0.005\n",
      "iteration: 2150 loss: 0.0223 lr: 0.005\n",
      "iteration: 2160 loss: 0.0194 lr: 0.005\n",
      "iteration: 2170 loss: 0.0202 lr: 0.005\n",
      "iteration: 2180 loss: 0.0186 lr: 0.005\n",
      "iteration: 2190 loss: 0.0196 lr: 0.005\n",
      "iteration: 2200 loss: 0.0184 lr: 0.005\n",
      "iteration: 2210 loss: 0.0170 lr: 0.005\n",
      "iteration: 2220 loss: 0.0199 lr: 0.005\n",
      "iteration: 2230 loss: 0.0190 lr: 0.005\n",
      "iteration: 2240 loss: 0.0162 lr: 0.005\n",
      "iteration: 2250 loss: 0.0193 lr: 0.005\n",
      "iteration: 2260 loss: 0.0224 lr: 0.005\n",
      "iteration: 2270 loss: 0.0179 lr: 0.005\n",
      "iteration: 2280 loss: 0.0148 lr: 0.005\n",
      "iteration: 2290 loss: 0.0188 lr: 0.005\n",
      "iteration: 2300 loss: 0.0176 lr: 0.005\n",
      "iteration: 2310 loss: 0.0190 lr: 0.005\n",
      "iteration: 2320 loss: 0.0212 lr: 0.005\n",
      "iteration: 2330 loss: 0.0153 lr: 0.005\n",
      "iteration: 2340 loss: 0.0190 lr: 0.005\n",
      "iteration: 2350 loss: 0.0178 lr: 0.005\n",
      "iteration: 2360 loss: 0.0207 lr: 0.005\n",
      "iteration: 2370 loss: 0.0204 lr: 0.005\n",
      "iteration: 2380 loss: 0.0205 lr: 0.005\n",
      "iteration: 2390 loss: 0.0173 lr: 0.005\n",
      "iteration: 2400 loss: 0.0169 lr: 0.005\n",
      "iteration: 2410 loss: 0.0140 lr: 0.005\n",
      "iteration: 2420 loss: 0.0200 lr: 0.005\n",
      "iteration: 2430 loss: 0.0183 lr: 0.005\n",
      "iteration: 2440 loss: 0.0153 lr: 0.005\n",
      "iteration: 2450 loss: 0.0167 lr: 0.005\n",
      "iteration: 2460 loss: 0.0148 lr: 0.005\n",
      "iteration: 2470 loss: 0.0163 lr: 0.005\n",
      "iteration: 2480 loss: 0.0130 lr: 0.005\n",
      "iteration: 2490 loss: 0.0160 lr: 0.005\n",
      "iteration: 2500 loss: 0.0184 lr: 0.005\n",
      "iteration: 2510 loss: 0.0178 lr: 0.005\n",
      "iteration: 2520 loss: 0.0160 lr: 0.005\n",
      "iteration: 2530 loss: 0.0140 lr: 0.005\n",
      "iteration: 2540 loss: 0.0157 lr: 0.005\n",
      "iteration: 2550 loss: 0.0225 lr: 0.005\n",
      "iteration: 2560 loss: 0.0230 lr: 0.005\n",
      "iteration: 2570 loss: 0.0192 lr: 0.005\n",
      "iteration: 2580 loss: 0.0174 lr: 0.005\n",
      "iteration: 2590 loss: 0.0174 lr: 0.005\n",
      "iteration: 2600 loss: 0.0148 lr: 0.005\n",
      "iteration: 2610 loss: 0.0148 lr: 0.005\n",
      "iteration: 2620 loss: 0.0187 lr: 0.005\n",
      "iteration: 2630 loss: 0.0167 lr: 0.005\n",
      "iteration: 2640 loss: 0.0209 lr: 0.005\n",
      "iteration: 2650 loss: 0.0180 lr: 0.005\n",
      "iteration: 2660 loss: 0.0189 lr: 0.005\n",
      "iteration: 2670 loss: 0.0147 lr: 0.005\n",
      "iteration: 2680 loss: 0.0152 lr: 0.005\n",
      "iteration: 2690 loss: 0.0178 lr: 0.005\n",
      "iteration: 2700 loss: 0.0124 lr: 0.005\n",
      "iteration: 2710 loss: 0.0146 lr: 0.005\n",
      "iteration: 2720 loss: 0.0145 lr: 0.005\n",
      "iteration: 2730 loss: 0.0136 lr: 0.005\n",
      "iteration: 2740 loss: 0.0208 lr: 0.005\n",
      "iteration: 2750 loss: 0.0229 lr: 0.005\n",
      "iteration: 2760 loss: 0.0169 lr: 0.005\n",
      "iteration: 2770 loss: 0.0150 lr: 0.005\n",
      "iteration: 2780 loss: 0.0134 lr: 0.005\n",
      "iteration: 2790 loss: 0.0156 lr: 0.005\n",
      "iteration: 2800 loss: 0.0163 lr: 0.005\n",
      "iteration: 2810 loss: 0.0148 lr: 0.005\n",
      "iteration: 2820 loss: 0.0194 lr: 0.005\n",
      "iteration: 2830 loss: 0.0191 lr: 0.005\n",
      "iteration: 2840 loss: 0.0155 lr: 0.005\n",
      "iteration: 2850 loss: 0.0154 lr: 0.005\n",
      "iteration: 2860 loss: 0.0207 lr: 0.005\n",
      "iteration: 2870 loss: 0.0177 lr: 0.005\n",
      "iteration: 2880 loss: 0.0168 lr: 0.005\n",
      "iteration: 2890 loss: 0.0175 lr: 0.005\n",
      "iteration: 2900 loss: 0.0174 lr: 0.005\n",
      "iteration: 2910 loss: 0.0167 lr: 0.005\n",
      "iteration: 2920 loss: 0.0144 lr: 0.005\n",
      "iteration: 2930 loss: 0.0149 lr: 0.005\n",
      "iteration: 2940 loss: 0.0182 lr: 0.005\n",
      "iteration: 2950 loss: 0.0207 lr: 0.005\n",
      "iteration: 2960 loss: 0.0139 lr: 0.005\n",
      "iteration: 2970 loss: 0.0167 lr: 0.005\n",
      "iteration: 2980 loss: 0.0169 lr: 0.005\n",
      "iteration: 2990 loss: 0.0174 lr: 0.005\n",
      "iteration: 3000 loss: 0.0136 lr: 0.005\n",
      "iteration: 3010 loss: 0.0164 lr: 0.005\n",
      "iteration: 3020 loss: 0.0185 lr: 0.005\n",
      "iteration: 3030 loss: 0.0192 lr: 0.005\n",
      "iteration: 3040 loss: 0.0147 lr: 0.005\n",
      "iteration: 3050 loss: 0.0160 lr: 0.005\n",
      "iteration: 3060 loss: 0.0176 lr: 0.005\n",
      "iteration: 3070 loss: 0.0181 lr: 0.005\n",
      "iteration: 3080 loss: 0.0138 lr: 0.005\n",
      "iteration: 3090 loss: 0.0169 lr: 0.005\n",
      "iteration: 3100 loss: 0.0177 lr: 0.005\n",
      "iteration: 3110 loss: 0.0167 lr: 0.005\n",
      "iteration: 3120 loss: 0.0160 lr: 0.005\n",
      "iteration: 3130 loss: 0.0160 lr: 0.005\n",
      "iteration: 3140 loss: 0.0193 lr: 0.005\n",
      "iteration: 3150 loss: 0.0187 lr: 0.005\n",
      "iteration: 3160 loss: 0.0157 lr: 0.005\n",
      "iteration: 3170 loss: 0.0134 lr: 0.005\n",
      "iteration: 3180 loss: 0.0119 lr: 0.005\n",
      "iteration: 3190 loss: 0.0181 lr: 0.005\n",
      "iteration: 3200 loss: 0.0168 lr: 0.005\n",
      "iteration: 3210 loss: 0.0170 lr: 0.005\n",
      "iteration: 3220 loss: 0.0127 lr: 0.005\n",
      "iteration: 3230 loss: 0.0132 lr: 0.005\n",
      "iteration: 3240 loss: 0.0177 lr: 0.005\n",
      "iteration: 3250 loss: 0.0194 lr: 0.005\n",
      "iteration: 3260 loss: 0.0194 lr: 0.005\n",
      "iteration: 3270 loss: 0.0155 lr: 0.005\n",
      "iteration: 3280 loss: 0.0137 lr: 0.005\n",
      "iteration: 3290 loss: 0.0178 lr: 0.005\n",
      "iteration: 3300 loss: 0.0169 lr: 0.005\n",
      "iteration: 3310 loss: 0.0118 lr: 0.005\n",
      "iteration: 3320 loss: 0.0187 lr: 0.005\n",
      "iteration: 3330 loss: 0.0148 lr: 0.005\n",
      "iteration: 3340 loss: 0.0149 lr: 0.005\n",
      "iteration: 3350 loss: 0.0161 lr: 0.005\n",
      "iteration: 3360 loss: 0.0150 lr: 0.005\n",
      "iteration: 3370 loss: 0.0135 lr: 0.005\n",
      "iteration: 3380 loss: 0.0146 lr: 0.005\n",
      "iteration: 3390 loss: 0.0188 lr: 0.005\n",
      "iteration: 3400 loss: 0.0142 lr: 0.005\n",
      "iteration: 3410 loss: 0.0176 lr: 0.005\n",
      "iteration: 3420 loss: 0.0164 lr: 0.005\n",
      "iteration: 3430 loss: 0.0173 lr: 0.005\n",
      "iteration: 3440 loss: 0.0175 lr: 0.005\n",
      "iteration: 3450 loss: 0.0145 lr: 0.005\n",
      "iteration: 3460 loss: 0.0196 lr: 0.005\n",
      "iteration: 3470 loss: 0.0167 lr: 0.005\n",
      "iteration: 3480 loss: 0.0139 lr: 0.005\n",
      "iteration: 3490 loss: 0.0147 lr: 0.005\n",
      "iteration: 3500 loss: 0.0146 lr: 0.005\n",
      "iteration: 3510 loss: 0.0126 lr: 0.005\n",
      "iteration: 3520 loss: 0.0153 lr: 0.005\n",
      "iteration: 3530 loss: 0.0154 lr: 0.005\n",
      "iteration: 3540 loss: 0.0177 lr: 0.005\n",
      "iteration: 3550 loss: 0.0144 lr: 0.005\n",
      "iteration: 3560 loss: 0.0162 lr: 0.005\n",
      "iteration: 3570 loss: 0.0169 lr: 0.005\n",
      "iteration: 3580 loss: 0.0190 lr: 0.005\n",
      "iteration: 3590 loss: 0.0149 lr: 0.005\n",
      "iteration: 3600 loss: 0.0157 lr: 0.005\n",
      "iteration: 3610 loss: 0.0161 lr: 0.005\n",
      "iteration: 3620 loss: 0.0147 lr: 0.005\n",
      "iteration: 3630 loss: 0.0176 lr: 0.005\n",
      "iteration: 3640 loss: 0.0167 lr: 0.005\n",
      "iteration: 3650 loss: 0.0153 lr: 0.005\n",
      "iteration: 3660 loss: 0.0139 lr: 0.005\n",
      "iteration: 3670 loss: 0.0157 lr: 0.005\n",
      "iteration: 3680 loss: 0.0140 lr: 0.005\n",
      "iteration: 3690 loss: 0.0139 lr: 0.005\n",
      "iteration: 3700 loss: 0.0164 lr: 0.005\n",
      "iteration: 3710 loss: 0.0119 lr: 0.005\n",
      "iteration: 3720 loss: 0.0152 lr: 0.005\n",
      "iteration: 3730 loss: 0.0127 lr: 0.005\n",
      "iteration: 3740 loss: 0.0143 lr: 0.005\n",
      "iteration: 3750 loss: 0.0188 lr: 0.005\n",
      "iteration: 3760 loss: 0.0118 lr: 0.005\n",
      "iteration: 3770 loss: 0.0170 lr: 0.005\n",
      "iteration: 3780 loss: 0.0150 lr: 0.005\n",
      "iteration: 3790 loss: 0.0146 lr: 0.005\n",
      "iteration: 3800 loss: 0.0141 lr: 0.005\n",
      "iteration: 3810 loss: 0.0166 lr: 0.005\n",
      "iteration: 3820 loss: 0.0175 lr: 0.005\n",
      "iteration: 3830 loss: 0.0138 lr: 0.005\n",
      "iteration: 3840 loss: 0.0135 lr: 0.005\n",
      "iteration: 3850 loss: 0.0156 lr: 0.005\n",
      "iteration: 3860 loss: 0.0159 lr: 0.005\n",
      "iteration: 3870 loss: 0.0148 lr: 0.005\n",
      "iteration: 3880 loss: 0.0142 lr: 0.005\n",
      "iteration: 3890 loss: 0.0206 lr: 0.005\n",
      "iteration: 3900 loss: 0.0165 lr: 0.005\n",
      "iteration: 3910 loss: 0.0121 lr: 0.005\n",
      "iteration: 3920 loss: 0.0163 lr: 0.005\n",
      "iteration: 3930 loss: 0.0144 lr: 0.005\n",
      "iteration: 3940 loss: 0.0146 lr: 0.005\n",
      "iteration: 3950 loss: 0.0170 lr: 0.005\n",
      "iteration: 3960 loss: 0.0136 lr: 0.005\n",
      "iteration: 3970 loss: 0.0122 lr: 0.005\n",
      "iteration: 3980 loss: 0.0134 lr: 0.005\n",
      "iteration: 3990 loss: 0.0180 lr: 0.005\n",
      "iteration: 4000 loss: 0.0162 lr: 0.005\n",
      "iteration: 4010 loss: 0.0155 lr: 0.005\n",
      "iteration: 4020 loss: 0.0210 lr: 0.005\n",
      "iteration: 4030 loss: 0.0159 lr: 0.005\n",
      "iteration: 4040 loss: 0.0191 lr: 0.005\n",
      "iteration: 4050 loss: 0.0126 lr: 0.005\n",
      "iteration: 4060 loss: 0.0169 lr: 0.005\n",
      "iteration: 4070 loss: 0.0125 lr: 0.005\n",
      "iteration: 4080 loss: 0.0147 lr: 0.005\n",
      "iteration: 4090 loss: 0.0161 lr: 0.005\n",
      "iteration: 4100 loss: 0.0122 lr: 0.005\n",
      "iteration: 4110 loss: 0.0154 lr: 0.005\n",
      "iteration: 4120 loss: 0.0184 lr: 0.005\n",
      "iteration: 4130 loss: 0.0149 lr: 0.005\n",
      "iteration: 4140 loss: 0.0148 lr: 0.005\n",
      "iteration: 4150 loss: 0.0132 lr: 0.005\n",
      "iteration: 4160 loss: 0.0140 lr: 0.005\n",
      "iteration: 4170 loss: 0.0127 lr: 0.005\n",
      "iteration: 4180 loss: 0.0141 lr: 0.005\n",
      "iteration: 4190 loss: 0.0114 lr: 0.005\n",
      "iteration: 4200 loss: 0.0098 lr: 0.005\n",
      "iteration: 4210 loss: 0.0168 lr: 0.005\n",
      "iteration: 4220 loss: 0.0165 lr: 0.005\n",
      "iteration: 4230 loss: 0.0172 lr: 0.005\n",
      "iteration: 4240 loss: 0.0132 lr: 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 4250 loss: 0.0116 lr: 0.005\n",
      "iteration: 4260 loss: 0.0130 lr: 0.005\n",
      "iteration: 4270 loss: 0.0147 lr: 0.005\n",
      "iteration: 4280 loss: 0.0149 lr: 0.005\n",
      "iteration: 4290 loss: 0.0119 lr: 0.005\n",
      "iteration: 4300 loss: 0.0136 lr: 0.005\n",
      "iteration: 4310 loss: 0.0204 lr: 0.005\n",
      "iteration: 4320 loss: 0.0136 lr: 0.005\n",
      "iteration: 4330 loss: 0.0142 lr: 0.005\n",
      "iteration: 4340 loss: 0.0152 lr: 0.005\n",
      "iteration: 4350 loss: 0.0169 lr: 0.005\n",
      "iteration: 4360 loss: 0.0150 lr: 0.005\n",
      "iteration: 4370 loss: 0.0162 lr: 0.005\n",
      "iteration: 4380 loss: 0.0172 lr: 0.005\n",
      "iteration: 4390 loss: 0.0144 lr: 0.005\n",
      "iteration: 4400 loss: 0.0158 lr: 0.005\n",
      "iteration: 4410 loss: 0.0165 lr: 0.005\n",
      "iteration: 4420 loss: 0.0139 lr: 0.005\n",
      "iteration: 4430 loss: 0.0154 lr: 0.005\n",
      "iteration: 4440 loss: 0.0164 lr: 0.005\n",
      "iteration: 4450 loss: 0.0168 lr: 0.005\n",
      "iteration: 4460 loss: 0.0123 lr: 0.005\n",
      "iteration: 4470 loss: 0.0171 lr: 0.005\n",
      "iteration: 4480 loss: 0.0151 lr: 0.005\n",
      "iteration: 4490 loss: 0.0198 lr: 0.005\n",
      "iteration: 4500 loss: 0.0138 lr: 0.005\n",
      "iteration: 4510 loss: 0.0127 lr: 0.005\n",
      "iteration: 4520 loss: 0.0146 lr: 0.005\n",
      "iteration: 4530 loss: 0.0128 lr: 0.005\n",
      "iteration: 4540 loss: 0.0125 lr: 0.005\n",
      "iteration: 4550 loss: 0.0143 lr: 0.005\n",
      "iteration: 4560 loss: 0.0142 lr: 0.005\n",
      "iteration: 4570 loss: 0.0168 lr: 0.005\n",
      "iteration: 4580 loss: 0.0175 lr: 0.005\n",
      "iteration: 4590 loss: 0.0106 lr: 0.005\n",
      "iteration: 4600 loss: 0.0163 lr: 0.005\n",
      "iteration: 4610 loss: 0.0140 lr: 0.005\n",
      "iteration: 4620 loss: 0.0115 lr: 0.005\n",
      "iteration: 4630 loss: 0.0154 lr: 0.005\n",
      "iteration: 4640 loss: 0.0134 lr: 0.005\n",
      "iteration: 4650 loss: 0.0127 lr: 0.005\n",
      "iteration: 4660 loss: 0.0124 lr: 0.005\n",
      "iteration: 4670 loss: 0.0151 lr: 0.005\n",
      "iteration: 4680 loss: 0.0109 lr: 0.005\n",
      "iteration: 4690 loss: 0.0111 lr: 0.005\n",
      "iteration: 4700 loss: 0.0132 lr: 0.005\n",
      "iteration: 4710 loss: 0.0156 lr: 0.005\n",
      "iteration: 4720 loss: 0.0140 lr: 0.005\n",
      "iteration: 4730 loss: 0.0145 lr: 0.005\n",
      "iteration: 4740 loss: 0.0144 lr: 0.005\n",
      "iteration: 4750 loss: 0.0167 lr: 0.005\n",
      "iteration: 4760 loss: 0.0136 lr: 0.005\n",
      "iteration: 4770 loss: 0.0169 lr: 0.005\n",
      "iteration: 4780 loss: 0.0121 lr: 0.005\n",
      "iteration: 4790 loss: 0.0174 lr: 0.005\n",
      "iteration: 4800 loss: 0.0149 lr: 0.005\n",
      "iteration: 4810 loss: 0.0124 lr: 0.005\n",
      "iteration: 4820 loss: 0.0130 lr: 0.005\n",
      "iteration: 4830 loss: 0.0140 lr: 0.005\n",
      "iteration: 4840 loss: 0.0119 lr: 0.005\n",
      "iteration: 4850 loss: 0.0118 lr: 0.005\n",
      "iteration: 4860 loss: 0.0126 lr: 0.005\n",
      "iteration: 4870 loss: 0.0147 lr: 0.005\n",
      "iteration: 4880 loss: 0.0124 lr: 0.005\n",
      "iteration: 4890 loss: 0.0168 lr: 0.005\n",
      "iteration: 4900 loss: 0.0136 lr: 0.005\n",
      "iteration: 4910 loss: 0.0140 lr: 0.005\n",
      "iteration: 4920 loss: 0.0109 lr: 0.005\n",
      "iteration: 4930 loss: 0.0128 lr: 0.005\n",
      "iteration: 4940 loss: 0.0133 lr: 0.005\n",
      "iteration: 4950 loss: 0.0163 lr: 0.005\n",
      "iteration: 4960 loss: 0.0157 lr: 0.005\n",
      "iteration: 4970 loss: 0.0189 lr: 0.005\n",
      "iteration: 4980 loss: 0.0142 lr: 0.005\n",
      "iteration: 4990 loss: 0.0138 lr: 0.005\n",
      "iteration: 5000 loss: 0.0130 lr: 0.005\n",
      "iteration: 5010 loss: 0.0140 lr: 0.005\n",
      "iteration: 5020 loss: 0.0147 lr: 0.005\n",
      "iteration: 5030 loss: 0.0124 lr: 0.005\n",
      "iteration: 5040 loss: 0.0118 lr: 0.005\n",
      "iteration: 5050 loss: 0.0121 lr: 0.005\n",
      "iteration: 5060 loss: 0.0098 lr: 0.005\n",
      "iteration: 5070 loss: 0.0107 lr: 0.005\n",
      "iteration: 5080 loss: 0.0150 lr: 0.005\n",
      "iteration: 5090 loss: 0.0137 lr: 0.005\n",
      "iteration: 5100 loss: 0.0152 lr: 0.005\n",
      "iteration: 5110 loss: 0.0120 lr: 0.005\n",
      "iteration: 5120 loss: 0.0136 lr: 0.005\n",
      "iteration: 5130 loss: 0.0155 lr: 0.005\n",
      "iteration: 5140 loss: 0.0131 lr: 0.005\n",
      "iteration: 5150 loss: 0.0131 lr: 0.005\n",
      "iteration: 5160 loss: 0.0139 lr: 0.005\n",
      "iteration: 5170 loss: 0.0124 lr: 0.005\n",
      "iteration: 5180 loss: 0.0127 lr: 0.005\n",
      "iteration: 5190 loss: 0.0160 lr: 0.005\n",
      "iteration: 5200 loss: 0.0126 lr: 0.005\n",
      "iteration: 5210 loss: 0.0175 lr: 0.005\n",
      "iteration: 5220 loss: 0.0139 lr: 0.005\n",
      "iteration: 5230 loss: 0.0175 lr: 0.005\n",
      "iteration: 5240 loss: 0.0135 lr: 0.005\n",
      "iteration: 5250 loss: 0.0132 lr: 0.005\n",
      "iteration: 5260 loss: 0.0155 lr: 0.005\n",
      "iteration: 5270 loss: 0.0150 lr: 0.005\n",
      "iteration: 5280 loss: 0.0124 lr: 0.005\n",
      "iteration: 5290 loss: 0.0105 lr: 0.005\n",
      "iteration: 5300 loss: 0.0186 lr: 0.005\n",
      "iteration: 5310 loss: 0.0152 lr: 0.005\n",
      "iteration: 5320 loss: 0.0146 lr: 0.005\n",
      "iteration: 5330 loss: 0.0163 lr: 0.005\n",
      "iteration: 5340 loss: 0.0107 lr: 0.005\n",
      "iteration: 5350 loss: 0.0146 lr: 0.005\n",
      "iteration: 5360 loss: 0.0119 lr: 0.005\n",
      "iteration: 5370 loss: 0.0141 lr: 0.005\n",
      "iteration: 5380 loss: 0.0197 lr: 0.005\n",
      "iteration: 5390 loss: 0.0160 lr: 0.005\n",
      "iteration: 5400 loss: 0.0127 lr: 0.005\n",
      "iteration: 5410 loss: 0.0127 lr: 0.005\n",
      "iteration: 5420 loss: 0.0157 lr: 0.005\n",
      "iteration: 5430 loss: 0.0129 lr: 0.005\n",
      "iteration: 5440 loss: 0.0127 lr: 0.005\n",
      "iteration: 5450 loss: 0.0147 lr: 0.005\n",
      "iteration: 5460 loss: 0.0193 lr: 0.005\n",
      "iteration: 5470 loss: 0.0166 lr: 0.005\n",
      "iteration: 5480 loss: 0.0111 lr: 0.005\n",
      "iteration: 5490 loss: 0.0113 lr: 0.005\n",
      "iteration: 5500 loss: 0.0147 lr: 0.005\n",
      "iteration: 5510 loss: 0.0106 lr: 0.005\n",
      "iteration: 5520 loss: 0.0174 lr: 0.005\n",
      "iteration: 5530 loss: 0.0125 lr: 0.005\n",
      "iteration: 5540 loss: 0.0135 lr: 0.005\n",
      "iteration: 5550 loss: 0.0096 lr: 0.005\n",
      "iteration: 5560 loss: 0.0141 lr: 0.005\n",
      "iteration: 5570 loss: 0.0135 lr: 0.005\n",
      "iteration: 5580 loss: 0.0112 lr: 0.005\n",
      "iteration: 5590 loss: 0.0154 lr: 0.005\n",
      "iteration: 5600 loss: 0.0119 lr: 0.005\n",
      "iteration: 5610 loss: 0.0114 lr: 0.005\n",
      "iteration: 5620 loss: 0.0125 lr: 0.005\n",
      "iteration: 5630 loss: 0.0128 lr: 0.005\n",
      "iteration: 5640 loss: 0.0151 lr: 0.005\n",
      "iteration: 5650 loss: 0.0153 lr: 0.005\n",
      "iteration: 5660 loss: 0.0170 lr: 0.005\n",
      "iteration: 5670 loss: 0.0113 lr: 0.005\n",
      "iteration: 5680 loss: 0.0113 lr: 0.005\n",
      "iteration: 5690 loss: 0.0170 lr: 0.005\n",
      "iteration: 5700 loss: 0.0105 lr: 0.005\n",
      "iteration: 5710 loss: 0.0116 lr: 0.005\n",
      "iteration: 5720 loss: 0.0135 lr: 0.005\n",
      "iteration: 5730 loss: 0.0113 lr: 0.005\n",
      "iteration: 5740 loss: 0.0108 lr: 0.005\n",
      "iteration: 5750 loss: 0.0127 lr: 0.005\n",
      "iteration: 5760 loss: 0.0136 lr: 0.005\n",
      "iteration: 5770 loss: 0.0161 lr: 0.005\n",
      "iteration: 5780 loss: 0.0186 lr: 0.005\n",
      "iteration: 5790 loss: 0.0108 lr: 0.005\n",
      "iteration: 5800 loss: 0.0118 lr: 0.005\n",
      "iteration: 5810 loss: 0.0163 lr: 0.005\n",
      "iteration: 5820 loss: 0.0139 lr: 0.005\n",
      "iteration: 5830 loss: 0.0115 lr: 0.005\n",
      "iteration: 5840 loss: 0.0112 lr: 0.005\n",
      "iteration: 5850 loss: 0.0140 lr: 0.005\n",
      "iteration: 5860 loss: 0.0103 lr: 0.005\n",
      "iteration: 5870 loss: 0.0128 lr: 0.005\n",
      "iteration: 5880 loss: 0.0117 lr: 0.005\n",
      "iteration: 5890 loss: 0.0100 lr: 0.005\n",
      "iteration: 5900 loss: 0.0099 lr: 0.005\n",
      "iteration: 5910 loss: 0.0121 lr: 0.005\n",
      "iteration: 5920 loss: 0.0117 lr: 0.005\n",
      "iteration: 5930 loss: 0.0131 lr: 0.005\n",
      "iteration: 5940 loss: 0.0123 lr: 0.005\n",
      "iteration: 5950 loss: 0.0127 lr: 0.005\n",
      "iteration: 5960 loss: 0.0107 lr: 0.005\n",
      "iteration: 5970 loss: 0.0112 lr: 0.005\n",
      "iteration: 5980 loss: 0.0134 lr: 0.005\n",
      "iteration: 5990 loss: 0.0147 lr: 0.005\n",
      "iteration: 6000 loss: 0.0122 lr: 0.005\n",
      "iteration: 6010 loss: 0.0158 lr: 0.005\n",
      "iteration: 6020 loss: 0.0125 lr: 0.005\n",
      "iteration: 6030 loss: 0.0150 lr: 0.005\n",
      "iteration: 6040 loss: 0.0133 lr: 0.005\n",
      "iteration: 6050 loss: 0.0132 lr: 0.005\n",
      "iteration: 6060 loss: 0.0148 lr: 0.005\n",
      "iteration: 6070 loss: 0.0126 lr: 0.005\n",
      "iteration: 6080 loss: 0.0128 lr: 0.005\n",
      "iteration: 6090 loss: 0.0129 lr: 0.005\n",
      "iteration: 6100 loss: 0.0090 lr: 0.005\n",
      "iteration: 6110 loss: 0.0116 lr: 0.005\n",
      "iteration: 6120 loss: 0.0116 lr: 0.005\n",
      "iteration: 6130 loss: 0.0107 lr: 0.005\n",
      "iteration: 6140 loss: 0.0118 lr: 0.005\n",
      "iteration: 6150 loss: 0.0152 lr: 0.005\n",
      "iteration: 6160 loss: 0.0138 lr: 0.005\n",
      "iteration: 6170 loss: 0.0123 lr: 0.005\n",
      "iteration: 6180 loss: 0.0100 lr: 0.005\n",
      "iteration: 6190 loss: 0.0127 lr: 0.005\n",
      "iteration: 6200 loss: 0.0130 lr: 0.005\n",
      "iteration: 6210 loss: 0.0125 lr: 0.005\n",
      "iteration: 6220 loss: 0.0135 lr: 0.005\n",
      "iteration: 6230 loss: 0.0105 lr: 0.005\n",
      "iteration: 6240 loss: 0.0116 lr: 0.005\n",
      "iteration: 6250 loss: 0.0116 lr: 0.005\n",
      "iteration: 6260 loss: 0.0104 lr: 0.005\n",
      "iteration: 6270 loss: 0.0136 lr: 0.005\n",
      "iteration: 6280 loss: 0.0149 lr: 0.005\n",
      "iteration: 6290 loss: 0.0101 lr: 0.005\n",
      "iteration: 6300 loss: 0.0109 lr: 0.005\n",
      "iteration: 6310 loss: 0.0112 lr: 0.005\n",
      "iteration: 6320 loss: 0.0174 lr: 0.005\n",
      "iteration: 6330 loss: 0.0126 lr: 0.005\n",
      "iteration: 6340 loss: 0.0110 lr: 0.005\n",
      "iteration: 6350 loss: 0.0116 lr: 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 6360 loss: 0.0106 lr: 0.005\n",
      "iteration: 6370 loss: 0.0119 lr: 0.005\n",
      "iteration: 6380 loss: 0.0116 lr: 0.005\n",
      "iteration: 6390 loss: 0.0105 lr: 0.005\n",
      "iteration: 6400 loss: 0.0149 lr: 0.005\n",
      "iteration: 6410 loss: 0.0124 lr: 0.005\n",
      "iteration: 6420 loss: 0.0129 lr: 0.005\n",
      "iteration: 6430 loss: 0.0122 lr: 0.005\n",
      "iteration: 6440 loss: 0.0107 lr: 0.005\n",
      "iteration: 6450 loss: 0.0154 lr: 0.005\n",
      "iteration: 6460 loss: 0.0145 lr: 0.005\n",
      "iteration: 6470 loss: 0.0108 lr: 0.005\n",
      "iteration: 6480 loss: 0.0129 lr: 0.005\n",
      "iteration: 6490 loss: 0.0123 lr: 0.005\n",
      "iteration: 6500 loss: 0.0128 lr: 0.005\n",
      "iteration: 6510 loss: 0.0108 lr: 0.005\n",
      "iteration: 6520 loss: 0.0104 lr: 0.005\n",
      "iteration: 6530 loss: 0.0096 lr: 0.005\n",
      "iteration: 6540 loss: 0.0128 lr: 0.005\n",
      "iteration: 6550 loss: 0.0153 lr: 0.005\n",
      "iteration: 6560 loss: 0.0123 lr: 0.005\n",
      "iteration: 6570 loss: 0.0113 lr: 0.005\n",
      "iteration: 6580 loss: 0.0152 lr: 0.005\n",
      "iteration: 6590 loss: 0.0134 lr: 0.005\n",
      "iteration: 6600 loss: 0.0150 lr: 0.005\n",
      "iteration: 6610 loss: 0.0100 lr: 0.005\n",
      "iteration: 6620 loss: 0.0123 lr: 0.005\n",
      "iteration: 6630 loss: 0.0122 lr: 0.005\n",
      "iteration: 6640 loss: 0.0119 lr: 0.005\n",
      "iteration: 6650 loss: 0.0123 lr: 0.005\n",
      "iteration: 6660 loss: 0.0133 lr: 0.005\n",
      "iteration: 6670 loss: 0.0095 lr: 0.005\n",
      "iteration: 6680 loss: 0.0103 lr: 0.005\n",
      "iteration: 6690 loss: 0.0139 lr: 0.005\n",
      "iteration: 6700 loss: 0.0095 lr: 0.005\n",
      "iteration: 6710 loss: 0.0141 lr: 0.005\n",
      "iteration: 6720 loss: 0.0111 lr: 0.005\n",
      "iteration: 6730 loss: 0.0166 lr: 0.005\n",
      "iteration: 6740 loss: 0.0131 lr: 0.005\n",
      "iteration: 6750 loss: 0.0124 lr: 0.005\n",
      "iteration: 6760 loss: 0.0096 lr: 0.005\n",
      "iteration: 6770 loss: 0.0137 lr: 0.005\n",
      "iteration: 6780 loss: 0.0123 lr: 0.005\n",
      "iteration: 6790 loss: 0.0115 lr: 0.005\n",
      "iteration: 6800 loss: 0.0115 lr: 0.005\n",
      "iteration: 6810 loss: 0.0092 lr: 0.005\n",
      "iteration: 6820 loss: 0.0112 lr: 0.005\n",
      "iteration: 6830 loss: 0.0098 lr: 0.005\n",
      "iteration: 6840 loss: 0.0146 lr: 0.005\n",
      "iteration: 6850 loss: 0.0142 lr: 0.005\n",
      "iteration: 6860 loss: 0.0119 lr: 0.005\n",
      "iteration: 6870 loss: 0.0137 lr: 0.005\n",
      "iteration: 6880 loss: 0.0108 lr: 0.005\n",
      "iteration: 6890 loss: 0.0125 lr: 0.005\n",
      "iteration: 6900 loss: 0.0127 lr: 0.005\n",
      "iteration: 6910 loss: 0.0109 lr: 0.005\n",
      "iteration: 6920 loss: 0.0103 lr: 0.005\n",
      "iteration: 6930 loss: 0.0140 lr: 0.005\n",
      "iteration: 6940 loss: 0.0144 lr: 0.005\n",
      "iteration: 6950 loss: 0.0124 lr: 0.005\n",
      "iteration: 6960 loss: 0.0107 lr: 0.005\n",
      "iteration: 6970 loss: 0.0120 lr: 0.005\n",
      "iteration: 6980 loss: 0.0101 lr: 0.005\n",
      "iteration: 6990 loss: 0.0150 lr: 0.005\n",
      "iteration: 7000 loss: 0.0120 lr: 0.005\n",
      "iteration: 7010 loss: 0.0120 lr: 0.005\n",
      "iteration: 7020 loss: 0.0166 lr: 0.005\n",
      "iteration: 7030 loss: 0.0159 lr: 0.005\n",
      "iteration: 7040 loss: 0.0107 lr: 0.005\n",
      "iteration: 7050 loss: 0.0106 lr: 0.005\n",
      "iteration: 7060 loss: 0.0123 lr: 0.005\n",
      "iteration: 7070 loss: 0.0120 lr: 0.005\n",
      "iteration: 7080 loss: 0.0144 lr: 0.005\n",
      "iteration: 7090 loss: 0.0109 lr: 0.005\n",
      "iteration: 7100 loss: 0.0159 lr: 0.005\n",
      "iteration: 7110 loss: 0.0121 lr: 0.005\n",
      "iteration: 7120 loss: 0.0128 lr: 0.005\n",
      "iteration: 7130 loss: 0.0103 lr: 0.005\n",
      "iteration: 7140 loss: 0.0137 lr: 0.005\n",
      "iteration: 7150 loss: 0.0101 lr: 0.005\n",
      "iteration: 7160 loss: 0.0125 lr: 0.005\n",
      "iteration: 7170 loss: 0.0112 lr: 0.005\n",
      "iteration: 7180 loss: 0.0107 lr: 0.005\n",
      "iteration: 7190 loss: 0.0108 lr: 0.005\n",
      "iteration: 7200 loss: 0.0084 lr: 0.005\n",
      "iteration: 7210 loss: 0.0114 lr: 0.005\n",
      "iteration: 7220 loss: 0.0107 lr: 0.005\n",
      "iteration: 7230 loss: 0.0163 lr: 0.005\n",
      "iteration: 7240 loss: 0.0133 lr: 0.005\n",
      "iteration: 7250 loss: 0.0129 lr: 0.005\n",
      "iteration: 7260 loss: 0.0153 lr: 0.005\n",
      "iteration: 7270 loss: 0.0111 lr: 0.005\n",
      "iteration: 7280 loss: 0.0134 lr: 0.005\n",
      "iteration: 7290 loss: 0.0098 lr: 0.005\n",
      "iteration: 7300 loss: 0.0125 lr: 0.005\n",
      "iteration: 7310 loss: 0.0108 lr: 0.005\n",
      "iteration: 7320 loss: 0.0132 lr: 0.005\n",
      "iteration: 7330 loss: 0.0130 lr: 0.005\n",
      "iteration: 7340 loss: 0.0114 lr: 0.005\n",
      "iteration: 7350 loss: 0.0117 lr: 0.005\n",
      "iteration: 7360 loss: 0.0125 lr: 0.005\n",
      "iteration: 7370 loss: 0.0123 lr: 0.005\n",
      "iteration: 7380 loss: 0.0102 lr: 0.005\n",
      "iteration: 7390 loss: 0.0082 lr: 0.005\n",
      "iteration: 7400 loss: 0.0130 lr: 0.005\n",
      "iteration: 7410 loss: 0.0135 lr: 0.005\n",
      "iteration: 7420 loss: 0.0103 lr: 0.005\n",
      "iteration: 7430 loss: 0.0118 lr: 0.005\n",
      "iteration: 7440 loss: 0.0077 lr: 0.005\n",
      "iteration: 7450 loss: 0.0115 lr: 0.005\n",
      "iteration: 7460 loss: 0.0125 lr: 0.005\n",
      "iteration: 7470 loss: 0.0131 lr: 0.005\n",
      "iteration: 7480 loss: 0.0125 lr: 0.005\n",
      "iteration: 7490 loss: 0.0132 lr: 0.005\n",
      "iteration: 7500 loss: 0.0107 lr: 0.005\n",
      "iteration: 7510 loss: 0.0125 lr: 0.005\n",
      "iteration: 7520 loss: 0.0096 lr: 0.005\n",
      "iteration: 7530 loss: 0.0121 lr: 0.005\n",
      "iteration: 7540 loss: 0.0104 lr: 0.005\n",
      "iteration: 7550 loss: 0.0106 lr: 0.005\n",
      "iteration: 7560 loss: 0.0099 lr: 0.005\n",
      "iteration: 7570 loss: 0.0163 lr: 0.005\n",
      "iteration: 7580 loss: 0.0154 lr: 0.005\n",
      "iteration: 7590 loss: 0.0116 lr: 0.005\n",
      "iteration: 7600 loss: 0.0133 lr: 0.005\n",
      "iteration: 7610 loss: 0.0158 lr: 0.005\n",
      "iteration: 7620 loss: 0.0123 lr: 0.005\n",
      "iteration: 7630 loss: 0.0125 lr: 0.005\n",
      "iteration: 7640 loss: 0.0092 lr: 0.005\n",
      "iteration: 7650 loss: 0.0124 lr: 0.005\n",
      "iteration: 7660 loss: 0.0124 lr: 0.005\n",
      "iteration: 7670 loss: 0.0100 lr: 0.005\n",
      "iteration: 7680 loss: 0.0118 lr: 0.005\n",
      "iteration: 7690 loss: 0.0142 lr: 0.005\n",
      "iteration: 7700 loss: 0.0132 lr: 0.005\n",
      "iteration: 7710 loss: 0.0145 lr: 0.005\n",
      "iteration: 7720 loss: 0.0111 lr: 0.005\n",
      "iteration: 7730 loss: 0.0088 lr: 0.005\n",
      "iteration: 7740 loss: 0.0111 lr: 0.005\n",
      "iteration: 7750 loss: 0.0149 lr: 0.005\n",
      "iteration: 7760 loss: 0.0092 lr: 0.005\n",
      "iteration: 7770 loss: 0.0108 lr: 0.005\n",
      "iteration: 7780 loss: 0.0111 lr: 0.005\n",
      "iteration: 7790 loss: 0.0133 lr: 0.005\n",
      "iteration: 7800 loss: 0.0085 lr: 0.005\n",
      "iteration: 7810 loss: 0.0098 lr: 0.005\n",
      "iteration: 7820 loss: 0.0097 lr: 0.005\n",
      "iteration: 7830 loss: 0.0131 lr: 0.005\n",
      "iteration: 7840 loss: 0.0112 lr: 0.005\n",
      "iteration: 7850 loss: 0.0093 lr: 0.005\n",
      "iteration: 7860 loss: 0.0099 lr: 0.005\n",
      "iteration: 7870 loss: 0.0069 lr: 0.005\n",
      "iteration: 7880 loss: 0.0102 lr: 0.005\n",
      "iteration: 7890 loss: 0.0092 lr: 0.005\n",
      "iteration: 7900 loss: 0.0115 lr: 0.005\n",
      "iteration: 7910 loss: 0.0135 lr: 0.005\n",
      "iteration: 7920 loss: 0.0114 lr: 0.005\n",
      "iteration: 7930 loss: 0.0114 lr: 0.005\n",
      "iteration: 7940 loss: 0.0134 lr: 0.005\n",
      "iteration: 7950 loss: 0.0101 lr: 0.005\n",
      "iteration: 7960 loss: 0.0109 lr: 0.005\n",
      "iteration: 7970 loss: 0.0110 lr: 0.005\n",
      "iteration: 7980 loss: 0.0089 lr: 0.005\n",
      "iteration: 7990 loss: 0.0090 lr: 0.005\n",
      "iteration: 8000 loss: 0.0093 lr: 0.005\n",
      "iteration: 8010 loss: 0.0082 lr: 0.005\n",
      "iteration: 8020 loss: 0.0097 lr: 0.005\n",
      "iteration: 8030 loss: 0.0094 lr: 0.005\n",
      "iteration: 8040 loss: 0.0126 lr: 0.005\n",
      "iteration: 8050 loss: 0.0129 lr: 0.005\n",
      "iteration: 8060 loss: 0.0130 lr: 0.005\n",
      "iteration: 8070 loss: 0.0106 lr: 0.005\n",
      "iteration: 8080 loss: 0.0120 lr: 0.005\n",
      "iteration: 8090 loss: 0.0130 lr: 0.005\n",
      "iteration: 8100 loss: 0.0138 lr: 0.005\n",
      "iteration: 8110 loss: 0.0129 lr: 0.005\n",
      "iteration: 8120 loss: 0.0087 lr: 0.005\n",
      "iteration: 8130 loss: 0.0125 lr: 0.005\n",
      "iteration: 8140 loss: 0.0111 lr: 0.005\n",
      "iteration: 8150 loss: 0.0118 lr: 0.005\n",
      "iteration: 8160 loss: 0.0119 lr: 0.005\n",
      "iteration: 8170 loss: 0.0129 lr: 0.005\n",
      "iteration: 8180 loss: 0.0116 lr: 0.005\n",
      "iteration: 8190 loss: 0.0104 lr: 0.005\n",
      "iteration: 8200 loss: 0.0148 lr: 0.005\n",
      "iteration: 8210 loss: 0.0105 lr: 0.005\n",
      "iteration: 8220 loss: 0.0102 lr: 0.005\n",
      "iteration: 8230 loss: 0.0124 lr: 0.005\n",
      "iteration: 8240 loss: 0.0139 lr: 0.005\n",
      "iteration: 8250 loss: 0.0100 lr: 0.005\n",
      "iteration: 8260 loss: 0.0091 lr: 0.005\n",
      "iteration: 8270 loss: 0.0119 lr: 0.005\n",
      "iteration: 8280 loss: 0.0105 lr: 0.005\n",
      "iteration: 8290 loss: 0.0096 lr: 0.005\n",
      "iteration: 8300 loss: 0.0116 lr: 0.005\n",
      "iteration: 8310 loss: 0.0129 lr: 0.005\n",
      "iteration: 8320 loss: 0.0093 lr: 0.005\n",
      "iteration: 8330 loss: 0.0099 lr: 0.005\n",
      "iteration: 8340 loss: 0.0145 lr: 0.005\n",
      "iteration: 8350 loss: 0.0131 lr: 0.005\n",
      "iteration: 8360 loss: 0.0088 lr: 0.005\n",
      "iteration: 8370 loss: 0.0093 lr: 0.005\n",
      "iteration: 8380 loss: 0.0090 lr: 0.005\n",
      "iteration: 8390 loss: 0.0107 lr: 0.005\n",
      "iteration: 8400 loss: 0.0146 lr: 0.005\n",
      "iteration: 8410 loss: 0.0119 lr: 0.005\n",
      "iteration: 8420 loss: 0.0102 lr: 0.005\n",
      "iteration: 8430 loss: 0.0152 lr: 0.005\n",
      "iteration: 8440 loss: 0.0106 lr: 0.005\n",
      "iteration: 8450 loss: 0.0111 lr: 0.005\n",
      "iteration: 8460 loss: 0.0104 lr: 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 8470 loss: 0.0154 lr: 0.005\n",
      "iteration: 8480 loss: 0.0102 lr: 0.005\n",
      "iteration: 8490 loss: 0.0112 lr: 0.005\n",
      "iteration: 8500 loss: 0.0098 lr: 0.005\n",
      "iteration: 8510 loss: 0.0104 lr: 0.005\n",
      "iteration: 8520 loss: 0.0098 lr: 0.005\n",
      "iteration: 8530 loss: 0.0102 lr: 0.005\n",
      "iteration: 8540 loss: 0.0111 lr: 0.005\n",
      "iteration: 8550 loss: 0.0111 lr: 0.005\n",
      "iteration: 8560 loss: 0.0087 lr: 0.005\n",
      "iteration: 8570 loss: 0.0096 lr: 0.005\n",
      "iteration: 8580 loss: 0.0094 lr: 0.005\n",
      "iteration: 8590 loss: 0.0126 lr: 0.005\n",
      "iteration: 8600 loss: 0.0106 lr: 0.005\n",
      "iteration: 8610 loss: 0.0114 lr: 0.005\n",
      "iteration: 8620 loss: 0.0087 lr: 0.005\n",
      "iteration: 8630 loss: 0.0109 lr: 0.005\n",
      "iteration: 8640 loss: 0.0101 lr: 0.005\n",
      "iteration: 8650 loss: 0.0129 lr: 0.005\n",
      "iteration: 8660 loss: 0.0113 lr: 0.005\n",
      "iteration: 8670 loss: 0.0138 lr: 0.005\n",
      "iteration: 8680 loss: 0.0124 lr: 0.005\n",
      "iteration: 8690 loss: 0.0121 lr: 0.005\n",
      "iteration: 8700 loss: 0.0083 lr: 0.005\n",
      "iteration: 8710 loss: 0.0099 lr: 0.005\n",
      "iteration: 8720 loss: 0.0108 lr: 0.005\n",
      "iteration: 8730 loss: 0.0097 lr: 0.005\n",
      "iteration: 8740 loss: 0.0095 lr: 0.005\n",
      "iteration: 8750 loss: 0.0115 lr: 0.005\n",
      "iteration: 8760 loss: 0.0091 lr: 0.005\n",
      "iteration: 8770 loss: 0.0097 lr: 0.005\n",
      "iteration: 8780 loss: 0.0126 lr: 0.005\n",
      "iteration: 8790 loss: 0.0110 lr: 0.005\n",
      "iteration: 8800 loss: 0.0119 lr: 0.005\n",
      "iteration: 8810 loss: 0.0122 lr: 0.005\n",
      "iteration: 8820 loss: 0.0129 lr: 0.005\n",
      "iteration: 8830 loss: 0.0100 lr: 0.005\n",
      "iteration: 8840 loss: 0.0118 lr: 0.005\n",
      "iteration: 8850 loss: 0.0101 lr: 0.005\n",
      "iteration: 8860 loss: 0.0110 lr: 0.005\n",
      "iteration: 8870 loss: 0.0111 lr: 0.005\n",
      "iteration: 8880 loss: 0.0106 lr: 0.005\n",
      "iteration: 8890 loss: 0.0118 lr: 0.005\n",
      "iteration: 8900 loss: 0.0098 lr: 0.005\n",
      "iteration: 8910 loss: 0.0133 lr: 0.005\n",
      "iteration: 8920 loss: 0.0119 lr: 0.005\n",
      "iteration: 8930 loss: 0.0097 lr: 0.005\n",
      "iteration: 8940 loss: 0.0117 lr: 0.005\n",
      "iteration: 8950 loss: 0.0103 lr: 0.005\n",
      "iteration: 8960 loss: 0.0111 lr: 0.005\n",
      "iteration: 8970 loss: 0.0088 lr: 0.005\n",
      "iteration: 8980 loss: 0.0107 lr: 0.005\n",
      "iteration: 8990 loss: 0.0086 lr: 0.005\n",
      "iteration: 9000 loss: 0.0117 lr: 0.005\n",
      "iteration: 9010 loss: 0.0104 lr: 0.005\n",
      "iteration: 9020 loss: 0.0141 lr: 0.005\n",
      "iteration: 9030 loss: 0.0122 lr: 0.005\n",
      "iteration: 9040 loss: 0.0106 lr: 0.005\n",
      "iteration: 9050 loss: 0.0083 lr: 0.005\n",
      "iteration: 9060 loss: 0.0126 lr: 0.005\n",
      "iteration: 9070 loss: 0.0106 lr: 0.005\n",
      "iteration: 9080 loss: 0.0117 lr: 0.005\n",
      "iteration: 9090 loss: 0.0085 lr: 0.005\n",
      "iteration: 9100 loss: 0.0081 lr: 0.005\n",
      "iteration: 9110 loss: 0.0137 lr: 0.005\n",
      "iteration: 9120 loss: 0.0093 lr: 0.005\n",
      "iteration: 9130 loss: 0.0103 lr: 0.005\n",
      "iteration: 9140 loss: 0.0109 lr: 0.005\n",
      "iteration: 9150 loss: 0.0098 lr: 0.005\n",
      "iteration: 9160 loss: 0.0113 lr: 0.005\n",
      "iteration: 9170 loss: 0.0103 lr: 0.005\n",
      "iteration: 9180 loss: 0.0092 lr: 0.005\n",
      "iteration: 9190 loss: 0.0126 lr: 0.005\n",
      "iteration: 9200 loss: 0.0102 lr: 0.005\n",
      "iteration: 9210 loss: 0.0101 lr: 0.005\n",
      "iteration: 9220 loss: 0.0098 lr: 0.005\n",
      "iteration: 9230 loss: 0.0084 lr: 0.005\n",
      "iteration: 9240 loss: 0.0079 lr: 0.005\n",
      "iteration: 9250 loss: 0.0087 lr: 0.005\n",
      "iteration: 9260 loss: 0.0099 lr: 0.005\n",
      "iteration: 9270 loss: 0.0102 lr: 0.005\n",
      "iteration: 9280 loss: 0.0113 lr: 0.005\n",
      "iteration: 9290 loss: 0.0099 lr: 0.005\n",
      "iteration: 9300 loss: 0.0115 lr: 0.005\n",
      "iteration: 9310 loss: 0.0086 lr: 0.005\n",
      "iteration: 9320 loss: 0.0084 lr: 0.005\n",
      "iteration: 9330 loss: 0.0087 lr: 0.005\n",
      "iteration: 9340 loss: 0.0100 lr: 0.005\n",
      "iteration: 9350 loss: 0.0109 lr: 0.005\n",
      "iteration: 9360 loss: 0.0100 lr: 0.005\n",
      "iteration: 9370 loss: 0.0129 lr: 0.005\n",
      "iteration: 9380 loss: 0.0097 lr: 0.005\n",
      "iteration: 9390 loss: 0.0110 lr: 0.005\n",
      "iteration: 9400 loss: 0.0119 lr: 0.005\n",
      "iteration: 9410 loss: 0.0087 lr: 0.005\n",
      "iteration: 9420 loss: 0.0090 lr: 0.005\n",
      "iteration: 9430 loss: 0.0095 lr: 0.005\n",
      "iteration: 9440 loss: 0.0091 lr: 0.005\n",
      "iteration: 9450 loss: 0.0118 lr: 0.005\n",
      "iteration: 9460 loss: 0.0086 lr: 0.005\n",
      "iteration: 9470 loss: 0.0103 lr: 0.005\n",
      "iteration: 9480 loss: 0.0108 lr: 0.005\n",
      "iteration: 9490 loss: 0.0135 lr: 0.005\n",
      "iteration: 9500 loss: 0.0106 lr: 0.005\n",
      "iteration: 9510 loss: 0.0111 lr: 0.005\n",
      "iteration: 9520 loss: 0.0097 lr: 0.005\n",
      "iteration: 9530 loss: 0.0123 lr: 0.005\n",
      "iteration: 9540 loss: 0.0110 lr: 0.005\n",
      "iteration: 9550 loss: 0.0102 lr: 0.005\n",
      "iteration: 9560 loss: 0.0100 lr: 0.005\n",
      "iteration: 9570 loss: 0.0126 lr: 0.005\n",
      "iteration: 9580 loss: 0.0087 lr: 0.005\n",
      "iteration: 9590 loss: 0.0091 lr: 0.005\n",
      "iteration: 9600 loss: 0.0100 lr: 0.005\n",
      "iteration: 9610 loss: 0.0106 lr: 0.005\n",
      "iteration: 9620 loss: 0.0081 lr: 0.005\n",
      "iteration: 9630 loss: 0.0109 lr: 0.005\n",
      "iteration: 9640 loss: 0.0111 lr: 0.005\n",
      "iteration: 9650 loss: 0.0107 lr: 0.005\n",
      "iteration: 9660 loss: 0.0116 lr: 0.005\n",
      "iteration: 9670 loss: 0.0154 lr: 0.005\n",
      "iteration: 9680 loss: 0.0105 lr: 0.005\n",
      "iteration: 9690 loss: 0.0077 lr: 0.005\n",
      "iteration: 9700 loss: 0.0104 lr: 0.005\n",
      "iteration: 9710 loss: 0.0142 lr: 0.005\n",
      "iteration: 9720 loss: 0.0121 lr: 0.005\n",
      "iteration: 9730 loss: 0.0078 lr: 0.005\n",
      "iteration: 9740 loss: 0.0082 lr: 0.005\n",
      "iteration: 9750 loss: 0.0121 lr: 0.005\n",
      "iteration: 9760 loss: 0.0081 lr: 0.005\n",
      "iteration: 9770 loss: 0.0094 lr: 0.005\n",
      "iteration: 9780 loss: 0.0088 lr: 0.005\n",
      "iteration: 9790 loss: 0.0094 lr: 0.005\n",
      "iteration: 9800 loss: 0.0104 lr: 0.005\n",
      "iteration: 9810 loss: 0.0106 lr: 0.005\n",
      "iteration: 9820 loss: 0.0103 lr: 0.005\n",
      "iteration: 9830 loss: 0.0098 lr: 0.005\n",
      "iteration: 9840 loss: 0.0099 lr: 0.005\n",
      "iteration: 9850 loss: 0.0093 lr: 0.005\n",
      "iteration: 9860 loss: 0.0097 lr: 0.005\n",
      "iteration: 9870 loss: 0.0090 lr: 0.005\n",
      "iteration: 9880 loss: 0.0076 lr: 0.005\n",
      "iteration: 9890 loss: 0.0093 lr: 0.005\n",
      "iteration: 9900 loss: 0.0110 lr: 0.005\n",
      "iteration: 9910 loss: 0.0116 lr: 0.005\n",
      "iteration: 9920 loss: 0.0115 lr: 0.005\n",
      "iteration: 9930 loss: 0.0115 lr: 0.005\n",
      "iteration: 9940 loss: 0.0095 lr: 0.005\n",
      "iteration: 9950 loss: 0.0107 lr: 0.005\n",
      "iteration: 9960 loss: 0.0094 lr: 0.005\n",
      "iteration: 9970 loss: 0.0097 lr: 0.005\n",
      "iteration: 9980 loss: 0.0092 lr: 0.005\n",
      "iteration: 9990 loss: 0.0084 lr: 0.005\n",
      "iteration: 10000 loss: 0.0092 lr: 0.005\n",
      "iteration: 10010 loss: 0.0115 lr: 0.02\n",
      "iteration: 10020 loss: 0.0133 lr: 0.02\n",
      "iteration: 10030 loss: 0.0158 lr: 0.02\n",
      "iteration: 10040 loss: 0.0111 lr: 0.02\n",
      "iteration: 10050 loss: 0.0139 lr: 0.02\n",
      "iteration: 10060 loss: 0.0168 lr: 0.02\n",
      "iteration: 10070 loss: 0.0217 lr: 0.02\n",
      "iteration: 10080 loss: 0.0192 lr: 0.02\n",
      "iteration: 10090 loss: 0.0157 lr: 0.02\n",
      "iteration: 10100 loss: 0.0184 lr: 0.02\n",
      "iteration: 10110 loss: 0.0247 lr: 0.02\n",
      "iteration: 10120 loss: 0.0196 lr: 0.02\n",
      "iteration: 10130 loss: 0.0179 lr: 0.02\n",
      "iteration: 10140 loss: 0.0210 lr: 0.02\n",
      "iteration: 10150 loss: 0.0207 lr: 0.02\n",
      "iteration: 10160 loss: 0.0136 lr: 0.02\n",
      "iteration: 10170 loss: 0.0157 lr: 0.02\n",
      "iteration: 10180 loss: 0.0164 lr: 0.02\n",
      "iteration: 10190 loss: 0.0162 lr: 0.02\n",
      "iteration: 10200 loss: 0.0164 lr: 0.02\n",
      "iteration: 10210 loss: 0.0196 lr: 0.02\n",
      "iteration: 10220 loss: 0.0178 lr: 0.02\n",
      "iteration: 10230 loss: 0.0185 lr: 0.02\n",
      "iteration: 10240 loss: 0.0196 lr: 0.02\n",
      "iteration: 10250 loss: 0.0161 lr: 0.02\n",
      "iteration: 10260 loss: 0.0200 lr: 0.02\n",
      "iteration: 10270 loss: 0.0157 lr: 0.02\n",
      "iteration: 10280 loss: 0.0235 lr: 0.02\n",
      "iteration: 10290 loss: 0.0180 lr: 0.02\n",
      "iteration: 10300 loss: 0.0147 lr: 0.02\n",
      "iteration: 10310 loss: 0.0181 lr: 0.02\n",
      "iteration: 10320 loss: 0.0168 lr: 0.02\n",
      "iteration: 10330 loss: 0.0184 lr: 0.02\n",
      "iteration: 10340 loss: 0.0118 lr: 0.02\n",
      "iteration: 10350 loss: 0.0179 lr: 0.02\n",
      "iteration: 10360 loss: 0.0189 lr: 0.02\n",
      "iteration: 10370 loss: 0.0154 lr: 0.02\n",
      "iteration: 10380 loss: 0.0127 lr: 0.02\n",
      "iteration: 10390 loss: 0.0165 lr: 0.02\n",
      "iteration: 10400 loss: 0.0142 lr: 0.02\n",
      "iteration: 10410 loss: 0.0144 lr: 0.02\n",
      "iteration: 10420 loss: 0.0160 lr: 0.02\n",
      "iteration: 10430 loss: 0.0165 lr: 0.02\n",
      "iteration: 10440 loss: 0.0143 lr: 0.02\n",
      "iteration: 10450 loss: 0.0176 lr: 0.02\n",
      "iteration: 10460 loss: 0.0139 lr: 0.02\n",
      "iteration: 10470 loss: 0.0162 lr: 0.02\n",
      "iteration: 10480 loss: 0.0158 lr: 0.02\n",
      "iteration: 10490 loss: 0.0159 lr: 0.02\n",
      "iteration: 10500 loss: 0.0156 lr: 0.02\n",
      "iteration: 10510 loss: 0.0147 lr: 0.02\n",
      "iteration: 10520 loss: 0.0271 lr: 0.02\n",
      "iteration: 10530 loss: 0.0169 lr: 0.02\n",
      "iteration: 10540 loss: 0.0182 lr: 0.02\n",
      "iteration: 10550 loss: 0.0152 lr: 0.02\n",
      "iteration: 10560 loss: 0.0173 lr: 0.02\n",
      "iteration: 10570 loss: 0.0156 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 10580 loss: 0.0156 lr: 0.02\n",
      "iteration: 10590 loss: 0.0129 lr: 0.02\n",
      "iteration: 10600 loss: 0.0196 lr: 0.02\n",
      "iteration: 10610 loss: 0.0163 lr: 0.02\n",
      "iteration: 10620 loss: 0.0120 lr: 0.02\n",
      "iteration: 10630 loss: 0.0138 lr: 0.02\n",
      "iteration: 10640 loss: 0.0140 lr: 0.02\n",
      "iteration: 10650 loss: 0.0155 lr: 0.02\n",
      "iteration: 10660 loss: 0.0132 lr: 0.02\n",
      "iteration: 10670 loss: 0.0138 lr: 0.02\n",
      "iteration: 10680 loss: 0.0153 lr: 0.02\n",
      "iteration: 10690 loss: 0.0124 lr: 0.02\n",
      "iteration: 10700 loss: 0.0153 lr: 0.02\n",
      "iteration: 10710 loss: 0.0094 lr: 0.02\n",
      "iteration: 10720 loss: 0.0150 lr: 0.02\n",
      "iteration: 10730 loss: 0.0155 lr: 0.02\n",
      "iteration: 10740 loss: 0.0137 lr: 0.02\n",
      "iteration: 10750 loss: 0.0139 lr: 0.02\n",
      "iteration: 10760 loss: 0.0139 lr: 0.02\n",
      "iteration: 10770 loss: 0.0143 lr: 0.02\n",
      "iteration: 10780 loss: 0.0162 lr: 0.02\n",
      "iteration: 10790 loss: 0.0138 lr: 0.02\n",
      "iteration: 10800 loss: 0.0138 lr: 0.02\n",
      "iteration: 10810 loss: 0.0147 lr: 0.02\n",
      "iteration: 10820 loss: 0.0148 lr: 0.02\n",
      "iteration: 10830 loss: 0.0151 lr: 0.02\n",
      "iteration: 10840 loss: 0.0153 lr: 0.02\n",
      "iteration: 10850 loss: 0.0205 lr: 0.02\n",
      "iteration: 10860 loss: 0.0112 lr: 0.02\n",
      "iteration: 10870 loss: 0.0092 lr: 0.02\n",
      "iteration: 10880 loss: 0.0150 lr: 0.02\n",
      "iteration: 10890 loss: 0.0156 lr: 0.02\n",
      "iteration: 10900 loss: 0.0165 lr: 0.02\n",
      "iteration: 10910 loss: 0.0138 lr: 0.02\n",
      "iteration: 10920 loss: 0.0138 lr: 0.02\n",
      "iteration: 10930 loss: 0.0166 lr: 0.02\n",
      "iteration: 10940 loss: 0.0122 lr: 0.02\n",
      "iteration: 10950 loss: 0.0165 lr: 0.02\n",
      "iteration: 10960 loss: 0.0153 lr: 0.02\n",
      "iteration: 10970 loss: 0.0122 lr: 0.02\n",
      "iteration: 10980 loss: 0.0175 lr: 0.02\n",
      "iteration: 10990 loss: 0.0104 lr: 0.02\n",
      "iteration: 11000 loss: 0.0136 lr: 0.02\n",
      "iteration: 11010 loss: 0.0131 lr: 0.02\n",
      "iteration: 11020 loss: 0.0149 lr: 0.02\n",
      "iteration: 11030 loss: 0.0152 lr: 0.02\n",
      "iteration: 11040 loss: 0.0171 lr: 0.02\n",
      "iteration: 11050 loss: 0.0159 lr: 0.02\n",
      "iteration: 11060 loss: 0.0139 lr: 0.02\n",
      "iteration: 11070 loss: 0.0144 lr: 0.02\n",
      "iteration: 11080 loss: 0.0133 lr: 0.02\n",
      "iteration: 11090 loss: 0.0180 lr: 0.02\n",
      "iteration: 11100 loss: 0.0122 lr: 0.02\n",
      "iteration: 11110 loss: 0.0134 lr: 0.02\n",
      "iteration: 11120 loss: 0.0168 lr: 0.02\n",
      "iteration: 11130 loss: 0.0117 lr: 0.02\n",
      "iteration: 11140 loss: 0.0176 lr: 0.02\n",
      "iteration: 11150 loss: 0.0146 lr: 0.02\n",
      "iteration: 11160 loss: 0.0155 lr: 0.02\n",
      "iteration: 11170 loss: 0.0113 lr: 0.02\n",
      "iteration: 11180 loss: 0.0111 lr: 0.02\n",
      "iteration: 11190 loss: 0.0114 lr: 0.02\n",
      "iteration: 11200 loss: 0.0125 lr: 0.02\n",
      "iteration: 11210 loss: 0.0108 lr: 0.02\n",
      "iteration: 11220 loss: 0.0145 lr: 0.02\n",
      "iteration: 11230 loss: 0.0116 lr: 0.02\n",
      "iteration: 11240 loss: 0.0135 lr: 0.02\n",
      "iteration: 11250 loss: 0.0127 lr: 0.02\n",
      "iteration: 11260 loss: 0.0107 lr: 0.02\n",
      "iteration: 11270 loss: 0.0135 lr: 0.02\n",
      "iteration: 11280 loss: 0.0130 lr: 0.02\n",
      "iteration: 11290 loss: 0.0143 lr: 0.02\n",
      "iteration: 11300 loss: 0.0127 lr: 0.02\n",
      "iteration: 11310 loss: 0.0160 lr: 0.02\n",
      "iteration: 11320 loss: 0.0149 lr: 0.02\n",
      "iteration: 11330 loss: 0.0128 lr: 0.02\n",
      "iteration: 11340 loss: 0.0116 lr: 0.02\n",
      "iteration: 11350 loss: 0.0121 lr: 0.02\n",
      "iteration: 11360 loss: 0.0137 lr: 0.02\n",
      "iteration: 11370 loss: 0.0148 lr: 0.02\n",
      "iteration: 11380 loss: 0.0138 lr: 0.02\n",
      "iteration: 11390 loss: 0.0130 lr: 0.02\n",
      "iteration: 11400 loss: 0.0138 lr: 0.02\n",
      "iteration: 11410 loss: 0.0169 lr: 0.02\n",
      "iteration: 11420 loss: 0.0125 lr: 0.02\n",
      "iteration: 11430 loss: 0.0128 lr: 0.02\n",
      "iteration: 11440 loss: 0.0149 lr: 0.02\n",
      "iteration: 11450 loss: 0.0120 lr: 0.02\n",
      "iteration: 11460 loss: 0.0149 lr: 0.02\n",
      "iteration: 11470 loss: 0.0114 lr: 0.02\n",
      "iteration: 11480 loss: 0.0142 lr: 0.02\n",
      "iteration: 11490 loss: 0.0128 lr: 0.02\n",
      "iteration: 11500 loss: 0.0141 lr: 0.02\n",
      "iteration: 11510 loss: 0.0121 lr: 0.02\n",
      "iteration: 11520 loss: 0.0154 lr: 0.02\n",
      "iteration: 11530 loss: 0.0139 lr: 0.02\n",
      "iteration: 11540 loss: 0.0114 lr: 0.02\n",
      "iteration: 11550 loss: 0.0143 lr: 0.02\n",
      "iteration: 11560 loss: 0.0171 lr: 0.02\n",
      "iteration: 11570 loss: 0.0147 lr: 0.02\n",
      "iteration: 11580 loss: 0.0132 lr: 0.02\n",
      "iteration: 11590 loss: 0.0128 lr: 0.02\n",
      "iteration: 11600 loss: 0.0109 lr: 0.02\n",
      "iteration: 11610 loss: 0.0113 lr: 0.02\n",
      "iteration: 11620 loss: 0.0126 lr: 0.02\n",
      "iteration: 11630 loss: 0.0105 lr: 0.02\n",
      "iteration: 11640 loss: 0.0153 lr: 0.02\n",
      "iteration: 11650 loss: 0.0155 lr: 0.02\n",
      "iteration: 11660 loss: 0.0118 lr: 0.02\n",
      "iteration: 11670 loss: 0.0140 lr: 0.02\n",
      "iteration: 11680 loss: 0.0135 lr: 0.02\n",
      "iteration: 11690 loss: 0.0135 lr: 0.02\n",
      "iteration: 11700 loss: 0.0138 lr: 0.02\n",
      "iteration: 11710 loss: 0.0131 lr: 0.02\n",
      "iteration: 11720 loss: 0.0137 lr: 0.02\n",
      "iteration: 11730 loss: 0.0119 lr: 0.02\n",
      "iteration: 11740 loss: 0.0110 lr: 0.02\n",
      "iteration: 11750 loss: 0.0146 lr: 0.02\n",
      "iteration: 11760 loss: 0.0129 lr: 0.02\n",
      "iteration: 11770 loss: 0.0121 lr: 0.02\n",
      "iteration: 11780 loss: 0.0128 lr: 0.02\n",
      "iteration: 11790 loss: 0.0114 lr: 0.02\n",
      "iteration: 11800 loss: 0.0121 lr: 0.02\n",
      "iteration: 11810 loss: 0.0122 lr: 0.02\n",
      "iteration: 11820 loss: 0.0131 lr: 0.02\n",
      "iteration: 11830 loss: 0.0126 lr: 0.02\n",
      "iteration: 11840 loss: 0.0139 lr: 0.02\n",
      "iteration: 11850 loss: 0.0133 lr: 0.02\n",
      "iteration: 11860 loss: 0.0142 lr: 0.02\n",
      "iteration: 11870 loss: 0.0150 lr: 0.02\n",
      "iteration: 11880 loss: 0.0110 lr: 0.02\n",
      "iteration: 11890 loss: 0.0149 lr: 0.02\n",
      "iteration: 11900 loss: 0.0157 lr: 0.02\n",
      "iteration: 11910 loss: 0.0136 lr: 0.02\n",
      "iteration: 11920 loss: 0.0118 lr: 0.02\n",
      "iteration: 11930 loss: 0.0125 lr: 0.02\n",
      "iteration: 11940 loss: 0.0129 lr: 0.02\n",
      "iteration: 11950 loss: 0.0136 lr: 0.02\n",
      "iteration: 11960 loss: 0.0114 lr: 0.02\n",
      "iteration: 11970 loss: 0.0110 lr: 0.02\n",
      "iteration: 11980 loss: 0.0134 lr: 0.02\n",
      "iteration: 11990 loss: 0.0121 lr: 0.02\n",
      "iteration: 12000 loss: 0.0169 lr: 0.02\n",
      "iteration: 12010 loss: 0.0115 lr: 0.02\n",
      "iteration: 12020 loss: 0.0125 lr: 0.02\n",
      "iteration: 12030 loss: 0.0147 lr: 0.02\n",
      "iteration: 12040 loss: 0.0139 lr: 0.02\n",
      "iteration: 12050 loss: 0.0138 lr: 0.02\n",
      "iteration: 12060 loss: 0.0124 lr: 0.02\n",
      "iteration: 12070 loss: 0.0109 lr: 0.02\n",
      "iteration: 12080 loss: 0.0130 lr: 0.02\n",
      "iteration: 12090 loss: 0.0091 lr: 0.02\n",
      "iteration: 12100 loss: 0.0092 lr: 0.02\n",
      "iteration: 12110 loss: 0.0125 lr: 0.02\n",
      "iteration: 12120 loss: 0.0133 lr: 0.02\n",
      "iteration: 12130 loss: 0.0151 lr: 0.02\n",
      "iteration: 12140 loss: 0.0139 lr: 0.02\n",
      "iteration: 12150 loss: 0.0095 lr: 0.02\n",
      "iteration: 12160 loss: 0.0105 lr: 0.02\n",
      "iteration: 12170 loss: 0.0114 lr: 0.02\n",
      "iteration: 12180 loss: 0.0110 lr: 0.02\n",
      "iteration: 12190 loss: 0.0140 lr: 0.02\n",
      "iteration: 12200 loss: 0.0120 lr: 0.02\n",
      "iteration: 12210 loss: 0.0130 lr: 0.02\n",
      "iteration: 12220 loss: 0.0126 lr: 0.02\n",
      "iteration: 12230 loss: 0.0102 lr: 0.02\n",
      "iteration: 12240 loss: 0.0144 lr: 0.02\n",
      "iteration: 12250 loss: 0.0121 lr: 0.02\n",
      "iteration: 12260 loss: 0.0119 lr: 0.02\n",
      "iteration: 12270 loss: 0.0104 lr: 0.02\n",
      "iteration: 12280 loss: 0.0106 lr: 0.02\n",
      "iteration: 12290 loss: 0.0135 lr: 0.02\n",
      "iteration: 12300 loss: 0.0103 lr: 0.02\n",
      "iteration: 12310 loss: 0.0100 lr: 0.02\n",
      "iteration: 12320 loss: 0.0118 lr: 0.02\n",
      "iteration: 12330 loss: 0.0144 lr: 0.02\n",
      "iteration: 12340 loss: 0.0117 lr: 0.02\n",
      "iteration: 12350 loss: 0.0149 lr: 0.02\n",
      "iteration: 12360 loss: 0.0112 lr: 0.02\n",
      "iteration: 12370 loss: 0.0082 lr: 0.02\n",
      "iteration: 12380 loss: 0.0152 lr: 0.02\n",
      "iteration: 12390 loss: 0.0110 lr: 0.02\n",
      "iteration: 12400 loss: 0.0125 lr: 0.02\n",
      "iteration: 12410 loss: 0.0089 lr: 0.02\n",
      "iteration: 12420 loss: 0.0102 lr: 0.02\n",
      "iteration: 12430 loss: 0.0124 lr: 0.02\n",
      "iteration: 12440 loss: 0.0113 lr: 0.02\n",
      "iteration: 12450 loss: 0.0128 lr: 0.02\n",
      "iteration: 12460 loss: 0.0106 lr: 0.02\n",
      "iteration: 12470 loss: 0.0121 lr: 0.02\n",
      "iteration: 12480 loss: 0.0124 lr: 0.02\n",
      "iteration: 12490 loss: 0.0123 lr: 0.02\n",
      "iteration: 12500 loss: 0.0110 lr: 0.02\n",
      "iteration: 12510 loss: 0.0147 lr: 0.02\n",
      "iteration: 12520 loss: 0.0110 lr: 0.02\n",
      "iteration: 12530 loss: 0.0127 lr: 0.02\n",
      "iteration: 12540 loss: 0.0097 lr: 0.02\n",
      "iteration: 12550 loss: 0.0129 lr: 0.02\n",
      "iteration: 12560 loss: 0.0109 lr: 0.02\n",
      "iteration: 12570 loss: 0.0171 lr: 0.02\n",
      "iteration: 12580 loss: 0.0112 lr: 0.02\n",
      "iteration: 12590 loss: 0.0141 lr: 0.02\n",
      "iteration: 12600 loss: 0.0099 lr: 0.02\n",
      "iteration: 12610 loss: 0.0117 lr: 0.02\n",
      "iteration: 12620 loss: 0.0113 lr: 0.02\n",
      "iteration: 12630 loss: 0.0106 lr: 0.02\n",
      "iteration: 12640 loss: 0.0112 lr: 0.02\n",
      "iteration: 12650 loss: 0.0108 lr: 0.02\n",
      "iteration: 12660 loss: 0.0109 lr: 0.02\n",
      "iteration: 12670 loss: 0.0096 lr: 0.02\n",
      "iteration: 12680 loss: 0.0156 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 12690 loss: 0.0115 lr: 0.02\n",
      "iteration: 12700 loss: 0.0119 lr: 0.02\n",
      "iteration: 12710 loss: 0.0131 lr: 0.02\n",
      "iteration: 12720 loss: 0.0129 lr: 0.02\n",
      "iteration: 12730 loss: 0.0143 lr: 0.02\n",
      "iteration: 12740 loss: 0.0094 lr: 0.02\n",
      "iteration: 12750 loss: 0.0140 lr: 0.02\n",
      "iteration: 12760 loss: 0.0138 lr: 0.02\n",
      "iteration: 12770 loss: 0.0102 lr: 0.02\n",
      "iteration: 12780 loss: 0.0117 lr: 0.02\n",
      "iteration: 12790 loss: 0.0143 lr: 0.02\n",
      "iteration: 12800 loss: 0.0092 lr: 0.02\n",
      "iteration: 12810 loss: 0.0083 lr: 0.02\n",
      "iteration: 12820 loss: 0.0109 lr: 0.02\n",
      "iteration: 12830 loss: 0.0124 lr: 0.02\n",
      "iteration: 12840 loss: 0.0123 lr: 0.02\n",
      "iteration: 12850 loss: 0.0087 lr: 0.02\n",
      "iteration: 12860 loss: 0.0085 lr: 0.02\n",
      "iteration: 12870 loss: 0.0120 lr: 0.02\n",
      "iteration: 12880 loss: 0.0111 lr: 0.02\n",
      "iteration: 12890 loss: 0.0115 lr: 0.02\n",
      "iteration: 12900 loss: 0.0119 lr: 0.02\n",
      "iteration: 12910 loss: 0.0110 lr: 0.02\n",
      "iteration: 12920 loss: 0.0103 lr: 0.02\n",
      "iteration: 12930 loss: 0.0096 lr: 0.02\n",
      "iteration: 12940 loss: 0.0120 lr: 0.02\n",
      "iteration: 12950 loss: 0.0099 lr: 0.02\n",
      "iteration: 12960 loss: 0.0120 lr: 0.02\n",
      "iteration: 12970 loss: 0.0107 lr: 0.02\n",
      "iteration: 12980 loss: 0.0113 lr: 0.02\n",
      "iteration: 12990 loss: 0.0103 lr: 0.02\n",
      "iteration: 13000 loss: 0.0118 lr: 0.02\n",
      "iteration: 13010 loss: 0.0147 lr: 0.02\n",
      "iteration: 13020 loss: 0.0133 lr: 0.02\n",
      "iteration: 13030 loss: 0.0132 lr: 0.02\n",
      "iteration: 13040 loss: 0.0113 lr: 0.02\n",
      "iteration: 13050 loss: 0.0112 lr: 0.02\n",
      "iteration: 13060 loss: 0.0112 lr: 0.02\n",
      "iteration: 13070 loss: 0.0125 lr: 0.02\n",
      "iteration: 13080 loss: 0.0118 lr: 0.02\n",
      "iteration: 13090 loss: 0.0112 lr: 0.02\n",
      "iteration: 13100 loss: 0.0103 lr: 0.02\n",
      "iteration: 13110 loss: 0.0126 lr: 0.02\n",
      "iteration: 13120 loss: 0.0119 lr: 0.02\n",
      "iteration: 13130 loss: 0.0091 lr: 0.02\n",
      "iteration: 13140 loss: 0.0103 lr: 0.02\n",
      "iteration: 13150 loss: 0.0100 lr: 0.02\n",
      "iteration: 13160 loss: 0.0128 lr: 0.02\n",
      "iteration: 13170 loss: 0.0108 lr: 0.02\n",
      "iteration: 13180 loss: 0.0153 lr: 0.02\n",
      "iteration: 13190 loss: 0.0120 lr: 0.02\n",
      "iteration: 13200 loss: 0.0125 lr: 0.02\n",
      "iteration: 13210 loss: 0.0140 lr: 0.02\n",
      "iteration: 13220 loss: 0.0125 lr: 0.02\n",
      "iteration: 13230 loss: 0.0090 lr: 0.02\n",
      "iteration: 13240 loss: 0.0109 lr: 0.02\n",
      "iteration: 13250 loss: 0.0116 lr: 0.02\n",
      "iteration: 13260 loss: 0.0124 lr: 0.02\n",
      "iteration: 13270 loss: 0.0090 lr: 0.02\n",
      "iteration: 13280 loss: 0.0131 lr: 0.02\n",
      "iteration: 13290 loss: 0.0096 lr: 0.02\n",
      "iteration: 13300 loss: 0.0104 lr: 0.02\n",
      "iteration: 13310 loss: 0.0090 lr: 0.02\n",
      "iteration: 13320 loss: 0.0133 lr: 0.02\n",
      "iteration: 13330 loss: 0.0124 lr: 0.02\n",
      "iteration: 13340 loss: 0.0098 lr: 0.02\n",
      "iteration: 13350 loss: 0.0112 lr: 0.02\n",
      "iteration: 13360 loss: 0.0104 lr: 0.02\n",
      "iteration: 13370 loss: 0.0104 lr: 0.02\n",
      "iteration: 13380 loss: 0.0114 lr: 0.02\n",
      "iteration: 13390 loss: 0.0096 lr: 0.02\n",
      "iteration: 13400 loss: 0.0117 lr: 0.02\n",
      "iteration: 13410 loss: 0.0089 lr: 0.02\n",
      "iteration: 13420 loss: 0.0128 lr: 0.02\n",
      "iteration: 13430 loss: 0.0100 lr: 0.02\n",
      "iteration: 13440 loss: 0.0099 lr: 0.02\n",
      "iteration: 13450 loss: 0.0088 lr: 0.02\n",
      "iteration: 13460 loss: 0.0105 lr: 0.02\n",
      "iteration: 13470 loss: 0.0096 lr: 0.02\n",
      "iteration: 13480 loss: 0.0103 lr: 0.02\n",
      "iteration: 13490 loss: 0.0112 lr: 0.02\n",
      "iteration: 13500 loss: 0.0090 lr: 0.02\n",
      "iteration: 13510 loss: 0.0119 lr: 0.02\n",
      "iteration: 13520 loss: 0.0116 lr: 0.02\n",
      "iteration: 13530 loss: 0.0093 lr: 0.02\n",
      "iteration: 13540 loss: 0.0102 lr: 0.02\n",
      "iteration: 13550 loss: 0.0093 lr: 0.02\n",
      "iteration: 13560 loss: 0.0099 lr: 0.02\n",
      "iteration: 13570 loss: 0.0102 lr: 0.02\n",
      "iteration: 13580 loss: 0.0089 lr: 0.02\n",
      "iteration: 13590 loss: 0.0131 lr: 0.02\n",
      "iteration: 13600 loss: 0.0099 lr: 0.02\n",
      "iteration: 13610 loss: 0.0118 lr: 0.02\n",
      "iteration: 13620 loss: 0.0166 lr: 0.02\n",
      "iteration: 13630 loss: 0.0102 lr: 0.02\n",
      "iteration: 13640 loss: 0.0146 lr: 0.02\n",
      "iteration: 13650 loss: 0.0081 lr: 0.02\n",
      "iteration: 13660 loss: 0.0070 lr: 0.02\n",
      "iteration: 13670 loss: 0.0078 lr: 0.02\n",
      "iteration: 13680 loss: 0.0126 lr: 0.02\n",
      "iteration: 13690 loss: 0.0092 lr: 0.02\n",
      "iteration: 13700 loss: 0.0106 lr: 0.02\n",
      "iteration: 13710 loss: 0.0118 lr: 0.02\n",
      "iteration: 13720 loss: 0.0084 lr: 0.02\n",
      "iteration: 13730 loss: 0.0102 lr: 0.02\n",
      "iteration: 13740 loss: 0.0119 lr: 0.02\n",
      "iteration: 13750 loss: 0.0103 lr: 0.02\n",
      "iteration: 13760 loss: 0.0098 lr: 0.02\n",
      "iteration: 13770 loss: 0.0118 lr: 0.02\n",
      "iteration: 13780 loss: 0.0105 lr: 0.02\n",
      "iteration: 13790 loss: 0.0075 lr: 0.02\n",
      "iteration: 13800 loss: 0.0067 lr: 0.02\n",
      "iteration: 13810 loss: 0.0111 lr: 0.02\n",
      "iteration: 13820 loss: 0.0111 lr: 0.02\n",
      "iteration: 13830 loss: 0.0116 lr: 0.02\n",
      "iteration: 13840 loss: 0.0149 lr: 0.02\n",
      "iteration: 13850 loss: 0.0105 lr: 0.02\n",
      "iteration: 13860 loss: 0.0100 lr: 0.02\n",
      "iteration: 13870 loss: 0.0096 lr: 0.02\n",
      "iteration: 13880 loss: 0.0075 lr: 0.02\n",
      "iteration: 13890 loss: 0.0118 lr: 0.02\n",
      "iteration: 13900 loss: 0.0128 lr: 0.02\n",
      "iteration: 13910 loss: 0.0093 lr: 0.02\n",
      "iteration: 13920 loss: 0.0090 lr: 0.02\n",
      "iteration: 13930 loss: 0.0111 lr: 0.02\n",
      "iteration: 13940 loss: 0.0127 lr: 0.02\n",
      "iteration: 13950 loss: 0.0099 lr: 0.02\n",
      "iteration: 13960 loss: 0.0095 lr: 0.02\n",
      "iteration: 13970 loss: 0.0087 lr: 0.02\n",
      "iteration: 13980 loss: 0.0119 lr: 0.02\n",
      "iteration: 13990 loss: 0.0110 lr: 0.02\n",
      "iteration: 14000 loss: 0.0096 lr: 0.02\n",
      "iteration: 14010 loss: 0.0111 lr: 0.02\n",
      "iteration: 14020 loss: 0.0108 lr: 0.02\n",
      "iteration: 14030 loss: 0.0144 lr: 0.02\n",
      "iteration: 14040 loss: 0.0132 lr: 0.02\n",
      "iteration: 14050 loss: 0.0117 lr: 0.02\n",
      "iteration: 14060 loss: 0.0114 lr: 0.02\n",
      "iteration: 14070 loss: 0.0110 lr: 0.02\n",
      "iteration: 14080 loss: 0.0115 lr: 0.02\n",
      "iteration: 14090 loss: 0.0104 lr: 0.02\n",
      "iteration: 14100 loss: 0.0106 lr: 0.02\n",
      "iteration: 14110 loss: 0.0081 lr: 0.02\n",
      "iteration: 14120 loss: 0.0098 lr: 0.02\n",
      "iteration: 14130 loss: 0.0091 lr: 0.02\n",
      "iteration: 14140 loss: 0.0093 lr: 0.02\n",
      "iteration: 14150 loss: 0.0090 lr: 0.02\n",
      "iteration: 14160 loss: 0.0115 lr: 0.02\n",
      "iteration: 14170 loss: 0.0104 lr: 0.02\n",
      "iteration: 14180 loss: 0.0133 lr: 0.02\n",
      "iteration: 14190 loss: 0.0082 lr: 0.02\n",
      "iteration: 14200 loss: 0.0117 lr: 0.02\n",
      "iteration: 14210 loss: 0.0117 lr: 0.02\n",
      "iteration: 14220 loss: 0.0099 lr: 0.02\n",
      "iteration: 14230 loss: 0.0093 lr: 0.02\n",
      "iteration: 14240 loss: 0.0100 lr: 0.02\n",
      "iteration: 14250 loss: 0.0094 lr: 0.02\n",
      "iteration: 14260 loss: 0.0102 lr: 0.02\n",
      "iteration: 14270 loss: 0.0109 lr: 0.02\n",
      "iteration: 14280 loss: 0.0088 lr: 0.02\n",
      "iteration: 14290 loss: 0.0092 lr: 0.02\n",
      "iteration: 14300 loss: 0.0095 lr: 0.02\n",
      "iteration: 14310 loss: 0.0089 lr: 0.02\n",
      "iteration: 14320 loss: 0.0106 lr: 0.02\n",
      "iteration: 14330 loss: 0.0081 lr: 0.02\n",
      "iteration: 14340 loss: 0.0101 lr: 0.02\n",
      "iteration: 14350 loss: 0.0102 lr: 0.02\n",
      "iteration: 14360 loss: 0.0078 lr: 0.02\n",
      "iteration: 14370 loss: 0.0109 lr: 0.02\n",
      "iteration: 14380 loss: 0.0076 lr: 0.02\n",
      "iteration: 14390 loss: 0.0089 lr: 0.02\n",
      "iteration: 14400 loss: 0.0124 lr: 0.02\n",
      "iteration: 14410 loss: 0.0092 lr: 0.02\n",
      "iteration: 14420 loss: 0.0110 lr: 0.02\n",
      "iteration: 14430 loss: 0.0109 lr: 0.02\n",
      "iteration: 14440 loss: 0.0100 lr: 0.02\n",
      "iteration: 14450 loss: 0.0085 lr: 0.02\n",
      "iteration: 14460 loss: 0.0118 lr: 0.02\n",
      "iteration: 14470 loss: 0.0099 lr: 0.02\n",
      "iteration: 14480 loss: 0.0122 lr: 0.02\n",
      "iteration: 14490 loss: 0.0108 lr: 0.02\n",
      "iteration: 14500 loss: 0.0115 lr: 0.02\n",
      "iteration: 14510 loss: 0.0086 lr: 0.02\n",
      "iteration: 14520 loss: 0.0130 lr: 0.02\n",
      "iteration: 14530 loss: 0.0098 lr: 0.02\n",
      "iteration: 14540 loss: 0.0096 lr: 0.02\n",
      "iteration: 14550 loss: 0.0117 lr: 0.02\n",
      "iteration: 14560 loss: 0.0093 lr: 0.02\n",
      "iteration: 14570 loss: 0.0096 lr: 0.02\n",
      "iteration: 14580 loss: 0.0082 lr: 0.02\n",
      "iteration: 14590 loss: 0.0096 lr: 0.02\n",
      "iteration: 14600 loss: 0.0124 lr: 0.02\n",
      "iteration: 14610 loss: 0.0127 lr: 0.02\n",
      "iteration: 14620 loss: 0.0104 lr: 0.02\n",
      "iteration: 14630 loss: 0.0086 lr: 0.02\n",
      "iteration: 14640 loss: 0.0087 lr: 0.02\n",
      "iteration: 14650 loss: 0.0092 lr: 0.02\n",
      "iteration: 14660 loss: 0.0079 lr: 0.02\n",
      "iteration: 14670 loss: 0.0113 lr: 0.02\n",
      "iteration: 14680 loss: 0.0064 lr: 0.02\n",
      "iteration: 14690 loss: 0.0105 lr: 0.02\n",
      "iteration: 14700 loss: 0.0098 lr: 0.02\n",
      "iteration: 14710 loss: 0.0072 lr: 0.02\n",
      "iteration: 14720 loss: 0.0076 lr: 0.02\n",
      "iteration: 14730 loss: 0.0092 lr: 0.02\n",
      "iteration: 14740 loss: 0.0074 lr: 0.02\n",
      "iteration: 14750 loss: 0.0096 lr: 0.02\n",
      "iteration: 14760 loss: 0.0093 lr: 0.02\n",
      "iteration: 14770 loss: 0.0120 lr: 0.02\n",
      "iteration: 14780 loss: 0.0096 lr: 0.02\n",
      "iteration: 14790 loss: 0.0083 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 14800 loss: 0.0114 lr: 0.02\n",
      "iteration: 14810 loss: 0.0102 lr: 0.02\n",
      "iteration: 14820 loss: 0.0121 lr: 0.02\n",
      "iteration: 14830 loss: 0.0116 lr: 0.02\n",
      "iteration: 14840 loss: 0.0094 lr: 0.02\n",
      "iteration: 14850 loss: 0.0122 lr: 0.02\n",
      "iteration: 14860 loss: 0.0123 lr: 0.02\n",
      "iteration: 14870 loss: 0.0115 lr: 0.02\n",
      "iteration: 14880 loss: 0.0086 lr: 0.02\n",
      "iteration: 14890 loss: 0.0129 lr: 0.02\n",
      "iteration: 14900 loss: 0.0136 lr: 0.02\n",
      "iteration: 14910 loss: 0.0086 lr: 0.02\n",
      "iteration: 14920 loss: 0.0094 lr: 0.02\n",
      "iteration: 14930 loss: 0.0088 lr: 0.02\n",
      "iteration: 14940 loss: 0.0095 lr: 0.02\n",
      "iteration: 14950 loss: 0.0099 lr: 0.02\n",
      "iteration: 14960 loss: 0.0094 lr: 0.02\n",
      "iteration: 14970 loss: 0.0140 lr: 0.02\n",
      "iteration: 14980 loss: 0.0082 lr: 0.02\n",
      "iteration: 14990 loss: 0.0097 lr: 0.02\n",
      "iteration: 15000 loss: 0.0085 lr: 0.02\n",
      "iteration: 15010 loss: 0.0084 lr: 0.02\n",
      "iteration: 15020 loss: 0.0089 lr: 0.02\n",
      "iteration: 15030 loss: 0.0122 lr: 0.02\n",
      "iteration: 15040 loss: 0.0095 lr: 0.02\n",
      "iteration: 15050 loss: 0.0100 lr: 0.02\n",
      "iteration: 15060 loss: 0.0098 lr: 0.02\n",
      "iteration: 15070 loss: 0.0079 lr: 0.02\n",
      "iteration: 15080 loss: 0.0099 lr: 0.02\n",
      "iteration: 15090 loss: 0.0146 lr: 0.02\n",
      "iteration: 15100 loss: 0.0107 lr: 0.02\n",
      "iteration: 15110 loss: 0.0077 lr: 0.02\n",
      "iteration: 15120 loss: 0.0082 lr: 0.02\n",
      "iteration: 15130 loss: 0.0086 lr: 0.02\n",
      "iteration: 15140 loss: 0.0098 lr: 0.02\n",
      "iteration: 15150 loss: 0.0094 lr: 0.02\n",
      "iteration: 15160 loss: 0.0113 lr: 0.02\n",
      "iteration: 15170 loss: 0.0079 lr: 0.02\n",
      "iteration: 15180 loss: 0.0100 lr: 0.02\n",
      "iteration: 15190 loss: 0.0096 lr: 0.02\n",
      "iteration: 15200 loss: 0.0071 lr: 0.02\n",
      "iteration: 15210 loss: 0.0082 lr: 0.02\n",
      "iteration: 15220 loss: 0.0089 lr: 0.02\n",
      "iteration: 15230 loss: 0.0101 lr: 0.02\n",
      "iteration: 15240 loss: 0.0079 lr: 0.02\n",
      "iteration: 15250 loss: 0.0098 lr: 0.02\n",
      "iteration: 15260 loss: 0.0064 lr: 0.02\n",
      "iteration: 15270 loss: 0.0083 lr: 0.02\n",
      "iteration: 15280 loss: 0.0086 lr: 0.02\n",
      "iteration: 15290 loss: 0.0101 lr: 0.02\n",
      "iteration: 15300 loss: 0.0077 lr: 0.02\n",
      "iteration: 15310 loss: 0.0123 lr: 0.02\n",
      "iteration: 15320 loss: 0.0106 lr: 0.02\n",
      "iteration: 15330 loss: 0.0106 lr: 0.02\n",
      "iteration: 15340 loss: 0.0097 lr: 0.02\n",
      "iteration: 15350 loss: 0.0082 lr: 0.02\n",
      "iteration: 15360 loss: 0.0102 lr: 0.02\n",
      "iteration: 15370 loss: 0.0086 lr: 0.02\n",
      "iteration: 15380 loss: 0.0105 lr: 0.02\n",
      "iteration: 15390 loss: 0.0107 lr: 0.02\n",
      "iteration: 15400 loss: 0.0105 lr: 0.02\n",
      "iteration: 15410 loss: 0.0093 lr: 0.02\n",
      "iteration: 15420 loss: 0.0093 lr: 0.02\n",
      "iteration: 15430 loss: 0.0116 lr: 0.02\n",
      "iteration: 15440 loss: 0.0118 lr: 0.02\n",
      "iteration: 15450 loss: 0.0082 lr: 0.02\n",
      "iteration: 15460 loss: 0.0084 lr: 0.02\n",
      "iteration: 15470 loss: 0.0110 lr: 0.02\n",
      "iteration: 15480 loss: 0.0092 lr: 0.02\n",
      "iteration: 15490 loss: 0.0093 lr: 0.02\n",
      "iteration: 15500 loss: 0.0093 lr: 0.02\n",
      "iteration: 15510 loss: 0.0086 lr: 0.02\n",
      "iteration: 15520 loss: 0.0103 lr: 0.02\n",
      "iteration: 15530 loss: 0.0112 lr: 0.02\n",
      "iteration: 15540 loss: 0.0099 lr: 0.02\n",
      "iteration: 15550 loss: 0.0117 lr: 0.02\n",
      "iteration: 15560 loss: 0.0118 lr: 0.02\n",
      "iteration: 15570 loss: 0.0099 lr: 0.02\n",
      "iteration: 15580 loss: 0.0081 lr: 0.02\n",
      "iteration: 15590 loss: 0.0101 lr: 0.02\n",
      "iteration: 15600 loss: 0.0105 lr: 0.02\n",
      "iteration: 15610 loss: 0.0103 lr: 0.02\n",
      "iteration: 15620 loss: 0.0094 lr: 0.02\n",
      "iteration: 15630 loss: 0.0079 lr: 0.02\n",
      "iteration: 15640 loss: 0.0093 lr: 0.02\n",
      "iteration: 15650 loss: 0.0070 lr: 0.02\n",
      "iteration: 15660 loss: 0.0074 lr: 0.02\n",
      "iteration: 15670 loss: 0.0099 lr: 0.02\n",
      "iteration: 15680 loss: 0.0093 lr: 0.02\n",
      "iteration: 15690 loss: 0.0107 lr: 0.02\n",
      "iteration: 15700 loss: 0.0119 lr: 0.02\n",
      "iteration: 15710 loss: 0.0097 lr: 0.02\n",
      "iteration: 15720 loss: 0.0096 lr: 0.02\n",
      "iteration: 15730 loss: 0.0086 lr: 0.02\n",
      "iteration: 15740 loss: 0.0096 lr: 0.02\n",
      "iteration: 15750 loss: 0.0083 lr: 0.02\n",
      "iteration: 15760 loss: 0.0084 lr: 0.02\n",
      "iteration: 15770 loss: 0.0090 lr: 0.02\n",
      "iteration: 15780 loss: 0.0077 lr: 0.02\n",
      "iteration: 15790 loss: 0.0097 lr: 0.02\n",
      "iteration: 15800 loss: 0.0077 lr: 0.02\n",
      "iteration: 15810 loss: 0.0104 lr: 0.02\n",
      "iteration: 15820 loss: 0.0082 lr: 0.02\n",
      "iteration: 15830 loss: 0.0093 lr: 0.02\n",
      "iteration: 15840 loss: 0.0100 lr: 0.02\n",
      "iteration: 15850 loss: 0.0114 lr: 0.02\n",
      "iteration: 15860 loss: 0.0070 lr: 0.02\n",
      "iteration: 15870 loss: 0.0125 lr: 0.02\n",
      "iteration: 15880 loss: 0.0102 lr: 0.02\n",
      "iteration: 15890 loss: 0.0131 lr: 0.02\n",
      "iteration: 15900 loss: 0.0081 lr: 0.02\n",
      "iteration: 15910 loss: 0.0088 lr: 0.02\n",
      "iteration: 15920 loss: 0.0087 lr: 0.02\n",
      "iteration: 15930 loss: 0.0089 lr: 0.02\n",
      "iteration: 15940 loss: 0.0102 lr: 0.02\n",
      "iteration: 15950 loss: 0.0074 lr: 0.02\n",
      "iteration: 15960 loss: 0.0084 lr: 0.02\n",
      "iteration: 15970 loss: 0.0082 lr: 0.02\n",
      "iteration: 15980 loss: 0.0097 lr: 0.02\n",
      "iteration: 15990 loss: 0.0078 lr: 0.02\n",
      "iteration: 16000 loss: 0.0125 lr: 0.02\n",
      "iteration: 16010 loss: 0.0111 lr: 0.02\n",
      "iteration: 16020 loss: 0.0111 lr: 0.02\n",
      "iteration: 16030 loss: 0.0111 lr: 0.02\n",
      "iteration: 16040 loss: 0.0098 lr: 0.02\n",
      "iteration: 16050 loss: 0.0069 lr: 0.02\n",
      "iteration: 16060 loss: 0.0081 lr: 0.02\n",
      "iteration: 16070 loss: 0.0089 lr: 0.02\n",
      "iteration: 16080 loss: 0.0117 lr: 0.02\n",
      "iteration: 16090 loss: 0.0092 lr: 0.02\n",
      "iteration: 16100 loss: 0.0075 lr: 0.02\n",
      "iteration: 16110 loss: 0.0093 lr: 0.02\n",
      "iteration: 16120 loss: 0.0107 lr: 0.02\n",
      "iteration: 16130 loss: 0.0089 lr: 0.02\n",
      "iteration: 16140 loss: 0.0071 lr: 0.02\n",
      "iteration: 16150 loss: 0.0080 lr: 0.02\n",
      "iteration: 16160 loss: 0.0083 lr: 0.02\n",
      "iteration: 16170 loss: 0.0076 lr: 0.02\n",
      "iteration: 16180 loss: 0.0072 lr: 0.02\n",
      "iteration: 16190 loss: 0.0095 lr: 0.02\n",
      "iteration: 16200 loss: 0.0105 lr: 0.02\n",
      "iteration: 16210 loss: 0.0089 lr: 0.02\n",
      "iteration: 16220 loss: 0.0090 lr: 0.02\n",
      "iteration: 16230 loss: 0.0074 lr: 0.02\n",
      "iteration: 16240 loss: 0.0057 lr: 0.02\n",
      "iteration: 16250 loss: 0.0084 lr: 0.02\n",
      "iteration: 16260 loss: 0.0074 lr: 0.02\n",
      "iteration: 16270 loss: 0.0083 lr: 0.02\n",
      "iteration: 16280 loss: 0.0081 lr: 0.02\n",
      "iteration: 16290 loss: 0.0099 lr: 0.02\n",
      "iteration: 16300 loss: 0.0085 lr: 0.02\n",
      "iteration: 16310 loss: 0.0084 lr: 0.02\n",
      "iteration: 16320 loss: 0.0065 lr: 0.02\n",
      "iteration: 16330 loss: 0.0080 lr: 0.02\n",
      "iteration: 16340 loss: 0.0074 lr: 0.02\n",
      "iteration: 16350 loss: 0.0101 lr: 0.02\n",
      "iteration: 16360 loss: 0.0117 lr: 0.02\n",
      "iteration: 16370 loss: 0.0083 lr: 0.02\n",
      "iteration: 16380 loss: 0.0101 lr: 0.02\n",
      "iteration: 16390 loss: 0.0084 lr: 0.02\n",
      "iteration: 16400 loss: 0.0065 lr: 0.02\n",
      "iteration: 16410 loss: 0.0082 lr: 0.02\n",
      "iteration: 16420 loss: 0.0067 lr: 0.02\n",
      "iteration: 16430 loss: 0.0101 lr: 0.02\n",
      "iteration: 16440 loss: 0.0100 lr: 0.02\n",
      "iteration: 16450 loss: 0.0093 lr: 0.02\n",
      "iteration: 16460 loss: 0.0079 lr: 0.02\n",
      "iteration: 16470 loss: 0.0101 lr: 0.02\n",
      "iteration: 16480 loss: 0.0072 lr: 0.02\n",
      "iteration: 16490 loss: 0.0077 lr: 0.02\n",
      "iteration: 16500 loss: 0.0102 lr: 0.02\n",
      "iteration: 16510 loss: 0.0098 lr: 0.02\n",
      "iteration: 16520 loss: 0.0097 lr: 0.02\n",
      "iteration: 16530 loss: 0.0099 lr: 0.02\n",
      "iteration: 16540 loss: 0.0086 lr: 0.02\n",
      "iteration: 16550 loss: 0.0081 lr: 0.02\n",
      "iteration: 16560 loss: 0.0081 lr: 0.02\n",
      "iteration: 16570 loss: 0.0078 lr: 0.02\n",
      "iteration: 16580 loss: 0.0099 lr: 0.02\n",
      "iteration: 16590 loss: 0.0081 lr: 0.02\n",
      "iteration: 16600 loss: 0.0073 lr: 0.02\n",
      "iteration: 16610 loss: 0.0077 lr: 0.02\n",
      "iteration: 16620 loss: 0.0080 lr: 0.02\n",
      "iteration: 16630 loss: 0.0078 lr: 0.02\n",
      "iteration: 16640 loss: 0.0080 lr: 0.02\n",
      "iteration: 16650 loss: 0.0092 lr: 0.02\n",
      "iteration: 16660 loss: 0.0080 lr: 0.02\n",
      "iteration: 16670 loss: 0.0086 lr: 0.02\n",
      "iteration: 16680 loss: 0.0090 lr: 0.02\n",
      "iteration: 16690 loss: 0.0088 lr: 0.02\n",
      "iteration: 16700 loss: 0.0097 lr: 0.02\n",
      "iteration: 16710 loss: 0.0073 lr: 0.02\n",
      "iteration: 16720 loss: 0.0104 lr: 0.02\n",
      "iteration: 16730 loss: 0.0071 lr: 0.02\n",
      "iteration: 16740 loss: 0.0069 lr: 0.02\n",
      "iteration: 16750 loss: 0.0061 lr: 0.02\n",
      "iteration: 16760 loss: 0.0054 lr: 0.02\n",
      "iteration: 16770 loss: 0.0068 lr: 0.02\n",
      "iteration: 16780 loss: 0.0092 lr: 0.02\n",
      "iteration: 16790 loss: 0.0075 lr: 0.02\n",
      "iteration: 16800 loss: 0.0068 lr: 0.02\n",
      "iteration: 16810 loss: 0.0068 lr: 0.02\n",
      "iteration: 16820 loss: 0.0088 lr: 0.02\n",
      "iteration: 16830 loss: 0.0079 lr: 0.02\n",
      "iteration: 16840 loss: 0.0078 lr: 0.02\n",
      "iteration: 16850 loss: 0.0113 lr: 0.02\n",
      "iteration: 16860 loss: 0.0108 lr: 0.02\n",
      "iteration: 16870 loss: 0.0076 lr: 0.02\n",
      "iteration: 16880 loss: 0.0082 lr: 0.02\n",
      "iteration: 16890 loss: 0.0098 lr: 0.02\n",
      "iteration: 16900 loss: 0.0067 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 16910 loss: 0.0072 lr: 0.02\n",
      "iteration: 16920 loss: 0.0084 lr: 0.02\n",
      "iteration: 16930 loss: 0.0074 lr: 0.02\n",
      "iteration: 16940 loss: 0.0082 lr: 0.02\n",
      "iteration: 16950 loss: 0.0096 lr: 0.02\n",
      "iteration: 16960 loss: 0.0085 lr: 0.02\n",
      "iteration: 16970 loss: 0.0071 lr: 0.02\n",
      "iteration: 16980 loss: 0.0070 lr: 0.02\n",
      "iteration: 16990 loss: 0.0079 lr: 0.02\n",
      "iteration: 17000 loss: 0.0084 lr: 0.02\n",
      "iteration: 17010 loss: 0.0082 lr: 0.02\n",
      "iteration: 17020 loss: 0.0082 lr: 0.02\n",
      "iteration: 17030 loss: 0.0095 lr: 0.02\n",
      "iteration: 17040 loss: 0.0075 lr: 0.02\n",
      "iteration: 17050 loss: 0.0073 lr: 0.02\n",
      "iteration: 17060 loss: 0.0079 lr: 0.02\n",
      "iteration: 17070 loss: 0.0085 lr: 0.02\n",
      "iteration: 17080 loss: 0.0085 lr: 0.02\n",
      "iteration: 17090 loss: 0.0086 lr: 0.02\n",
      "iteration: 17100 loss: 0.0083 lr: 0.02\n",
      "iteration: 17110 loss: 0.0087 lr: 0.02\n",
      "iteration: 17120 loss: 0.0081 lr: 0.02\n",
      "iteration: 17130 loss: 0.0106 lr: 0.02\n",
      "iteration: 17140 loss: 0.0077 lr: 0.02\n",
      "iteration: 17150 loss: 0.0091 lr: 0.02\n",
      "iteration: 17160 loss: 0.0065 lr: 0.02\n",
      "iteration: 17170 loss: 0.0065 lr: 0.02\n",
      "iteration: 17180 loss: 0.0089 lr: 0.02\n",
      "iteration: 17190 loss: 0.0116 lr: 0.02\n",
      "iteration: 17200 loss: 0.0090 lr: 0.02\n",
      "iteration: 17210 loss: 0.0082 lr: 0.02\n",
      "iteration: 17220 loss: 0.0083 lr: 0.02\n",
      "iteration: 17230 loss: 0.0094 lr: 0.02\n",
      "iteration: 17240 loss: 0.0089 lr: 0.02\n",
      "iteration: 17250 loss: 0.0075 lr: 0.02\n",
      "iteration: 17260 loss: 0.0096 lr: 0.02\n",
      "iteration: 17270 loss: 0.0096 lr: 0.02\n",
      "iteration: 17280 loss: 0.0097 lr: 0.02\n",
      "iteration: 17290 loss: 0.0095 lr: 0.02\n",
      "iteration: 17300 loss: 0.0101 lr: 0.02\n",
      "iteration: 17310 loss: 0.0097 lr: 0.02\n",
      "iteration: 17320 loss: 0.0066 lr: 0.02\n",
      "iteration: 17330 loss: 0.0081 lr: 0.02\n",
      "iteration: 17340 loss: 0.0092 lr: 0.02\n",
      "iteration: 17350 loss: 0.0075 lr: 0.02\n",
      "iteration: 17360 loss: 0.0095 lr: 0.02\n",
      "iteration: 17370 loss: 0.0084 lr: 0.02\n",
      "iteration: 17380 loss: 0.0108 lr: 0.02\n",
      "iteration: 17390 loss: 0.0099 lr: 0.02\n",
      "iteration: 17400 loss: 0.0091 lr: 0.02\n",
      "iteration: 17410 loss: 0.0081 lr: 0.02\n",
      "iteration: 17420 loss: 0.0076 lr: 0.02\n",
      "iteration: 17430 loss: 0.0087 lr: 0.02\n",
      "iteration: 17440 loss: 0.0081 lr: 0.02\n",
      "iteration: 17450 loss: 0.0076 lr: 0.02\n",
      "iteration: 17460 loss: 0.0085 lr: 0.02\n",
      "iteration: 17470 loss: 0.0079 lr: 0.02\n",
      "iteration: 17480 loss: 0.0110 lr: 0.02\n",
      "iteration: 17490 loss: 0.0071 lr: 0.02\n",
      "iteration: 17500 loss: 0.0065 lr: 0.02\n",
      "iteration: 17510 loss: 0.0078 lr: 0.02\n",
      "iteration: 17520 loss: 0.0068 lr: 0.02\n",
      "iteration: 17530 loss: 0.0077 lr: 0.02\n",
      "iteration: 17540 loss: 0.0083 lr: 0.02\n",
      "iteration: 17550 loss: 0.0068 lr: 0.02\n",
      "iteration: 17560 loss: 0.0082 lr: 0.02\n",
      "iteration: 17570 loss: 0.0070 lr: 0.02\n",
      "iteration: 17580 loss: 0.0084 lr: 0.02\n",
      "iteration: 17590 loss: 0.0071 lr: 0.02\n",
      "iteration: 17600 loss: 0.0095 lr: 0.02\n",
      "iteration: 17610 loss: 0.0077 lr: 0.02\n",
      "iteration: 17620 loss: 0.0077 lr: 0.02\n",
      "iteration: 17630 loss: 0.0083 lr: 0.02\n",
      "iteration: 17640 loss: 0.0093 lr: 0.02\n",
      "iteration: 17650 loss: 0.0076 lr: 0.02\n",
      "iteration: 17660 loss: 0.0075 lr: 0.02\n",
      "iteration: 17670 loss: 0.0073 lr: 0.02\n",
      "iteration: 17680 loss: 0.0074 lr: 0.02\n",
      "iteration: 17690 loss: 0.0082 lr: 0.02\n",
      "iteration: 17700 loss: 0.0083 lr: 0.02\n",
      "iteration: 17710 loss: 0.0074 lr: 0.02\n",
      "iteration: 17720 loss: 0.0095 lr: 0.02\n",
      "iteration: 17730 loss: 0.0073 lr: 0.02\n",
      "iteration: 17740 loss: 0.0061 lr: 0.02\n",
      "iteration: 17750 loss: 0.0084 lr: 0.02\n",
      "iteration: 17760 loss: 0.0110 lr: 0.02\n",
      "iteration: 17770 loss: 0.0091 lr: 0.02\n",
      "iteration: 17780 loss: 0.0091 lr: 0.02\n",
      "iteration: 17790 loss: 0.0090 lr: 0.02\n",
      "iteration: 17800 loss: 0.0073 lr: 0.02\n",
      "iteration: 17810 loss: 0.0102 lr: 0.02\n",
      "iteration: 17820 loss: 0.0080 lr: 0.02\n",
      "iteration: 17830 loss: 0.0084 lr: 0.02\n",
      "iteration: 17840 loss: 0.0064 lr: 0.02\n",
      "iteration: 17850 loss: 0.0072 lr: 0.02\n",
      "iteration: 17860 loss: 0.0063 lr: 0.02\n",
      "iteration: 17870 loss: 0.0073 lr: 0.02\n",
      "iteration: 17880 loss: 0.0085 lr: 0.02\n",
      "iteration: 17890 loss: 0.0075 lr: 0.02\n",
      "iteration: 17900 loss: 0.0093 lr: 0.02\n",
      "iteration: 17910 loss: 0.0091 lr: 0.02\n",
      "iteration: 17920 loss: 0.0076 lr: 0.02\n",
      "iteration: 17930 loss: 0.0089 lr: 0.02\n",
      "iteration: 17940 loss: 0.0091 lr: 0.02\n",
      "iteration: 17950 loss: 0.0079 lr: 0.02\n",
      "iteration: 17960 loss: 0.0081 lr: 0.02\n",
      "iteration: 17970 loss: 0.0069 lr: 0.02\n",
      "iteration: 17980 loss: 0.0092 lr: 0.02\n",
      "iteration: 17990 loss: 0.0070 lr: 0.02\n",
      "iteration: 18000 loss: 0.0064 lr: 0.02\n",
      "iteration: 18010 loss: 0.0082 lr: 0.02\n",
      "iteration: 18020 loss: 0.0095 lr: 0.02\n",
      "iteration: 18030 loss: 0.0057 lr: 0.02\n",
      "iteration: 18040 loss: 0.0065 lr: 0.02\n",
      "iteration: 18050 loss: 0.0097 lr: 0.02\n",
      "iteration: 18060 loss: 0.0110 lr: 0.02\n",
      "iteration: 18070 loss: 0.0083 lr: 0.02\n",
      "iteration: 18080 loss: 0.0077 lr: 0.02\n",
      "iteration: 18090 loss: 0.0091 lr: 0.02\n",
      "iteration: 18100 loss: 0.0073 lr: 0.02\n",
      "iteration: 18110 loss: 0.0066 lr: 0.02\n",
      "iteration: 18120 loss: 0.0071 lr: 0.02\n",
      "iteration: 18130 loss: 0.0091 lr: 0.02\n",
      "iteration: 18140 loss: 0.0080 lr: 0.02\n",
      "iteration: 18150 loss: 0.0093 lr: 0.02\n",
      "iteration: 18160 loss: 0.0086 lr: 0.02\n",
      "iteration: 18170 loss: 0.0073 lr: 0.02\n",
      "iteration: 18180 loss: 0.0091 lr: 0.02\n",
      "iteration: 18190 loss: 0.0084 lr: 0.02\n",
      "iteration: 18200 loss: 0.0097 lr: 0.02\n",
      "iteration: 18210 loss: 0.0083 lr: 0.02\n",
      "iteration: 18220 loss: 0.0095 lr: 0.02\n",
      "iteration: 18230 loss: 0.0086 lr: 0.02\n",
      "iteration: 18240 loss: 0.0079 lr: 0.02\n",
      "iteration: 18250 loss: 0.0073 lr: 0.02\n",
      "iteration: 18260 loss: 0.0071 lr: 0.02\n",
      "iteration: 18270 loss: 0.0113 lr: 0.02\n",
      "iteration: 18280 loss: 0.0100 lr: 0.02\n",
      "iteration: 18290 loss: 0.0065 lr: 0.02\n",
      "iteration: 18300 loss: 0.0078 lr: 0.02\n",
      "iteration: 18310 loss: 0.0084 lr: 0.02\n",
      "iteration: 18320 loss: 0.0064 lr: 0.02\n",
      "iteration: 18330 loss: 0.0086 lr: 0.02\n",
      "iteration: 18340 loss: 0.0077 lr: 0.02\n",
      "iteration: 18350 loss: 0.0080 lr: 0.02\n",
      "iteration: 18360 loss: 0.0083 lr: 0.02\n",
      "iteration: 18370 loss: 0.0077 lr: 0.02\n",
      "iteration: 18380 loss: 0.0080 lr: 0.02\n",
      "iteration: 18390 loss: 0.0078 lr: 0.02\n",
      "iteration: 18400 loss: 0.0082 lr: 0.02\n",
      "iteration: 18410 loss: 0.0086 lr: 0.02\n",
      "iteration: 18420 loss: 0.0077 lr: 0.02\n",
      "iteration: 18430 loss: 0.0075 lr: 0.02\n",
      "iteration: 18440 loss: 0.0091 lr: 0.02\n",
      "iteration: 18450 loss: 0.0072 lr: 0.02\n",
      "iteration: 18460 loss: 0.0076 lr: 0.02\n",
      "iteration: 18470 loss: 0.0084 lr: 0.02\n",
      "iteration: 18480 loss: 0.0058 lr: 0.02\n",
      "iteration: 18490 loss: 0.0068 lr: 0.02\n",
      "iteration: 18500 loss: 0.0051 lr: 0.02\n",
      "iteration: 18510 loss: 0.0088 lr: 0.02\n",
      "iteration: 18520 loss: 0.0063 lr: 0.02\n",
      "iteration: 18530 loss: 0.0070 lr: 0.02\n",
      "iteration: 18540 loss: 0.0064 lr: 0.02\n",
      "iteration: 18550 loss: 0.0054 lr: 0.02\n",
      "iteration: 18560 loss: 0.0082 lr: 0.02\n",
      "iteration: 18570 loss: 0.0084 lr: 0.02\n",
      "iteration: 18580 loss: 0.0078 lr: 0.02\n",
      "iteration: 18590 loss: 0.0086 lr: 0.02\n",
      "iteration: 18600 loss: 0.0071 lr: 0.02\n",
      "iteration: 18610 loss: 0.0079 lr: 0.02\n",
      "iteration: 18620 loss: 0.0078 lr: 0.02\n",
      "iteration: 18630 loss: 0.0093 lr: 0.02\n",
      "iteration: 18640 loss: 0.0077 lr: 0.02\n",
      "iteration: 18650 loss: 0.0103 lr: 0.02\n",
      "iteration: 18660 loss: 0.0095 lr: 0.02\n",
      "iteration: 18670 loss: 0.0078 lr: 0.02\n",
      "iteration: 18680 loss: 0.0072 lr: 0.02\n",
      "iteration: 18690 loss: 0.0078 lr: 0.02\n",
      "iteration: 18700 loss: 0.0067 lr: 0.02\n",
      "iteration: 18710 loss: 0.0070 lr: 0.02\n",
      "iteration: 18720 loss: 0.0058 lr: 0.02\n",
      "iteration: 18730 loss: 0.0082 lr: 0.02\n",
      "iteration: 18740 loss: 0.0063 lr: 0.02\n",
      "iteration: 18750 loss: 0.0057 lr: 0.02\n",
      "iteration: 18760 loss: 0.0057 lr: 0.02\n",
      "iteration: 18770 loss: 0.0074 lr: 0.02\n",
      "iteration: 18780 loss: 0.0062 lr: 0.02\n",
      "iteration: 18790 loss: 0.0065 lr: 0.02\n",
      "iteration: 18800 loss: 0.0083 lr: 0.02\n",
      "iteration: 18810 loss: 0.0078 lr: 0.02\n",
      "iteration: 18820 loss: 0.0070 lr: 0.02\n",
      "iteration: 18830 loss: 0.0070 lr: 0.02\n",
      "iteration: 18840 loss: 0.0082 lr: 0.02\n",
      "iteration: 18850 loss: 0.0062 lr: 0.02\n",
      "iteration: 18860 loss: 0.0093 lr: 0.02\n",
      "iteration: 18870 loss: 0.0093 lr: 0.02\n",
      "iteration: 18880 loss: 0.0087 lr: 0.02\n",
      "iteration: 18890 loss: 0.0099 lr: 0.02\n",
      "iteration: 18900 loss: 0.0106 lr: 0.02\n",
      "iteration: 18910 loss: 0.0108 lr: 0.02\n",
      "iteration: 18920 loss: 0.0084 lr: 0.02\n",
      "iteration: 18930 loss: 0.0068 lr: 0.02\n",
      "iteration: 18940 loss: 0.0077 lr: 0.02\n",
      "iteration: 18950 loss: 0.0082 lr: 0.02\n",
      "iteration: 18960 loss: 0.0079 lr: 0.02\n",
      "iteration: 18970 loss: 0.0065 lr: 0.02\n",
      "iteration: 18980 loss: 0.0074 lr: 0.02\n",
      "iteration: 18990 loss: 0.0055 lr: 0.02\n",
      "iteration: 19000 loss: 0.0060 lr: 0.02\n",
      "iteration: 19010 loss: 0.0056 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 19020 loss: 0.0059 lr: 0.02\n",
      "iteration: 19030 loss: 0.0093 lr: 0.02\n",
      "iteration: 19040 loss: 0.0078 lr: 0.02\n",
      "iteration: 19050 loss: 0.0093 lr: 0.02\n",
      "iteration: 19060 loss: 0.0067 lr: 0.02\n",
      "iteration: 19070 loss: 0.0083 lr: 0.02\n",
      "iteration: 19080 loss: 0.0072 lr: 0.02\n",
      "iteration: 19090 loss: 0.0077 lr: 0.02\n",
      "iteration: 19100 loss: 0.0082 lr: 0.02\n",
      "iteration: 19110 loss: 0.0068 lr: 0.02\n",
      "iteration: 19120 loss: 0.0071 lr: 0.02\n",
      "iteration: 19130 loss: 0.0076 lr: 0.02\n",
      "iteration: 19140 loss: 0.0076 lr: 0.02\n",
      "iteration: 19150 loss: 0.0073 lr: 0.02\n",
      "iteration: 19160 loss: 0.0083 lr: 0.02\n",
      "iteration: 19170 loss: 0.0061 lr: 0.02\n",
      "iteration: 19180 loss: 0.0072 lr: 0.02\n",
      "iteration: 19190 loss: 0.0073 lr: 0.02\n",
      "iteration: 19200 loss: 0.0071 lr: 0.02\n",
      "iteration: 19210 loss: 0.0069 lr: 0.02\n",
      "iteration: 19220 loss: 0.0092 lr: 0.02\n",
      "iteration: 19230 loss: 0.0098 lr: 0.02\n",
      "iteration: 19240 loss: 0.0074 lr: 0.02\n",
      "iteration: 19250 loss: 0.0081 lr: 0.02\n",
      "iteration: 19260 loss: 0.0069 lr: 0.02\n",
      "iteration: 19270 loss: 0.0061 lr: 0.02\n",
      "iteration: 19280 loss: 0.0069 lr: 0.02\n",
      "iteration: 19290 loss: 0.0074 lr: 0.02\n",
      "iteration: 19300 loss: 0.0059 lr: 0.02\n",
      "iteration: 19310 loss: 0.0072 lr: 0.02\n",
      "iteration: 19320 loss: 0.0075 lr: 0.02\n",
      "iteration: 19330 loss: 0.0080 lr: 0.02\n",
      "iteration: 19340 loss: 0.0091 lr: 0.02\n",
      "iteration: 19350 loss: 0.0084 lr: 0.02\n",
      "iteration: 19360 loss: 0.0087 lr: 0.02\n",
      "iteration: 19370 loss: 0.0081 lr: 0.02\n",
      "iteration: 19380 loss: 0.0098 lr: 0.02\n",
      "iteration: 19390 loss: 0.0074 lr: 0.02\n",
      "iteration: 19400 loss: 0.0071 lr: 0.02\n",
      "iteration: 19410 loss: 0.0079 lr: 0.02\n",
      "iteration: 19420 loss: 0.0080 lr: 0.02\n",
      "iteration: 19430 loss: 0.0066 lr: 0.02\n",
      "iteration: 19440 loss: 0.0078 lr: 0.02\n",
      "iteration: 19450 loss: 0.0106 lr: 0.02\n",
      "iteration: 19460 loss: 0.0080 lr: 0.02\n",
      "iteration: 19470 loss: 0.0080 lr: 0.02\n",
      "iteration: 19480 loss: 0.0109 lr: 0.02\n",
      "iteration: 19490 loss: 0.0066 lr: 0.02\n",
      "iteration: 19500 loss: 0.0091 lr: 0.02\n",
      "iteration: 19510 loss: 0.0076 lr: 0.02\n",
      "iteration: 19520 loss: 0.0089 lr: 0.02\n",
      "iteration: 19530 loss: 0.0073 lr: 0.02\n",
      "iteration: 19540 loss: 0.0074 lr: 0.02\n",
      "iteration: 19550 loss: 0.0069 lr: 0.02\n",
      "iteration: 19560 loss: 0.0064 lr: 0.02\n",
      "iteration: 19570 loss: 0.0075 lr: 0.02\n",
      "iteration: 19580 loss: 0.0070 lr: 0.02\n",
      "iteration: 19590 loss: 0.0067 lr: 0.02\n",
      "iteration: 19600 loss: 0.0078 lr: 0.02\n",
      "iteration: 19610 loss: 0.0061 lr: 0.02\n",
      "iteration: 19620 loss: 0.0081 lr: 0.02\n",
      "iteration: 19630 loss: 0.0078 lr: 0.02\n",
      "iteration: 19640 loss: 0.0076 lr: 0.02\n",
      "iteration: 19650 loss: 0.0066 lr: 0.02\n",
      "iteration: 19660 loss: 0.0081 lr: 0.02\n",
      "iteration: 19670 loss: 0.0074 lr: 0.02\n",
      "iteration: 19680 loss: 0.0085 lr: 0.02\n",
      "iteration: 19690 loss: 0.0091 lr: 0.02\n",
      "iteration: 19700 loss: 0.0091 lr: 0.02\n",
      "iteration: 19710 loss: 0.0063 lr: 0.02\n",
      "iteration: 19720 loss: 0.0050 lr: 0.02\n",
      "iteration: 19730 loss: 0.0078 lr: 0.02\n",
      "iteration: 19740 loss: 0.0057 lr: 0.02\n",
      "iteration: 19750 loss: 0.0068 lr: 0.02\n",
      "iteration: 19760 loss: 0.0074 lr: 0.02\n",
      "iteration: 19770 loss: 0.0057 lr: 0.02\n",
      "iteration: 19780 loss: 0.0072 lr: 0.02\n",
      "iteration: 19790 loss: 0.0057 lr: 0.02\n",
      "iteration: 19800 loss: 0.0071 lr: 0.02\n",
      "iteration: 19810 loss: 0.0063 lr: 0.02\n",
      "iteration: 19820 loss: 0.0078 lr: 0.02\n",
      "iteration: 19830 loss: 0.0080 lr: 0.02\n",
      "iteration: 19840 loss: 0.0069 lr: 0.02\n",
      "iteration: 19850 loss: 0.0064 lr: 0.02\n",
      "iteration: 19860 loss: 0.0099 lr: 0.02\n",
      "iteration: 19870 loss: 0.0073 lr: 0.02\n",
      "iteration: 19880 loss: 0.0073 lr: 0.02\n",
      "iteration: 19890 loss: 0.0074 lr: 0.02\n",
      "iteration: 19900 loss: 0.0073 lr: 0.02\n",
      "iteration: 19910 loss: 0.0065 lr: 0.02\n",
      "iteration: 19920 loss: 0.0066 lr: 0.02\n",
      "iteration: 19930 loss: 0.0072 lr: 0.02\n",
      "iteration: 19940 loss: 0.0080 lr: 0.02\n",
      "iteration: 19950 loss: 0.0086 lr: 0.02\n",
      "iteration: 19960 loss: 0.0084 lr: 0.02\n",
      "iteration: 19970 loss: 0.0069 lr: 0.02\n",
      "iteration: 19980 loss: 0.0073 lr: 0.02\n",
      "iteration: 19990 loss: 0.0071 lr: 0.02\n",
      "iteration: 20000 loss: 0.0077 lr: 0.02\n",
      "iteration: 20010 loss: 0.0080 lr: 0.02\n",
      "iteration: 20020 loss: 0.0066 lr: 0.02\n",
      "iteration: 20030 loss: 0.0075 lr: 0.02\n",
      "iteration: 20040 loss: 0.0048 lr: 0.02\n",
      "iteration: 20050 loss: 0.0078 lr: 0.02\n",
      "iteration: 20060 loss: 0.0050 lr: 0.02\n",
      "iteration: 20070 loss: 0.0045 lr: 0.02\n",
      "iteration: 20080 loss: 0.0066 lr: 0.02\n",
      "iteration: 20090 loss: 0.0068 lr: 0.02\n",
      "iteration: 20100 loss: 0.0050 lr: 0.02\n",
      "iteration: 20110 loss: 0.0072 lr: 0.02\n",
      "iteration: 20120 loss: 0.0061 lr: 0.02\n",
      "iteration: 20130 loss: 0.0075 lr: 0.02\n",
      "iteration: 20140 loss: 0.0062 lr: 0.02\n",
      "iteration: 20150 loss: 0.0053 lr: 0.02\n",
      "iteration: 20160 loss: 0.0068 lr: 0.02\n",
      "iteration: 20170 loss: 0.0068 lr: 0.02\n",
      "iteration: 20180 loss: 0.0071 lr: 0.02\n",
      "iteration: 20190 loss: 0.0071 lr: 0.02\n",
      "iteration: 20200 loss: 0.0056 lr: 0.02\n",
      "iteration: 20210 loss: 0.0084 lr: 0.02\n",
      "iteration: 20220 loss: 0.0056 lr: 0.02\n",
      "iteration: 20230 loss: 0.0069 lr: 0.02\n",
      "iteration: 20240 loss: 0.0089 lr: 0.02\n",
      "iteration: 20250 loss: 0.0073 lr: 0.02\n",
      "iteration: 20260 loss: 0.0062 lr: 0.02\n",
      "iteration: 20270 loss: 0.0061 lr: 0.02\n",
      "iteration: 20280 loss: 0.0076 lr: 0.02\n",
      "iteration: 20290 loss: 0.0052 lr: 0.02\n",
      "iteration: 20300 loss: 0.0070 lr: 0.02\n",
      "iteration: 20310 loss: 0.0076 lr: 0.02\n",
      "iteration: 20320 loss: 0.0059 lr: 0.02\n",
      "iteration: 20330 loss: 0.0074 lr: 0.02\n",
      "iteration: 20340 loss: 0.0074 lr: 0.02\n",
      "iteration: 20350 loss: 0.0067 lr: 0.02\n",
      "iteration: 20360 loss: 0.0057 lr: 0.02\n",
      "iteration: 20370 loss: 0.0077 lr: 0.02\n",
      "iteration: 20380 loss: 0.0061 lr: 0.02\n",
      "iteration: 20390 loss: 0.0067 lr: 0.02\n",
      "iteration: 20400 loss: 0.0083 lr: 0.02\n",
      "iteration: 20410 loss: 0.0090 lr: 0.02\n",
      "iteration: 20420 loss: 0.0071 lr: 0.02\n",
      "iteration: 20430 loss: 0.0068 lr: 0.02\n",
      "iteration: 20440 loss: 0.0051 lr: 0.02\n",
      "iteration: 20450 loss: 0.0076 lr: 0.02\n",
      "iteration: 20460 loss: 0.0082 lr: 0.02\n",
      "iteration: 20470 loss: 0.0055 lr: 0.02\n",
      "iteration: 20480 loss: 0.0081 lr: 0.02\n",
      "iteration: 20490 loss: 0.0074 lr: 0.02\n",
      "iteration: 20500 loss: 0.0065 lr: 0.02\n",
      "iteration: 20510 loss: 0.0043 lr: 0.02\n",
      "iteration: 20520 loss: 0.0068 lr: 0.02\n",
      "iteration: 20530 loss: 0.0064 lr: 0.02\n",
      "iteration: 20540 loss: 0.0063 lr: 0.02\n",
      "iteration: 20550 loss: 0.0074 lr: 0.02\n",
      "iteration: 20560 loss: 0.0057 lr: 0.02\n",
      "iteration: 20570 loss: 0.0089 lr: 0.02\n",
      "iteration: 20580 loss: 0.0077 lr: 0.02\n",
      "iteration: 20590 loss: 0.0070 lr: 0.02\n",
      "iteration: 20600 loss: 0.0090 lr: 0.02\n",
      "iteration: 20610 loss: 0.0064 lr: 0.02\n",
      "iteration: 20620 loss: 0.0078 lr: 0.02\n",
      "iteration: 20630 loss: 0.0064 lr: 0.02\n",
      "iteration: 20640 loss: 0.0069 lr: 0.02\n",
      "iteration: 20650 loss: 0.0053 lr: 0.02\n",
      "iteration: 20660 loss: 0.0058 lr: 0.02\n",
      "iteration: 20670 loss: 0.0064 lr: 0.02\n",
      "iteration: 20680 loss: 0.0076 lr: 0.02\n",
      "iteration: 20690 loss: 0.0083 lr: 0.02\n",
      "iteration: 20700 loss: 0.0081 lr: 0.02\n",
      "iteration: 20710 loss: 0.0063 lr: 0.02\n",
      "iteration: 20720 loss: 0.0087 lr: 0.02\n",
      "iteration: 20730 loss: 0.0066 lr: 0.02\n",
      "iteration: 20740 loss: 0.0076 lr: 0.02\n",
      "iteration: 20750 loss: 0.0085 lr: 0.02\n",
      "iteration: 20760 loss: 0.0094 lr: 0.02\n",
      "iteration: 20770 loss: 0.0079 lr: 0.02\n",
      "iteration: 20780 loss: 0.0088 lr: 0.02\n",
      "iteration: 20790 loss: 0.0076 lr: 0.02\n",
      "iteration: 20800 loss: 0.0081 lr: 0.02\n",
      "iteration: 20810 loss: 0.0077 lr: 0.02\n",
      "iteration: 20820 loss: 0.0052 lr: 0.02\n",
      "iteration: 20830 loss: 0.0090 lr: 0.02\n",
      "iteration: 20840 loss: 0.0074 lr: 0.02\n",
      "iteration: 20850 loss: 0.0062 lr: 0.02\n",
      "iteration: 20860 loss: 0.0083 lr: 0.02\n",
      "iteration: 20870 loss: 0.0062 lr: 0.02\n",
      "iteration: 20880 loss: 0.0074 lr: 0.02\n",
      "iteration: 20890 loss: 0.0066 lr: 0.02\n",
      "iteration: 20900 loss: 0.0071 lr: 0.02\n",
      "iteration: 20910 loss: 0.0066 lr: 0.02\n",
      "iteration: 20920 loss: 0.0063 lr: 0.02\n",
      "iteration: 20930 loss: 0.0068 lr: 0.02\n",
      "iteration: 20940 loss: 0.0074 lr: 0.02\n",
      "iteration: 20950 loss: 0.0086 lr: 0.02\n",
      "iteration: 20960 loss: 0.0072 lr: 0.02\n",
      "iteration: 20970 loss: 0.0067 lr: 0.02\n",
      "iteration: 20980 loss: 0.0066 lr: 0.02\n",
      "iteration: 20990 loss: 0.0054 lr: 0.02\n",
      "iteration: 21000 loss: 0.0060 lr: 0.02\n",
      "iteration: 21010 loss: 0.0053 lr: 0.02\n",
      "iteration: 21020 loss: 0.0059 lr: 0.02\n",
      "iteration: 21030 loss: 0.0050 lr: 0.02\n",
      "iteration: 21040 loss: 0.0056 lr: 0.02\n",
      "iteration: 21050 loss: 0.0088 lr: 0.02\n",
      "iteration: 21060 loss: 0.0075 lr: 0.02\n",
      "iteration: 21070 loss: 0.0073 lr: 0.02\n",
      "iteration: 21080 loss: 0.0060 lr: 0.02\n",
      "iteration: 21090 loss: 0.0062 lr: 0.02\n",
      "iteration: 21100 loss: 0.0062 lr: 0.02\n",
      "iteration: 21110 loss: 0.0055 lr: 0.02\n",
      "iteration: 21120 loss: 0.0055 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 21130 loss: 0.0063 lr: 0.02\n",
      "iteration: 21140 loss: 0.0061 lr: 0.02\n",
      "iteration: 21150 loss: 0.0064 lr: 0.02\n",
      "iteration: 21160 loss: 0.0077 lr: 0.02\n",
      "iteration: 21170 loss: 0.0073 lr: 0.02\n",
      "iteration: 21180 loss: 0.0060 lr: 0.02\n",
      "iteration: 21190 loss: 0.0060 lr: 0.02\n",
      "iteration: 21200 loss: 0.0078 lr: 0.02\n",
      "iteration: 21210 loss: 0.0079 lr: 0.02\n",
      "iteration: 21220 loss: 0.0066 lr: 0.02\n",
      "iteration: 21230 loss: 0.0061 lr: 0.02\n",
      "iteration: 21240 loss: 0.0077 lr: 0.02\n",
      "iteration: 21250 loss: 0.0079 lr: 0.02\n",
      "iteration: 21260 loss: 0.0063 lr: 0.02\n",
      "iteration: 21270 loss: 0.0059 lr: 0.02\n",
      "iteration: 21280 loss: 0.0067 lr: 0.02\n",
      "iteration: 21290 loss: 0.0073 lr: 0.02\n",
      "iteration: 21300 loss: 0.0053 lr: 0.02\n",
      "iteration: 21310 loss: 0.0064 lr: 0.02\n",
      "iteration: 21320 loss: 0.0055 lr: 0.02\n",
      "iteration: 21330 loss: 0.0060 lr: 0.02\n",
      "iteration: 21340 loss: 0.0052 lr: 0.02\n",
      "iteration: 21350 loss: 0.0082 lr: 0.02\n",
      "iteration: 21360 loss: 0.0069 lr: 0.02\n",
      "iteration: 21370 loss: 0.0065 lr: 0.02\n",
      "iteration: 21380 loss: 0.0055 lr: 0.02\n",
      "iteration: 21390 loss: 0.0074 lr: 0.02\n",
      "iteration: 21400 loss: 0.0050 lr: 0.02\n",
      "iteration: 21410 loss: 0.0071 lr: 0.02\n",
      "iteration: 21420 loss: 0.0075 lr: 0.02\n",
      "iteration: 21430 loss: 0.0086 lr: 0.02\n",
      "iteration: 21440 loss: 0.0084 lr: 0.02\n",
      "iteration: 21450 loss: 0.0084 lr: 0.02\n",
      "iteration: 21460 loss: 0.0102 lr: 0.02\n",
      "iteration: 21470 loss: 0.0079 lr: 0.02\n",
      "iteration: 21480 loss: 0.0071 lr: 0.02\n",
      "iteration: 21490 loss: 0.0072 lr: 0.02\n",
      "iteration: 21500 loss: 0.0081 lr: 0.02\n",
      "iteration: 21510 loss: 0.0081 lr: 0.02\n",
      "iteration: 21520 loss: 0.0065 lr: 0.02\n",
      "iteration: 21530 loss: 0.0062 lr: 0.02\n",
      "iteration: 21540 loss: 0.0056 lr: 0.02\n",
      "iteration: 21550 loss: 0.0069 lr: 0.02\n",
      "iteration: 21560 loss: 0.0047 lr: 0.02\n",
      "iteration: 21570 loss: 0.0073 lr: 0.02\n",
      "iteration: 21580 loss: 0.0071 lr: 0.02\n",
      "iteration: 21590 loss: 0.0066 lr: 0.02\n",
      "iteration: 21600 loss: 0.0064 lr: 0.02\n",
      "iteration: 21610 loss: 0.0044 lr: 0.02\n",
      "iteration: 21620 loss: 0.0080 lr: 0.02\n",
      "iteration: 21630 loss: 0.0068 lr: 0.02\n",
      "iteration: 21640 loss: 0.0073 lr: 0.02\n",
      "iteration: 21650 loss: 0.0077 lr: 0.02\n",
      "iteration: 21660 loss: 0.0061 lr: 0.02\n",
      "iteration: 21670 loss: 0.0055 lr: 0.02\n",
      "iteration: 21680 loss: 0.0054 lr: 0.02\n",
      "iteration: 21690 loss: 0.0051 lr: 0.02\n",
      "iteration: 21700 loss: 0.0060 lr: 0.02\n",
      "iteration: 21710 loss: 0.0084 lr: 0.02\n",
      "iteration: 21720 loss: 0.0055 lr: 0.02\n",
      "iteration: 21730 loss: 0.0066 lr: 0.02\n",
      "iteration: 21740 loss: 0.0075 lr: 0.02\n",
      "iteration: 21750 loss: 0.0068 lr: 0.02\n",
      "iteration: 21760 loss: 0.0048 lr: 0.02\n",
      "iteration: 21770 loss: 0.0066 lr: 0.02\n",
      "iteration: 21780 loss: 0.0063 lr: 0.02\n",
      "iteration: 21790 loss: 0.0068 lr: 0.02\n",
      "iteration: 21800 loss: 0.0072 lr: 0.02\n",
      "iteration: 21810 loss: 0.0052 lr: 0.02\n",
      "iteration: 21820 loss: 0.0075 lr: 0.02\n",
      "iteration: 21830 loss: 0.0076 lr: 0.02\n",
      "iteration: 21840 loss: 0.0075 lr: 0.02\n",
      "iteration: 21850 loss: 0.0067 lr: 0.02\n",
      "iteration: 21860 loss: 0.0070 lr: 0.02\n",
      "iteration: 21870 loss: 0.0053 lr: 0.02\n",
      "iteration: 21880 loss: 0.0058 lr: 0.02\n",
      "iteration: 21890 loss: 0.0057 lr: 0.02\n",
      "iteration: 21900 loss: 0.0090 lr: 0.02\n",
      "iteration: 21910 loss: 0.0066 lr: 0.02\n",
      "iteration: 21920 loss: 0.0053 lr: 0.02\n",
      "iteration: 21930 loss: 0.0055 lr: 0.02\n",
      "iteration: 21940 loss: 0.0069 lr: 0.02\n",
      "iteration: 21950 loss: 0.0069 lr: 0.02\n",
      "iteration: 21960 loss: 0.0057 lr: 0.02\n",
      "iteration: 21970 loss: 0.0069 lr: 0.02\n",
      "iteration: 21980 loss: 0.0072 lr: 0.02\n",
      "iteration: 21990 loss: 0.0056 lr: 0.02\n",
      "iteration: 22000 loss: 0.0058 lr: 0.02\n",
      "iteration: 22010 loss: 0.0072 lr: 0.02\n",
      "iteration: 22020 loss: 0.0055 lr: 0.02\n",
      "iteration: 22030 loss: 0.0052 lr: 0.02\n",
      "iteration: 22040 loss: 0.0070 lr: 0.02\n",
      "iteration: 22050 loss: 0.0067 lr: 0.02\n",
      "iteration: 22060 loss: 0.0087 lr: 0.02\n",
      "iteration: 22070 loss: 0.0065 lr: 0.02\n",
      "iteration: 22080 loss: 0.0057 lr: 0.02\n",
      "iteration: 22090 loss: 0.0100 lr: 0.02\n",
      "iteration: 22100 loss: 0.0094 lr: 0.02\n",
      "iteration: 22110 loss: 0.0075 lr: 0.02\n",
      "iteration: 22120 loss: 0.0079 lr: 0.02\n",
      "iteration: 22130 loss: 0.0056 lr: 0.02\n",
      "iteration: 22140 loss: 0.0084 lr: 0.02\n",
      "iteration: 22150 loss: 0.0061 lr: 0.02\n",
      "iteration: 22160 loss: 0.0064 lr: 0.02\n",
      "iteration: 22170 loss: 0.0071 lr: 0.02\n",
      "iteration: 22180 loss: 0.0072 lr: 0.02\n",
      "iteration: 22190 loss: 0.0068 lr: 0.02\n",
      "iteration: 22200 loss: 0.0078 lr: 0.02\n",
      "iteration: 22210 loss: 0.0065 lr: 0.02\n",
      "iteration: 22220 loss: 0.0064 lr: 0.02\n",
      "iteration: 22230 loss: 0.0061 lr: 0.02\n",
      "iteration: 22240 loss: 0.0096 lr: 0.02\n",
      "iteration: 22250 loss: 0.0062 lr: 0.02\n",
      "iteration: 22260 loss: 0.0058 lr: 0.02\n",
      "iteration: 22270 loss: 0.0056 lr: 0.02\n",
      "iteration: 22280 loss: 0.0075 lr: 0.02\n",
      "iteration: 22290 loss: 0.0071 lr: 0.02\n",
      "iteration: 22300 loss: 0.0079 lr: 0.02\n",
      "iteration: 22310 loss: 0.0064 lr: 0.02\n",
      "iteration: 22320 loss: 0.0067 lr: 0.02\n",
      "iteration: 22330 loss: 0.0062 lr: 0.02\n",
      "iteration: 22340 loss: 0.0070 lr: 0.02\n",
      "iteration: 22350 loss: 0.0059 lr: 0.02\n",
      "iteration: 22360 loss: 0.0070 lr: 0.02\n",
      "iteration: 22370 loss: 0.0073 lr: 0.02\n",
      "iteration: 22380 loss: 0.0078 lr: 0.02\n",
      "iteration: 22390 loss: 0.0079 lr: 0.02\n",
      "iteration: 22400 loss: 0.0057 lr: 0.02\n",
      "iteration: 22410 loss: 0.0055 lr: 0.02\n",
      "iteration: 22420 loss: 0.0071 lr: 0.02\n",
      "iteration: 22430 loss: 0.0064 lr: 0.02\n",
      "iteration: 22440 loss: 0.0068 lr: 0.02\n",
      "iteration: 22450 loss: 0.0053 lr: 0.02\n",
      "iteration: 22460 loss: 0.0048 lr: 0.02\n",
      "iteration: 22470 loss: 0.0089 lr: 0.02\n",
      "iteration: 22480 loss: 0.0055 lr: 0.02\n",
      "iteration: 22490 loss: 0.0040 lr: 0.02\n",
      "iteration: 22500 loss: 0.0057 lr: 0.02\n",
      "iteration: 22510 loss: 0.0076 lr: 0.02\n",
      "iteration: 22520 loss: 0.0061 lr: 0.02\n",
      "iteration: 22530 loss: 0.0091 lr: 0.02\n",
      "iteration: 22540 loss: 0.0067 lr: 0.02\n",
      "iteration: 22550 loss: 0.0072 lr: 0.02\n",
      "iteration: 22560 loss: 0.0061 lr: 0.02\n",
      "iteration: 22570 loss: 0.0068 lr: 0.02\n",
      "iteration: 22580 loss: 0.0060 lr: 0.02\n",
      "iteration: 22590 loss: 0.0051 lr: 0.02\n",
      "iteration: 22600 loss: 0.0049 lr: 0.02\n",
      "iteration: 22610 loss: 0.0069 lr: 0.02\n",
      "iteration: 22620 loss: 0.0053 lr: 0.02\n",
      "iteration: 22630 loss: 0.0089 lr: 0.02\n",
      "iteration: 22640 loss: 0.0083 lr: 0.02\n",
      "iteration: 22650 loss: 0.0075 lr: 0.02\n",
      "iteration: 22660 loss: 0.0054 lr: 0.02\n",
      "iteration: 22670 loss: 0.0061 lr: 0.02\n",
      "iteration: 22680 loss: 0.0058 lr: 0.02\n",
      "iteration: 22690 loss: 0.0070 lr: 0.02\n",
      "iteration: 22700 loss: 0.0082 lr: 0.02\n",
      "iteration: 22710 loss: 0.0068 lr: 0.02\n",
      "iteration: 22720 loss: 0.0063 lr: 0.02\n",
      "iteration: 22730 loss: 0.0061 lr: 0.02\n",
      "iteration: 22740 loss: 0.0056 lr: 0.02\n",
      "iteration: 22750 loss: 0.0076 lr: 0.02\n",
      "iteration: 22760 loss: 0.0064 lr: 0.02\n",
      "iteration: 22770 loss: 0.0083 lr: 0.02\n",
      "iteration: 22780 loss: 0.0052 lr: 0.02\n",
      "iteration: 22790 loss: 0.0081 lr: 0.02\n",
      "iteration: 22800 loss: 0.0057 lr: 0.02\n",
      "iteration: 22810 loss: 0.0066 lr: 0.02\n",
      "iteration: 22820 loss: 0.0063 lr: 0.02\n",
      "iteration: 22830 loss: 0.0068 lr: 0.02\n",
      "iteration: 22840 loss: 0.0068 lr: 0.02\n",
      "iteration: 22850 loss: 0.0076 lr: 0.02\n",
      "iteration: 22860 loss: 0.0056 lr: 0.02\n",
      "iteration: 22870 loss: 0.0066 lr: 0.02\n",
      "iteration: 22880 loss: 0.0057 lr: 0.02\n",
      "iteration: 22890 loss: 0.0044 lr: 0.02\n",
      "iteration: 22900 loss: 0.0049 lr: 0.02\n",
      "iteration: 22910 loss: 0.0063 lr: 0.02\n",
      "iteration: 22920 loss: 0.0057 lr: 0.02\n",
      "iteration: 22930 loss: 0.0077 lr: 0.02\n",
      "iteration: 22940 loss: 0.0063 lr: 0.02\n",
      "iteration: 22950 loss: 0.0064 lr: 0.02\n",
      "iteration: 22960 loss: 0.0063 lr: 0.02\n",
      "iteration: 22970 loss: 0.0066 lr: 0.02\n",
      "iteration: 22980 loss: 0.0057 lr: 0.02\n",
      "iteration: 22990 loss: 0.0061 lr: 0.02\n",
      "iteration: 23000 loss: 0.0061 lr: 0.02\n",
      "iteration: 23010 loss: 0.0056 lr: 0.02\n",
      "iteration: 23020 loss: 0.0053 lr: 0.02\n",
      "iteration: 23030 loss: 0.0052 lr: 0.02\n",
      "iteration: 23040 loss: 0.0061 lr: 0.02\n",
      "iteration: 23050 loss: 0.0060 lr: 0.02\n",
      "iteration: 23060 loss: 0.0064 lr: 0.02\n",
      "iteration: 23070 loss: 0.0065 lr: 0.02\n",
      "iteration: 23080 loss: 0.0069 lr: 0.02\n",
      "iteration: 23090 loss: 0.0070 lr: 0.02\n",
      "iteration: 23100 loss: 0.0072 lr: 0.02\n",
      "iteration: 23110 loss: 0.0058 lr: 0.02\n",
      "iteration: 23120 loss: 0.0054 lr: 0.02\n",
      "iteration: 23130 loss: 0.0062 lr: 0.02\n",
      "iteration: 23140 loss: 0.0061 lr: 0.02\n",
      "iteration: 23150 loss: 0.0051 lr: 0.02\n",
      "iteration: 23160 loss: 0.0049 lr: 0.02\n",
      "iteration: 23170 loss: 0.0059 lr: 0.02\n",
      "iteration: 23180 loss: 0.0061 lr: 0.02\n",
      "iteration: 23190 loss: 0.0067 lr: 0.02\n",
      "iteration: 23200 loss: 0.0076 lr: 0.02\n",
      "iteration: 23210 loss: 0.0058 lr: 0.02\n",
      "iteration: 23220 loss: 0.0066 lr: 0.02\n",
      "iteration: 23230 loss: 0.0057 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 23240 loss: 0.0078 lr: 0.02\n",
      "iteration: 23250 loss: 0.0071 lr: 0.02\n",
      "iteration: 23260 loss: 0.0049 lr: 0.02\n",
      "iteration: 23270 loss: 0.0058 lr: 0.02\n",
      "iteration: 23280 loss: 0.0069 lr: 0.02\n",
      "iteration: 23290 loss: 0.0071 lr: 0.02\n",
      "iteration: 23300 loss: 0.0072 lr: 0.02\n",
      "iteration: 23310 loss: 0.0095 lr: 0.02\n",
      "iteration: 23320 loss: 0.0054 lr: 0.02\n",
      "iteration: 23330 loss: 0.0072 lr: 0.02\n",
      "iteration: 23340 loss: 0.0078 lr: 0.02\n",
      "iteration: 23350 loss: 0.0066 lr: 0.02\n",
      "iteration: 23360 loss: 0.0059 lr: 0.02\n",
      "iteration: 23370 loss: 0.0055 lr: 0.02\n",
      "iteration: 23380 loss: 0.0069 lr: 0.02\n",
      "iteration: 23390 loss: 0.0071 lr: 0.02\n",
      "iteration: 23400 loss: 0.0072 lr: 0.02\n",
      "iteration: 23410 loss: 0.0059 lr: 0.02\n",
      "iteration: 23420 loss: 0.0083 lr: 0.02\n",
      "iteration: 23430 loss: 0.0062 lr: 0.02\n",
      "iteration: 23440 loss: 0.0055 lr: 0.02\n",
      "iteration: 23450 loss: 0.0074 lr: 0.02\n",
      "iteration: 23460 loss: 0.0066 lr: 0.02\n",
      "iteration: 23470 loss: 0.0052 lr: 0.02\n",
      "iteration: 23480 loss: 0.0056 lr: 0.02\n",
      "iteration: 23490 loss: 0.0080 lr: 0.02\n",
      "iteration: 23500 loss: 0.0054 lr: 0.02\n",
      "iteration: 23510 loss: 0.0067 lr: 0.02\n",
      "iteration: 23520 loss: 0.0063 lr: 0.02\n",
      "iteration: 23530 loss: 0.0070 lr: 0.02\n",
      "iteration: 23540 loss: 0.0069 lr: 0.02\n",
      "iteration: 23550 loss: 0.0069 lr: 0.02\n",
      "iteration: 23560 loss: 0.0053 lr: 0.02\n",
      "iteration: 23570 loss: 0.0049 lr: 0.02\n",
      "iteration: 23580 loss: 0.0054 lr: 0.02\n",
      "iteration: 23590 loss: 0.0063 lr: 0.02\n",
      "iteration: 23600 loss: 0.0058 lr: 0.02\n",
      "iteration: 23610 loss: 0.0096 lr: 0.02\n",
      "iteration: 23620 loss: 0.0060 lr: 0.02\n",
      "iteration: 23630 loss: 0.0086 lr: 0.02\n",
      "iteration: 23640 loss: 0.0075 lr: 0.02\n",
      "iteration: 23650 loss: 0.0069 lr: 0.02\n",
      "iteration: 23660 loss: 0.0054 lr: 0.02\n",
      "iteration: 23670 loss: 0.0063 lr: 0.02\n",
      "iteration: 23680 loss: 0.0055 lr: 0.02\n",
      "iteration: 23690 loss: 0.0053 lr: 0.02\n",
      "iteration: 23700 loss: 0.0059 lr: 0.02\n",
      "iteration: 23710 loss: 0.0069 lr: 0.02\n",
      "iteration: 23720 loss: 0.0060 lr: 0.02\n",
      "iteration: 23730 loss: 0.0062 lr: 0.02\n",
      "iteration: 23740 loss: 0.0063 lr: 0.02\n",
      "iteration: 23750 loss: 0.0058 lr: 0.02\n",
      "iteration: 23760 loss: 0.0057 lr: 0.02\n",
      "iteration: 23770 loss: 0.0056 lr: 0.02\n",
      "iteration: 23780 loss: 0.0085 lr: 0.02\n",
      "iteration: 23790 loss: 0.0058 lr: 0.02\n",
      "iteration: 23800 loss: 0.0052 lr: 0.02\n",
      "iteration: 23810 loss: 0.0050 lr: 0.02\n",
      "iteration: 23820 loss: 0.0065 lr: 0.02\n",
      "iteration: 23830 loss: 0.0057 lr: 0.02\n",
      "iteration: 23840 loss: 0.0056 lr: 0.02\n",
      "iteration: 23850 loss: 0.0055 lr: 0.02\n",
      "iteration: 23860 loss: 0.0041 lr: 0.02\n",
      "iteration: 23870 loss: 0.0080 lr: 0.02\n",
      "iteration: 23880 loss: 0.0073 lr: 0.02\n",
      "iteration: 23890 loss: 0.0053 lr: 0.02\n",
      "iteration: 23900 loss: 0.0063 lr: 0.02\n",
      "iteration: 23910 loss: 0.0061 lr: 0.02\n",
      "iteration: 23920 loss: 0.0044 lr: 0.02\n",
      "iteration: 23930 loss: 0.0062 lr: 0.02\n",
      "iteration: 23940 loss: 0.0078 lr: 0.02\n",
      "iteration: 23950 loss: 0.0064 lr: 0.02\n",
      "iteration: 23960 loss: 0.0042 lr: 0.02\n",
      "iteration: 23970 loss: 0.0055 lr: 0.02\n",
      "iteration: 23980 loss: 0.0062 lr: 0.02\n",
      "iteration: 23990 loss: 0.0053 lr: 0.02\n",
      "iteration: 24000 loss: 0.0049 lr: 0.02\n",
      "iteration: 24010 loss: 0.0051 lr: 0.02\n",
      "iteration: 24020 loss: 0.0043 lr: 0.02\n",
      "iteration: 24030 loss: 0.0051 lr: 0.02\n",
      "iteration: 24040 loss: 0.0062 lr: 0.02\n",
      "iteration: 24050 loss: 0.0054 lr: 0.02\n",
      "iteration: 24060 loss: 0.0078 lr: 0.02\n",
      "iteration: 24070 loss: 0.0051 lr: 0.02\n",
      "iteration: 24080 loss: 0.0063 lr: 0.02\n",
      "iteration: 24090 loss: 0.0083 lr: 0.02\n",
      "iteration: 24100 loss: 0.0063 lr: 0.02\n",
      "iteration: 24110 loss: 0.0061 lr: 0.02\n",
      "iteration: 24120 loss: 0.0056 lr: 0.02\n",
      "iteration: 24130 loss: 0.0052 lr: 0.02\n",
      "iteration: 24140 loss: 0.0054 lr: 0.02\n",
      "iteration: 24150 loss: 0.0047 lr: 0.02\n",
      "iteration: 24160 loss: 0.0053 lr: 0.02\n",
      "iteration: 24170 loss: 0.0054 lr: 0.02\n",
      "iteration: 24180 loss: 0.0063 lr: 0.02\n",
      "iteration: 24190 loss: 0.0055 lr: 0.02\n",
      "iteration: 24200 loss: 0.0061 lr: 0.02\n",
      "iteration: 24210 loss: 0.0071 lr: 0.02\n",
      "iteration: 24220 loss: 0.0058 lr: 0.02\n",
      "iteration: 24230 loss: 0.0072 lr: 0.02\n",
      "iteration: 24240 loss: 0.0047 lr: 0.02\n",
      "iteration: 24250 loss: 0.0059 lr: 0.02\n",
      "iteration: 24260 loss: 0.0054 lr: 0.02\n",
      "iteration: 24270 loss: 0.0054 lr: 0.02\n",
      "iteration: 24280 loss: 0.0075 lr: 0.02\n",
      "iteration: 24290 loss: 0.0061 lr: 0.02\n",
      "iteration: 24300 loss: 0.0055 lr: 0.02\n",
      "iteration: 24310 loss: 0.0065 lr: 0.02\n",
      "iteration: 24320 loss: 0.0048 lr: 0.02\n",
      "iteration: 24330 loss: 0.0075 lr: 0.02\n",
      "iteration: 24340 loss: 0.0054 lr: 0.02\n",
      "iteration: 24350 loss: 0.0053 lr: 0.02\n",
      "iteration: 24360 loss: 0.0057 lr: 0.02\n",
      "iteration: 24370 loss: 0.0063 lr: 0.02\n",
      "iteration: 24380 loss: 0.0041 lr: 0.02\n",
      "iteration: 24390 loss: 0.0069 lr: 0.02\n",
      "iteration: 24400 loss: 0.0058 lr: 0.02\n",
      "iteration: 24410 loss: 0.0057 lr: 0.02\n",
      "iteration: 24420 loss: 0.0059 lr: 0.02\n",
      "iteration: 24430 loss: 0.0070 lr: 0.02\n",
      "iteration: 24440 loss: 0.0063 lr: 0.02\n",
      "iteration: 24450 loss: 0.0055 lr: 0.02\n",
      "iteration: 24460 loss: 0.0053 lr: 0.02\n",
      "iteration: 24470 loss: 0.0049 lr: 0.02\n",
      "iteration: 24480 loss: 0.0044 lr: 0.02\n",
      "iteration: 24490 loss: 0.0058 lr: 0.02\n",
      "iteration: 24500 loss: 0.0053 lr: 0.02\n",
      "iteration: 24510 loss: 0.0097 lr: 0.02\n",
      "iteration: 24520 loss: 0.0058 lr: 0.02\n",
      "iteration: 24530 loss: 0.0046 lr: 0.02\n",
      "iteration: 24540 loss: 0.0047 lr: 0.02\n",
      "iteration: 24550 loss: 0.0050 lr: 0.02\n",
      "iteration: 24560 loss: 0.0056 lr: 0.02\n",
      "iteration: 24570 loss: 0.0055 lr: 0.02\n",
      "iteration: 24580 loss: 0.0045 lr: 0.02\n",
      "iteration: 24590 loss: 0.0059 lr: 0.02\n",
      "iteration: 24600 loss: 0.0054 lr: 0.02\n",
      "iteration: 24610 loss: 0.0055 lr: 0.02\n",
      "iteration: 24620 loss: 0.0049 lr: 0.02\n",
      "iteration: 24630 loss: 0.0060 lr: 0.02\n",
      "iteration: 24640 loss: 0.0062 lr: 0.02\n",
      "iteration: 24650 loss: 0.0068 lr: 0.02\n",
      "iteration: 24660 loss: 0.0057 lr: 0.02\n",
      "iteration: 24670 loss: 0.0070 lr: 0.02\n",
      "iteration: 24680 loss: 0.0049 lr: 0.02\n",
      "iteration: 24690 loss: 0.0062 lr: 0.02\n",
      "iteration: 24700 loss: 0.0055 lr: 0.02\n",
      "iteration: 24710 loss: 0.0051 lr: 0.02\n",
      "iteration: 24720 loss: 0.0046 lr: 0.02\n",
      "iteration: 24730 loss: 0.0060 lr: 0.02\n",
      "iteration: 24740 loss: 0.0061 lr: 0.02\n",
      "iteration: 24750 loss: 0.0058 lr: 0.02\n",
      "iteration: 24760 loss: 0.0079 lr: 0.02\n",
      "iteration: 24770 loss: 0.0061 lr: 0.02\n",
      "iteration: 24780 loss: 0.0055 lr: 0.02\n",
      "iteration: 24790 loss: 0.0060 lr: 0.02\n",
      "iteration: 24800 loss: 0.0081 lr: 0.02\n",
      "iteration: 24810 loss: 0.0053 lr: 0.02\n",
      "iteration: 24820 loss: 0.0049 lr: 0.02\n",
      "iteration: 24830 loss: 0.0081 lr: 0.02\n",
      "iteration: 24840 loss: 0.0057 lr: 0.02\n",
      "iteration: 24850 loss: 0.0060 lr: 0.02\n",
      "iteration: 24860 loss: 0.0062 lr: 0.02\n",
      "iteration: 24870 loss: 0.0088 lr: 0.02\n",
      "iteration: 24880 loss: 0.0056 lr: 0.02\n",
      "iteration: 24890 loss: 0.0061 lr: 0.02\n",
      "iteration: 24900 loss: 0.0054 lr: 0.02\n",
      "iteration: 24910 loss: 0.0049 lr: 0.02\n",
      "iteration: 24920 loss: 0.0074 lr: 0.02\n",
      "iteration: 24930 loss: 0.0061 lr: 0.02\n",
      "iteration: 24940 loss: 0.0052 lr: 0.02\n",
      "iteration: 24950 loss: 0.0063 lr: 0.02\n",
      "iteration: 24960 loss: 0.0061 lr: 0.02\n",
      "iteration: 24970 loss: 0.0070 lr: 0.02\n",
      "iteration: 24980 loss: 0.0052 lr: 0.02\n",
      "iteration: 24990 loss: 0.0053 lr: 0.02\n",
      "iteration: 25000 loss: 0.0061 lr: 0.02\n",
      "iteration: 25010 loss: 0.0054 lr: 0.02\n",
      "iteration: 25020 loss: 0.0053 lr: 0.02\n",
      "iteration: 25030 loss: 0.0066 lr: 0.02\n",
      "iteration: 25040 loss: 0.0069 lr: 0.02\n",
      "iteration: 25050 loss: 0.0064 lr: 0.02\n",
      "iteration: 25060 loss: 0.0073 lr: 0.02\n",
      "iteration: 25070 loss: 0.0045 lr: 0.02\n",
      "iteration: 25080 loss: 0.0052 lr: 0.02\n",
      "iteration: 25090 loss: 0.0060 lr: 0.02\n",
      "iteration: 25100 loss: 0.0060 lr: 0.02\n",
      "iteration: 25110 loss: 0.0059 lr: 0.02\n",
      "iteration: 25120 loss: 0.0058 lr: 0.02\n",
      "iteration: 25130 loss: 0.0056 lr: 0.02\n",
      "iteration: 25140 loss: 0.0049 lr: 0.02\n",
      "iteration: 25150 loss: 0.0049 lr: 0.02\n",
      "iteration: 25160 loss: 0.0057 lr: 0.02\n",
      "iteration: 25170 loss: 0.0068 lr: 0.02\n",
      "iteration: 25180 loss: 0.0052 lr: 0.02\n",
      "iteration: 25190 loss: 0.0060 lr: 0.02\n",
      "iteration: 25200 loss: 0.0058 lr: 0.02\n",
      "iteration: 25210 loss: 0.0060 lr: 0.02\n",
      "iteration: 25220 loss: 0.0073 lr: 0.02\n",
      "iteration: 25230 loss: 0.0054 lr: 0.02\n",
      "iteration: 25240 loss: 0.0059 lr: 0.02\n",
      "iteration: 25250 loss: 0.0076 lr: 0.02\n",
      "iteration: 25260 loss: 0.0063 lr: 0.02\n",
      "iteration: 25270 loss: 0.0062 lr: 0.02\n",
      "iteration: 25280 loss: 0.0056 lr: 0.02\n",
      "iteration: 25290 loss: 0.0060 lr: 0.02\n",
      "iteration: 25300 loss: 0.0046 lr: 0.02\n",
      "iteration: 25310 loss: 0.0066 lr: 0.02\n",
      "iteration: 25320 loss: 0.0060 lr: 0.02\n",
      "iteration: 25330 loss: 0.0061 lr: 0.02\n",
      "iteration: 25340 loss: 0.0054 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 25350 loss: 0.0050 lr: 0.02\n",
      "iteration: 25360 loss: 0.0044 lr: 0.02\n",
      "iteration: 25370 loss: 0.0080 lr: 0.02\n",
      "iteration: 25380 loss: 0.0050 lr: 0.02\n",
      "iteration: 25390 loss: 0.0057 lr: 0.02\n",
      "iteration: 25400 loss: 0.0057 lr: 0.02\n",
      "iteration: 25410 loss: 0.0058 lr: 0.02\n",
      "iteration: 25420 loss: 0.0043 lr: 0.02\n",
      "iteration: 25430 loss: 0.0066 lr: 0.02\n",
      "iteration: 25440 loss: 0.0067 lr: 0.02\n",
      "iteration: 25450 loss: 0.0059 lr: 0.02\n",
      "iteration: 25460 loss: 0.0066 lr: 0.02\n",
      "iteration: 25470 loss: 0.0079 lr: 0.02\n",
      "iteration: 25480 loss: 0.0053 lr: 0.02\n",
      "iteration: 25490 loss: 0.0065 lr: 0.02\n",
      "iteration: 25500 loss: 0.0082 lr: 0.02\n",
      "iteration: 25510 loss: 0.0049 lr: 0.02\n",
      "iteration: 25520 loss: 0.0081 lr: 0.02\n",
      "iteration: 25530 loss: 0.0065 lr: 0.02\n",
      "iteration: 25540 loss: 0.0071 lr: 0.02\n",
      "iteration: 25550 loss: 0.0067 lr: 0.02\n",
      "iteration: 25560 loss: 0.0076 lr: 0.02\n",
      "iteration: 25570 loss: 0.0068 lr: 0.02\n",
      "iteration: 25580 loss: 0.0051 lr: 0.02\n",
      "iteration: 25590 loss: 0.0062 lr: 0.02\n",
      "iteration: 25600 loss: 0.0067 lr: 0.02\n",
      "iteration: 25610 loss: 0.0057 lr: 0.02\n",
      "iteration: 25620 loss: 0.0063 lr: 0.02\n",
      "iteration: 25630 loss: 0.0071 lr: 0.02\n",
      "iteration: 25640 loss: 0.0067 lr: 0.02\n",
      "iteration: 25650 loss: 0.0076 lr: 0.02\n",
      "iteration: 25660 loss: 0.0064 lr: 0.02\n",
      "iteration: 25670 loss: 0.0044 lr: 0.02\n",
      "iteration: 25680 loss: 0.0051 lr: 0.02\n",
      "iteration: 25690 loss: 0.0049 lr: 0.02\n",
      "iteration: 25700 loss: 0.0044 lr: 0.02\n",
      "iteration: 25710 loss: 0.0072 lr: 0.02\n",
      "iteration: 25720 loss: 0.0060 lr: 0.02\n",
      "iteration: 25730 loss: 0.0071 lr: 0.02\n",
      "iteration: 25740 loss: 0.0059 lr: 0.02\n",
      "iteration: 25750 loss: 0.0073 lr: 0.02\n",
      "iteration: 25760 loss: 0.0057 lr: 0.02\n",
      "iteration: 25770 loss: 0.0068 lr: 0.02\n",
      "iteration: 25780 loss: 0.0062 lr: 0.02\n",
      "iteration: 25790 loss: 0.0062 lr: 0.02\n",
      "iteration: 25800 loss: 0.0057 lr: 0.02\n",
      "iteration: 25810 loss: 0.0062 lr: 0.02\n",
      "iteration: 25820 loss: 0.0052 lr: 0.02\n",
      "iteration: 25830 loss: 0.0047 lr: 0.02\n",
      "iteration: 25840 loss: 0.0056 lr: 0.02\n",
      "iteration: 25850 loss: 0.0065 lr: 0.02\n",
      "iteration: 25860 loss: 0.0068 lr: 0.02\n",
      "iteration: 25870 loss: 0.0055 lr: 0.02\n",
      "iteration: 25880 loss: 0.0050 lr: 0.02\n",
      "iteration: 25890 loss: 0.0061 lr: 0.02\n",
      "iteration: 25900 loss: 0.0058 lr: 0.02\n",
      "iteration: 25910 loss: 0.0047 lr: 0.02\n",
      "iteration: 25920 loss: 0.0058 lr: 0.02\n",
      "iteration: 25930 loss: 0.0049 lr: 0.02\n",
      "iteration: 25940 loss: 0.0042 lr: 0.02\n",
      "iteration: 25950 loss: 0.0046 lr: 0.02\n",
      "iteration: 25960 loss: 0.0048 lr: 0.02\n",
      "iteration: 25970 loss: 0.0094 lr: 0.02\n",
      "iteration: 25980 loss: 0.0058 lr: 0.02\n",
      "iteration: 25990 loss: 0.0072 lr: 0.02\n",
      "iteration: 26000 loss: 0.0059 lr: 0.02\n",
      "iteration: 26010 loss: 0.0050 lr: 0.02\n",
      "iteration: 26020 loss: 0.0051 lr: 0.02\n",
      "iteration: 26030 loss: 0.0054 lr: 0.02\n",
      "iteration: 26040 loss: 0.0076 lr: 0.02\n",
      "iteration: 26050 loss: 0.0054 lr: 0.02\n",
      "iteration: 26060 loss: 0.0055 lr: 0.02\n",
      "iteration: 26070 loss: 0.0076 lr: 0.02\n",
      "iteration: 26080 loss: 0.0074 lr: 0.02\n",
      "iteration: 26090 loss: 0.0052 lr: 0.02\n",
      "iteration: 26100 loss: 0.0053 lr: 0.02\n",
      "iteration: 26110 loss: 0.0071 lr: 0.02\n",
      "iteration: 26120 loss: 0.0056 lr: 0.02\n",
      "iteration: 26130 loss: 0.0071 lr: 0.02\n",
      "iteration: 26140 loss: 0.0052 lr: 0.02\n",
      "iteration: 26150 loss: 0.0047 lr: 0.02\n",
      "iteration: 26160 loss: 0.0047 lr: 0.02\n",
      "iteration: 26170 loss: 0.0052 lr: 0.02\n",
      "iteration: 26180 loss: 0.0065 lr: 0.02\n",
      "iteration: 26190 loss: 0.0046 lr: 0.02\n",
      "iteration: 26200 loss: 0.0075 lr: 0.02\n",
      "iteration: 26210 loss: 0.0049 lr: 0.02\n",
      "iteration: 26220 loss: 0.0055 lr: 0.02\n",
      "iteration: 26230 loss: 0.0059 lr: 0.02\n",
      "iteration: 26240 loss: 0.0057 lr: 0.02\n",
      "iteration: 26250 loss: 0.0063 lr: 0.02\n",
      "iteration: 26260 loss: 0.0060 lr: 0.02\n",
      "iteration: 26270 loss: 0.0049 lr: 0.02\n",
      "iteration: 26280 loss: 0.0043 lr: 0.02\n",
      "iteration: 26290 loss: 0.0057 lr: 0.02\n",
      "iteration: 26300 loss: 0.0054 lr: 0.02\n",
      "iteration: 26310 loss: 0.0048 lr: 0.02\n",
      "iteration: 26320 loss: 0.0065 lr: 0.02\n",
      "iteration: 26330 loss: 0.0049 lr: 0.02\n",
      "iteration: 26340 loss: 0.0057 lr: 0.02\n",
      "iteration: 26350 loss: 0.0045 lr: 0.02\n",
      "iteration: 26360 loss: 0.0068 lr: 0.02\n",
      "iteration: 26370 loss: 0.0056 lr: 0.02\n",
      "iteration: 26380 loss: 0.0060 lr: 0.02\n",
      "iteration: 26390 loss: 0.0065 lr: 0.02\n",
      "iteration: 26400 loss: 0.0075 lr: 0.02\n",
      "iteration: 26410 loss: 0.0050 lr: 0.02\n",
      "iteration: 26420 loss: 0.0082 lr: 0.02\n",
      "iteration: 26430 loss: 0.0058 lr: 0.02\n",
      "iteration: 26440 loss: 0.0070 lr: 0.02\n",
      "iteration: 26450 loss: 0.0066 lr: 0.02\n",
      "iteration: 26460 loss: 0.0063 lr: 0.02\n",
      "iteration: 26470 loss: 0.0061 lr: 0.02\n",
      "iteration: 26480 loss: 0.0063 lr: 0.02\n",
      "iteration: 26490 loss: 0.0065 lr: 0.02\n",
      "iteration: 26500 loss: 0.0054 lr: 0.02\n",
      "iteration: 26510 loss: 0.0059 lr: 0.02\n",
      "iteration: 26520 loss: 0.0037 lr: 0.02\n",
      "iteration: 26530 loss: 0.0067 lr: 0.02\n",
      "iteration: 26540 loss: 0.0053 lr: 0.02\n",
      "iteration: 26550 loss: 0.0062 lr: 0.02\n",
      "iteration: 26560 loss: 0.0072 lr: 0.02\n",
      "iteration: 26570 loss: 0.0051 lr: 0.02\n",
      "iteration: 26580 loss: 0.0057 lr: 0.02\n",
      "iteration: 26590 loss: 0.0058 lr: 0.02\n",
      "iteration: 26600 loss: 0.0051 lr: 0.02\n",
      "iteration: 26610 loss: 0.0056 lr: 0.02\n",
      "iteration: 26620 loss: 0.0060 lr: 0.02\n",
      "iteration: 26630 loss: 0.0067 lr: 0.02\n",
      "iteration: 26640 loss: 0.0059 lr: 0.02\n",
      "iteration: 26650 loss: 0.0049 lr: 0.02\n",
      "iteration: 26660 loss: 0.0057 lr: 0.02\n",
      "iteration: 26670 loss: 0.0052 lr: 0.02\n",
      "iteration: 26680 loss: 0.0047 lr: 0.02\n",
      "iteration: 26690 loss: 0.0068 lr: 0.02\n",
      "iteration: 26700 loss: 0.0046 lr: 0.02\n",
      "iteration: 26710 loss: 0.0065 lr: 0.02\n",
      "iteration: 26720 loss: 0.0048 lr: 0.02\n",
      "iteration: 26730 loss: 0.0040 lr: 0.02\n",
      "iteration: 26740 loss: 0.0038 lr: 0.02\n",
      "iteration: 26750 loss: 0.0045 lr: 0.02\n",
      "iteration: 26760 loss: 0.0068 lr: 0.02\n",
      "iteration: 26770 loss: 0.0056 lr: 0.02\n",
      "iteration: 26780 loss: 0.0064 lr: 0.02\n",
      "iteration: 26790 loss: 0.0054 lr: 0.02\n",
      "iteration: 26800 loss: 0.0048 lr: 0.02\n",
      "iteration: 26810 loss: 0.0048 lr: 0.02\n",
      "iteration: 26820 loss: 0.0065 lr: 0.02\n",
      "iteration: 26830 loss: 0.0062 lr: 0.02\n",
      "iteration: 26840 loss: 0.0057 lr: 0.02\n",
      "iteration: 26850 loss: 0.0077 lr: 0.02\n",
      "iteration: 26860 loss: 0.0056 lr: 0.02\n",
      "iteration: 26870 loss: 0.0045 lr: 0.02\n",
      "iteration: 26880 loss: 0.0054 lr: 0.02\n",
      "iteration: 26890 loss: 0.0050 lr: 0.02\n",
      "iteration: 26900 loss: 0.0057 lr: 0.02\n",
      "iteration: 26910 loss: 0.0053 lr: 0.02\n",
      "iteration: 26920 loss: 0.0045 lr: 0.02\n",
      "iteration: 26930 loss: 0.0064 lr: 0.02\n",
      "iteration: 26940 loss: 0.0047 lr: 0.02\n",
      "iteration: 26950 loss: 0.0071 lr: 0.02\n",
      "iteration: 26960 loss: 0.0045 lr: 0.02\n",
      "iteration: 26970 loss: 0.0063 lr: 0.02\n",
      "iteration: 26980 loss: 0.0067 lr: 0.02\n",
      "iteration: 26990 loss: 0.0058 lr: 0.02\n",
      "iteration: 27000 loss: 0.0064 lr: 0.02\n",
      "iteration: 27010 loss: 0.0051 lr: 0.02\n",
      "iteration: 27020 loss: 0.0076 lr: 0.02\n",
      "iteration: 27030 loss: 0.0055 lr: 0.02\n",
      "iteration: 27040 loss: 0.0057 lr: 0.02\n",
      "iteration: 27050 loss: 0.0045 lr: 0.02\n",
      "iteration: 27060 loss: 0.0054 lr: 0.02\n",
      "iteration: 27070 loss: 0.0049 lr: 0.02\n",
      "iteration: 27080 loss: 0.0055 lr: 0.02\n",
      "iteration: 27090 loss: 0.0062 lr: 0.02\n",
      "iteration: 27100 loss: 0.0049 lr: 0.02\n",
      "iteration: 27110 loss: 0.0043 lr: 0.02\n",
      "iteration: 27120 loss: 0.0049 lr: 0.02\n",
      "iteration: 27130 loss: 0.0049 lr: 0.02\n",
      "iteration: 27140 loss: 0.0054 lr: 0.02\n",
      "iteration: 27150 loss: 0.0056 lr: 0.02\n",
      "iteration: 27160 loss: 0.0048 lr: 0.02\n",
      "iteration: 27170 loss: 0.0059 lr: 0.02\n",
      "iteration: 27180 loss: 0.0068 lr: 0.02\n",
      "iteration: 27190 loss: 0.0040 lr: 0.02\n",
      "iteration: 27200 loss: 0.0086 lr: 0.02\n",
      "iteration: 27210 loss: 0.0070 lr: 0.02\n",
      "iteration: 27220 loss: 0.0070 lr: 0.02\n",
      "iteration: 27230 loss: 0.0050 lr: 0.02\n",
      "iteration: 27240 loss: 0.0069 lr: 0.02\n",
      "iteration: 27250 loss: 0.0073 lr: 0.02\n",
      "iteration: 27260 loss: 0.0055 lr: 0.02\n",
      "iteration: 27270 loss: 0.0049 lr: 0.02\n",
      "iteration: 27280 loss: 0.0051 lr: 0.02\n",
      "iteration: 27290 loss: 0.0076 lr: 0.02\n",
      "iteration: 27300 loss: 0.0057 lr: 0.02\n",
      "iteration: 27310 loss: 0.0053 lr: 0.02\n",
      "iteration: 27320 loss: 0.0073 lr: 0.02\n",
      "iteration: 27330 loss: 0.0054 lr: 0.02\n",
      "iteration: 27340 loss: 0.0058 lr: 0.02\n",
      "iteration: 27350 loss: 0.0070 lr: 0.02\n",
      "iteration: 27360 loss: 0.0058 lr: 0.02\n",
      "iteration: 27370 loss: 0.0054 lr: 0.02\n",
      "iteration: 27380 loss: 0.0062 lr: 0.02\n",
      "iteration: 27390 loss: 0.0040 lr: 0.02\n",
      "iteration: 27400 loss: 0.0050 lr: 0.02\n",
      "iteration: 27410 loss: 0.0062 lr: 0.02\n",
      "iteration: 27420 loss: 0.0044 lr: 0.02\n",
      "iteration: 27430 loss: 0.0045 lr: 0.02\n",
      "iteration: 27440 loss: 0.0080 lr: 0.02\n",
      "iteration: 27450 loss: 0.0080 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 27460 loss: 0.0045 lr: 0.02\n",
      "iteration: 27470 loss: 0.0056 lr: 0.02\n",
      "iteration: 27480 loss: 0.0071 lr: 0.02\n",
      "iteration: 27490 loss: 0.0080 lr: 0.02\n",
      "iteration: 27500 loss: 0.0054 lr: 0.02\n",
      "iteration: 27510 loss: 0.0054 lr: 0.02\n",
      "iteration: 27520 loss: 0.0044 lr: 0.02\n",
      "iteration: 27530 loss: 0.0057 lr: 0.02\n",
      "iteration: 27540 loss: 0.0047 lr: 0.02\n",
      "iteration: 27550 loss: 0.0068 lr: 0.02\n",
      "iteration: 27560 loss: 0.0047 lr: 0.02\n",
      "iteration: 27570 loss: 0.0042 lr: 0.02\n",
      "iteration: 27580 loss: 0.0083 lr: 0.02\n",
      "iteration: 27590 loss: 0.0070 lr: 0.02\n",
      "iteration: 27600 loss: 0.0053 lr: 0.02\n",
      "iteration: 27610 loss: 0.0059 lr: 0.02\n",
      "iteration: 27620 loss: 0.0054 lr: 0.02\n",
      "iteration: 27630 loss: 0.0076 lr: 0.02\n",
      "iteration: 27640 loss: 0.0059 lr: 0.02\n",
      "iteration: 27650 loss: 0.0050 lr: 0.02\n",
      "iteration: 27660 loss: 0.0047 lr: 0.02\n",
      "iteration: 27670 loss: 0.0046 lr: 0.02\n",
      "iteration: 27680 loss: 0.0045 lr: 0.02\n",
      "iteration: 27690 loss: 0.0038 lr: 0.02\n",
      "iteration: 27700 loss: 0.0046 lr: 0.02\n",
      "iteration: 27710 loss: 0.0059 lr: 0.02\n",
      "iteration: 27720 loss: 0.0057 lr: 0.02\n",
      "iteration: 27730 loss: 0.0052 lr: 0.02\n",
      "iteration: 27740 loss: 0.0045 lr: 0.02\n",
      "iteration: 27750 loss: 0.0043 lr: 0.02\n",
      "iteration: 27760 loss: 0.0064 lr: 0.02\n",
      "iteration: 27770 loss: 0.0073 lr: 0.02\n",
      "iteration: 27780 loss: 0.0061 lr: 0.02\n",
      "iteration: 27790 loss: 0.0057 lr: 0.02\n",
      "iteration: 27800 loss: 0.0040 lr: 0.02\n",
      "iteration: 27810 loss: 0.0048 lr: 0.02\n",
      "iteration: 27820 loss: 0.0057 lr: 0.02\n",
      "iteration: 27830 loss: 0.0035 lr: 0.02\n",
      "iteration: 27840 loss: 0.0059 lr: 0.02\n",
      "iteration: 27850 loss: 0.0065 lr: 0.02\n",
      "iteration: 27860 loss: 0.0052 lr: 0.02\n",
      "iteration: 27870 loss: 0.0053 lr: 0.02\n",
      "iteration: 27880 loss: 0.0065 lr: 0.02\n",
      "iteration: 27890 loss: 0.0045 lr: 0.02\n",
      "iteration: 27900 loss: 0.0064 lr: 0.02\n",
      "iteration: 27910 loss: 0.0051 lr: 0.02\n",
      "iteration: 27920 loss: 0.0046 lr: 0.02\n",
      "iteration: 27930 loss: 0.0066 lr: 0.02\n",
      "iteration: 27940 loss: 0.0042 lr: 0.02\n",
      "iteration: 27950 loss: 0.0051 lr: 0.02\n",
      "iteration: 27960 loss: 0.0052 lr: 0.02\n",
      "iteration: 27970 loss: 0.0050 lr: 0.02\n",
      "iteration: 27980 loss: 0.0052 lr: 0.02\n",
      "iteration: 27990 loss: 0.0061 lr: 0.02\n",
      "iteration: 28000 loss: 0.0044 lr: 0.02\n",
      "iteration: 28010 loss: 0.0053 lr: 0.02\n",
      "iteration: 28020 loss: 0.0078 lr: 0.02\n",
      "iteration: 28030 loss: 0.0080 lr: 0.02\n",
      "iteration: 28040 loss: 0.0045 lr: 0.02\n",
      "iteration: 28050 loss: 0.0050 lr: 0.02\n",
      "iteration: 28060 loss: 0.0050 lr: 0.02\n",
      "iteration: 28070 loss: 0.0085 lr: 0.02\n",
      "iteration: 28080 loss: 0.0056 lr: 0.02\n",
      "iteration: 28090 loss: 0.0052 lr: 0.02\n",
      "iteration: 28100 loss: 0.0054 lr: 0.02\n",
      "iteration: 28110 loss: 0.0058 lr: 0.02\n",
      "iteration: 28120 loss: 0.0047 lr: 0.02\n",
      "iteration: 28130 loss: 0.0047 lr: 0.02\n",
      "iteration: 28140 loss: 0.0058 lr: 0.02\n",
      "iteration: 28150 loss: 0.0035 lr: 0.02\n",
      "iteration: 28160 loss: 0.0048 lr: 0.02\n",
      "iteration: 28170 loss: 0.0048 lr: 0.02\n",
      "iteration: 28180 loss: 0.0056 lr: 0.02\n",
      "iteration: 28190 loss: 0.0043 lr: 0.02\n",
      "iteration: 28200 loss: 0.0048 lr: 0.02\n",
      "iteration: 28210 loss: 0.0055 lr: 0.02\n",
      "iteration: 28220 loss: 0.0051 lr: 0.02\n",
      "iteration: 28230 loss: 0.0057 lr: 0.02\n",
      "iteration: 28240 loss: 0.0040 lr: 0.02\n",
      "iteration: 28250 loss: 0.0054 lr: 0.02\n",
      "iteration: 28260 loss: 0.0064 lr: 0.02\n",
      "iteration: 28270 loss: 0.0057 lr: 0.02\n",
      "iteration: 28280 loss: 0.0053 lr: 0.02\n",
      "iteration: 28290 loss: 0.0052 lr: 0.02\n",
      "iteration: 28300 loss: 0.0051 lr: 0.02\n",
      "iteration: 28310 loss: 0.0055 lr: 0.02\n",
      "iteration: 28320 loss: 0.0045 lr: 0.02\n",
      "iteration: 28330 loss: 0.0061 lr: 0.02\n",
      "iteration: 28340 loss: 0.0051 lr: 0.02\n",
      "iteration: 28350 loss: 0.0059 lr: 0.02\n",
      "iteration: 28360 loss: 0.0048 lr: 0.02\n",
      "iteration: 28370 loss: 0.0063 lr: 0.02\n",
      "iteration: 28380 loss: 0.0038 lr: 0.02\n",
      "iteration: 28390 loss: 0.0055 lr: 0.02\n",
      "iteration: 28400 loss: 0.0040 lr: 0.02\n",
      "iteration: 28410 loss: 0.0056 lr: 0.02\n",
      "iteration: 28420 loss: 0.0044 lr: 0.02\n",
      "iteration: 28430 loss: 0.0055 lr: 0.02\n",
      "iteration: 28440 loss: 0.0044 lr: 0.02\n",
      "iteration: 28450 loss: 0.0045 lr: 0.02\n",
      "iteration: 28460 loss: 0.0044 lr: 0.02\n",
      "iteration: 28470 loss: 0.0061 lr: 0.02\n",
      "iteration: 28480 loss: 0.0048 lr: 0.02\n",
      "iteration: 28490 loss: 0.0065 lr: 0.02\n",
      "iteration: 28500 loss: 0.0057 lr: 0.02\n",
      "iteration: 28510 loss: 0.0052 lr: 0.02\n",
      "iteration: 28520 loss: 0.0038 lr: 0.02\n",
      "iteration: 28530 loss: 0.0039 lr: 0.02\n",
      "iteration: 28540 loss: 0.0056 lr: 0.02\n",
      "iteration: 28550 loss: 0.0043 lr: 0.02\n",
      "iteration: 28560 loss: 0.0059 lr: 0.02\n",
      "iteration: 28570 loss: 0.0051 lr: 0.02\n",
      "iteration: 28580 loss: 0.0051 lr: 0.02\n",
      "iteration: 28590 loss: 0.0076 lr: 0.02\n",
      "iteration: 28600 loss: 0.0062 lr: 0.02\n",
      "iteration: 28610 loss: 0.0051 lr: 0.02\n",
      "iteration: 28620 loss: 0.0053 lr: 0.02\n",
      "iteration: 28630 loss: 0.0058 lr: 0.02\n",
      "iteration: 28640 loss: 0.0049 lr: 0.02\n",
      "iteration: 28650 loss: 0.0054 lr: 0.02\n",
      "iteration: 28660 loss: 0.0048 lr: 0.02\n",
      "iteration: 28670 loss: 0.0056 lr: 0.02\n",
      "iteration: 28680 loss: 0.0050 lr: 0.02\n",
      "iteration: 28690 loss: 0.0053 lr: 0.02\n",
      "iteration: 28700 loss: 0.0053 lr: 0.02\n",
      "iteration: 28710 loss: 0.0046 lr: 0.02\n",
      "iteration: 28720 loss: 0.0064 lr: 0.02\n",
      "iteration: 28730 loss: 0.0069 lr: 0.02\n",
      "iteration: 28740 loss: 0.0054 lr: 0.02\n",
      "iteration: 28750 loss: 0.0050 lr: 0.02\n",
      "iteration: 28760 loss: 0.0039 lr: 0.02\n",
      "iteration: 28770 loss: 0.0043 lr: 0.02\n",
      "iteration: 28780 loss: 0.0065 lr: 0.02\n",
      "iteration: 28790 loss: 0.0072 lr: 0.02\n",
      "iteration: 28800 loss: 0.0056 lr: 0.02\n",
      "iteration: 28810 loss: 0.0053 lr: 0.02\n",
      "iteration: 28820 loss: 0.0049 lr: 0.02\n",
      "iteration: 28830 loss: 0.0065 lr: 0.02\n",
      "iteration: 28840 loss: 0.0050 lr: 0.02\n",
      "iteration: 28850 loss: 0.0056 lr: 0.02\n",
      "iteration: 28860 loss: 0.0050 lr: 0.02\n",
      "iteration: 28870 loss: 0.0044 lr: 0.02\n",
      "iteration: 28880 loss: 0.0059 lr: 0.02\n",
      "iteration: 28890 loss: 0.0049 lr: 0.02\n",
      "iteration: 28900 loss: 0.0044 lr: 0.02\n",
      "iteration: 28910 loss: 0.0041 lr: 0.02\n",
      "iteration: 28920 loss: 0.0069 lr: 0.02\n",
      "iteration: 28930 loss: 0.0060 lr: 0.02\n",
      "iteration: 28940 loss: 0.0045 lr: 0.02\n",
      "iteration: 28950 loss: 0.0045 lr: 0.02\n",
      "iteration: 28960 loss: 0.0037 lr: 0.02\n",
      "iteration: 28970 loss: 0.0047 lr: 0.02\n",
      "iteration: 28980 loss: 0.0054 lr: 0.02\n",
      "iteration: 28990 loss: 0.0061 lr: 0.02\n",
      "iteration: 29000 loss: 0.0048 lr: 0.02\n",
      "iteration: 29010 loss: 0.0055 lr: 0.02\n",
      "iteration: 29020 loss: 0.0044 lr: 0.02\n",
      "iteration: 29030 loss: 0.0039 lr: 0.02\n",
      "iteration: 29040 loss: 0.0055 lr: 0.02\n",
      "iteration: 29050 loss: 0.0053 lr: 0.02\n",
      "iteration: 29060 loss: 0.0050 lr: 0.02\n",
      "iteration: 29070 loss: 0.0051 lr: 0.02\n",
      "iteration: 29080 loss: 0.0058 lr: 0.02\n",
      "iteration: 29090 loss: 0.0047 lr: 0.02\n",
      "iteration: 29100 loss: 0.0041 lr: 0.02\n",
      "iteration: 29110 loss: 0.0066 lr: 0.02\n",
      "iteration: 29120 loss: 0.0057 lr: 0.02\n",
      "iteration: 29130 loss: 0.0056 lr: 0.02\n",
      "iteration: 29140 loss: 0.0058 lr: 0.02\n",
      "iteration: 29150 loss: 0.0057 lr: 0.02\n",
      "iteration: 29160 loss: 0.0056 lr: 0.02\n",
      "iteration: 29170 loss: 0.0051 lr: 0.02\n",
      "iteration: 29180 loss: 0.0059 lr: 0.02\n",
      "iteration: 29190 loss: 0.0048 lr: 0.02\n",
      "iteration: 29200 loss: 0.0048 lr: 0.02\n",
      "iteration: 29210 loss: 0.0054 lr: 0.02\n",
      "iteration: 29220 loss: 0.0038 lr: 0.02\n",
      "iteration: 29230 loss: 0.0041 lr: 0.02\n",
      "iteration: 29240 loss: 0.0047 lr: 0.02\n",
      "iteration: 29250 loss: 0.0046 lr: 0.02\n",
      "iteration: 29260 loss: 0.0064 lr: 0.02\n",
      "iteration: 29270 loss: 0.0057 lr: 0.02\n",
      "iteration: 29280 loss: 0.0049 lr: 0.02\n",
      "iteration: 29290 loss: 0.0053 lr: 0.02\n",
      "iteration: 29300 loss: 0.0054 lr: 0.02\n",
      "iteration: 29310 loss: 0.0055 lr: 0.02\n",
      "iteration: 29320 loss: 0.0055 lr: 0.02\n",
      "iteration: 29330 loss: 0.0058 lr: 0.02\n",
      "iteration: 29340 loss: 0.0046 lr: 0.02\n",
      "iteration: 29350 loss: 0.0059 lr: 0.02\n",
      "iteration: 29360 loss: 0.0045 lr: 0.02\n",
      "iteration: 29370 loss: 0.0053 lr: 0.02\n",
      "iteration: 29380 loss: 0.0045 lr: 0.02\n",
      "iteration: 29390 loss: 0.0047 lr: 0.02\n",
      "iteration: 29400 loss: 0.0070 lr: 0.02\n",
      "iteration: 29410 loss: 0.0083 lr: 0.02\n",
      "iteration: 29420 loss: 0.0046 lr: 0.02\n",
      "iteration: 29430 loss: 0.0050 lr: 0.02\n",
      "iteration: 29440 loss: 0.0057 lr: 0.02\n",
      "iteration: 29450 loss: 0.0057 lr: 0.02\n",
      "iteration: 29460 loss: 0.0066 lr: 0.02\n",
      "iteration: 29470 loss: 0.0054 lr: 0.02\n",
      "iteration: 29480 loss: 0.0048 lr: 0.02\n",
      "iteration: 29490 loss: 0.0044 lr: 0.02\n",
      "iteration: 29500 loss: 0.0047 lr: 0.02\n",
      "iteration: 29510 loss: 0.0057 lr: 0.02\n",
      "iteration: 29520 loss: 0.0057 lr: 0.02\n",
      "iteration: 29530 loss: 0.0039 lr: 0.02\n",
      "iteration: 29540 loss: 0.0067 lr: 0.02\n",
      "iteration: 29550 loss: 0.0053 lr: 0.02\n",
      "iteration: 29560 loss: 0.0054 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 29570 loss: 0.0045 lr: 0.02\n",
      "iteration: 29580 loss: 0.0047 lr: 0.02\n",
      "iteration: 29590 loss: 0.0072 lr: 0.02\n",
      "iteration: 29600 loss: 0.0066 lr: 0.02\n",
      "iteration: 29610 loss: 0.0055 lr: 0.02\n",
      "iteration: 29620 loss: 0.0056 lr: 0.02\n",
      "iteration: 29630 loss: 0.0059 lr: 0.02\n",
      "iteration: 29640 loss: 0.0047 lr: 0.02\n",
      "iteration: 29650 loss: 0.0064 lr: 0.02\n",
      "iteration: 29660 loss: 0.0059 lr: 0.02\n",
      "iteration: 29670 loss: 0.0064 lr: 0.02\n",
      "iteration: 29680 loss: 0.0043 lr: 0.02\n",
      "iteration: 29690 loss: 0.0053 lr: 0.02\n",
      "iteration: 29700 loss: 0.0053 lr: 0.02\n",
      "iteration: 29710 loss: 0.0052 lr: 0.02\n",
      "iteration: 29720 loss: 0.0058 lr: 0.02\n",
      "iteration: 29730 loss: 0.0044 lr: 0.02\n",
      "iteration: 29740 loss: 0.0038 lr: 0.02\n",
      "iteration: 29750 loss: 0.0047 lr: 0.02\n",
      "iteration: 29760 loss: 0.0057 lr: 0.02\n",
      "iteration: 29770 loss: 0.0048 lr: 0.02\n",
      "iteration: 29780 loss: 0.0060 lr: 0.02\n",
      "iteration: 29790 loss: 0.0057 lr: 0.02\n",
      "iteration: 29800 loss: 0.0050 lr: 0.02\n",
      "iteration: 29810 loss: 0.0048 lr: 0.02\n",
      "iteration: 29820 loss: 0.0061 lr: 0.02\n",
      "iteration: 29830 loss: 0.0061 lr: 0.02\n",
      "iteration: 29840 loss: 0.0047 lr: 0.02\n",
      "iteration: 29850 loss: 0.0052 lr: 0.02\n",
      "iteration: 29860 loss: 0.0050 lr: 0.02\n",
      "iteration: 29870 loss: 0.0065 lr: 0.02\n",
      "iteration: 29880 loss: 0.0061 lr: 0.02\n",
      "iteration: 29890 loss: 0.0040 lr: 0.02\n",
      "iteration: 29900 loss: 0.0049 lr: 0.02\n",
      "iteration: 29910 loss: 0.0059 lr: 0.02\n",
      "iteration: 29920 loss: 0.0053 lr: 0.02\n",
      "iteration: 29930 loss: 0.0046 lr: 0.02\n",
      "iteration: 29940 loss: 0.0055 lr: 0.02\n",
      "iteration: 29950 loss: 0.0051 lr: 0.02\n",
      "iteration: 29960 loss: 0.0054 lr: 0.02\n",
      "iteration: 29970 loss: 0.0064 lr: 0.02\n",
      "iteration: 29980 loss: 0.0044 lr: 0.02\n",
      "iteration: 29990 loss: 0.0054 lr: 0.02\n",
      "iteration: 30000 loss: 0.0062 lr: 0.02\n",
      "iteration: 30010 loss: 0.0053 lr: 0.02\n",
      "iteration: 30020 loss: 0.0051 lr: 0.02\n",
      "iteration: 30030 loss: 0.0059 lr: 0.02\n",
      "iteration: 30040 loss: 0.0047 lr: 0.02\n",
      "iteration: 30050 loss: 0.0052 lr: 0.02\n",
      "iteration: 30060 loss: 0.0048 lr: 0.02\n",
      "iteration: 30070 loss: 0.0046 lr: 0.02\n",
      "iteration: 30080 loss: 0.0041 lr: 0.02\n",
      "iteration: 30090 loss: 0.0045 lr: 0.02\n",
      "iteration: 30100 loss: 0.0041 lr: 0.02\n",
      "iteration: 30110 loss: 0.0076 lr: 0.02\n",
      "iteration: 30120 loss: 0.0061 lr: 0.02\n",
      "iteration: 30130 loss: 0.0057 lr: 0.02\n",
      "iteration: 30140 loss: 0.0058 lr: 0.02\n",
      "iteration: 30150 loss: 0.0068 lr: 0.02\n",
      "iteration: 30160 loss: 0.0044 lr: 0.02\n",
      "iteration: 30170 loss: 0.0048 lr: 0.02\n",
      "iteration: 30180 loss: 0.0053 lr: 0.02\n",
      "iteration: 30190 loss: 0.0065 lr: 0.02\n",
      "iteration: 30200 loss: 0.0049 lr: 0.02\n",
      "iteration: 30210 loss: 0.0064 lr: 0.02\n",
      "iteration: 30220 loss: 0.0061 lr: 0.02\n",
      "iteration: 30230 loss: 0.0047 lr: 0.02\n",
      "iteration: 30240 loss: 0.0066 lr: 0.02\n",
      "iteration: 30250 loss: 0.0074 lr: 0.02\n",
      "iteration: 30260 loss: 0.0053 lr: 0.02\n",
      "iteration: 30270 loss: 0.0056 lr: 0.02\n",
      "iteration: 30280 loss: 0.0044 lr: 0.02\n",
      "iteration: 30290 loss: 0.0052 lr: 0.02\n",
      "iteration: 30300 loss: 0.0041 lr: 0.02\n",
      "iteration: 30310 loss: 0.0051 lr: 0.02\n",
      "iteration: 30320 loss: 0.0054 lr: 0.02\n",
      "iteration: 30330 loss: 0.0055 lr: 0.02\n",
      "iteration: 30340 loss: 0.0066 lr: 0.02\n",
      "iteration: 30350 loss: 0.0061 lr: 0.02\n",
      "iteration: 30360 loss: 0.0049 lr: 0.02\n",
      "iteration: 30370 loss: 0.0070 lr: 0.02\n",
      "iteration: 30380 loss: 0.0048 lr: 0.02\n",
      "iteration: 30390 loss: 0.0047 lr: 0.02\n",
      "iteration: 30400 loss: 0.0056 lr: 0.02\n",
      "iteration: 30410 loss: 0.0047 lr: 0.02\n",
      "iteration: 30420 loss: 0.0042 lr: 0.02\n",
      "iteration: 30430 loss: 0.0052 lr: 0.02\n",
      "iteration: 30440 loss: 0.0057 lr: 0.02\n",
      "iteration: 30450 loss: 0.0072 lr: 0.02\n",
      "iteration: 30460 loss: 0.0062 lr: 0.02\n",
      "iteration: 30470 loss: 0.0050 lr: 0.02\n",
      "iteration: 30480 loss: 0.0054 lr: 0.02\n",
      "iteration: 30490 loss: 0.0056 lr: 0.02\n",
      "iteration: 30500 loss: 0.0059 lr: 0.02\n",
      "iteration: 30510 loss: 0.0069 lr: 0.02\n",
      "iteration: 30520 loss: 0.0053 lr: 0.02\n",
      "iteration: 30530 loss: 0.0046 lr: 0.02\n",
      "iteration: 30540 loss: 0.0052 lr: 0.02\n",
      "iteration: 30550 loss: 0.0064 lr: 0.02\n",
      "iteration: 30560 loss: 0.0063 lr: 0.02\n",
      "iteration: 30570 loss: 0.0052 lr: 0.02\n",
      "iteration: 30580 loss: 0.0058 lr: 0.02\n",
      "iteration: 30590 loss: 0.0047 lr: 0.02\n",
      "iteration: 30600 loss: 0.0054 lr: 0.02\n",
      "iteration: 30610 loss: 0.0064 lr: 0.02\n",
      "iteration: 30620 loss: 0.0050 lr: 0.02\n",
      "iteration: 30630 loss: 0.0049 lr: 0.02\n",
      "iteration: 30640 loss: 0.0063 lr: 0.02\n",
      "iteration: 30650 loss: 0.0037 lr: 0.02\n",
      "iteration: 30660 loss: 0.0053 lr: 0.02\n",
      "iteration: 30670 loss: 0.0048 lr: 0.02\n",
      "iteration: 30680 loss: 0.0047 lr: 0.02\n",
      "iteration: 30690 loss: 0.0060 lr: 0.02\n",
      "iteration: 30700 loss: 0.0058 lr: 0.02\n",
      "iteration: 30710 loss: 0.0060 lr: 0.02\n",
      "iteration: 30720 loss: 0.0055 lr: 0.02\n",
      "iteration: 30730 loss: 0.0037 lr: 0.02\n",
      "iteration: 30740 loss: 0.0070 lr: 0.02\n",
      "iteration: 30750 loss: 0.0059 lr: 0.02\n",
      "iteration: 30760 loss: 0.0063 lr: 0.02\n",
      "iteration: 30770 loss: 0.0041 lr: 0.02\n",
      "iteration: 30780 loss: 0.0065 lr: 0.02\n",
      "iteration: 30790 loss: 0.0053 lr: 0.02\n",
      "iteration: 30800 loss: 0.0046 lr: 0.02\n",
      "iteration: 30810 loss: 0.0042 lr: 0.02\n",
      "iteration: 30820 loss: 0.0046 lr: 0.02\n",
      "iteration: 30830 loss: 0.0046 lr: 0.02\n",
      "iteration: 30840 loss: 0.0073 lr: 0.02\n",
      "iteration: 30850 loss: 0.0048 lr: 0.02\n",
      "iteration: 30860 loss: 0.0043 lr: 0.02\n",
      "iteration: 30870 loss: 0.0043 lr: 0.02\n",
      "iteration: 30880 loss: 0.0050 lr: 0.02\n",
      "iteration: 30890 loss: 0.0040 lr: 0.02\n",
      "iteration: 30900 loss: 0.0056 lr: 0.02\n",
      "iteration: 30910 loss: 0.0049 lr: 0.02\n",
      "iteration: 30920 loss: 0.0039 lr: 0.02\n",
      "iteration: 30930 loss: 0.0048 lr: 0.02\n",
      "iteration: 30940 loss: 0.0065 lr: 0.02\n",
      "iteration: 30950 loss: 0.0060 lr: 0.02\n",
      "iteration: 30960 loss: 0.0047 lr: 0.02\n",
      "iteration: 30970 loss: 0.0068 lr: 0.02\n",
      "iteration: 30980 loss: 0.0051 lr: 0.02\n",
      "iteration: 30990 loss: 0.0060 lr: 0.02\n",
      "iteration: 31000 loss: 0.0051 lr: 0.02\n",
      "iteration: 31010 loss: 0.0050 lr: 0.02\n",
      "iteration: 31020 loss: 0.0045 lr: 0.02\n",
      "iteration: 31030 loss: 0.0050 lr: 0.02\n",
      "iteration: 31040 loss: 0.0044 lr: 0.02\n",
      "iteration: 31050 loss: 0.0061 lr: 0.02\n",
      "iteration: 31060 loss: 0.0058 lr: 0.02\n",
      "iteration: 31070 loss: 0.0068 lr: 0.02\n",
      "iteration: 31080 loss: 0.0058 lr: 0.02\n",
      "iteration: 31090 loss: 0.0052 lr: 0.02\n",
      "iteration: 31100 loss: 0.0056 lr: 0.02\n",
      "iteration: 31110 loss: 0.0061 lr: 0.02\n",
      "iteration: 31120 loss: 0.0065 lr: 0.02\n",
      "iteration: 31130 loss: 0.0077 lr: 0.02\n",
      "iteration: 31140 loss: 0.0051 lr: 0.02\n",
      "iteration: 31150 loss: 0.0040 lr: 0.02\n",
      "iteration: 31160 loss: 0.0042 lr: 0.02\n",
      "iteration: 31170 loss: 0.0036 lr: 0.02\n",
      "iteration: 31180 loss: 0.0037 lr: 0.02\n",
      "iteration: 31190 loss: 0.0068 lr: 0.02\n",
      "iteration: 31200 loss: 0.0049 lr: 0.02\n",
      "iteration: 31210 loss: 0.0052 lr: 0.02\n",
      "iteration: 31220 loss: 0.0058 lr: 0.02\n",
      "iteration: 31230 loss: 0.0059 lr: 0.02\n",
      "iteration: 31240 loss: 0.0046 lr: 0.02\n",
      "iteration: 31250 loss: 0.0038 lr: 0.02\n",
      "iteration: 31260 loss: 0.0042 lr: 0.02\n",
      "iteration: 31270 loss: 0.0057 lr: 0.02\n",
      "iteration: 31280 loss: 0.0063 lr: 0.02\n",
      "iteration: 31290 loss: 0.0050 lr: 0.02\n",
      "iteration: 31300 loss: 0.0039 lr: 0.02\n",
      "iteration: 31310 loss: 0.0039 lr: 0.02\n",
      "iteration: 31320 loss: 0.0037 lr: 0.02\n",
      "iteration: 31330 loss: 0.0059 lr: 0.02\n",
      "iteration: 31340 loss: 0.0065 lr: 0.02\n",
      "iteration: 31350 loss: 0.0050 lr: 0.02\n",
      "iteration: 31360 loss: 0.0067 lr: 0.02\n",
      "iteration: 31370 loss: 0.0046 lr: 0.02\n",
      "iteration: 31380 loss: 0.0044 lr: 0.02\n",
      "iteration: 31390 loss: 0.0044 lr: 0.02\n",
      "iteration: 31400 loss: 0.0039 lr: 0.02\n",
      "iteration: 31410 loss: 0.0054 lr: 0.02\n",
      "iteration: 31420 loss: 0.0042 lr: 0.02\n",
      "iteration: 31430 loss: 0.0053 lr: 0.02\n",
      "iteration: 31440 loss: 0.0056 lr: 0.02\n",
      "iteration: 31450 loss: 0.0051 lr: 0.02\n",
      "iteration: 31460 loss: 0.0046 lr: 0.02\n",
      "iteration: 31470 loss: 0.0059 lr: 0.02\n",
      "iteration: 31480 loss: 0.0052 lr: 0.02\n",
      "iteration: 31490 loss: 0.0058 lr: 0.02\n",
      "iteration: 31500 loss: 0.0048 lr: 0.02\n",
      "iteration: 31510 loss: 0.0046 lr: 0.02\n",
      "iteration: 31520 loss: 0.0059 lr: 0.02\n",
      "iteration: 31530 loss: 0.0052 lr: 0.02\n",
      "iteration: 31540 loss: 0.0042 lr: 0.02\n",
      "iteration: 31550 loss: 0.0055 lr: 0.02\n",
      "iteration: 31560 loss: 0.0040 lr: 0.02\n",
      "iteration: 31570 loss: 0.0055 lr: 0.02\n",
      "iteration: 31580 loss: 0.0037 lr: 0.02\n",
      "iteration: 31590 loss: 0.0046 lr: 0.02\n",
      "iteration: 31600 loss: 0.0040 lr: 0.02\n",
      "iteration: 31610 loss: 0.0060 lr: 0.02\n",
      "iteration: 31620 loss: 0.0061 lr: 0.02\n",
      "iteration: 31630 loss: 0.0045 lr: 0.02\n",
      "iteration: 31640 loss: 0.0044 lr: 0.02\n",
      "iteration: 31650 loss: 0.0051 lr: 0.02\n",
      "iteration: 31660 loss: 0.0067 lr: 0.02\n",
      "iteration: 31670 loss: 0.0074 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 31680 loss: 0.0062 lr: 0.02\n",
      "iteration: 31690 loss: 0.0040 lr: 0.02\n",
      "iteration: 31700 loss: 0.0061 lr: 0.02\n",
      "iteration: 31710 loss: 0.0033 lr: 0.02\n",
      "iteration: 31720 loss: 0.0060 lr: 0.02\n",
      "iteration: 31730 loss: 0.0054 lr: 0.02\n",
      "iteration: 31740 loss: 0.0062 lr: 0.02\n",
      "iteration: 31750 loss: 0.0036 lr: 0.02\n",
      "iteration: 31760 loss: 0.0047 lr: 0.02\n",
      "iteration: 31770 loss: 0.0050 lr: 0.02\n",
      "iteration: 31780 loss: 0.0034 lr: 0.02\n",
      "iteration: 31790 loss: 0.0059 lr: 0.02\n",
      "iteration: 31800 loss: 0.0043 lr: 0.02\n",
      "iteration: 31810 loss: 0.0040 lr: 0.02\n",
      "iteration: 31820 loss: 0.0045 lr: 0.02\n",
      "iteration: 31830 loss: 0.0051 lr: 0.02\n",
      "iteration: 31840 loss: 0.0038 lr: 0.02\n",
      "iteration: 31850 loss: 0.0043 lr: 0.02\n",
      "iteration: 31860 loss: 0.0048 lr: 0.02\n",
      "iteration: 31870 loss: 0.0039 lr: 0.02\n",
      "iteration: 31880 loss: 0.0043 lr: 0.02\n",
      "iteration: 31890 loss: 0.0049 lr: 0.02\n",
      "iteration: 31900 loss: 0.0054 lr: 0.02\n",
      "iteration: 31910 loss: 0.0062 lr: 0.02\n",
      "iteration: 31920 loss: 0.0049 lr: 0.02\n",
      "iteration: 31930 loss: 0.0067 lr: 0.02\n",
      "iteration: 31940 loss: 0.0049 lr: 0.02\n",
      "iteration: 31950 loss: 0.0066 lr: 0.02\n",
      "iteration: 31960 loss: 0.0056 lr: 0.02\n",
      "iteration: 31970 loss: 0.0055 lr: 0.02\n",
      "iteration: 31980 loss: 0.0070 lr: 0.02\n",
      "iteration: 31990 loss: 0.0043 lr: 0.02\n",
      "iteration: 32000 loss: 0.0056 lr: 0.02\n",
      "iteration: 32010 loss: 0.0057 lr: 0.02\n",
      "iteration: 32020 loss: 0.0059 lr: 0.02\n",
      "iteration: 32030 loss: 0.0048 lr: 0.02\n",
      "iteration: 32040 loss: 0.0068 lr: 0.02\n",
      "iteration: 32050 loss: 0.0047 lr: 0.02\n",
      "iteration: 32060 loss: 0.0044 lr: 0.02\n",
      "iteration: 32070 loss: 0.0048 lr: 0.02\n",
      "iteration: 32080 loss: 0.0037 lr: 0.02\n",
      "iteration: 32090 loss: 0.0073 lr: 0.02\n",
      "iteration: 32100 loss: 0.0041 lr: 0.02\n",
      "iteration: 32110 loss: 0.0046 lr: 0.02\n",
      "iteration: 32120 loss: 0.0050 lr: 0.02\n",
      "iteration: 32130 loss: 0.0056 lr: 0.02\n",
      "iteration: 32140 loss: 0.0036 lr: 0.02\n",
      "iteration: 32150 loss: 0.0058 lr: 0.02\n",
      "iteration: 32160 loss: 0.0054 lr: 0.02\n",
      "iteration: 32170 loss: 0.0038 lr: 0.02\n",
      "iteration: 32180 loss: 0.0055 lr: 0.02\n",
      "iteration: 32190 loss: 0.0049 lr: 0.02\n",
      "iteration: 32200 loss: 0.0053 lr: 0.02\n",
      "iteration: 32210 loss: 0.0043 lr: 0.02\n",
      "iteration: 32220 loss: 0.0045 lr: 0.02\n",
      "iteration: 32230 loss: 0.0060 lr: 0.02\n",
      "iteration: 32240 loss: 0.0059 lr: 0.02\n",
      "iteration: 32250 loss: 0.0052 lr: 0.02\n",
      "iteration: 32260 loss: 0.0043 lr: 0.02\n",
      "iteration: 32270 loss: 0.0056 lr: 0.02\n",
      "iteration: 32280 loss: 0.0045 lr: 0.02\n",
      "iteration: 32290 loss: 0.0054 lr: 0.02\n",
      "iteration: 32300 loss: 0.0064 lr: 0.02\n",
      "iteration: 32310 loss: 0.0048 lr: 0.02\n",
      "iteration: 32320 loss: 0.0049 lr: 0.02\n",
      "iteration: 32330 loss: 0.0061 lr: 0.02\n",
      "iteration: 32340 loss: 0.0041 lr: 0.02\n",
      "iteration: 32350 loss: 0.0053 lr: 0.02\n",
      "iteration: 32360 loss: 0.0068 lr: 0.02\n",
      "iteration: 32370 loss: 0.0051 lr: 0.02\n",
      "iteration: 32380 loss: 0.0072 lr: 0.02\n",
      "iteration: 32390 loss: 0.0056 lr: 0.02\n",
      "iteration: 32400 loss: 0.0057 lr: 0.02\n",
      "iteration: 32410 loss: 0.0050 lr: 0.02\n",
      "iteration: 32420 loss: 0.0063 lr: 0.02\n",
      "iteration: 32430 loss: 0.0056 lr: 0.02\n",
      "iteration: 32440 loss: 0.0053 lr: 0.02\n",
      "iteration: 32450 loss: 0.0047 lr: 0.02\n",
      "iteration: 32460 loss: 0.0058 lr: 0.02\n",
      "iteration: 32470 loss: 0.0054 lr: 0.02\n",
      "iteration: 32480 loss: 0.0044 lr: 0.02\n",
      "iteration: 32490 loss: 0.0041 lr: 0.02\n",
      "iteration: 32500 loss: 0.0049 lr: 0.02\n",
      "iteration: 32510 loss: 0.0046 lr: 0.02\n",
      "iteration: 32520 loss: 0.0045 lr: 0.02\n",
      "iteration: 32530 loss: 0.0045 lr: 0.02\n",
      "iteration: 32540 loss: 0.0059 lr: 0.02\n",
      "iteration: 32550 loss: 0.0041 lr: 0.02\n",
      "iteration: 32560 loss: 0.0055 lr: 0.02\n",
      "iteration: 32570 loss: 0.0050 lr: 0.02\n",
      "iteration: 32580 loss: 0.0055 lr: 0.02\n",
      "iteration: 32590 loss: 0.0034 lr: 0.02\n",
      "iteration: 32600 loss: 0.0038 lr: 0.02\n",
      "iteration: 32610 loss: 0.0059 lr: 0.02\n",
      "iteration: 32620 loss: 0.0049 lr: 0.02\n",
      "iteration: 32630 loss: 0.0042 lr: 0.02\n",
      "iteration: 32640 loss: 0.0053 lr: 0.02\n",
      "iteration: 32650 loss: 0.0047 lr: 0.02\n",
      "iteration: 32660 loss: 0.0053 lr: 0.02\n",
      "iteration: 32670 loss: 0.0049 lr: 0.02\n",
      "iteration: 32680 loss: 0.0044 lr: 0.02\n",
      "iteration: 32690 loss: 0.0065 lr: 0.02\n",
      "iteration: 32700 loss: 0.0049 lr: 0.02\n",
      "iteration: 32710 loss: 0.0050 lr: 0.02\n",
      "iteration: 32720 loss: 0.0063 lr: 0.02\n",
      "iteration: 32730 loss: 0.0047 lr: 0.02\n",
      "iteration: 32740 loss: 0.0058 lr: 0.02\n",
      "iteration: 32750 loss: 0.0044 lr: 0.02\n",
      "iteration: 32760 loss: 0.0059 lr: 0.02\n",
      "iteration: 32770 loss: 0.0043 lr: 0.02\n",
      "iteration: 32780 loss: 0.0058 lr: 0.02\n",
      "iteration: 32790 loss: 0.0050 lr: 0.02\n",
      "iteration: 32800 loss: 0.0058 lr: 0.02\n",
      "iteration: 32810 loss: 0.0037 lr: 0.02\n",
      "iteration: 32820 loss: 0.0049 lr: 0.02\n",
      "iteration: 32830 loss: 0.0053 lr: 0.02\n",
      "iteration: 32840 loss: 0.0051 lr: 0.02\n",
      "iteration: 32850 loss: 0.0041 lr: 0.02\n",
      "iteration: 32860 loss: 0.0046 lr: 0.02\n",
      "iteration: 32870 loss: 0.0043 lr: 0.02\n",
      "iteration: 32880 loss: 0.0052 lr: 0.02\n",
      "iteration: 32890 loss: 0.0041 lr: 0.02\n",
      "iteration: 32900 loss: 0.0045 lr: 0.02\n",
      "iteration: 32910 loss: 0.0057 lr: 0.02\n",
      "iteration: 32920 loss: 0.0050 lr: 0.02\n",
      "iteration: 32930 loss: 0.0045 lr: 0.02\n",
      "iteration: 32940 loss: 0.0052 lr: 0.02\n",
      "iteration: 32950 loss: 0.0042 lr: 0.02\n",
      "iteration: 32960 loss: 0.0045 lr: 0.02\n",
      "iteration: 32970 loss: 0.0043 lr: 0.02\n",
      "iteration: 32980 loss: 0.0036 lr: 0.02\n",
      "iteration: 32990 loss: 0.0039 lr: 0.02\n",
      "iteration: 33000 loss: 0.0046 lr: 0.02\n",
      "iteration: 33010 loss: 0.0061 lr: 0.02\n",
      "iteration: 33020 loss: 0.0041 lr: 0.02\n",
      "iteration: 33030 loss: 0.0040 lr: 0.02\n",
      "iteration: 33040 loss: 0.0074 lr: 0.02\n",
      "iteration: 33050 loss: 0.0054 lr: 0.02\n",
      "iteration: 33060 loss: 0.0047 lr: 0.02\n",
      "iteration: 33070 loss: 0.0053 lr: 0.02\n",
      "iteration: 33080 loss: 0.0037 lr: 0.02\n",
      "iteration: 33090 loss: 0.0050 lr: 0.02\n",
      "iteration: 33100 loss: 0.0050 lr: 0.02\n",
      "iteration: 33110 loss: 0.0042 lr: 0.02\n",
      "iteration: 33120 loss: 0.0035 lr: 0.02\n",
      "iteration: 33130 loss: 0.0038 lr: 0.02\n",
      "iteration: 33140 loss: 0.0039 lr: 0.02\n",
      "iteration: 33150 loss: 0.0053 lr: 0.02\n",
      "iteration: 33160 loss: 0.0039 lr: 0.02\n",
      "iteration: 33170 loss: 0.0066 lr: 0.02\n",
      "iteration: 33180 loss: 0.0033 lr: 0.02\n",
      "iteration: 33190 loss: 0.0044 lr: 0.02\n",
      "iteration: 33200 loss: 0.0051 lr: 0.02\n",
      "iteration: 33210 loss: 0.0043 lr: 0.02\n",
      "iteration: 33220 loss: 0.0045 lr: 0.02\n",
      "iteration: 33230 loss: 0.0045 lr: 0.02\n",
      "iteration: 33240 loss: 0.0052 lr: 0.02\n",
      "iteration: 33250 loss: 0.0055 lr: 0.02\n",
      "iteration: 33260 loss: 0.0047 lr: 0.02\n",
      "iteration: 33270 loss: 0.0046 lr: 0.02\n",
      "iteration: 33280 loss: 0.0061 lr: 0.02\n",
      "iteration: 33290 loss: 0.0038 lr: 0.02\n",
      "iteration: 33300 loss: 0.0042 lr: 0.02\n",
      "iteration: 33310 loss: 0.0047 lr: 0.02\n",
      "iteration: 33320 loss: 0.0045 lr: 0.02\n",
      "iteration: 33330 loss: 0.0051 lr: 0.02\n",
      "iteration: 33340 loss: 0.0049 lr: 0.02\n",
      "iteration: 33350 loss: 0.0047 lr: 0.02\n",
      "iteration: 33360 loss: 0.0069 lr: 0.02\n",
      "iteration: 33370 loss: 0.0064 lr: 0.02\n",
      "iteration: 33380 loss: 0.0055 lr: 0.02\n",
      "iteration: 33390 loss: 0.0048 lr: 0.02\n",
      "iteration: 33400 loss: 0.0046 lr: 0.02\n",
      "iteration: 33410 loss: 0.0042 lr: 0.02\n",
      "iteration: 33420 loss: 0.0045 lr: 0.02\n",
      "iteration: 33430 loss: 0.0053 lr: 0.02\n",
      "iteration: 33440 loss: 0.0045 lr: 0.02\n",
      "iteration: 33450 loss: 0.0065 lr: 0.02\n",
      "iteration: 33460 loss: 0.0052 lr: 0.02\n",
      "iteration: 33470 loss: 0.0052 lr: 0.02\n",
      "iteration: 33480 loss: 0.0053 lr: 0.02\n",
      "iteration: 33490 loss: 0.0055 lr: 0.02\n",
      "iteration: 33500 loss: 0.0051 lr: 0.02\n",
      "iteration: 33510 loss: 0.0056 lr: 0.02\n",
      "iteration: 33520 loss: 0.0058 lr: 0.02\n",
      "iteration: 33530 loss: 0.0051 lr: 0.02\n",
      "iteration: 33540 loss: 0.0044 lr: 0.02\n",
      "iteration: 33550 loss: 0.0051 lr: 0.02\n",
      "iteration: 33560 loss: 0.0081 lr: 0.02\n",
      "iteration: 33570 loss: 0.0060 lr: 0.02\n",
      "iteration: 33580 loss: 0.0051 lr: 0.02\n",
      "iteration: 33590 loss: 0.0054 lr: 0.02\n",
      "iteration: 33600 loss: 0.0070 lr: 0.02\n",
      "iteration: 33610 loss: 0.0045 lr: 0.02\n",
      "iteration: 33620 loss: 0.0056 lr: 0.02\n",
      "iteration: 33630 loss: 0.0050 lr: 0.02\n",
      "iteration: 33640 loss: 0.0056 lr: 0.02\n",
      "iteration: 33650 loss: 0.0045 lr: 0.02\n",
      "iteration: 33660 loss: 0.0061 lr: 0.02\n",
      "iteration: 33670 loss: 0.0039 lr: 0.02\n",
      "iteration: 33680 loss: 0.0056 lr: 0.02\n",
      "iteration: 33690 loss: 0.0065 lr: 0.02\n",
      "iteration: 33700 loss: 0.0052 lr: 0.02\n",
      "iteration: 33710 loss: 0.0049 lr: 0.02\n",
      "iteration: 33720 loss: 0.0048 lr: 0.02\n",
      "iteration: 33730 loss: 0.0073 lr: 0.02\n",
      "iteration: 33740 loss: 0.0051 lr: 0.02\n",
      "iteration: 33750 loss: 0.0044 lr: 0.02\n",
      "iteration: 33760 loss: 0.0039 lr: 0.02\n",
      "iteration: 33770 loss: 0.0072 lr: 0.02\n",
      "iteration: 33780 loss: 0.0055 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 33790 loss: 0.0064 lr: 0.02\n",
      "iteration: 33800 loss: 0.0048 lr: 0.02\n",
      "iteration: 33810 loss: 0.0049 lr: 0.02\n",
      "iteration: 33820 loss: 0.0053 lr: 0.02\n",
      "iteration: 33830 loss: 0.0051 lr: 0.02\n",
      "iteration: 33840 loss: 0.0043 lr: 0.02\n",
      "iteration: 33850 loss: 0.0053 lr: 0.02\n",
      "iteration: 33860 loss: 0.0035 lr: 0.02\n",
      "iteration: 33870 loss: 0.0048 lr: 0.02\n",
      "iteration: 33880 loss: 0.0051 lr: 0.02\n",
      "iteration: 33890 loss: 0.0037 lr: 0.02\n",
      "iteration: 33900 loss: 0.0040 lr: 0.02\n",
      "iteration: 33910 loss: 0.0043 lr: 0.02\n",
      "iteration: 33920 loss: 0.0043 lr: 0.02\n",
      "iteration: 33930 loss: 0.0043 lr: 0.02\n",
      "iteration: 33940 loss: 0.0040 lr: 0.02\n",
      "iteration: 33950 loss: 0.0056 lr: 0.02\n",
      "iteration: 33960 loss: 0.0044 lr: 0.02\n",
      "iteration: 33970 loss: 0.0062 lr: 0.02\n",
      "iteration: 33980 loss: 0.0047 lr: 0.02\n",
      "iteration: 33990 loss: 0.0052 lr: 0.02\n",
      "iteration: 34000 loss: 0.0047 lr: 0.02\n",
      "iteration: 34010 loss: 0.0065 lr: 0.02\n",
      "iteration: 34020 loss: 0.0039 lr: 0.02\n",
      "iteration: 34030 loss: 0.0043 lr: 0.02\n",
      "iteration: 34040 loss: 0.0040 lr: 0.02\n",
      "iteration: 34050 loss: 0.0058 lr: 0.02\n",
      "iteration: 34060 loss: 0.0053 lr: 0.02\n",
      "iteration: 34070 loss: 0.0039 lr: 0.02\n",
      "iteration: 34080 loss: 0.0046 lr: 0.02\n",
      "iteration: 34090 loss: 0.0043 lr: 0.02\n",
      "iteration: 34100 loss: 0.0059 lr: 0.02\n",
      "iteration: 34110 loss: 0.0041 lr: 0.02\n",
      "iteration: 34120 loss: 0.0036 lr: 0.02\n",
      "iteration: 34130 loss: 0.0050 lr: 0.02\n",
      "iteration: 34140 loss: 0.0052 lr: 0.02\n",
      "iteration: 34150 loss: 0.0046 lr: 0.02\n",
      "iteration: 34160 loss: 0.0050 lr: 0.02\n",
      "iteration: 34170 loss: 0.0046 lr: 0.02\n",
      "iteration: 34180 loss: 0.0042 lr: 0.02\n",
      "iteration: 34190 loss: 0.0048 lr: 0.02\n",
      "iteration: 34200 loss: 0.0038 lr: 0.02\n",
      "iteration: 34210 loss: 0.0055 lr: 0.02\n",
      "iteration: 34220 loss: 0.0043 lr: 0.02\n",
      "iteration: 34230 loss: 0.0046 lr: 0.02\n",
      "iteration: 34240 loss: 0.0050 lr: 0.02\n",
      "iteration: 34250 loss: 0.0050 lr: 0.02\n",
      "iteration: 34260 loss: 0.0063 lr: 0.02\n",
      "iteration: 34270 loss: 0.0059 lr: 0.02\n",
      "iteration: 34280 loss: 0.0055 lr: 0.02\n",
      "iteration: 34290 loss: 0.0046 lr: 0.02\n",
      "iteration: 34300 loss: 0.0050 lr: 0.02\n",
      "iteration: 34310 loss: 0.0052 lr: 0.02\n",
      "iteration: 34320 loss: 0.0045 lr: 0.02\n",
      "iteration: 34330 loss: 0.0048 lr: 0.02\n",
      "iteration: 34340 loss: 0.0029 lr: 0.02\n",
      "iteration: 34350 loss: 0.0035 lr: 0.02\n",
      "iteration: 34360 loss: 0.0047 lr: 0.02\n",
      "iteration: 34370 loss: 0.0044 lr: 0.02\n",
      "iteration: 34380 loss: 0.0045 lr: 0.02\n",
      "iteration: 34390 loss: 0.0045 lr: 0.02\n",
      "iteration: 34400 loss: 0.0049 lr: 0.02\n",
      "iteration: 34410 loss: 0.0060 lr: 0.02\n",
      "iteration: 34420 loss: 0.0071 lr: 0.02\n",
      "iteration: 34430 loss: 0.0067 lr: 0.02\n",
      "iteration: 34440 loss: 0.0051 lr: 0.02\n",
      "iteration: 34450 loss: 0.0051 lr: 0.02\n",
      "iteration: 34460 loss: 0.0049 lr: 0.02\n",
      "iteration: 34470 loss: 0.0048 lr: 0.02\n",
      "iteration: 34480 loss: 0.0048 lr: 0.02\n",
      "iteration: 34490 loss: 0.0051 lr: 0.02\n",
      "iteration: 34500 loss: 0.0080 lr: 0.02\n",
      "iteration: 34510 loss: 0.0046 lr: 0.02\n",
      "iteration: 34520 loss: 0.0047 lr: 0.02\n",
      "iteration: 34530 loss: 0.0046 lr: 0.02\n",
      "iteration: 34540 loss: 0.0050 lr: 0.02\n",
      "iteration: 34550 loss: 0.0055 lr: 0.02\n",
      "iteration: 34560 loss: 0.0039 lr: 0.02\n",
      "iteration: 34570 loss: 0.0041 lr: 0.02\n",
      "iteration: 34580 loss: 0.0052 lr: 0.02\n",
      "iteration: 34590 loss: 0.0070 lr: 0.02\n",
      "iteration: 34600 loss: 0.0045 lr: 0.02\n",
      "iteration: 34610 loss: 0.0054 lr: 0.02\n",
      "iteration: 34620 loss: 0.0055 lr: 0.02\n",
      "iteration: 34630 loss: 0.0061 lr: 0.02\n",
      "iteration: 34640 loss: 0.0056 lr: 0.02\n",
      "iteration: 34650 loss: 0.0045 lr: 0.02\n",
      "iteration: 34660 loss: 0.0048 lr: 0.02\n",
      "iteration: 34670 loss: 0.0049 lr: 0.02\n",
      "iteration: 34680 loss: 0.0052 lr: 0.02\n",
      "iteration: 34690 loss: 0.0065 lr: 0.02\n",
      "iteration: 34700 loss: 0.0044 lr: 0.02\n",
      "iteration: 34710 loss: 0.0034 lr: 0.02\n",
      "iteration: 34720 loss: 0.0042 lr: 0.02\n",
      "iteration: 34730 loss: 0.0060 lr: 0.02\n",
      "iteration: 34740 loss: 0.0048 lr: 0.02\n",
      "iteration: 34750 loss: 0.0052 lr: 0.02\n",
      "iteration: 34760 loss: 0.0047 lr: 0.02\n",
      "iteration: 34770 loss: 0.0042 lr: 0.02\n",
      "iteration: 34780 loss: 0.0046 lr: 0.02\n",
      "iteration: 34790 loss: 0.0045 lr: 0.02\n",
      "iteration: 34800 loss: 0.0041 lr: 0.02\n",
      "iteration: 34810 loss: 0.0040 lr: 0.02\n",
      "iteration: 34820 loss: 0.0046 lr: 0.02\n",
      "iteration: 34830 loss: 0.0039 lr: 0.02\n",
      "iteration: 34840 loss: 0.0047 lr: 0.02\n",
      "iteration: 34850 loss: 0.0052 lr: 0.02\n",
      "iteration: 34860 loss: 0.0055 lr: 0.02\n",
      "iteration: 34870 loss: 0.0044 lr: 0.02\n",
      "iteration: 34880 loss: 0.0049 lr: 0.02\n",
      "iteration: 34890 loss: 0.0054 lr: 0.02\n",
      "iteration: 34900 loss: 0.0044 lr: 0.02\n",
      "iteration: 34910 loss: 0.0046 lr: 0.02\n",
      "iteration: 34920 loss: 0.0065 lr: 0.02\n",
      "iteration: 34930 loss: 0.0039 lr: 0.02\n",
      "iteration: 34940 loss: 0.0045 lr: 0.02\n",
      "iteration: 34950 loss: 0.0036 lr: 0.02\n",
      "iteration: 34960 loss: 0.0042 lr: 0.02\n",
      "iteration: 34970 loss: 0.0042 lr: 0.02\n",
      "iteration: 34980 loss: 0.0046 lr: 0.02\n",
      "iteration: 34990 loss: 0.0035 lr: 0.02\n",
      "iteration: 35000 loss: 0.0039 lr: 0.02\n",
      "iteration: 35010 loss: 0.0038 lr: 0.02\n",
      "iteration: 35020 loss: 0.0043 lr: 0.02\n",
      "iteration: 35030 loss: 0.0048 lr: 0.02\n",
      "iteration: 35040 loss: 0.0050 lr: 0.02\n",
      "iteration: 35050 loss: 0.0045 lr: 0.02\n",
      "iteration: 35060 loss: 0.0033 lr: 0.02\n",
      "iteration: 35070 loss: 0.0040 lr: 0.02\n",
      "iteration: 35080 loss: 0.0046 lr: 0.02\n",
      "iteration: 35090 loss: 0.0049 lr: 0.02\n",
      "iteration: 35100 loss: 0.0049 lr: 0.02\n",
      "iteration: 35110 loss: 0.0044 lr: 0.02\n",
      "iteration: 35120 loss: 0.0049 lr: 0.02\n",
      "iteration: 35130 loss: 0.0053 lr: 0.02\n",
      "iteration: 35140 loss: 0.0053 lr: 0.02\n",
      "iteration: 35150 loss: 0.0046 lr: 0.02\n",
      "iteration: 35160 loss: 0.0061 lr: 0.02\n",
      "iteration: 35170 loss: 0.0052 lr: 0.02\n",
      "iteration: 35180 loss: 0.0043 lr: 0.02\n",
      "iteration: 35190 loss: 0.0050 lr: 0.02\n",
      "iteration: 35200 loss: 0.0052 lr: 0.02\n",
      "iteration: 35210 loss: 0.0047 lr: 0.02\n",
      "iteration: 35220 loss: 0.0048 lr: 0.02\n",
      "iteration: 35230 loss: 0.0049 lr: 0.02\n",
      "iteration: 35240 loss: 0.0048 lr: 0.02\n",
      "iteration: 35250 loss: 0.0043 lr: 0.02\n",
      "iteration: 35260 loss: 0.0037 lr: 0.02\n",
      "iteration: 35270 loss: 0.0061 lr: 0.02\n",
      "iteration: 35280 loss: 0.0040 lr: 0.02\n",
      "iteration: 35290 loss: 0.0059 lr: 0.02\n",
      "iteration: 35300 loss: 0.0069 lr: 0.02\n",
      "iteration: 35310 loss: 0.0050 lr: 0.02\n",
      "iteration: 35320 loss: 0.0047 lr: 0.02\n",
      "iteration: 35330 loss: 0.0041 lr: 0.02\n",
      "iteration: 35340 loss: 0.0034 lr: 0.02\n",
      "iteration: 35350 loss: 0.0041 lr: 0.02\n",
      "iteration: 35360 loss: 0.0052 lr: 0.02\n",
      "iteration: 35370 loss: 0.0056 lr: 0.02\n",
      "iteration: 35380 loss: 0.0056 lr: 0.02\n",
      "iteration: 35390 loss: 0.0036 lr: 0.02\n",
      "iteration: 35400 loss: 0.0045 lr: 0.02\n",
      "iteration: 35410 loss: 0.0062 lr: 0.02\n",
      "iteration: 35420 loss: 0.0060 lr: 0.02\n",
      "iteration: 35430 loss: 0.0045 lr: 0.02\n",
      "iteration: 35440 loss: 0.0047 lr: 0.02\n",
      "iteration: 35450 loss: 0.0054 lr: 0.02\n",
      "iteration: 35460 loss: 0.0040 lr: 0.02\n",
      "iteration: 35470 loss: 0.0060 lr: 0.02\n",
      "iteration: 35480 loss: 0.0054 lr: 0.02\n",
      "iteration: 35490 loss: 0.0040 lr: 0.02\n",
      "iteration: 35500 loss: 0.0049 lr: 0.02\n",
      "iteration: 35510 loss: 0.0045 lr: 0.02\n",
      "iteration: 35520 loss: 0.0042 lr: 0.02\n",
      "iteration: 35530 loss: 0.0044 lr: 0.02\n",
      "iteration: 35540 loss: 0.0051 lr: 0.02\n",
      "iteration: 35550 loss: 0.0044 lr: 0.02\n",
      "iteration: 35560 loss: 0.0046 lr: 0.02\n",
      "iteration: 35570 loss: 0.0041 lr: 0.02\n",
      "iteration: 35580 loss: 0.0042 lr: 0.02\n",
      "iteration: 35590 loss: 0.0041 lr: 0.02\n",
      "iteration: 35600 loss: 0.0050 lr: 0.02\n",
      "iteration: 35610 loss: 0.0044 lr: 0.02\n",
      "iteration: 35620 loss: 0.0037 lr: 0.02\n",
      "iteration: 35630 loss: 0.0039 lr: 0.02\n",
      "iteration: 35640 loss: 0.0043 lr: 0.02\n",
      "iteration: 35650 loss: 0.0030 lr: 0.02\n",
      "iteration: 35660 loss: 0.0044 lr: 0.02\n",
      "iteration: 35670 loss: 0.0031 lr: 0.02\n",
      "iteration: 35680 loss: 0.0035 lr: 0.02\n",
      "iteration: 35690 loss: 0.0050 lr: 0.02\n",
      "iteration: 35700 loss: 0.0039 lr: 0.02\n",
      "iteration: 35710 loss: 0.0053 lr: 0.02\n",
      "iteration: 35720 loss: 0.0050 lr: 0.02\n",
      "iteration: 35730 loss: 0.0050 lr: 0.02\n",
      "iteration: 35740 loss: 0.0038 lr: 0.02\n",
      "iteration: 35750 loss: 0.0045 lr: 0.02\n",
      "iteration: 35760 loss: 0.0037 lr: 0.02\n",
      "iteration: 35770 loss: 0.0053 lr: 0.02\n",
      "iteration: 35780 loss: 0.0037 lr: 0.02\n",
      "iteration: 35790 loss: 0.0043 lr: 0.02\n",
      "iteration: 35800 loss: 0.0047 lr: 0.02\n",
      "iteration: 35810 loss: 0.0044 lr: 0.02\n",
      "iteration: 35820 loss: 0.0047 lr: 0.02\n",
      "iteration: 35830 loss: 0.0041 lr: 0.02\n",
      "iteration: 35840 loss: 0.0050 lr: 0.02\n",
      "iteration: 35850 loss: 0.0039 lr: 0.02\n",
      "iteration: 35860 loss: 0.0035 lr: 0.02\n",
      "iteration: 35870 loss: 0.0055 lr: 0.02\n",
      "iteration: 35880 loss: 0.0041 lr: 0.02\n",
      "iteration: 35890 loss: 0.0035 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 35900 loss: 0.0040 lr: 0.02\n",
      "iteration: 35910 loss: 0.0051 lr: 0.02\n",
      "iteration: 35920 loss: 0.0048 lr: 0.02\n",
      "iteration: 35930 loss: 0.0035 lr: 0.02\n",
      "iteration: 35940 loss: 0.0051 lr: 0.02\n",
      "iteration: 35950 loss: 0.0072 lr: 0.02\n",
      "iteration: 35960 loss: 0.0046 lr: 0.02\n",
      "iteration: 35970 loss: 0.0040 lr: 0.02\n",
      "iteration: 35980 loss: 0.0054 lr: 0.02\n",
      "iteration: 35990 loss: 0.0071 lr: 0.02\n",
      "iteration: 36000 loss: 0.0048 lr: 0.02\n",
      "iteration: 36010 loss: 0.0055 lr: 0.02\n",
      "iteration: 36020 loss: 0.0050 lr: 0.02\n",
      "iteration: 36030 loss: 0.0039 lr: 0.02\n",
      "iteration: 36040 loss: 0.0040 lr: 0.02\n",
      "iteration: 36050 loss: 0.0046 lr: 0.02\n",
      "iteration: 36060 loss: 0.0045 lr: 0.02\n",
      "iteration: 36070 loss: 0.0051 lr: 0.02\n",
      "iteration: 36080 loss: 0.0057 lr: 0.02\n",
      "iteration: 36090 loss: 0.0041 lr: 0.02\n",
      "iteration: 36100 loss: 0.0043 lr: 0.02\n",
      "iteration: 36110 loss: 0.0055 lr: 0.02\n",
      "iteration: 36120 loss: 0.0040 lr: 0.02\n",
      "iteration: 36130 loss: 0.0066 lr: 0.02\n",
      "iteration: 36140 loss: 0.0048 lr: 0.02\n",
      "iteration: 36150 loss: 0.0048 lr: 0.02\n",
      "iteration: 36160 loss: 0.0056 lr: 0.02\n",
      "iteration: 36170 loss: 0.0042 lr: 0.02\n",
      "iteration: 36180 loss: 0.0040 lr: 0.02\n",
      "iteration: 36190 loss: 0.0049 lr: 0.02\n",
      "iteration: 36200 loss: 0.0043 lr: 0.02\n",
      "iteration: 36210 loss: 0.0042 lr: 0.02\n",
      "iteration: 36220 loss: 0.0045 lr: 0.02\n",
      "iteration: 36230 loss: 0.0068 lr: 0.02\n",
      "iteration: 36240 loss: 0.0067 lr: 0.02\n",
      "iteration: 36250 loss: 0.0037 lr: 0.02\n",
      "iteration: 36260 loss: 0.0038 lr: 0.02\n",
      "iteration: 36270 loss: 0.0044 lr: 0.02\n",
      "iteration: 36280 loss: 0.0037 lr: 0.02\n",
      "iteration: 36290 loss: 0.0035 lr: 0.02\n",
      "iteration: 36300 loss: 0.0044 lr: 0.02\n",
      "iteration: 36310 loss: 0.0048 lr: 0.02\n",
      "iteration: 36320 loss: 0.0049 lr: 0.02\n",
      "iteration: 36330 loss: 0.0060 lr: 0.02\n",
      "iteration: 36340 loss: 0.0044 lr: 0.02\n",
      "iteration: 36350 loss: 0.0048 lr: 0.02\n",
      "iteration: 36360 loss: 0.0038 lr: 0.02\n",
      "iteration: 36370 loss: 0.0041 lr: 0.02\n",
      "iteration: 36380 loss: 0.0055 lr: 0.02\n",
      "iteration: 36390 loss: 0.0037 lr: 0.02\n",
      "iteration: 36400 loss: 0.0033 lr: 0.02\n",
      "iteration: 36410 loss: 0.0040 lr: 0.02\n",
      "iteration: 36420 loss: 0.0088 lr: 0.02\n",
      "iteration: 36430 loss: 0.0049 lr: 0.02\n",
      "iteration: 36440 loss: 0.0062 lr: 0.02\n",
      "iteration: 36450 loss: 0.0057 lr: 0.02\n",
      "iteration: 36460 loss: 0.0042 lr: 0.02\n",
      "iteration: 36470 loss: 0.0052 lr: 0.02\n",
      "iteration: 36480 loss: 0.0046 lr: 0.02\n",
      "iteration: 36490 loss: 0.0044 lr: 0.02\n",
      "iteration: 36500 loss: 0.0044 lr: 0.02\n",
      "iteration: 36510 loss: 0.0070 lr: 0.02\n",
      "iteration: 36520 loss: 0.0037 lr: 0.02\n",
      "iteration: 36530 loss: 0.0038 lr: 0.02\n",
      "iteration: 36540 loss: 0.0046 lr: 0.02\n",
      "iteration: 36550 loss: 0.0043 lr: 0.02\n",
      "iteration: 36560 loss: 0.0034 lr: 0.02\n",
      "iteration: 36570 loss: 0.0037 lr: 0.02\n",
      "iteration: 36580 loss: 0.0033 lr: 0.02\n",
      "iteration: 36590 loss: 0.0054 lr: 0.02\n",
      "iteration: 36600 loss: 0.0056 lr: 0.02\n",
      "iteration: 36610 loss: 0.0075 lr: 0.02\n",
      "iteration: 36620 loss: 0.0043 lr: 0.02\n",
      "iteration: 36630 loss: 0.0069 lr: 0.02\n",
      "iteration: 36640 loss: 0.0049 lr: 0.02\n",
      "iteration: 36650 loss: 0.0044 lr: 0.02\n",
      "iteration: 36660 loss: 0.0059 lr: 0.02\n",
      "iteration: 36670 loss: 0.0051 lr: 0.02\n",
      "iteration: 36680 loss: 0.0039 lr: 0.02\n",
      "iteration: 36690 loss: 0.0063 lr: 0.02\n",
      "iteration: 36700 loss: 0.0053 lr: 0.02\n",
      "iteration: 36710 loss: 0.0035 lr: 0.02\n",
      "iteration: 36720 loss: 0.0048 lr: 0.02\n",
      "iteration: 36730 loss: 0.0050 lr: 0.02\n",
      "iteration: 36740 loss: 0.0045 lr: 0.02\n",
      "iteration: 36750 loss: 0.0050 lr: 0.02\n",
      "iteration: 36760 loss: 0.0057 lr: 0.02\n",
      "iteration: 36770 loss: 0.0038 lr: 0.02\n",
      "iteration: 36780 loss: 0.0050 lr: 0.02\n",
      "iteration: 36790 loss: 0.0054 lr: 0.02\n",
      "iteration: 36800 loss: 0.0051 lr: 0.02\n",
      "iteration: 36810 loss: 0.0054 lr: 0.02\n",
      "iteration: 36820 loss: 0.0045 lr: 0.02\n",
      "iteration: 36830 loss: 0.0048 lr: 0.02\n",
      "iteration: 36840 loss: 0.0039 lr: 0.02\n",
      "iteration: 36850 loss: 0.0043 lr: 0.02\n",
      "iteration: 36860 loss: 0.0052 lr: 0.02\n",
      "iteration: 36870 loss: 0.0067 lr: 0.02\n",
      "iteration: 36880 loss: 0.0041 lr: 0.02\n",
      "iteration: 36890 loss: 0.0046 lr: 0.02\n",
      "iteration: 36900 loss: 0.0053 lr: 0.02\n",
      "iteration: 36910 loss: 0.0046 lr: 0.02\n",
      "iteration: 36920 loss: 0.0059 lr: 0.02\n",
      "iteration: 36930 loss: 0.0041 lr: 0.02\n",
      "iteration: 36940 loss: 0.0040 lr: 0.02\n",
      "iteration: 36950 loss: 0.0038 lr: 0.02\n",
      "iteration: 36960 loss: 0.0051 lr: 0.02\n",
      "iteration: 36970 loss: 0.0049 lr: 0.02\n",
      "iteration: 36980 loss: 0.0040 lr: 0.02\n",
      "iteration: 36990 loss: 0.0043 lr: 0.02\n",
      "iteration: 37000 loss: 0.0041 lr: 0.02\n",
      "iteration: 37010 loss: 0.0045 lr: 0.02\n",
      "iteration: 37020 loss: 0.0067 lr: 0.02\n",
      "iteration: 37030 loss: 0.0045 lr: 0.02\n",
      "iteration: 37040 loss: 0.0044 lr: 0.02\n",
      "iteration: 37050 loss: 0.0039 lr: 0.02\n",
      "iteration: 37060 loss: 0.0054 lr: 0.02\n",
      "iteration: 37070 loss: 0.0051 lr: 0.02\n",
      "iteration: 37080 loss: 0.0066 lr: 0.02\n",
      "iteration: 37090 loss: 0.0055 lr: 0.02\n",
      "iteration: 37100 loss: 0.0061 lr: 0.02\n",
      "iteration: 37110 loss: 0.0047 lr: 0.02\n",
      "iteration: 37120 loss: 0.0045 lr: 0.02\n",
      "iteration: 37130 loss: 0.0047 lr: 0.02\n",
      "iteration: 37140 loss: 0.0047 lr: 0.02\n",
      "iteration: 37150 loss: 0.0044 lr: 0.02\n",
      "iteration: 37160 loss: 0.0049 lr: 0.02\n",
      "iteration: 37170 loss: 0.0033 lr: 0.02\n",
      "iteration: 37180 loss: 0.0053 lr: 0.02\n",
      "iteration: 37190 loss: 0.0041 lr: 0.02\n",
      "iteration: 37200 loss: 0.0046 lr: 0.02\n",
      "iteration: 37210 loss: 0.0043 lr: 0.02\n",
      "iteration: 37220 loss: 0.0046 lr: 0.02\n",
      "iteration: 37230 loss: 0.0038 lr: 0.02\n",
      "iteration: 37240 loss: 0.0042 lr: 0.02\n",
      "iteration: 37250 loss: 0.0052 lr: 0.02\n",
      "iteration: 37260 loss: 0.0032 lr: 0.02\n",
      "iteration: 37270 loss: 0.0049 lr: 0.02\n",
      "iteration: 37280 loss: 0.0042 lr: 0.02\n",
      "iteration: 37290 loss: 0.0036 lr: 0.02\n",
      "iteration: 37300 loss: 0.0038 lr: 0.02\n",
      "iteration: 37310 loss: 0.0051 lr: 0.02\n",
      "iteration: 37320 loss: 0.0040 lr: 0.02\n",
      "iteration: 37330 loss: 0.0051 lr: 0.02\n",
      "iteration: 37340 loss: 0.0034 lr: 0.02\n",
      "iteration: 37350 loss: 0.0051 lr: 0.02\n",
      "iteration: 37360 loss: 0.0037 lr: 0.02\n",
      "iteration: 37370 loss: 0.0049 lr: 0.02\n",
      "iteration: 37380 loss: 0.0055 lr: 0.02\n",
      "iteration: 37390 loss: 0.0046 lr: 0.02\n",
      "iteration: 37400 loss: 0.0057 lr: 0.02\n",
      "iteration: 37410 loss: 0.0044 lr: 0.02\n",
      "iteration: 37420 loss: 0.0035 lr: 0.02\n",
      "iteration: 37430 loss: 0.0045 lr: 0.02\n",
      "iteration: 37440 loss: 0.0044 lr: 0.02\n",
      "iteration: 37450 loss: 0.0079 lr: 0.02\n",
      "iteration: 37460 loss: 0.0041 lr: 0.02\n",
      "iteration: 37470 loss: 0.0047 lr: 0.02\n",
      "iteration: 37480 loss: 0.0041 lr: 0.02\n",
      "iteration: 37490 loss: 0.0057 lr: 0.02\n",
      "iteration: 37500 loss: 0.0054 lr: 0.02\n",
      "iteration: 37510 loss: 0.0046 lr: 0.02\n",
      "iteration: 37520 loss: 0.0035 lr: 0.02\n",
      "iteration: 37530 loss: 0.0049 lr: 0.02\n",
      "iteration: 37540 loss: 0.0054 lr: 0.02\n",
      "iteration: 37550 loss: 0.0044 lr: 0.02\n",
      "iteration: 37560 loss: 0.0039 lr: 0.02\n",
      "iteration: 37570 loss: 0.0042 lr: 0.02\n",
      "iteration: 37580 loss: 0.0051 lr: 0.02\n",
      "iteration: 37590 loss: 0.0052 lr: 0.02\n",
      "iteration: 37600 loss: 0.0038 lr: 0.02\n",
      "iteration: 37610 loss: 0.0058 lr: 0.02\n",
      "iteration: 37620 loss: 0.0043 lr: 0.02\n",
      "iteration: 37630 loss: 0.0037 lr: 0.02\n",
      "iteration: 37640 loss: 0.0039 lr: 0.02\n",
      "iteration: 37650 loss: 0.0041 lr: 0.02\n",
      "iteration: 37660 loss: 0.0044 lr: 0.02\n",
      "iteration: 37670 loss: 0.0046 lr: 0.02\n",
      "iteration: 37680 loss: 0.0050 lr: 0.02\n",
      "iteration: 37690 loss: 0.0051 lr: 0.02\n",
      "iteration: 37700 loss: 0.0057 lr: 0.02\n",
      "iteration: 37710 loss: 0.0046 lr: 0.02\n",
      "iteration: 37720 loss: 0.0048 lr: 0.02\n",
      "iteration: 37730 loss: 0.0051 lr: 0.02\n",
      "iteration: 37740 loss: 0.0036 lr: 0.02\n",
      "iteration: 37750 loss: 0.0050 lr: 0.02\n",
      "iteration: 37760 loss: 0.0046 lr: 0.02\n",
      "iteration: 37770 loss: 0.0038 lr: 0.02\n",
      "iteration: 37780 loss: 0.0031 lr: 0.02\n",
      "iteration: 37790 loss: 0.0049 lr: 0.02\n",
      "iteration: 37800 loss: 0.0042 lr: 0.02\n",
      "iteration: 37810 loss: 0.0038 lr: 0.02\n",
      "iteration: 37820 loss: 0.0062 lr: 0.02\n",
      "iteration: 37830 loss: 0.0048 lr: 0.02\n",
      "iteration: 37840 loss: 0.0041 lr: 0.02\n",
      "iteration: 37850 loss: 0.0064 lr: 0.02\n",
      "iteration: 37860 loss: 0.0041 lr: 0.02\n",
      "iteration: 37870 loss: 0.0044 lr: 0.02\n",
      "iteration: 37880 loss: 0.0042 lr: 0.02\n",
      "iteration: 37890 loss: 0.0047 lr: 0.02\n",
      "iteration: 37900 loss: 0.0050 lr: 0.02\n",
      "iteration: 37910 loss: 0.0039 lr: 0.02\n",
      "iteration: 37920 loss: 0.0036 lr: 0.02\n",
      "iteration: 37930 loss: 0.0036 lr: 0.02\n",
      "iteration: 37940 loss: 0.0051 lr: 0.02\n",
      "iteration: 37950 loss: 0.0053 lr: 0.02\n",
      "iteration: 37960 loss: 0.0039 lr: 0.02\n",
      "iteration: 37970 loss: 0.0057 lr: 0.02\n",
      "iteration: 37980 loss: 0.0034 lr: 0.02\n",
      "iteration: 37990 loss: 0.0047 lr: 0.02\n",
      "iteration: 38000 loss: 0.0052 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 38010 loss: 0.0043 lr: 0.02\n",
      "iteration: 38020 loss: 0.0041 lr: 0.02\n",
      "iteration: 38030 loss: 0.0049 lr: 0.02\n",
      "iteration: 38040 loss: 0.0046 lr: 0.02\n",
      "iteration: 38050 loss: 0.0046 lr: 0.02\n",
      "iteration: 38060 loss: 0.0047 lr: 0.02\n",
      "iteration: 38070 loss: 0.0045 lr: 0.02\n",
      "iteration: 38080 loss: 0.0051 lr: 0.02\n",
      "iteration: 38090 loss: 0.0055 lr: 0.02\n",
      "iteration: 38100 loss: 0.0080 lr: 0.02\n",
      "iteration: 38110 loss: 0.0071 lr: 0.02\n",
      "iteration: 38120 loss: 0.0043 lr: 0.02\n",
      "iteration: 38130 loss: 0.0053 lr: 0.02\n",
      "iteration: 38140 loss: 0.0040 lr: 0.02\n",
      "iteration: 38150 loss: 0.0031 lr: 0.02\n",
      "iteration: 38160 loss: 0.0036 lr: 0.02\n",
      "iteration: 38170 loss: 0.0037 lr: 0.02\n",
      "iteration: 38180 loss: 0.0041 lr: 0.02\n",
      "iteration: 38190 loss: 0.0051 lr: 0.02\n",
      "iteration: 38200 loss: 0.0056 lr: 0.02\n",
      "iteration: 38210 loss: 0.0058 lr: 0.02\n",
      "iteration: 38220 loss: 0.0041 lr: 0.02\n",
      "iteration: 38230 loss: 0.0043 lr: 0.02\n",
      "iteration: 38240 loss: 0.0043 lr: 0.02\n",
      "iteration: 38250 loss: 0.0042 lr: 0.02\n",
      "iteration: 38260 loss: 0.0030 lr: 0.02\n",
      "iteration: 38270 loss: 0.0039 lr: 0.02\n",
      "iteration: 38280 loss: 0.0046 lr: 0.02\n",
      "iteration: 38290 loss: 0.0038 lr: 0.02\n",
      "iteration: 38300 loss: 0.0062 lr: 0.02\n",
      "iteration: 38310 loss: 0.0039 lr: 0.02\n",
      "iteration: 38320 loss: 0.0044 lr: 0.02\n",
      "iteration: 38330 loss: 0.0041 lr: 0.02\n",
      "iteration: 38340 loss: 0.0037 lr: 0.02\n",
      "iteration: 38350 loss: 0.0036 lr: 0.02\n",
      "iteration: 38360 loss: 0.0040 lr: 0.02\n",
      "iteration: 38370 loss: 0.0052 lr: 0.02\n",
      "iteration: 38380 loss: 0.0044 lr: 0.02\n",
      "iteration: 38390 loss: 0.0042 lr: 0.02\n",
      "iteration: 38400 loss: 0.0041 lr: 0.02\n",
      "iteration: 38410 loss: 0.0040 lr: 0.02\n",
      "iteration: 38420 loss: 0.0046 lr: 0.02\n",
      "iteration: 38430 loss: 0.0050 lr: 0.02\n",
      "iteration: 38440 loss: 0.0047 lr: 0.02\n",
      "iteration: 38450 loss: 0.0043 lr: 0.02\n",
      "iteration: 38460 loss: 0.0029 lr: 0.02\n",
      "iteration: 38470 loss: 0.0061 lr: 0.02\n",
      "iteration: 38480 loss: 0.0043 lr: 0.02\n",
      "iteration: 38490 loss: 0.0040 lr: 0.02\n",
      "iteration: 38500 loss: 0.0038 lr: 0.02\n",
      "iteration: 38510 loss: 0.0060 lr: 0.02\n",
      "iteration: 38520 loss: 0.0038 lr: 0.02\n",
      "iteration: 38530 loss: 0.0053 lr: 0.02\n",
      "iteration: 38540 loss: 0.0031 lr: 0.02\n",
      "iteration: 38550 loss: 0.0039 lr: 0.02\n",
      "iteration: 38560 loss: 0.0052 lr: 0.02\n",
      "iteration: 38570 loss: 0.0038 lr: 0.02\n",
      "iteration: 38580 loss: 0.0038 lr: 0.02\n",
      "iteration: 38590 loss: 0.0053 lr: 0.02\n",
      "iteration: 38600 loss: 0.0063 lr: 0.02\n",
      "iteration: 38610 loss: 0.0040 lr: 0.02\n",
      "iteration: 38620 loss: 0.0034 lr: 0.02\n",
      "iteration: 38630 loss: 0.0054 lr: 0.02\n",
      "iteration: 38640 loss: 0.0044 lr: 0.02\n",
      "iteration: 38650 loss: 0.0049 lr: 0.02\n",
      "iteration: 38660 loss: 0.0048 lr: 0.02\n",
      "iteration: 38670 loss: 0.0045 lr: 0.02\n",
      "iteration: 38680 loss: 0.0040 lr: 0.02\n",
      "iteration: 38690 loss: 0.0034 lr: 0.02\n",
      "iteration: 38700 loss: 0.0050 lr: 0.02\n",
      "iteration: 38710 loss: 0.0050 lr: 0.02\n",
      "iteration: 38720 loss: 0.0041 lr: 0.02\n",
      "iteration: 38730 loss: 0.0048 lr: 0.02\n",
      "iteration: 38740 loss: 0.0041 lr: 0.02\n",
      "iteration: 38750 loss: 0.0034 lr: 0.02\n",
      "iteration: 38760 loss: 0.0045 lr: 0.02\n",
      "iteration: 38770 loss: 0.0037 lr: 0.02\n",
      "iteration: 38780 loss: 0.0037 lr: 0.02\n",
      "iteration: 38790 loss: 0.0043 lr: 0.02\n",
      "iteration: 38800 loss: 0.0040 lr: 0.02\n",
      "iteration: 38810 loss: 0.0049 lr: 0.02\n",
      "iteration: 38820 loss: 0.0036 lr: 0.02\n",
      "iteration: 38830 loss: 0.0055 lr: 0.02\n",
      "iteration: 38840 loss: 0.0040 lr: 0.02\n",
      "iteration: 38850 loss: 0.0039 lr: 0.02\n",
      "iteration: 38860 loss: 0.0037 lr: 0.02\n",
      "iteration: 38870 loss: 0.0045 lr: 0.02\n",
      "iteration: 38880 loss: 0.0044 lr: 0.02\n",
      "iteration: 38890 loss: 0.0039 lr: 0.02\n",
      "iteration: 38900 loss: 0.0037 lr: 0.02\n",
      "iteration: 38910 loss: 0.0047 lr: 0.02\n",
      "iteration: 38920 loss: 0.0041 lr: 0.02\n",
      "iteration: 38930 loss: 0.0032 lr: 0.02\n",
      "iteration: 38940 loss: 0.0040 lr: 0.02\n",
      "iteration: 38950 loss: 0.0046 lr: 0.02\n",
      "iteration: 38960 loss: 0.0042 lr: 0.02\n",
      "iteration: 38970 loss: 0.0036 lr: 0.02\n",
      "iteration: 38980 loss: 0.0040 lr: 0.02\n",
      "iteration: 38990 loss: 0.0042 lr: 0.02\n",
      "iteration: 39000 loss: 0.0034 lr: 0.02\n",
      "iteration: 39010 loss: 0.0050 lr: 0.02\n",
      "iteration: 39020 loss: 0.0058 lr: 0.02\n",
      "iteration: 39030 loss: 0.0052 lr: 0.02\n",
      "iteration: 39040 loss: 0.0038 lr: 0.02\n",
      "iteration: 39050 loss: 0.0045 lr: 0.02\n",
      "iteration: 39060 loss: 0.0046 lr: 0.02\n",
      "iteration: 39070 loss: 0.0054 lr: 0.02\n",
      "iteration: 39080 loss: 0.0043 lr: 0.02\n",
      "iteration: 39090 loss: 0.0037 lr: 0.02\n",
      "iteration: 39100 loss: 0.0041 lr: 0.02\n",
      "iteration: 39110 loss: 0.0047 lr: 0.02\n",
      "iteration: 39120 loss: 0.0035 lr: 0.02\n",
      "iteration: 39130 loss: 0.0036 lr: 0.02\n",
      "iteration: 39140 loss: 0.0031 lr: 0.02\n",
      "iteration: 39150 loss: 0.0060 lr: 0.02\n",
      "iteration: 39160 loss: 0.0044 lr: 0.02\n",
      "iteration: 39170 loss: 0.0041 lr: 0.02\n",
      "iteration: 39180 loss: 0.0046 lr: 0.02\n",
      "iteration: 39190 loss: 0.0053 lr: 0.02\n",
      "iteration: 39200 loss: 0.0056 lr: 0.02\n",
      "iteration: 39210 loss: 0.0049 lr: 0.02\n",
      "iteration: 39220 loss: 0.0035 lr: 0.02\n",
      "iteration: 39230 loss: 0.0037 lr: 0.02\n",
      "iteration: 39240 loss: 0.0042 lr: 0.02\n",
      "iteration: 39250 loss: 0.0058 lr: 0.02\n",
      "iteration: 39260 loss: 0.0044 lr: 0.02\n",
      "iteration: 39270 loss: 0.0052 lr: 0.02\n",
      "iteration: 39280 loss: 0.0047 lr: 0.02\n",
      "iteration: 39290 loss: 0.0038 lr: 0.02\n",
      "iteration: 39300 loss: 0.0041 lr: 0.02\n",
      "iteration: 39310 loss: 0.0054 lr: 0.02\n",
      "iteration: 39320 loss: 0.0045 lr: 0.02\n",
      "iteration: 39330 loss: 0.0060 lr: 0.02\n",
      "iteration: 39340 loss: 0.0053 lr: 0.02\n",
      "iteration: 39350 loss: 0.0037 lr: 0.02\n",
      "iteration: 39360 loss: 0.0048 lr: 0.02\n",
      "iteration: 39370 loss: 0.0047 lr: 0.02\n",
      "iteration: 39380 loss: 0.0038 lr: 0.02\n",
      "iteration: 39390 loss: 0.0043 lr: 0.02\n",
      "iteration: 39400 loss: 0.0033 lr: 0.02\n",
      "iteration: 39410 loss: 0.0040 lr: 0.02\n",
      "iteration: 39420 loss: 0.0033 lr: 0.02\n",
      "iteration: 39430 loss: 0.0043 lr: 0.02\n",
      "iteration: 39440 loss: 0.0038 lr: 0.02\n",
      "iteration: 39450 loss: 0.0038 lr: 0.02\n",
      "iteration: 39460 loss: 0.0038 lr: 0.02\n",
      "iteration: 39470 loss: 0.0060 lr: 0.02\n",
      "iteration: 39480 loss: 0.0050 lr: 0.02\n",
      "iteration: 39490 loss: 0.0053 lr: 0.02\n",
      "iteration: 39500 loss: 0.0044 lr: 0.02\n",
      "iteration: 39510 loss: 0.0049 lr: 0.02\n",
      "iteration: 39520 loss: 0.0041 lr: 0.02\n",
      "iteration: 39530 loss: 0.0052 lr: 0.02\n",
      "iteration: 39540 loss: 0.0042 lr: 0.02\n",
      "iteration: 39550 loss: 0.0047 lr: 0.02\n",
      "iteration: 39560 loss: 0.0041 lr: 0.02\n",
      "iteration: 39570 loss: 0.0035 lr: 0.02\n",
      "iteration: 39580 loss: 0.0042 lr: 0.02\n",
      "iteration: 39590 loss: 0.0065 lr: 0.02\n",
      "iteration: 39600 loss: 0.0056 lr: 0.02\n",
      "iteration: 39610 loss: 0.0040 lr: 0.02\n",
      "iteration: 39620 loss: 0.0056 lr: 0.02\n",
      "iteration: 39630 loss: 0.0042 lr: 0.02\n",
      "iteration: 39640 loss: 0.0035 lr: 0.02\n",
      "iteration: 39650 loss: 0.0046 lr: 0.02\n",
      "iteration: 39660 loss: 0.0048 lr: 0.02\n",
      "iteration: 39670 loss: 0.0053 lr: 0.02\n",
      "iteration: 39680 loss: 0.0044 lr: 0.02\n",
      "iteration: 39690 loss: 0.0052 lr: 0.02\n",
      "iteration: 39700 loss: 0.0056 lr: 0.02\n",
      "iteration: 39710 loss: 0.0037 lr: 0.02\n",
      "iteration: 39720 loss: 0.0047 lr: 0.02\n",
      "iteration: 39730 loss: 0.0056 lr: 0.02\n",
      "iteration: 39740 loss: 0.0045 lr: 0.02\n",
      "iteration: 39750 loss: 0.0037 lr: 0.02\n",
      "iteration: 39760 loss: 0.0040 lr: 0.02\n",
      "iteration: 39770 loss: 0.0040 lr: 0.02\n",
      "iteration: 39780 loss: 0.0051 lr: 0.02\n",
      "iteration: 39790 loss: 0.0051 lr: 0.02\n",
      "iteration: 39800 loss: 0.0036 lr: 0.02\n",
      "iteration: 39810 loss: 0.0040 lr: 0.02\n",
      "iteration: 39820 loss: 0.0044 lr: 0.02\n",
      "iteration: 39830 loss: 0.0045 lr: 0.02\n",
      "iteration: 39840 loss: 0.0048 lr: 0.02\n",
      "iteration: 39850 loss: 0.0048 lr: 0.02\n",
      "iteration: 39860 loss: 0.0034 lr: 0.02\n",
      "iteration: 39870 loss: 0.0063 lr: 0.02\n",
      "iteration: 39880 loss: 0.0035 lr: 0.02\n",
      "iteration: 39890 loss: 0.0058 lr: 0.02\n",
      "iteration: 39900 loss: 0.0048 lr: 0.02\n",
      "iteration: 39910 loss: 0.0045 lr: 0.02\n",
      "iteration: 39920 loss: 0.0038 lr: 0.02\n",
      "iteration: 39930 loss: 0.0064 lr: 0.02\n",
      "iteration: 39940 loss: 0.0045 lr: 0.02\n",
      "iteration: 39950 loss: 0.0038 lr: 0.02\n",
      "iteration: 39960 loss: 0.0040 lr: 0.02\n",
      "iteration: 39970 loss: 0.0051 lr: 0.02\n",
      "iteration: 39980 loss: 0.0043 lr: 0.02\n",
      "iteration: 39990 loss: 0.0048 lr: 0.02\n",
      "iteration: 40000 loss: 0.0053 lr: 0.02\n",
      "iteration: 40010 loss: 0.0046 lr: 0.02\n",
      "iteration: 40020 loss: 0.0051 lr: 0.02\n",
      "iteration: 40030 loss: 0.0038 lr: 0.02\n",
      "iteration: 40040 loss: 0.0048 lr: 0.02\n",
      "iteration: 40050 loss: 0.0043 lr: 0.02\n",
      "iteration: 40060 loss: 0.0060 lr: 0.02\n",
      "iteration: 40070 loss: 0.0049 lr: 0.02\n",
      "iteration: 40080 loss: 0.0047 lr: 0.02\n",
      "iteration: 40090 loss: 0.0039 lr: 0.02\n",
      "iteration: 40100 loss: 0.0033 lr: 0.02\n",
      "iteration: 40110 loss: 0.0034 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 40120 loss: 0.0040 lr: 0.02\n",
      "iteration: 40130 loss: 0.0042 lr: 0.02\n",
      "iteration: 40140 loss: 0.0045 lr: 0.02\n",
      "iteration: 40150 loss: 0.0040 lr: 0.02\n",
      "iteration: 40160 loss: 0.0051 lr: 0.02\n",
      "iteration: 40170 loss: 0.0057 lr: 0.02\n",
      "iteration: 40180 loss: 0.0047 lr: 0.02\n",
      "iteration: 40190 loss: 0.0057 lr: 0.02\n",
      "iteration: 40200 loss: 0.0052 lr: 0.02\n",
      "iteration: 40210 loss: 0.0053 lr: 0.02\n",
      "iteration: 40220 loss: 0.0038 lr: 0.02\n",
      "iteration: 40230 loss: 0.0046 lr: 0.02\n",
      "iteration: 40240 loss: 0.0043 lr: 0.02\n",
      "iteration: 40250 loss: 0.0039 lr: 0.02\n",
      "iteration: 40260 loss: 0.0043 lr: 0.02\n",
      "iteration: 40270 loss: 0.0044 lr: 0.02\n",
      "iteration: 40280 loss: 0.0045 lr: 0.02\n",
      "iteration: 40290 loss: 0.0055 lr: 0.02\n",
      "iteration: 40300 loss: 0.0044 lr: 0.02\n",
      "iteration: 40310 loss: 0.0043 lr: 0.02\n",
      "iteration: 40320 loss: 0.0050 lr: 0.02\n",
      "iteration: 40330 loss: 0.0049 lr: 0.02\n",
      "iteration: 40340 loss: 0.0063 lr: 0.02\n",
      "iteration: 40350 loss: 0.0036 lr: 0.02\n",
      "iteration: 40360 loss: 0.0069 lr: 0.02\n",
      "iteration: 40370 loss: 0.0047 lr: 0.02\n",
      "iteration: 40380 loss: 0.0046 lr: 0.02\n",
      "iteration: 40390 loss: 0.0059 lr: 0.02\n",
      "iteration: 40400 loss: 0.0050 lr: 0.02\n",
      "iteration: 40410 loss: 0.0051 lr: 0.02\n",
      "iteration: 40420 loss: 0.0038 lr: 0.02\n",
      "iteration: 40430 loss: 0.0034 lr: 0.02\n",
      "iteration: 40440 loss: 0.0049 lr: 0.02\n",
      "iteration: 40450 loss: 0.0042 lr: 0.02\n",
      "iteration: 40460 loss: 0.0055 lr: 0.02\n",
      "iteration: 40470 loss: 0.0040 lr: 0.02\n",
      "iteration: 40480 loss: 0.0041 lr: 0.02\n",
      "iteration: 40490 loss: 0.0032 lr: 0.02\n",
      "iteration: 40500 loss: 0.0044 lr: 0.02\n",
      "iteration: 40510 loss: 0.0049 lr: 0.02\n",
      "iteration: 40520 loss: 0.0042 lr: 0.02\n",
      "iteration: 40530 loss: 0.0040 lr: 0.02\n",
      "iteration: 40540 loss: 0.0042 lr: 0.02\n",
      "iteration: 40550 loss: 0.0037 lr: 0.02\n",
      "iteration: 40560 loss: 0.0049 lr: 0.02\n",
      "iteration: 40570 loss: 0.0050 lr: 0.02\n",
      "iteration: 40580 loss: 0.0039 lr: 0.02\n",
      "iteration: 40590 loss: 0.0044 lr: 0.02\n",
      "iteration: 40600 loss: 0.0035 lr: 0.02\n",
      "iteration: 40610 loss: 0.0044 lr: 0.02\n",
      "iteration: 40620 loss: 0.0036 lr: 0.02\n",
      "iteration: 40630 loss: 0.0036 lr: 0.02\n",
      "iteration: 40640 loss: 0.0045 lr: 0.02\n",
      "iteration: 40650 loss: 0.0043 lr: 0.02\n",
      "iteration: 40660 loss: 0.0046 lr: 0.02\n",
      "iteration: 40670 loss: 0.0062 lr: 0.02\n",
      "iteration: 40680 loss: 0.0035 lr: 0.02\n",
      "iteration: 40690 loss: 0.0045 lr: 0.02\n",
      "iteration: 40700 loss: 0.0036 lr: 0.02\n",
      "iteration: 40710 loss: 0.0050 lr: 0.02\n",
      "iteration: 40720 loss: 0.0050 lr: 0.02\n",
      "iteration: 40730 loss: 0.0072 lr: 0.02\n",
      "iteration: 40740 loss: 0.0067 lr: 0.02\n",
      "iteration: 40750 loss: 0.0042 lr: 0.02\n",
      "iteration: 40760 loss: 0.0051 lr: 0.02\n",
      "iteration: 40770 loss: 0.0056 lr: 0.02\n",
      "iteration: 40780 loss: 0.0046 lr: 0.02\n",
      "iteration: 40790 loss: 0.0037 lr: 0.02\n",
      "iteration: 40800 loss: 0.0048 lr: 0.02\n",
      "iteration: 40810 loss: 0.0059 lr: 0.02\n",
      "iteration: 40820 loss: 0.0044 lr: 0.02\n",
      "iteration: 40830 loss: 0.0050 lr: 0.02\n",
      "iteration: 40840 loss: 0.0042 lr: 0.02\n",
      "iteration: 40850 loss: 0.0032 lr: 0.02\n",
      "iteration: 40860 loss: 0.0049 lr: 0.02\n",
      "iteration: 40870 loss: 0.0036 lr: 0.02\n",
      "iteration: 40880 loss: 0.0043 lr: 0.02\n",
      "iteration: 40890 loss: 0.0048 lr: 0.02\n",
      "iteration: 40900 loss: 0.0040 lr: 0.02\n",
      "iteration: 40910 loss: 0.0063 lr: 0.02\n",
      "iteration: 40920 loss: 0.0045 lr: 0.02\n",
      "iteration: 40930 loss: 0.0035 lr: 0.02\n",
      "iteration: 40940 loss: 0.0052 lr: 0.02\n",
      "iteration: 40950 loss: 0.0036 lr: 0.02\n",
      "iteration: 40960 loss: 0.0039 lr: 0.02\n",
      "iteration: 40970 loss: 0.0044 lr: 0.02\n",
      "iteration: 40980 loss: 0.0043 lr: 0.02\n",
      "iteration: 40990 loss: 0.0047 lr: 0.02\n",
      "iteration: 41000 loss: 0.0036 lr: 0.02\n",
      "iteration: 41010 loss: 0.0040 lr: 0.02\n",
      "iteration: 41020 loss: 0.0047 lr: 0.02\n",
      "iteration: 41030 loss: 0.0049 lr: 0.02\n",
      "iteration: 41040 loss: 0.0044 lr: 0.02\n",
      "iteration: 41050 loss: 0.0033 lr: 0.02\n",
      "iteration: 41060 loss: 0.0044 lr: 0.02\n",
      "iteration: 41070 loss: 0.0029 lr: 0.02\n",
      "iteration: 41080 loss: 0.0043 lr: 0.02\n",
      "iteration: 41090 loss: 0.0045 lr: 0.02\n",
      "iteration: 41100 loss: 0.0045 lr: 0.02\n",
      "iteration: 41110 loss: 0.0042 lr: 0.02\n",
      "iteration: 41120 loss: 0.0055 lr: 0.02\n",
      "iteration: 41130 loss: 0.0038 lr: 0.02\n",
      "iteration: 41140 loss: 0.0035 lr: 0.02\n",
      "iteration: 41150 loss: 0.0035 lr: 0.02\n",
      "iteration: 41160 loss: 0.0052 lr: 0.02\n",
      "iteration: 41170 loss: 0.0044 lr: 0.02\n",
      "iteration: 41180 loss: 0.0043 lr: 0.02\n",
      "iteration: 41190 loss: 0.0036 lr: 0.02\n",
      "iteration: 41200 loss: 0.0037 lr: 0.02\n",
      "iteration: 41210 loss: 0.0039 lr: 0.02\n",
      "iteration: 41220 loss: 0.0055 lr: 0.02\n",
      "iteration: 41230 loss: 0.0029 lr: 0.02\n",
      "iteration: 41240 loss: 0.0042 lr: 0.02\n",
      "iteration: 41250 loss: 0.0036 lr: 0.02\n",
      "iteration: 41260 loss: 0.0037 lr: 0.02\n",
      "iteration: 41270 loss: 0.0041 lr: 0.02\n",
      "iteration: 41280 loss: 0.0035 lr: 0.02\n",
      "iteration: 41290 loss: 0.0040 lr: 0.02\n",
      "iteration: 41300 loss: 0.0041 lr: 0.02\n",
      "iteration: 41310 loss: 0.0041 lr: 0.02\n",
      "iteration: 41320 loss: 0.0043 lr: 0.02\n",
      "iteration: 41330 loss: 0.0035 lr: 0.02\n",
      "iteration: 41340 loss: 0.0037 lr: 0.02\n",
      "iteration: 41350 loss: 0.0059 lr: 0.02\n",
      "iteration: 41360 loss: 0.0032 lr: 0.02\n",
      "iteration: 41370 loss: 0.0040 lr: 0.02\n",
      "iteration: 41380 loss: 0.0037 lr: 0.02\n",
      "iteration: 41390 loss: 0.0046 lr: 0.02\n",
      "iteration: 41400 loss: 0.0049 lr: 0.02\n",
      "iteration: 41410 loss: 0.0044 lr: 0.02\n",
      "iteration: 41420 loss: 0.0039 lr: 0.02\n",
      "iteration: 41430 loss: 0.0038 lr: 0.02\n",
      "iteration: 41440 loss: 0.0041 lr: 0.02\n",
      "iteration: 41450 loss: 0.0054 lr: 0.02\n",
      "iteration: 41460 loss: 0.0046 lr: 0.02\n",
      "iteration: 41470 loss: 0.0037 lr: 0.02\n",
      "iteration: 41480 loss: 0.0033 lr: 0.02\n",
      "iteration: 41490 loss: 0.0039 lr: 0.02\n",
      "iteration: 41500 loss: 0.0043 lr: 0.02\n",
      "iteration: 41510 loss: 0.0031 lr: 0.02\n",
      "iteration: 41520 loss: 0.0045 lr: 0.02\n",
      "iteration: 41530 loss: 0.0040 lr: 0.02\n",
      "iteration: 41540 loss: 0.0044 lr: 0.02\n",
      "iteration: 41550 loss: 0.0040 lr: 0.02\n",
      "iteration: 41560 loss: 0.0043 lr: 0.02\n",
      "iteration: 41570 loss: 0.0045 lr: 0.02\n",
      "iteration: 41580 loss: 0.0061 lr: 0.02\n",
      "iteration: 41590 loss: 0.0050 lr: 0.02\n",
      "iteration: 41600 loss: 0.0051 lr: 0.02\n",
      "iteration: 41610 loss: 0.0049 lr: 0.02\n",
      "iteration: 41620 loss: 0.0047 lr: 0.02\n",
      "iteration: 41630 loss: 0.0041 lr: 0.02\n",
      "iteration: 41640 loss: 0.0048 lr: 0.02\n",
      "iteration: 41650 loss: 0.0037 lr: 0.02\n",
      "iteration: 41660 loss: 0.0038 lr: 0.02\n",
      "iteration: 41670 loss: 0.0030 lr: 0.02\n",
      "iteration: 41680 loss: 0.0032 lr: 0.02\n",
      "iteration: 41690 loss: 0.0039 lr: 0.02\n",
      "iteration: 41700 loss: 0.0031 lr: 0.02\n",
      "iteration: 41710 loss: 0.0053 lr: 0.02\n",
      "iteration: 41720 loss: 0.0062 lr: 0.02\n",
      "iteration: 41730 loss: 0.0045 lr: 0.02\n",
      "iteration: 41740 loss: 0.0047 lr: 0.02\n",
      "iteration: 41750 loss: 0.0046 lr: 0.02\n",
      "iteration: 41760 loss: 0.0044 lr: 0.02\n",
      "iteration: 41770 loss: 0.0051 lr: 0.02\n",
      "iteration: 41780 loss: 0.0045 lr: 0.02\n",
      "iteration: 41790 loss: 0.0039 lr: 0.02\n",
      "iteration: 41800 loss: 0.0042 lr: 0.02\n",
      "iteration: 41810 loss: 0.0044 lr: 0.02\n",
      "iteration: 41820 loss: 0.0043 lr: 0.02\n",
      "iteration: 41830 loss: 0.0047 lr: 0.02\n",
      "iteration: 41840 loss: 0.0051 lr: 0.02\n",
      "iteration: 41850 loss: 0.0046 lr: 0.02\n",
      "iteration: 41860 loss: 0.0042 lr: 0.02\n",
      "iteration: 41870 loss: 0.0050 lr: 0.02\n",
      "iteration: 41880 loss: 0.0054 lr: 0.02\n",
      "iteration: 41890 loss: 0.0054 lr: 0.02\n",
      "iteration: 41900 loss: 0.0038 lr: 0.02\n",
      "iteration: 41910 loss: 0.0041 lr: 0.02\n",
      "iteration: 41920 loss: 0.0047 lr: 0.02\n",
      "iteration: 41930 loss: 0.0043 lr: 0.02\n",
      "iteration: 41940 loss: 0.0048 lr: 0.02\n",
      "iteration: 41950 loss: 0.0050 lr: 0.02\n",
      "iteration: 41960 loss: 0.0054 lr: 0.02\n",
      "iteration: 41970 loss: 0.0034 lr: 0.02\n",
      "iteration: 41980 loss: 0.0040 lr: 0.02\n",
      "iteration: 41990 loss: 0.0043 lr: 0.02\n",
      "iteration: 42000 loss: 0.0040 lr: 0.02\n",
      "iteration: 42010 loss: 0.0041 lr: 0.02\n",
      "iteration: 42020 loss: 0.0032 lr: 0.02\n",
      "iteration: 42030 loss: 0.0046 lr: 0.02\n",
      "iteration: 42040 loss: 0.0044 lr: 0.02\n",
      "iteration: 42050 loss: 0.0044 lr: 0.02\n",
      "iteration: 42060 loss: 0.0048 lr: 0.02\n",
      "iteration: 42070 loss: 0.0040 lr: 0.02\n",
      "iteration: 42080 loss: 0.0042 lr: 0.02\n",
      "iteration: 42090 loss: 0.0033 lr: 0.02\n",
      "iteration: 42100 loss: 0.0036 lr: 0.02\n",
      "iteration: 42110 loss: 0.0043 lr: 0.02\n",
      "iteration: 42120 loss: 0.0052 lr: 0.02\n",
      "iteration: 42130 loss: 0.0033 lr: 0.02\n",
      "iteration: 42140 loss: 0.0050 lr: 0.02\n",
      "iteration: 42150 loss: 0.0035 lr: 0.02\n",
      "iteration: 42160 loss: 0.0037 lr: 0.02\n",
      "iteration: 42170 loss: 0.0035 lr: 0.02\n",
      "iteration: 42180 loss: 0.0034 lr: 0.02\n",
      "iteration: 42190 loss: 0.0038 lr: 0.02\n",
      "iteration: 42200 loss: 0.0055 lr: 0.02\n",
      "iteration: 42210 loss: 0.0034 lr: 0.02\n",
      "iteration: 42220 loss: 0.0041 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 42230 loss: 0.0040 lr: 0.02\n",
      "iteration: 42240 loss: 0.0043 lr: 0.02\n",
      "iteration: 42250 loss: 0.0046 lr: 0.02\n",
      "iteration: 42260 loss: 0.0041 lr: 0.02\n",
      "iteration: 42270 loss: 0.0032 lr: 0.02\n",
      "iteration: 42280 loss: 0.0047 lr: 0.02\n",
      "iteration: 42290 loss: 0.0047 lr: 0.02\n",
      "iteration: 42300 loss: 0.0046 lr: 0.02\n",
      "iteration: 42310 loss: 0.0043 lr: 0.02\n",
      "iteration: 42320 loss: 0.0057 lr: 0.02\n",
      "iteration: 42330 loss: 0.0050 lr: 0.02\n",
      "iteration: 42340 loss: 0.0044 lr: 0.02\n",
      "iteration: 42350 loss: 0.0042 lr: 0.02\n",
      "iteration: 42360 loss: 0.0040 lr: 0.02\n",
      "iteration: 42370 loss: 0.0047 lr: 0.02\n",
      "iteration: 42380 loss: 0.0036 lr: 0.02\n",
      "iteration: 42390 loss: 0.0046 lr: 0.02\n",
      "iteration: 42400 loss: 0.0033 lr: 0.02\n",
      "iteration: 42410 loss: 0.0051 lr: 0.02\n",
      "iteration: 42420 loss: 0.0040 lr: 0.02\n",
      "iteration: 42430 loss: 0.0043 lr: 0.02\n",
      "iteration: 42440 loss: 0.0035 lr: 0.02\n",
      "iteration: 42450 loss: 0.0040 lr: 0.02\n",
      "iteration: 42460 loss: 0.0086 lr: 0.02\n",
      "iteration: 42470 loss: 0.0036 lr: 0.02\n",
      "iteration: 42480 loss: 0.0055 lr: 0.02\n",
      "iteration: 42490 loss: 0.0034 lr: 0.02\n",
      "iteration: 42500 loss: 0.0036 lr: 0.02\n",
      "iteration: 42510 loss: 0.0033 lr: 0.02\n",
      "iteration: 42520 loss: 0.0038 lr: 0.02\n",
      "iteration: 42530 loss: 0.0035 lr: 0.02\n",
      "iteration: 42540 loss: 0.0048 lr: 0.02\n",
      "iteration: 42550 loss: 0.0048 lr: 0.02\n",
      "iteration: 42560 loss: 0.0039 lr: 0.02\n",
      "iteration: 42570 loss: 0.0039 lr: 0.02\n",
      "iteration: 42580 loss: 0.0037 lr: 0.02\n",
      "iteration: 42590 loss: 0.0040 lr: 0.02\n",
      "iteration: 42600 loss: 0.0033 lr: 0.02\n",
      "iteration: 42610 loss: 0.0055 lr: 0.02\n",
      "iteration: 42620 loss: 0.0048 lr: 0.02\n",
      "iteration: 42630 loss: 0.0037 lr: 0.02\n",
      "iteration: 42640 loss: 0.0037 lr: 0.02\n",
      "iteration: 42650 loss: 0.0061 lr: 0.02\n",
      "iteration: 42660 loss: 0.0041 lr: 0.02\n",
      "iteration: 42670 loss: 0.0049 lr: 0.02\n",
      "iteration: 42680 loss: 0.0042 lr: 0.02\n",
      "iteration: 42690 loss: 0.0042 lr: 0.02\n",
      "iteration: 42700 loss: 0.0043 lr: 0.02\n",
      "iteration: 42710 loss: 0.0037 lr: 0.02\n",
      "iteration: 42720 loss: 0.0052 lr: 0.02\n",
      "iteration: 42730 loss: 0.0033 lr: 0.02\n",
      "iteration: 42740 loss: 0.0050 lr: 0.02\n",
      "iteration: 42750 loss: 0.0032 lr: 0.02\n",
      "iteration: 42760 loss: 0.0038 lr: 0.02\n",
      "iteration: 42770 loss: 0.0039 lr: 0.02\n",
      "iteration: 42780 loss: 0.0034 lr: 0.02\n",
      "iteration: 42790 loss: 0.0041 lr: 0.02\n",
      "iteration: 42800 loss: 0.0039 lr: 0.02\n",
      "iteration: 42810 loss: 0.0038 lr: 0.02\n",
      "iteration: 42820 loss: 0.0035 lr: 0.02\n",
      "iteration: 42830 loss: 0.0033 lr: 0.02\n",
      "iteration: 42840 loss: 0.0044 lr: 0.02\n",
      "iteration: 42850 loss: 0.0036 lr: 0.02\n",
      "iteration: 42860 loss: 0.0033 lr: 0.02\n",
      "iteration: 42870 loss: 0.0034 lr: 0.02\n",
      "iteration: 42880 loss: 0.0043 lr: 0.02\n",
      "iteration: 42890 loss: 0.0047 lr: 0.02\n",
      "iteration: 42900 loss: 0.0040 lr: 0.02\n",
      "iteration: 42910 loss: 0.0033 lr: 0.02\n",
      "iteration: 42920 loss: 0.0040 lr: 0.02\n",
      "iteration: 42930 loss: 0.0037 lr: 0.02\n",
      "iteration: 42940 loss: 0.0037 lr: 0.02\n",
      "iteration: 42950 loss: 0.0037 lr: 0.02\n",
      "iteration: 42960 loss: 0.0034 lr: 0.02\n",
      "iteration: 42970 loss: 0.0038 lr: 0.02\n",
      "iteration: 42980 loss: 0.0040 lr: 0.02\n",
      "iteration: 42990 loss: 0.0032 lr: 0.02\n",
      "iteration: 43000 loss: 0.0032 lr: 0.02\n",
      "iteration: 43010 loss: 0.0036 lr: 0.02\n",
      "iteration: 43020 loss: 0.0044 lr: 0.02\n",
      "iteration: 43030 loss: 0.0056 lr: 0.02\n",
      "iteration: 43040 loss: 0.0038 lr: 0.02\n",
      "iteration: 43050 loss: 0.0044 lr: 0.02\n",
      "iteration: 43060 loss: 0.0046 lr: 0.02\n",
      "iteration: 43070 loss: 0.0043 lr: 0.02\n",
      "iteration: 43080 loss: 0.0042 lr: 0.02\n",
      "iteration: 43090 loss: 0.0040 lr: 0.02\n",
      "iteration: 43100 loss: 0.0029 lr: 0.02\n",
      "iteration: 43110 loss: 0.0034 lr: 0.02\n",
      "iteration: 43120 loss: 0.0029 lr: 0.02\n",
      "iteration: 43130 loss: 0.0055 lr: 0.02\n",
      "iteration: 43140 loss: 0.0040 lr: 0.02\n",
      "iteration: 43150 loss: 0.0038 lr: 0.02\n",
      "iteration: 43160 loss: 0.0044 lr: 0.02\n",
      "iteration: 43170 loss: 0.0039 lr: 0.02\n",
      "iteration: 43180 loss: 0.0051 lr: 0.02\n",
      "iteration: 43190 loss: 0.0033 lr: 0.02\n",
      "iteration: 43200 loss: 0.0035 lr: 0.02\n",
      "iteration: 43210 loss: 0.0024 lr: 0.02\n",
      "iteration: 43220 loss: 0.0041 lr: 0.02\n",
      "iteration: 43230 loss: 0.0036 lr: 0.02\n",
      "iteration: 43240 loss: 0.0037 lr: 0.02\n",
      "iteration: 43250 loss: 0.0045 lr: 0.02\n",
      "iteration: 43260 loss: 0.0046 lr: 0.02\n",
      "iteration: 43270 loss: 0.0035 lr: 0.02\n",
      "iteration: 43280 loss: 0.0047 lr: 0.02\n",
      "iteration: 43290 loss: 0.0032 lr: 0.02\n",
      "iteration: 43300 loss: 0.0049 lr: 0.02\n",
      "iteration: 43310 loss: 0.0028 lr: 0.02\n",
      "iteration: 43320 loss: 0.0038 lr: 0.02\n",
      "iteration: 43330 loss: 0.0038 lr: 0.02\n",
      "iteration: 43340 loss: 0.0037 lr: 0.02\n",
      "iteration: 43350 loss: 0.0044 lr: 0.02\n",
      "iteration: 43360 loss: 0.0045 lr: 0.02\n",
      "iteration: 43370 loss: 0.0028 lr: 0.02\n",
      "iteration: 43380 loss: 0.0028 lr: 0.02\n",
      "iteration: 43390 loss: 0.0038 lr: 0.02\n",
      "iteration: 43400 loss: 0.0041 lr: 0.02\n",
      "iteration: 43410 loss: 0.0035 lr: 0.02\n",
      "iteration: 43420 loss: 0.0037 lr: 0.02\n",
      "iteration: 43430 loss: 0.0042 lr: 0.02\n",
      "iteration: 43440 loss: 0.0039 lr: 0.02\n",
      "iteration: 43450 loss: 0.0034 lr: 0.02\n",
      "iteration: 43460 loss: 0.0037 lr: 0.02\n",
      "iteration: 43470 loss: 0.0038 lr: 0.02\n",
      "iteration: 43480 loss: 0.0048 lr: 0.02\n",
      "iteration: 43490 loss: 0.0050 lr: 0.02\n",
      "iteration: 43500 loss: 0.0059 lr: 0.02\n",
      "iteration: 43510 loss: 0.0046 lr: 0.02\n",
      "iteration: 43520 loss: 0.0038 lr: 0.02\n",
      "iteration: 43530 loss: 0.0048 lr: 0.02\n",
      "iteration: 43540 loss: 0.0036 lr: 0.02\n",
      "iteration: 43550 loss: 0.0034 lr: 0.02\n",
      "iteration: 43560 loss: 0.0052 lr: 0.02\n",
      "iteration: 43570 loss: 0.0045 lr: 0.02\n",
      "iteration: 43580 loss: 0.0056 lr: 0.02\n",
      "iteration: 43590 loss: 0.0036 lr: 0.02\n",
      "iteration: 43600 loss: 0.0040 lr: 0.02\n",
      "iteration: 43610 loss: 0.0037 lr: 0.02\n",
      "iteration: 43620 loss: 0.0032 lr: 0.02\n",
      "iteration: 43630 loss: 0.0059 lr: 0.02\n",
      "iteration: 43640 loss: 0.0038 lr: 0.02\n",
      "iteration: 43650 loss: 0.0036 lr: 0.02\n",
      "iteration: 43660 loss: 0.0033 lr: 0.02\n",
      "iteration: 43670 loss: 0.0053 lr: 0.02\n",
      "iteration: 43680 loss: 0.0040 lr: 0.02\n",
      "iteration: 43690 loss: 0.0050 lr: 0.02\n",
      "iteration: 43700 loss: 0.0038 lr: 0.02\n",
      "iteration: 43710 loss: 0.0039 lr: 0.02\n",
      "iteration: 43720 loss: 0.0043 lr: 0.02\n",
      "iteration: 43730 loss: 0.0048 lr: 0.02\n",
      "iteration: 43740 loss: 0.0033 lr: 0.02\n",
      "iteration: 43750 loss: 0.0039 lr: 0.02\n",
      "iteration: 43760 loss: 0.0053 lr: 0.02\n",
      "iteration: 43770 loss: 0.0080 lr: 0.02\n",
      "iteration: 43780 loss: 0.0043 lr: 0.02\n",
      "iteration: 43790 loss: 0.0041 lr: 0.02\n",
      "iteration: 43800 loss: 0.0051 lr: 0.02\n",
      "iteration: 43810 loss: 0.0059 lr: 0.02\n",
      "iteration: 43820 loss: 0.0039 lr: 0.02\n",
      "iteration: 43830 loss: 0.0034 lr: 0.02\n",
      "iteration: 43840 loss: 0.0044 lr: 0.02\n",
      "iteration: 43850 loss: 0.0041 lr: 0.02\n",
      "iteration: 43860 loss: 0.0044 lr: 0.02\n",
      "iteration: 43870 loss: 0.0058 lr: 0.02\n",
      "iteration: 43880 loss: 0.0032 lr: 0.02\n",
      "iteration: 43890 loss: 0.0039 lr: 0.02\n",
      "iteration: 43900 loss: 0.0041 lr: 0.02\n",
      "iteration: 43910 loss: 0.0035 lr: 0.02\n",
      "iteration: 43920 loss: 0.0046 lr: 0.02\n",
      "iteration: 43930 loss: 0.0032 lr: 0.02\n",
      "iteration: 43940 loss: 0.0047 lr: 0.02\n",
      "iteration: 43950 loss: 0.0037 lr: 0.02\n",
      "iteration: 43960 loss: 0.0045 lr: 0.02\n",
      "iteration: 43970 loss: 0.0039 lr: 0.02\n",
      "iteration: 43980 loss: 0.0056 lr: 0.02\n",
      "iteration: 43990 loss: 0.0041 lr: 0.02\n",
      "iteration: 44000 loss: 0.0036 lr: 0.02\n",
      "iteration: 44010 loss: 0.0050 lr: 0.02\n",
      "iteration: 44020 loss: 0.0044 lr: 0.02\n",
      "iteration: 44030 loss: 0.0038 lr: 0.02\n",
      "iteration: 44040 loss: 0.0038 lr: 0.02\n",
      "iteration: 44050 loss: 0.0049 lr: 0.02\n",
      "iteration: 44060 loss: 0.0041 lr: 0.02\n",
      "iteration: 44070 loss: 0.0045 lr: 0.02\n",
      "iteration: 44080 loss: 0.0053 lr: 0.02\n",
      "iteration: 44090 loss: 0.0046 lr: 0.02\n",
      "iteration: 44100 loss: 0.0070 lr: 0.02\n",
      "iteration: 44110 loss: 0.0055 lr: 0.02\n",
      "iteration: 44120 loss: 0.0032 lr: 0.02\n",
      "iteration: 44130 loss: 0.0041 lr: 0.02\n",
      "iteration: 44140 loss: 0.0033 lr: 0.02\n",
      "iteration: 44150 loss: 0.0040 lr: 0.02\n",
      "iteration: 44160 loss: 0.0039 lr: 0.02\n",
      "iteration: 44170 loss: 0.0044 lr: 0.02\n",
      "iteration: 44180 loss: 0.0046 lr: 0.02\n",
      "iteration: 44190 loss: 0.0048 lr: 0.02\n",
      "iteration: 44200 loss: 0.0035 lr: 0.02\n",
      "iteration: 44210 loss: 0.0035 lr: 0.02\n",
      "iteration: 44220 loss: 0.0034 lr: 0.02\n",
      "iteration: 44230 loss: 0.0030 lr: 0.02\n",
      "iteration: 44240 loss: 0.0039 lr: 0.02\n",
      "iteration: 44250 loss: 0.0045 lr: 0.02\n",
      "iteration: 44260 loss: 0.0045 lr: 0.02\n",
      "iteration: 44270 loss: 0.0036 lr: 0.02\n",
      "iteration: 44280 loss: 0.0039 lr: 0.02\n",
      "iteration: 44290 loss: 0.0038 lr: 0.02\n",
      "iteration: 44300 loss: 0.0035 lr: 0.02\n",
      "iteration: 44310 loss: 0.0041 lr: 0.02\n",
      "iteration: 44320 loss: 0.0045 lr: 0.02\n",
      "iteration: 44330 loss: 0.0040 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 44340 loss: 0.0071 lr: 0.02\n",
      "iteration: 44350 loss: 0.0030 lr: 0.02\n",
      "iteration: 44360 loss: 0.0051 lr: 0.02\n",
      "iteration: 44370 loss: 0.0045 lr: 0.02\n",
      "iteration: 44380 loss: 0.0036 lr: 0.02\n",
      "iteration: 44390 loss: 0.0056 lr: 0.02\n",
      "iteration: 44400 loss: 0.0051 lr: 0.02\n",
      "iteration: 44410 loss: 0.0052 lr: 0.02\n",
      "iteration: 44420 loss: 0.0042 lr: 0.02\n",
      "iteration: 44430 loss: 0.0037 lr: 0.02\n",
      "iteration: 44440 loss: 0.0072 lr: 0.02\n",
      "iteration: 44450 loss: 0.0044 lr: 0.02\n",
      "iteration: 44460 loss: 0.0040 lr: 0.02\n",
      "iteration: 44470 loss: 0.0038 lr: 0.02\n",
      "iteration: 44480 loss: 0.0053 lr: 0.02\n",
      "iteration: 44490 loss: 0.0045 lr: 0.02\n",
      "iteration: 44500 loss: 0.0048 lr: 0.02\n",
      "iteration: 44510 loss: 0.0037 lr: 0.02\n",
      "iteration: 44520 loss: 0.0036 lr: 0.02\n",
      "iteration: 44530 loss: 0.0031 lr: 0.02\n",
      "iteration: 44540 loss: 0.0037 lr: 0.02\n",
      "iteration: 44550 loss: 0.0039 lr: 0.02\n",
      "iteration: 44560 loss: 0.0034 lr: 0.02\n",
      "iteration: 44570 loss: 0.0035 lr: 0.02\n",
      "iteration: 44580 loss: 0.0031 lr: 0.02\n",
      "iteration: 44590 loss: 0.0052 lr: 0.02\n",
      "iteration: 44600 loss: 0.0029 lr: 0.02\n",
      "iteration: 44610 loss: 0.0039 lr: 0.02\n",
      "iteration: 44620 loss: 0.0045 lr: 0.02\n",
      "iteration: 44630 loss: 0.0048 lr: 0.02\n",
      "iteration: 44640 loss: 0.0047 lr: 0.02\n",
      "iteration: 44650 loss: 0.0031 lr: 0.02\n",
      "iteration: 44660 loss: 0.0043 lr: 0.02\n",
      "iteration: 44670 loss: 0.0035 lr: 0.02\n",
      "iteration: 44680 loss: 0.0044 lr: 0.02\n",
      "iteration: 44690 loss: 0.0038 lr: 0.02\n",
      "iteration: 44700 loss: 0.0038 lr: 0.02\n",
      "iteration: 44710 loss: 0.0038 lr: 0.02\n",
      "iteration: 44720 loss: 0.0039 lr: 0.02\n",
      "iteration: 44730 loss: 0.0032 lr: 0.02\n",
      "iteration: 44740 loss: 0.0034 lr: 0.02\n",
      "iteration: 44750 loss: 0.0039 lr: 0.02\n",
      "iteration: 44760 loss: 0.0046 lr: 0.02\n",
      "iteration: 44770 loss: 0.0058 lr: 0.02\n",
      "iteration: 44780 loss: 0.0032 lr: 0.02\n",
      "iteration: 44790 loss: 0.0038 lr: 0.02\n",
      "iteration: 44800 loss: 0.0045 lr: 0.02\n",
      "iteration: 44810 loss: 0.0056 lr: 0.02\n",
      "iteration: 44820 loss: 0.0046 lr: 0.02\n",
      "iteration: 44830 loss: 0.0052 lr: 0.02\n",
      "iteration: 44840 loss: 0.0030 lr: 0.02\n",
      "iteration: 44850 loss: 0.0035 lr: 0.02\n",
      "iteration: 44860 loss: 0.0038 lr: 0.02\n",
      "iteration: 44870 loss: 0.0046 lr: 0.02\n",
      "iteration: 44880 loss: 0.0037 lr: 0.02\n",
      "iteration: 44890 loss: 0.0044 lr: 0.02\n",
      "iteration: 44900 loss: 0.0050 lr: 0.02\n",
      "iteration: 44910 loss: 0.0044 lr: 0.02\n",
      "iteration: 44920 loss: 0.0033 lr: 0.02\n",
      "iteration: 44930 loss: 0.0044 lr: 0.02\n",
      "iteration: 44940 loss: 0.0047 lr: 0.02\n",
      "iteration: 44950 loss: 0.0029 lr: 0.02\n",
      "iteration: 44960 loss: 0.0045 lr: 0.02\n",
      "iteration: 44970 loss: 0.0041 lr: 0.02\n",
      "iteration: 44980 loss: 0.0050 lr: 0.02\n",
      "iteration: 44990 loss: 0.0037 lr: 0.02\n",
      "iteration: 45000 loss: 0.0038 lr: 0.02\n",
      "iteration: 45010 loss: 0.0040 lr: 0.02\n",
      "iteration: 45020 loss: 0.0059 lr: 0.02\n",
      "iteration: 45030 loss: 0.0046 lr: 0.02\n",
      "iteration: 45040 loss: 0.0071 lr: 0.02\n",
      "iteration: 45050 loss: 0.0063 lr: 0.02\n",
      "iteration: 45060 loss: 0.0046 lr: 0.02\n",
      "iteration: 45070 loss: 0.0053 lr: 0.02\n",
      "iteration: 45080 loss: 0.0058 lr: 0.02\n",
      "iteration: 45090 loss: 0.0040 lr: 0.02\n",
      "iteration: 45100 loss: 0.0036 lr: 0.02\n",
      "iteration: 45110 loss: 0.0034 lr: 0.02\n",
      "iteration: 45120 loss: 0.0035 lr: 0.02\n",
      "iteration: 45130 loss: 0.0038 lr: 0.02\n",
      "iteration: 45140 loss: 0.0040 lr: 0.02\n",
      "iteration: 45150 loss: 0.0040 lr: 0.02\n",
      "iteration: 45160 loss: 0.0033 lr: 0.02\n",
      "iteration: 45170 loss: 0.0043 lr: 0.02\n",
      "iteration: 45180 loss: 0.0037 lr: 0.02\n",
      "iteration: 45190 loss: 0.0045 lr: 0.02\n",
      "iteration: 45200 loss: 0.0047 lr: 0.02\n",
      "iteration: 45210 loss: 0.0051 lr: 0.02\n",
      "iteration: 45220 loss: 0.0041 lr: 0.02\n",
      "iteration: 45230 loss: 0.0046 lr: 0.02\n",
      "iteration: 45240 loss: 0.0043 lr: 0.02\n",
      "iteration: 45250 loss: 0.0047 lr: 0.02\n",
      "iteration: 45260 loss: 0.0040 lr: 0.02\n",
      "iteration: 45270 loss: 0.0048 lr: 0.02\n",
      "iteration: 45280 loss: 0.0044 lr: 0.02\n",
      "iteration: 45290 loss: 0.0067 lr: 0.02\n",
      "iteration: 45300 loss: 0.0039 lr: 0.02\n",
      "iteration: 45310 loss: 0.0039 lr: 0.02\n",
      "iteration: 45320 loss: 0.0041 lr: 0.02\n",
      "iteration: 45330 loss: 0.0037 lr: 0.02\n",
      "iteration: 45340 loss: 0.0057 lr: 0.02\n",
      "iteration: 45350 loss: 0.0038 lr: 0.02\n",
      "iteration: 45360 loss: 0.0043 lr: 0.02\n",
      "iteration: 45370 loss: 0.0052 lr: 0.02\n",
      "iteration: 45380 loss: 0.0040 lr: 0.02\n",
      "iteration: 45390 loss: 0.0037 lr: 0.02\n",
      "iteration: 45400 loss: 0.0036 lr: 0.02\n",
      "iteration: 45410 loss: 0.0037 lr: 0.02\n",
      "iteration: 45420 loss: 0.0041 lr: 0.02\n",
      "iteration: 45430 loss: 0.0050 lr: 0.02\n",
      "iteration: 45440 loss: 0.0052 lr: 0.02\n",
      "iteration: 45450 loss: 0.0048 lr: 0.02\n",
      "iteration: 45460 loss: 0.0038 lr: 0.02\n",
      "iteration: 45470 loss: 0.0056 lr: 0.02\n",
      "iteration: 45480 loss: 0.0040 lr: 0.02\n",
      "iteration: 45490 loss: 0.0043 lr: 0.02\n",
      "iteration: 45500 loss: 0.0035 lr: 0.02\n",
      "iteration: 45510 loss: 0.0044 lr: 0.02\n",
      "iteration: 45520 loss: 0.0039 lr: 0.02\n",
      "iteration: 45530 loss: 0.0034 lr: 0.02\n",
      "iteration: 45540 loss: 0.0042 lr: 0.02\n",
      "iteration: 45550 loss: 0.0050 lr: 0.02\n",
      "iteration: 45560 loss: 0.0043 lr: 0.02\n",
      "iteration: 45570 loss: 0.0037 lr: 0.02\n",
      "iteration: 45580 loss: 0.0035 lr: 0.02\n",
      "iteration: 45590 loss: 0.0034 lr: 0.02\n",
      "iteration: 45600 loss: 0.0045 lr: 0.02\n",
      "iteration: 45610 loss: 0.0045 lr: 0.02\n",
      "iteration: 45620 loss: 0.0032 lr: 0.02\n",
      "iteration: 45630 loss: 0.0033 lr: 0.02\n",
      "iteration: 45640 loss: 0.0041 lr: 0.02\n",
      "iteration: 45650 loss: 0.0039 lr: 0.02\n",
      "iteration: 45660 loss: 0.0036 lr: 0.02\n",
      "iteration: 45670 loss: 0.0039 lr: 0.02\n",
      "iteration: 45680 loss: 0.0033 lr: 0.02\n",
      "iteration: 45690 loss: 0.0051 lr: 0.02\n",
      "iteration: 45700 loss: 0.0043 lr: 0.02\n",
      "iteration: 45710 loss: 0.0038 lr: 0.02\n",
      "iteration: 45720 loss: 0.0044 lr: 0.02\n",
      "iteration: 45730 loss: 0.0038 lr: 0.02\n",
      "iteration: 45740 loss: 0.0039 lr: 0.02\n",
      "iteration: 45750 loss: 0.0038 lr: 0.02\n",
      "iteration: 45760 loss: 0.0034 lr: 0.02\n",
      "iteration: 45770 loss: 0.0034 lr: 0.02\n",
      "iteration: 45780 loss: 0.0039 lr: 0.02\n",
      "iteration: 45790 loss: 0.0037 lr: 0.02\n",
      "iteration: 45800 loss: 0.0043 lr: 0.02\n",
      "iteration: 45810 loss: 0.0030 lr: 0.02\n",
      "iteration: 45820 loss: 0.0040 lr: 0.02\n",
      "iteration: 45830 loss: 0.0041 lr: 0.02\n",
      "iteration: 45840 loss: 0.0041 lr: 0.02\n",
      "iteration: 45850 loss: 0.0048 lr: 0.02\n",
      "iteration: 45860 loss: 0.0045 lr: 0.02\n",
      "iteration: 45870 loss: 0.0054 lr: 0.02\n",
      "iteration: 45880 loss: 0.0043 lr: 0.02\n",
      "iteration: 45890 loss: 0.0039 lr: 0.02\n",
      "iteration: 45900 loss: 0.0046 lr: 0.02\n",
      "iteration: 45910 loss: 0.0045 lr: 0.02\n",
      "iteration: 45920 loss: 0.0045 lr: 0.02\n",
      "iteration: 45930 loss: 0.0048 lr: 0.02\n",
      "iteration: 45940 loss: 0.0054 lr: 0.02\n",
      "iteration: 45950 loss: 0.0034 lr: 0.02\n",
      "iteration: 45960 loss: 0.0055 lr: 0.02\n",
      "iteration: 45970 loss: 0.0037 lr: 0.02\n",
      "iteration: 45980 loss: 0.0042 lr: 0.02\n",
      "iteration: 45990 loss: 0.0038 lr: 0.02\n",
      "iteration: 46000 loss: 0.0042 lr: 0.02\n",
      "iteration: 46010 loss: 0.0039 lr: 0.02\n",
      "iteration: 46020 loss: 0.0031 lr: 0.02\n",
      "iteration: 46030 loss: 0.0033 lr: 0.02\n",
      "iteration: 46040 loss: 0.0037 lr: 0.02\n",
      "iteration: 46050 loss: 0.0054 lr: 0.02\n",
      "iteration: 46060 loss: 0.0043 lr: 0.02\n",
      "iteration: 46070 loss: 0.0035 lr: 0.02\n",
      "iteration: 46080 loss: 0.0040 lr: 0.02\n",
      "iteration: 46090 loss: 0.0052 lr: 0.02\n",
      "iteration: 46100 loss: 0.0033 lr: 0.02\n",
      "iteration: 46110 loss: 0.0057 lr: 0.02\n",
      "iteration: 46120 loss: 0.0038 lr: 0.02\n",
      "iteration: 46130 loss: 0.0031 lr: 0.02\n",
      "iteration: 46140 loss: 0.0045 lr: 0.02\n",
      "iteration: 46150 loss: 0.0038 lr: 0.02\n",
      "iteration: 46160 loss: 0.0037 lr: 0.02\n",
      "iteration: 46170 loss: 0.0041 lr: 0.02\n",
      "iteration: 46180 loss: 0.0027 lr: 0.02\n",
      "iteration: 46190 loss: 0.0033 lr: 0.02\n",
      "iteration: 46200 loss: 0.0043 lr: 0.02\n",
      "iteration: 46210 loss: 0.0043 lr: 0.02\n",
      "iteration: 46220 loss: 0.0037 lr: 0.02\n",
      "iteration: 46230 loss: 0.0048 lr: 0.02\n",
      "iteration: 46240 loss: 0.0039 lr: 0.02\n",
      "iteration: 46250 loss: 0.0046 lr: 0.02\n",
      "iteration: 46260 loss: 0.0030 lr: 0.02\n",
      "iteration: 46270 loss: 0.0041 lr: 0.02\n",
      "iteration: 46280 loss: 0.0041 lr: 0.02\n",
      "iteration: 46290 loss: 0.0045 lr: 0.02\n",
      "iteration: 46300 loss: 0.0025 lr: 0.02\n",
      "iteration: 46310 loss: 0.0034 lr: 0.02\n",
      "iteration: 46320 loss: 0.0051 lr: 0.02\n",
      "iteration: 46330 loss: 0.0043 lr: 0.02\n",
      "iteration: 46340 loss: 0.0048 lr: 0.02\n",
      "iteration: 46350 loss: 0.0056 lr: 0.02\n",
      "iteration: 46360 loss: 0.0048 lr: 0.02\n",
      "iteration: 46370 loss: 0.0041 lr: 0.02\n",
      "iteration: 46380 loss: 0.0052 lr: 0.02\n",
      "iteration: 46390 loss: 0.0065 lr: 0.02\n",
      "iteration: 46400 loss: 0.0052 lr: 0.02\n",
      "iteration: 46410 loss: 0.0046 lr: 0.02\n",
      "iteration: 46420 loss: 0.0049 lr: 0.02\n",
      "iteration: 46430 loss: 0.0056 lr: 0.02\n",
      "iteration: 46440 loss: 0.0038 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 46450 loss: 0.0080 lr: 0.02\n",
      "iteration: 46460 loss: 0.0040 lr: 0.02\n",
      "iteration: 46470 loss: 0.0044 lr: 0.02\n",
      "iteration: 46480 loss: 0.0045 lr: 0.02\n",
      "iteration: 46490 loss: 0.0033 lr: 0.02\n",
      "iteration: 46500 loss: 0.0038 lr: 0.02\n",
      "iteration: 46510 loss: 0.0047 lr: 0.02\n",
      "iteration: 46520 loss: 0.0034 lr: 0.02\n",
      "iteration: 46530 loss: 0.0042 lr: 0.02\n",
      "iteration: 46540 loss: 0.0052 lr: 0.02\n",
      "iteration: 46550 loss: 0.0039 lr: 0.02\n",
      "iteration: 46560 loss: 0.0055 lr: 0.02\n",
      "iteration: 46570 loss: 0.0057 lr: 0.02\n",
      "iteration: 46580 loss: 0.0039 lr: 0.02\n",
      "iteration: 46590 loss: 0.0047 lr: 0.02\n",
      "iteration: 46600 loss: 0.0032 lr: 0.02\n",
      "iteration: 46610 loss: 0.0048 lr: 0.02\n",
      "iteration: 46620 loss: 0.0060 lr: 0.02\n",
      "iteration: 46630 loss: 0.0040 lr: 0.02\n",
      "iteration: 46640 loss: 0.0045 lr: 0.02\n",
      "iteration: 46650 loss: 0.0034 lr: 0.02\n",
      "iteration: 46660 loss: 0.0046 lr: 0.02\n",
      "iteration: 46670 loss: 0.0039 lr: 0.02\n",
      "iteration: 46680 loss: 0.0046 lr: 0.02\n",
      "iteration: 46690 loss: 0.0040 lr: 0.02\n",
      "iteration: 46700 loss: 0.0036 lr: 0.02\n",
      "iteration: 46710 loss: 0.0040 lr: 0.02\n",
      "iteration: 46720 loss: 0.0029 lr: 0.02\n",
      "iteration: 46730 loss: 0.0036 lr: 0.02\n",
      "iteration: 46740 loss: 0.0036 lr: 0.02\n",
      "iteration: 46750 loss: 0.0035 lr: 0.02\n",
      "iteration: 46760 loss: 0.0037 lr: 0.02\n",
      "iteration: 46770 loss: 0.0047 lr: 0.02\n",
      "iteration: 46780 loss: 0.0044 lr: 0.02\n",
      "iteration: 46790 loss: 0.0032 lr: 0.02\n",
      "iteration: 46800 loss: 0.0044 lr: 0.02\n",
      "iteration: 46810 loss: 0.0049 lr: 0.02\n",
      "iteration: 46820 loss: 0.0041 lr: 0.02\n",
      "iteration: 46830 loss: 0.0043 lr: 0.02\n",
      "iteration: 46840 loss: 0.0039 lr: 0.02\n",
      "iteration: 46850 loss: 0.0040 lr: 0.02\n",
      "iteration: 46860 loss: 0.0041 lr: 0.02\n",
      "iteration: 46870 loss: 0.0031 lr: 0.02\n",
      "iteration: 46880 loss: 0.0039 lr: 0.02\n",
      "iteration: 46890 loss: 0.0054 lr: 0.02\n",
      "iteration: 46900 loss: 0.0037 lr: 0.02\n",
      "iteration: 46910 loss: 0.0041 lr: 0.02\n",
      "iteration: 46920 loss: 0.0029 lr: 0.02\n",
      "iteration: 46930 loss: 0.0033 lr: 0.02\n",
      "iteration: 46940 loss: 0.0036 lr: 0.02\n",
      "iteration: 46950 loss: 0.0048 lr: 0.02\n",
      "iteration: 46960 loss: 0.0046 lr: 0.02\n",
      "iteration: 46970 loss: 0.0040 lr: 0.02\n",
      "iteration: 46980 loss: 0.0045 lr: 0.02\n",
      "iteration: 46990 loss: 0.0047 lr: 0.02\n",
      "iteration: 47000 loss: 0.0051 lr: 0.02\n",
      "iteration: 47010 loss: 0.0048 lr: 0.02\n",
      "iteration: 47020 loss: 0.0038 lr: 0.02\n",
      "iteration: 47030 loss: 0.0033 lr: 0.02\n",
      "iteration: 47040 loss: 0.0056 lr: 0.02\n",
      "iteration: 47050 loss: 0.0037 lr: 0.02\n",
      "iteration: 47060 loss: 0.0046 lr: 0.02\n",
      "iteration: 47070 loss: 0.0037 lr: 0.02\n",
      "iteration: 47080 loss: 0.0036 lr: 0.02\n",
      "iteration: 47090 loss: 0.0041 lr: 0.02\n",
      "iteration: 47100 loss: 0.0039 lr: 0.02\n",
      "iteration: 47110 loss: 0.0039 lr: 0.02\n",
      "iteration: 47120 loss: 0.0047 lr: 0.02\n",
      "iteration: 47130 loss: 0.0046 lr: 0.02\n",
      "iteration: 47140 loss: 0.0043 lr: 0.02\n",
      "iteration: 47150 loss: 0.0045 lr: 0.02\n",
      "iteration: 47160 loss: 0.0032 lr: 0.02\n",
      "iteration: 47170 loss: 0.0043 lr: 0.02\n",
      "iteration: 47180 loss: 0.0034 lr: 0.02\n",
      "iteration: 47190 loss: 0.0039 lr: 0.02\n",
      "iteration: 47200 loss: 0.0032 lr: 0.02\n",
      "iteration: 47210 loss: 0.0035 lr: 0.02\n",
      "iteration: 47220 loss: 0.0032 lr: 0.02\n",
      "iteration: 47230 loss: 0.0043 lr: 0.02\n",
      "iteration: 47240 loss: 0.0040 lr: 0.02\n",
      "iteration: 47250 loss: 0.0031 lr: 0.02\n",
      "iteration: 47260 loss: 0.0046 lr: 0.02\n",
      "iteration: 47270 loss: 0.0032 lr: 0.02\n",
      "iteration: 47280 loss: 0.0054 lr: 0.02\n",
      "iteration: 47290 loss: 0.0043 lr: 0.02\n",
      "iteration: 47300 loss: 0.0036 lr: 0.02\n",
      "iteration: 47310 loss: 0.0051 lr: 0.02\n",
      "iteration: 47320 loss: 0.0046 lr: 0.02\n",
      "iteration: 47330 loss: 0.0036 lr: 0.02\n",
      "iteration: 47340 loss: 0.0033 lr: 0.02\n",
      "iteration: 47350 loss: 0.0037 lr: 0.02\n",
      "iteration: 47360 loss: 0.0049 lr: 0.02\n",
      "iteration: 47370 loss: 0.0037 lr: 0.02\n",
      "iteration: 47380 loss: 0.0038 lr: 0.02\n",
      "iteration: 47390 loss: 0.0033 lr: 0.02\n",
      "iteration: 47400 loss: 0.0041 lr: 0.02\n",
      "iteration: 47410 loss: 0.0034 lr: 0.02\n",
      "iteration: 47420 loss: 0.0037 lr: 0.02\n",
      "iteration: 47430 loss: 0.0039 lr: 0.02\n",
      "iteration: 47440 loss: 0.0035 lr: 0.02\n",
      "iteration: 47450 loss: 0.0039 lr: 0.02\n",
      "iteration: 47460 loss: 0.0050 lr: 0.02\n",
      "iteration: 47470 loss: 0.0041 lr: 0.02\n",
      "iteration: 47480 loss: 0.0070 lr: 0.02\n",
      "iteration: 47490 loss: 0.0044 lr: 0.02\n",
      "iteration: 47500 loss: 0.0036 lr: 0.02\n",
      "iteration: 47510 loss: 0.0043 lr: 0.02\n",
      "iteration: 47520 loss: 0.0038 lr: 0.02\n",
      "iteration: 47530 loss: 0.0036 lr: 0.02\n",
      "iteration: 47540 loss: 0.0036 lr: 0.02\n",
      "iteration: 47550 loss: 0.0036 lr: 0.02\n",
      "iteration: 47560 loss: 0.0054 lr: 0.02\n",
      "iteration: 47570 loss: 0.0039 lr: 0.02\n",
      "iteration: 47580 loss: 0.0044 lr: 0.02\n",
      "iteration: 47590 loss: 0.0039 lr: 0.02\n",
      "iteration: 47600 loss: 0.0037 lr: 0.02\n",
      "iteration: 47610 loss: 0.0045 lr: 0.02\n",
      "iteration: 47620 loss: 0.0044 lr: 0.02\n",
      "iteration: 47630 loss: 0.0043 lr: 0.02\n",
      "iteration: 47640 loss: 0.0052 lr: 0.02\n",
      "iteration: 47650 loss: 0.0043 lr: 0.02\n",
      "iteration: 47660 loss: 0.0050 lr: 0.02\n",
      "iteration: 47670 loss: 0.0038 lr: 0.02\n",
      "iteration: 47680 loss: 0.0047 lr: 0.02\n",
      "iteration: 47690 loss: 0.0039 lr: 0.02\n",
      "iteration: 47700 loss: 0.0045 lr: 0.02\n",
      "iteration: 47710 loss: 0.0045 lr: 0.02\n",
      "iteration: 47720 loss: 0.0041 lr: 0.02\n",
      "iteration: 47730 loss: 0.0045 lr: 0.02\n",
      "iteration: 47740 loss: 0.0035 lr: 0.02\n",
      "iteration: 47750 loss: 0.0047 lr: 0.02\n",
      "iteration: 47760 loss: 0.0045 lr: 0.02\n",
      "iteration: 47770 loss: 0.0043 lr: 0.02\n",
      "iteration: 47780 loss: 0.0046 lr: 0.02\n",
      "iteration: 47790 loss: 0.0048 lr: 0.02\n",
      "iteration: 47800 loss: 0.0042 lr: 0.02\n",
      "iteration: 47810 loss: 0.0044 lr: 0.02\n",
      "iteration: 47820 loss: 0.0042 lr: 0.02\n",
      "iteration: 47830 loss: 0.0035 lr: 0.02\n",
      "iteration: 47840 loss: 0.0033 lr: 0.02\n",
      "iteration: 47850 loss: 0.0038 lr: 0.02\n",
      "iteration: 47860 loss: 0.0049 lr: 0.02\n",
      "iteration: 47870 loss: 0.0051 lr: 0.02\n",
      "iteration: 47880 loss: 0.0065 lr: 0.02\n",
      "iteration: 47890 loss: 0.0057 lr: 0.02\n",
      "iteration: 47900 loss: 0.0045 lr: 0.02\n",
      "iteration: 47910 loss: 0.0046 lr: 0.02\n",
      "iteration: 47920 loss: 0.0044 lr: 0.02\n",
      "iteration: 47930 loss: 0.0034 lr: 0.02\n",
      "iteration: 47940 loss: 0.0035 lr: 0.02\n",
      "iteration: 47950 loss: 0.0043 lr: 0.02\n",
      "iteration: 47960 loss: 0.0031 lr: 0.02\n",
      "iteration: 47970 loss: 0.0042 lr: 0.02\n",
      "iteration: 47980 loss: 0.0043 lr: 0.02\n",
      "iteration: 47990 loss: 0.0032 lr: 0.02\n",
      "iteration: 48000 loss: 0.0046 lr: 0.02\n",
      "iteration: 48010 loss: 0.0040 lr: 0.02\n",
      "iteration: 48020 loss: 0.0037 lr: 0.02\n",
      "iteration: 48030 loss: 0.0050 lr: 0.02\n",
      "iteration: 48040 loss: 0.0036 lr: 0.02\n",
      "iteration: 48050 loss: 0.0045 lr: 0.02\n",
      "iteration: 48060 loss: 0.0051 lr: 0.02\n",
      "iteration: 48070 loss: 0.0040 lr: 0.02\n",
      "iteration: 48080 loss: 0.0052 lr: 0.02\n",
      "iteration: 48090 loss: 0.0039 lr: 0.02\n",
      "iteration: 48100 loss: 0.0035 lr: 0.02\n",
      "iteration: 48110 loss: 0.0037 lr: 0.02\n",
      "iteration: 48120 loss: 0.0029 lr: 0.02\n",
      "iteration: 48130 loss: 0.0031 lr: 0.02\n",
      "iteration: 48140 loss: 0.0029 lr: 0.02\n",
      "iteration: 48150 loss: 0.0041 lr: 0.02\n",
      "iteration: 48160 loss: 0.0036 lr: 0.02\n",
      "iteration: 48170 loss: 0.0043 lr: 0.02\n",
      "iteration: 48180 loss: 0.0034 lr: 0.02\n",
      "iteration: 48190 loss: 0.0044 lr: 0.02\n",
      "iteration: 48200 loss: 0.0031 lr: 0.02\n",
      "iteration: 48210 loss: 0.0041 lr: 0.02\n",
      "iteration: 48220 loss: 0.0047 lr: 0.02\n",
      "iteration: 48230 loss: 0.0053 lr: 0.02\n",
      "iteration: 48240 loss: 0.0046 lr: 0.02\n",
      "iteration: 48250 loss: 0.0049 lr: 0.02\n",
      "iteration: 48260 loss: 0.0036 lr: 0.02\n",
      "iteration: 48270 loss: 0.0035 lr: 0.02\n",
      "iteration: 48280 loss: 0.0039 lr: 0.02\n",
      "iteration: 48290 loss: 0.0045 lr: 0.02\n",
      "iteration: 48300 loss: 0.0026 lr: 0.02\n",
      "iteration: 48310 loss: 0.0053 lr: 0.02\n",
      "iteration: 48320 loss: 0.0035 lr: 0.02\n",
      "iteration: 48330 loss: 0.0040 lr: 0.02\n",
      "iteration: 48340 loss: 0.0036 lr: 0.02\n",
      "iteration: 48350 loss: 0.0035 lr: 0.02\n",
      "iteration: 48360 loss: 0.0037 lr: 0.02\n",
      "iteration: 48370 loss: 0.0050 lr: 0.02\n",
      "iteration: 48380 loss: 0.0051 lr: 0.02\n",
      "iteration: 48390 loss: 0.0033 lr: 0.02\n",
      "iteration: 48400 loss: 0.0047 lr: 0.02\n",
      "iteration: 48410 loss: 0.0043 lr: 0.02\n",
      "iteration: 48420 loss: 0.0062 lr: 0.02\n",
      "iteration: 48430 loss: 0.0036 lr: 0.02\n",
      "iteration: 48440 loss: 0.0040 lr: 0.02\n",
      "iteration: 48450 loss: 0.0036 lr: 0.02\n",
      "iteration: 48460 loss: 0.0041 lr: 0.02\n",
      "iteration: 48470 loss: 0.0060 lr: 0.02\n",
      "iteration: 48480 loss: 0.0043 lr: 0.02\n",
      "iteration: 48490 loss: 0.0038 lr: 0.02\n",
      "iteration: 48500 loss: 0.0034 lr: 0.02\n",
      "iteration: 48510 loss: 0.0048 lr: 0.02\n",
      "iteration: 48520 loss: 0.0044 lr: 0.02\n",
      "iteration: 48530 loss: 0.0033 lr: 0.02\n",
      "iteration: 48540 loss: 0.0042 lr: 0.02\n",
      "iteration: 48550 loss: 0.0051 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 48560 loss: 0.0045 lr: 0.02\n",
      "iteration: 48570 loss: 0.0044 lr: 0.02\n",
      "iteration: 48580 loss: 0.0029 lr: 0.02\n",
      "iteration: 48590 loss: 0.0042 lr: 0.02\n",
      "iteration: 48600 loss: 0.0043 lr: 0.02\n",
      "iteration: 48610 loss: 0.0034 lr: 0.02\n",
      "iteration: 48620 loss: 0.0043 lr: 0.02\n",
      "iteration: 48630 loss: 0.0054 lr: 0.02\n",
      "iteration: 48640 loss: 0.0056 lr: 0.02\n",
      "iteration: 48650 loss: 0.0053 lr: 0.02\n",
      "iteration: 48660 loss: 0.0053 lr: 0.02\n",
      "iteration: 48670 loss: 0.0050 lr: 0.02\n",
      "iteration: 48680 loss: 0.0046 lr: 0.02\n",
      "iteration: 48690 loss: 0.0055 lr: 0.02\n",
      "iteration: 48700 loss: 0.0045 lr: 0.02\n",
      "iteration: 48710 loss: 0.0047 lr: 0.02\n",
      "iteration: 48720 loss: 0.0048 lr: 0.02\n",
      "iteration: 48730 loss: 0.0041 lr: 0.02\n",
      "iteration: 48740 loss: 0.0039 lr: 0.02\n",
      "iteration: 48750 loss: 0.0045 lr: 0.02\n",
      "iteration: 48760 loss: 0.0033 lr: 0.02\n",
      "iteration: 48770 loss: 0.0037 lr: 0.02\n",
      "iteration: 48780 loss: 0.0043 lr: 0.02\n",
      "iteration: 48790 loss: 0.0046 lr: 0.02\n",
      "iteration: 48800 loss: 0.0036 lr: 0.02\n",
      "iteration: 48810 loss: 0.0042 lr: 0.02\n",
      "iteration: 48820 loss: 0.0040 lr: 0.02\n",
      "iteration: 48830 loss: 0.0039 lr: 0.02\n",
      "iteration: 48840 loss: 0.0039 lr: 0.02\n",
      "iteration: 48850 loss: 0.0044 lr: 0.02\n",
      "iteration: 48860 loss: 0.0061 lr: 0.02\n",
      "iteration: 48870 loss: 0.0045 lr: 0.02\n",
      "iteration: 48880 loss: 0.0051 lr: 0.02\n",
      "iteration: 48890 loss: 0.0041 lr: 0.02\n",
      "iteration: 48900 loss: 0.0036 lr: 0.02\n",
      "iteration: 48910 loss: 0.0042 lr: 0.02\n",
      "iteration: 48920 loss: 0.0035 lr: 0.02\n",
      "iteration: 48930 loss: 0.0051 lr: 0.02\n",
      "iteration: 48940 loss: 0.0045 lr: 0.02\n",
      "iteration: 48950 loss: 0.0037 lr: 0.02\n",
      "iteration: 48960 loss: 0.0057 lr: 0.02\n",
      "iteration: 48970 loss: 0.0062 lr: 0.02\n",
      "iteration: 48980 loss: 0.0038 lr: 0.02\n",
      "iteration: 48990 loss: 0.0037 lr: 0.02\n",
      "iteration: 49000 loss: 0.0029 lr: 0.02\n",
      "iteration: 49010 loss: 0.0046 lr: 0.02\n",
      "iteration: 49020 loss: 0.0040 lr: 0.02\n",
      "iteration: 49030 loss: 0.0038 lr: 0.02\n",
      "iteration: 49040 loss: 0.0038 lr: 0.02\n",
      "iteration: 49050 loss: 0.0041 lr: 0.02\n",
      "iteration: 49060 loss: 0.0034 lr: 0.02\n",
      "iteration: 49070 loss: 0.0043 lr: 0.02\n",
      "iteration: 49080 loss: 0.0031 lr: 0.02\n",
      "iteration: 49090 loss: 0.0041 lr: 0.02\n",
      "iteration: 49100 loss: 0.0035 lr: 0.02\n",
      "iteration: 49110 loss: 0.0043 lr: 0.02\n",
      "iteration: 49120 loss: 0.0043 lr: 0.02\n",
      "iteration: 49130 loss: 0.0060 lr: 0.02\n",
      "iteration: 49140 loss: 0.0036 lr: 0.02\n",
      "iteration: 49150 loss: 0.0048 lr: 0.02\n",
      "iteration: 49160 loss: 0.0038 lr: 0.02\n",
      "iteration: 49170 loss: 0.0036 lr: 0.02\n",
      "iteration: 49180 loss: 0.0042 lr: 0.02\n",
      "iteration: 49190 loss: 0.0032 lr: 0.02\n",
      "iteration: 49200 loss: 0.0039 lr: 0.02\n",
      "iteration: 49210 loss: 0.0049 lr: 0.02\n",
      "iteration: 49220 loss: 0.0040 lr: 0.02\n",
      "iteration: 49230 loss: 0.0054 lr: 0.02\n",
      "iteration: 49240 loss: 0.0041 lr: 0.02\n",
      "iteration: 49250 loss: 0.0034 lr: 0.02\n",
      "iteration: 49260 loss: 0.0043 lr: 0.02\n",
      "iteration: 49270 loss: 0.0037 lr: 0.02\n",
      "iteration: 49280 loss: 0.0030 lr: 0.02\n",
      "iteration: 49290 loss: 0.0043 lr: 0.02\n",
      "iteration: 49300 loss: 0.0041 lr: 0.02\n",
      "iteration: 49310 loss: 0.0039 lr: 0.02\n",
      "iteration: 49320 loss: 0.0046 lr: 0.02\n",
      "iteration: 49330 loss: 0.0040 lr: 0.02\n",
      "iteration: 49340 loss: 0.0041 lr: 0.02\n",
      "iteration: 49350 loss: 0.0041 lr: 0.02\n",
      "iteration: 49360 loss: 0.0045 lr: 0.02\n",
      "iteration: 49370 loss: 0.0041 lr: 0.02\n",
      "iteration: 49380 loss: 0.0035 lr: 0.02\n",
      "iteration: 49390 loss: 0.0046 lr: 0.02\n",
      "iteration: 49400 loss: 0.0034 lr: 0.02\n",
      "iteration: 49410 loss: 0.0039 lr: 0.02\n",
      "iteration: 49420 loss: 0.0038 lr: 0.02\n",
      "iteration: 49430 loss: 0.0060 lr: 0.02\n",
      "iteration: 49440 loss: 0.0039 lr: 0.02\n",
      "iteration: 49450 loss: 0.0045 lr: 0.02\n",
      "iteration: 49460 loss: 0.0038 lr: 0.02\n",
      "iteration: 49470 loss: 0.0050 lr: 0.02\n",
      "iteration: 49480 loss: 0.0039 lr: 0.02\n",
      "iteration: 49490 loss: 0.0045 lr: 0.02\n",
      "iteration: 49500 loss: 0.0060 lr: 0.02\n",
      "iteration: 49510 loss: 0.0039 lr: 0.02\n",
      "iteration: 49520 loss: 0.0039 lr: 0.02\n",
      "iteration: 49530 loss: 0.0035 lr: 0.02\n",
      "iteration: 49540 loss: 0.0037 lr: 0.02\n",
      "iteration: 49550 loss: 0.0039 lr: 0.02\n",
      "iteration: 49560 loss: 0.0034 lr: 0.02\n",
      "iteration: 49570 loss: 0.0035 lr: 0.02\n",
      "iteration: 49580 loss: 0.0039 lr: 0.02\n",
      "iteration: 49590 loss: 0.0055 lr: 0.02\n",
      "iteration: 49600 loss: 0.0043 lr: 0.02\n",
      "iteration: 49610 loss: 0.0039 lr: 0.02\n",
      "iteration: 49620 loss: 0.0063 lr: 0.02\n",
      "iteration: 49630 loss: 0.0035 lr: 0.02\n",
      "iteration: 49640 loss: 0.0034 lr: 0.02\n",
      "iteration: 49650 loss: 0.0039 lr: 0.02\n",
      "iteration: 49660 loss: 0.0050 lr: 0.02\n",
      "iteration: 49670 loss: 0.0034 lr: 0.02\n",
      "iteration: 49680 loss: 0.0032 lr: 0.02\n",
      "iteration: 49690 loss: 0.0037 lr: 0.02\n",
      "iteration: 49700 loss: 0.0033 lr: 0.02\n",
      "iteration: 49710 loss: 0.0032 lr: 0.02\n",
      "iteration: 49720 loss: 0.0041 lr: 0.02\n",
      "iteration: 49730 loss: 0.0044 lr: 0.02\n",
      "iteration: 49740 loss: 0.0043 lr: 0.02\n",
      "iteration: 49750 loss: 0.0030 lr: 0.02\n",
      "iteration: 49760 loss: 0.0057 lr: 0.02\n",
      "iteration: 49770 loss: 0.0032 lr: 0.02\n",
      "iteration: 49780 loss: 0.0033 lr: 0.02\n",
      "iteration: 49790 loss: 0.0046 lr: 0.02\n",
      "iteration: 49800 loss: 0.0033 lr: 0.02\n",
      "iteration: 49810 loss: 0.0035 lr: 0.02\n",
      "iteration: 49820 loss: 0.0040 lr: 0.02\n",
      "iteration: 49830 loss: 0.0037 lr: 0.02\n",
      "iteration: 49840 loss: 0.0037 lr: 0.02\n",
      "iteration: 49850 loss: 0.0052 lr: 0.02\n",
      "iteration: 49860 loss: 0.0049 lr: 0.02\n",
      "iteration: 49870 loss: 0.0041 lr: 0.02\n",
      "iteration: 49880 loss: 0.0044 lr: 0.02\n",
      "iteration: 49890 loss: 0.0045 lr: 0.02\n",
      "iteration: 49900 loss: 0.0049 lr: 0.02\n",
      "iteration: 49910 loss: 0.0036 lr: 0.02\n",
      "iteration: 49920 loss: 0.0043 lr: 0.02\n",
      "iteration: 49930 loss: 0.0028 lr: 0.02\n",
      "iteration: 49940 loss: 0.0039 lr: 0.02\n",
      "iteration: 49950 loss: 0.0036 lr: 0.02\n",
      "iteration: 49960 loss: 0.0034 lr: 0.02\n",
      "iteration: 49970 loss: 0.0032 lr: 0.02\n",
      "iteration: 49980 loss: 0.0040 lr: 0.02\n",
      "iteration: 49990 loss: 0.0065 lr: 0.02\n",
      "iteration: 50000 loss: 0.0043 lr: 0.02\n",
      "iteration: 50010 loss: 0.0042 lr: 0.02\n",
      "iteration: 50020 loss: 0.0036 lr: 0.02\n",
      "iteration: 50030 loss: 0.0044 lr: 0.02\n",
      "iteration: 50040 loss: 0.0046 lr: 0.02\n",
      "iteration: 50050 loss: 0.0044 lr: 0.02\n",
      "iteration: 50060 loss: 0.0043 lr: 0.02\n",
      "iteration: 50070 loss: 0.0037 lr: 0.02\n",
      "iteration: 50080 loss: 0.0075 lr: 0.02\n",
      "iteration: 50090 loss: 0.0048 lr: 0.02\n",
      "iteration: 50100 loss: 0.0047 lr: 0.02\n",
      "iteration: 50110 loss: 0.0038 lr: 0.02\n",
      "iteration: 50120 loss: 0.0052 lr: 0.02\n",
      "iteration: 50130 loss: 0.0033 lr: 0.02\n",
      "iteration: 50140 loss: 0.0063 lr: 0.02\n",
      "iteration: 50150 loss: 0.0034 lr: 0.02\n",
      "iteration: 50160 loss: 0.0034 lr: 0.02\n",
      "iteration: 50170 loss: 0.0044 lr: 0.02\n",
      "iteration: 50180 loss: 0.0049 lr: 0.02\n",
      "iteration: 50190 loss: 0.0044 lr: 0.02\n",
      "iteration: 50200 loss: 0.0047 lr: 0.02\n",
      "iteration: 50210 loss: 0.0044 lr: 0.02\n",
      "iteration: 50220 loss: 0.0039 lr: 0.02\n",
      "iteration: 50230 loss: 0.0033 lr: 0.02\n",
      "iteration: 50240 loss: 0.0041 lr: 0.02\n",
      "iteration: 50250 loss: 0.0035 lr: 0.02\n",
      "iteration: 50260 loss: 0.0036 lr: 0.02\n",
      "iteration: 50270 loss: 0.0039 lr: 0.02\n",
      "iteration: 50280 loss: 0.0037 lr: 0.02\n",
      "iteration: 50290 loss: 0.0031 lr: 0.02\n",
      "iteration: 50300 loss: 0.0032 lr: 0.02\n",
      "iteration: 50310 loss: 0.0041 lr: 0.02\n",
      "iteration: 50320 loss: 0.0031 lr: 0.02\n",
      "iteration: 50330 loss: 0.0043 lr: 0.02\n",
      "iteration: 50340 loss: 0.0044 lr: 0.02\n",
      "iteration: 50350 loss: 0.0031 lr: 0.02\n",
      "iteration: 50360 loss: 0.0040 lr: 0.02\n",
      "iteration: 50370 loss: 0.0037 lr: 0.02\n",
      "iteration: 50380 loss: 0.0041 lr: 0.02\n",
      "iteration: 50390 loss: 0.0039 lr: 0.02\n",
      "iteration: 50400 loss: 0.0036 lr: 0.02\n",
      "iteration: 50410 loss: 0.0034 lr: 0.02\n",
      "iteration: 50420 loss: 0.0051 lr: 0.02\n",
      "iteration: 50430 loss: 0.0036 lr: 0.02\n",
      "iteration: 50440 loss: 0.0036 lr: 0.02\n",
      "iteration: 50450 loss: 0.0032 lr: 0.02\n",
      "iteration: 50460 loss: 0.0032 lr: 0.02\n",
      "iteration: 50470 loss: 0.0024 lr: 0.02\n",
      "iteration: 50480 loss: 0.0037 lr: 0.02\n",
      "iteration: 50490 loss: 0.0037 lr: 0.02\n",
      "iteration: 50500 loss: 0.0032 lr: 0.02\n",
      "iteration: 50510 loss: 0.0040 lr: 0.02\n",
      "iteration: 50520 loss: 0.0037 lr: 0.02\n",
      "iteration: 50530 loss: 0.0046 lr: 0.02\n",
      "iteration: 50540 loss: 0.0027 lr: 0.02\n",
      "iteration: 50550 loss: 0.0035 lr: 0.02\n",
      "iteration: 50560 loss: 0.0031 lr: 0.02\n",
      "iteration: 50570 loss: 0.0049 lr: 0.02\n",
      "iteration: 50580 loss: 0.0038 lr: 0.02\n",
      "iteration: 50590 loss: 0.0044 lr: 0.02\n",
      "iteration: 50600 loss: 0.0049 lr: 0.02\n",
      "iteration: 50610 loss: 0.0044 lr: 0.02\n",
      "iteration: 50620 loss: 0.0044 lr: 0.02\n",
      "iteration: 50630 loss: 0.0037 lr: 0.02\n",
      "iteration: 50640 loss: 0.0032 lr: 0.02\n",
      "iteration: 50650 loss: 0.0043 lr: 0.02\n",
      "iteration: 50660 loss: 0.0044 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 50670 loss: 0.0030 lr: 0.02\n",
      "iteration: 50680 loss: 0.0032 lr: 0.02\n",
      "iteration: 50690 loss: 0.0034 lr: 0.02\n",
      "iteration: 50700 loss: 0.0037 lr: 0.02\n",
      "iteration: 50710 loss: 0.0057 lr: 0.02\n",
      "iteration: 50720 loss: 0.0047 lr: 0.02\n",
      "iteration: 50730 loss: 0.0035 lr: 0.02\n",
      "iteration: 50740 loss: 0.0041 lr: 0.02\n",
      "iteration: 50750 loss: 0.0044 lr: 0.02\n",
      "iteration: 50760 loss: 0.0038 lr: 0.02\n",
      "iteration: 50770 loss: 0.0036 lr: 0.02\n",
      "iteration: 50780 loss: 0.0041 lr: 0.02\n",
      "iteration: 50790 loss: 0.0039 lr: 0.02\n",
      "iteration: 50800 loss: 0.0036 lr: 0.02\n",
      "iteration: 50810 loss: 0.0040 lr: 0.02\n",
      "iteration: 50820 loss: 0.0051 lr: 0.02\n",
      "iteration: 50830 loss: 0.0040 lr: 0.02\n",
      "iteration: 50840 loss: 0.0050 lr: 0.02\n",
      "iteration: 50850 loss: 0.0037 lr: 0.02\n",
      "iteration: 50860 loss: 0.0039 lr: 0.02\n",
      "iteration: 50870 loss: 0.0035 lr: 0.02\n",
      "iteration: 50880 loss: 0.0034 lr: 0.02\n",
      "iteration: 50890 loss: 0.0044 lr: 0.02\n",
      "iteration: 50900 loss: 0.0030 lr: 0.02\n",
      "iteration: 50910 loss: 0.0037 lr: 0.02\n",
      "iteration: 50920 loss: 0.0037 lr: 0.02\n",
      "iteration: 50930 loss: 0.0038 lr: 0.02\n",
      "iteration: 50940 loss: 0.0043 lr: 0.02\n",
      "iteration: 50950 loss: 0.0032 lr: 0.02\n",
      "iteration: 50960 loss: 0.0037 lr: 0.02\n",
      "iteration: 50970 loss: 0.0026 lr: 0.02\n",
      "iteration: 50980 loss: 0.0035 lr: 0.02\n",
      "iteration: 50990 loss: 0.0039 lr: 0.02\n",
      "iteration: 51000 loss: 0.0034 lr: 0.02\n",
      "iteration: 51010 loss: 0.0056 lr: 0.02\n",
      "iteration: 51020 loss: 0.0035 lr: 0.02\n",
      "iteration: 51030 loss: 0.0042 lr: 0.02\n",
      "iteration: 51040 loss: 0.0033 lr: 0.02\n",
      "iteration: 51050 loss: 0.0033 lr: 0.02\n",
      "iteration: 51060 loss: 0.0047 lr: 0.02\n",
      "iteration: 51070 loss: 0.0040 lr: 0.02\n",
      "iteration: 51080 loss: 0.0030 lr: 0.02\n",
      "iteration: 51090 loss: 0.0040 lr: 0.02\n",
      "iteration: 51100 loss: 0.0039 lr: 0.02\n",
      "iteration: 51110 loss: 0.0035 lr: 0.02\n",
      "iteration: 51120 loss: 0.0036 lr: 0.02\n",
      "iteration: 51130 loss: 0.0044 lr: 0.02\n",
      "iteration: 51140 loss: 0.0032 lr: 0.02\n",
      "iteration: 51150 loss: 0.0043 lr: 0.02\n",
      "iteration: 51160 loss: 0.0050 lr: 0.02\n",
      "iteration: 51170 loss: 0.0061 lr: 0.02\n",
      "iteration: 51180 loss: 0.0043 lr: 0.02\n",
      "iteration: 51190 loss: 0.0044 lr: 0.02\n",
      "iteration: 51200 loss: 0.0047 lr: 0.02\n",
      "iteration: 51210 loss: 0.0048 lr: 0.02\n",
      "iteration: 51220 loss: 0.0041 lr: 0.02\n",
      "iteration: 51230 loss: 0.0026 lr: 0.02\n",
      "iteration: 51240 loss: 0.0046 lr: 0.02\n",
      "iteration: 51250 loss: 0.0054 lr: 0.02\n",
      "iteration: 51260 loss: 0.0037 lr: 0.02\n",
      "iteration: 51270 loss: 0.0036 lr: 0.02\n",
      "iteration: 51280 loss: 0.0034 lr: 0.02\n",
      "iteration: 51290 loss: 0.0040 lr: 0.02\n",
      "iteration: 51300 loss: 0.0029 lr: 0.02\n",
      "iteration: 51310 loss: 0.0039 lr: 0.02\n",
      "iteration: 51320 loss: 0.0033 lr: 0.02\n",
      "iteration: 51330 loss: 0.0032 lr: 0.02\n",
      "iteration: 51340 loss: 0.0032 lr: 0.02\n",
      "iteration: 51350 loss: 0.0031 lr: 0.02\n",
      "iteration: 51360 loss: 0.0037 lr: 0.02\n",
      "iteration: 51370 loss: 0.0031 lr: 0.02\n",
      "iteration: 51380 loss: 0.0029 lr: 0.02\n",
      "iteration: 51390 loss: 0.0049 lr: 0.02\n",
      "iteration: 51400 loss: 0.0034 lr: 0.02\n",
      "iteration: 51410 loss: 0.0038 lr: 0.02\n",
      "iteration: 51420 loss: 0.0037 lr: 0.02\n",
      "iteration: 51430 loss: 0.0039 lr: 0.02\n",
      "iteration: 51440 loss: 0.0034 lr: 0.02\n",
      "iteration: 51450 loss: 0.0038 lr: 0.02\n",
      "iteration: 51460 loss: 0.0030 lr: 0.02\n",
      "iteration: 51470 loss: 0.0038 lr: 0.02\n",
      "iteration: 51480 loss: 0.0042 lr: 0.02\n",
      "iteration: 51490 loss: 0.0041 lr: 0.02\n",
      "iteration: 51500 loss: 0.0034 lr: 0.02\n",
      "iteration: 51510 loss: 0.0026 lr: 0.02\n",
      "iteration: 51520 loss: 0.0044 lr: 0.02\n",
      "iteration: 51530 loss: 0.0046 lr: 0.02\n",
      "iteration: 51540 loss: 0.0043 lr: 0.02\n",
      "iteration: 51550 loss: 0.0049 lr: 0.02\n",
      "iteration: 51560 loss: 0.0041 lr: 0.02\n",
      "iteration: 51570 loss: 0.0036 lr: 0.02\n",
      "iteration: 51580 loss: 0.0047 lr: 0.02\n",
      "iteration: 51590 loss: 0.0052 lr: 0.02\n",
      "iteration: 51600 loss: 0.0031 lr: 0.02\n",
      "iteration: 51610 loss: 0.0037 lr: 0.02\n",
      "iteration: 51620 loss: 0.0044 lr: 0.02\n",
      "iteration: 51630 loss: 0.0046 lr: 0.02\n",
      "iteration: 51640 loss: 0.0045 lr: 0.02\n",
      "iteration: 51650 loss: 0.0045 lr: 0.02\n",
      "iteration: 51660 loss: 0.0040 lr: 0.02\n",
      "iteration: 51670 loss: 0.0049 lr: 0.02\n",
      "iteration: 51680 loss: 0.0041 lr: 0.02\n",
      "iteration: 51690 loss: 0.0038 lr: 0.02\n",
      "iteration: 51700 loss: 0.0033 lr: 0.02\n",
      "iteration: 51710 loss: 0.0042 lr: 0.02\n",
      "iteration: 51720 loss: 0.0043 lr: 0.02\n",
      "iteration: 51730 loss: 0.0030 lr: 0.02\n",
      "iteration: 51740 loss: 0.0035 lr: 0.02\n",
      "iteration: 51750 loss: 0.0035 lr: 0.02\n",
      "iteration: 51760 loss: 0.0028 lr: 0.02\n",
      "iteration: 51770 loss: 0.0033 lr: 0.02\n",
      "iteration: 51780 loss: 0.0039 lr: 0.02\n",
      "iteration: 51790 loss: 0.0050 lr: 0.02\n",
      "iteration: 51800 loss: 0.0035 lr: 0.02\n",
      "iteration: 51810 loss: 0.0039 lr: 0.02\n",
      "iteration: 51820 loss: 0.0045 lr: 0.02\n",
      "iteration: 51830 loss: 0.0044 lr: 0.02\n",
      "iteration: 51840 loss: 0.0048 lr: 0.02\n",
      "iteration: 51850 loss: 0.0043 lr: 0.02\n",
      "iteration: 51860 loss: 0.0029 lr: 0.02\n",
      "iteration: 51870 loss: 0.0036 lr: 0.02\n",
      "iteration: 51880 loss: 0.0038 lr: 0.02\n",
      "iteration: 51890 loss: 0.0040 lr: 0.02\n",
      "iteration: 51900 loss: 0.0034 lr: 0.02\n",
      "iteration: 51910 loss: 0.0038 lr: 0.02\n",
      "iteration: 51920 loss: 0.0041 lr: 0.02\n",
      "iteration: 51930 loss: 0.0034 lr: 0.02\n",
      "iteration: 51940 loss: 0.0041 lr: 0.02\n",
      "iteration: 51950 loss: 0.0044 lr: 0.02\n",
      "iteration: 51960 loss: 0.0043 lr: 0.02\n",
      "iteration: 51970 loss: 0.0031 lr: 0.02\n",
      "iteration: 51980 loss: 0.0045 lr: 0.02\n",
      "iteration: 51990 loss: 0.0035 lr: 0.02\n",
      "iteration: 52000 loss: 0.0027 lr: 0.02\n",
      "iteration: 52010 loss: 0.0028 lr: 0.02\n",
      "iteration: 52020 loss: 0.0036 lr: 0.02\n",
      "iteration: 52030 loss: 0.0054 lr: 0.02\n",
      "iteration: 52040 loss: 0.0035 lr: 0.02\n",
      "iteration: 52050 loss: 0.0045 lr: 0.02\n",
      "iteration: 52060 loss: 0.0068 lr: 0.02\n",
      "iteration: 52070 loss: 0.0042 lr: 0.02\n",
      "iteration: 52080 loss: 0.0041 lr: 0.02\n",
      "iteration: 52090 loss: 0.0043 lr: 0.02\n",
      "iteration: 52100 loss: 0.0035 lr: 0.02\n",
      "iteration: 52110 loss: 0.0028 lr: 0.02\n",
      "iteration: 52120 loss: 0.0032 lr: 0.02\n",
      "iteration: 52130 loss: 0.0044 lr: 0.02\n",
      "iteration: 52140 loss: 0.0038 lr: 0.02\n",
      "iteration: 52150 loss: 0.0038 lr: 0.02\n",
      "iteration: 52160 loss: 0.0032 lr: 0.02\n",
      "iteration: 52170 loss: 0.0034 lr: 0.02\n",
      "iteration: 52180 loss: 0.0037 lr: 0.02\n",
      "iteration: 52190 loss: 0.0047 lr: 0.02\n",
      "iteration: 52200 loss: 0.0035 lr: 0.02\n",
      "iteration: 52210 loss: 0.0039 lr: 0.02\n",
      "iteration: 52220 loss: 0.0040 lr: 0.02\n",
      "iteration: 52230 loss: 0.0049 lr: 0.02\n",
      "iteration: 52240 loss: 0.0040 lr: 0.02\n",
      "iteration: 52250 loss: 0.0033 lr: 0.02\n",
      "iteration: 52260 loss: 0.0038 lr: 0.02\n",
      "iteration: 52270 loss: 0.0049 lr: 0.02\n",
      "iteration: 52280 loss: 0.0033 lr: 0.02\n",
      "iteration: 52290 loss: 0.0039 lr: 0.02\n",
      "iteration: 52300 loss: 0.0045 lr: 0.02\n",
      "iteration: 52310 loss: 0.0035 lr: 0.02\n",
      "iteration: 52320 loss: 0.0036 lr: 0.02\n",
      "iteration: 52330 loss: 0.0046 lr: 0.02\n",
      "iteration: 52340 loss: 0.0042 lr: 0.02\n",
      "iteration: 52350 loss: 0.0061 lr: 0.02\n",
      "iteration: 52360 loss: 0.0048 lr: 0.02\n",
      "iteration: 52370 loss: 0.0039 lr: 0.02\n",
      "iteration: 52380 loss: 0.0037 lr: 0.02\n",
      "iteration: 52390 loss: 0.0033 lr: 0.02\n",
      "iteration: 52400 loss: 0.0038 lr: 0.02\n",
      "iteration: 52410 loss: 0.0038 lr: 0.02\n",
      "iteration: 52420 loss: 0.0034 lr: 0.02\n",
      "iteration: 52430 loss: 0.0034 lr: 0.02\n",
      "iteration: 52440 loss: 0.0055 lr: 0.02\n",
      "iteration: 52450 loss: 0.0045 lr: 0.02\n",
      "iteration: 52460 loss: 0.0044 lr: 0.02\n",
      "iteration: 52470 loss: 0.0036 lr: 0.02\n",
      "iteration: 52480 loss: 0.0026 lr: 0.02\n",
      "iteration: 52490 loss: 0.0030 lr: 0.02\n",
      "iteration: 52500 loss: 0.0041 lr: 0.02\n",
      "iteration: 52510 loss: 0.0042 lr: 0.02\n",
      "iteration: 52520 loss: 0.0039 lr: 0.02\n",
      "iteration: 52530 loss: 0.0041 lr: 0.02\n",
      "iteration: 52540 loss: 0.0038 lr: 0.02\n",
      "iteration: 52550 loss: 0.0045 lr: 0.02\n",
      "iteration: 52560 loss: 0.0042 lr: 0.02\n",
      "iteration: 52570 loss: 0.0041 lr: 0.02\n",
      "iteration: 52580 loss: 0.0042 lr: 0.02\n",
      "iteration: 52590 loss: 0.0037 lr: 0.02\n",
      "iteration: 52600 loss: 0.0043 lr: 0.02\n",
      "iteration: 52610 loss: 0.0038 lr: 0.02\n",
      "iteration: 52620 loss: 0.0060 lr: 0.02\n",
      "iteration: 52630 loss: 0.0045 lr: 0.02\n",
      "iteration: 52640 loss: 0.0039 lr: 0.02\n",
      "iteration: 52650 loss: 0.0042 lr: 0.02\n",
      "iteration: 52660 loss: 0.0059 lr: 0.02\n",
      "iteration: 52670 loss: 0.0053 lr: 0.02\n",
      "iteration: 52680 loss: 0.0035 lr: 0.02\n",
      "iteration: 52690 loss: 0.0038 lr: 0.02\n",
      "iteration: 52700 loss: 0.0047 lr: 0.02\n",
      "iteration: 52710 loss: 0.0029 lr: 0.02\n",
      "iteration: 52720 loss: 0.0052 lr: 0.02\n",
      "iteration: 52730 loss: 0.0041 lr: 0.02\n",
      "iteration: 52740 loss: 0.0041 lr: 0.02\n",
      "iteration: 52750 loss: 0.0050 lr: 0.02\n",
      "iteration: 52760 loss: 0.0038 lr: 0.02\n",
      "iteration: 52770 loss: 0.0034 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 52780 loss: 0.0033 lr: 0.02\n",
      "iteration: 52790 loss: 0.0053 lr: 0.02\n",
      "iteration: 52800 loss: 0.0041 lr: 0.02\n",
      "iteration: 52810 loss: 0.0038 lr: 0.02\n",
      "iteration: 52820 loss: 0.0036 lr: 0.02\n",
      "iteration: 52830 loss: 0.0035 lr: 0.02\n",
      "iteration: 52840 loss: 0.0033 lr: 0.02\n",
      "iteration: 52850 loss: 0.0040 lr: 0.02\n",
      "iteration: 52860 loss: 0.0033 lr: 0.02\n",
      "iteration: 52870 loss: 0.0041 lr: 0.02\n",
      "iteration: 52880 loss: 0.0034 lr: 0.02\n",
      "iteration: 52890 loss: 0.0040 lr: 0.02\n",
      "iteration: 52900 loss: 0.0033 lr: 0.02\n",
      "iteration: 52910 loss: 0.0045 lr: 0.02\n",
      "iteration: 52920 loss: 0.0042 lr: 0.02\n",
      "iteration: 52930 loss: 0.0048 lr: 0.02\n",
      "iteration: 52940 loss: 0.0044 lr: 0.02\n",
      "iteration: 52950 loss: 0.0032 lr: 0.02\n",
      "iteration: 52960 loss: 0.0041 lr: 0.02\n",
      "iteration: 52970 loss: 0.0045 lr: 0.02\n",
      "iteration: 52980 loss: 0.0035 lr: 0.02\n",
      "iteration: 52990 loss: 0.0038 lr: 0.02\n",
      "iteration: 53000 loss: 0.0035 lr: 0.02\n",
      "iteration: 53010 loss: 0.0055 lr: 0.02\n",
      "iteration: 53020 loss: 0.0047 lr: 0.02\n",
      "iteration: 53030 loss: 0.0040 lr: 0.02\n",
      "iteration: 53040 loss: 0.0029 lr: 0.02\n",
      "iteration: 53050 loss: 0.0028 lr: 0.02\n",
      "iteration: 53060 loss: 0.0051 lr: 0.02\n",
      "iteration: 53070 loss: 0.0040 lr: 0.02\n",
      "iteration: 53080 loss: 0.0029 lr: 0.02\n",
      "iteration: 53090 loss: 0.0030 lr: 0.02\n",
      "iteration: 53100 loss: 0.0042 lr: 0.02\n",
      "iteration: 53110 loss: 0.0045 lr: 0.02\n",
      "iteration: 53120 loss: 0.0037 lr: 0.02\n",
      "iteration: 53130 loss: 0.0035 lr: 0.02\n",
      "iteration: 53140 loss: 0.0043 lr: 0.02\n",
      "iteration: 53150 loss: 0.0040 lr: 0.02\n",
      "iteration: 53160 loss: 0.0040 lr: 0.02\n",
      "iteration: 53170 loss: 0.0049 lr: 0.02\n",
      "iteration: 53180 loss: 0.0036 lr: 0.02\n",
      "iteration: 53190 loss: 0.0032 lr: 0.02\n",
      "iteration: 53200 loss: 0.0036 lr: 0.02\n",
      "iteration: 53210 loss: 0.0035 lr: 0.02\n",
      "iteration: 53220 loss: 0.0044 lr: 0.02\n",
      "iteration: 53230 loss: 0.0047 lr: 0.02\n",
      "iteration: 53240 loss: 0.0031 lr: 0.02\n",
      "iteration: 53250 loss: 0.0033 lr: 0.02\n",
      "iteration: 53260 loss: 0.0032 lr: 0.02\n",
      "iteration: 53270 loss: 0.0036 lr: 0.02\n",
      "iteration: 53280 loss: 0.0032 lr: 0.02\n",
      "iteration: 53290 loss: 0.0037 lr: 0.02\n",
      "iteration: 53300 loss: 0.0037 lr: 0.02\n",
      "iteration: 53310 loss: 0.0027 lr: 0.02\n",
      "iteration: 53320 loss: 0.0032 lr: 0.02\n",
      "iteration: 53330 loss: 0.0029 lr: 0.02\n",
      "iteration: 53340 loss: 0.0032 lr: 0.02\n",
      "iteration: 53350 loss: 0.0043 lr: 0.02\n",
      "iteration: 53360 loss: 0.0036 lr: 0.02\n",
      "iteration: 53370 loss: 0.0040 lr: 0.02\n",
      "iteration: 53380 loss: 0.0029 lr: 0.02\n",
      "iteration: 53390 loss: 0.0042 lr: 0.02\n",
      "iteration: 53400 loss: 0.0054 lr: 0.02\n",
      "iteration: 53410 loss: 0.0035 lr: 0.02\n",
      "iteration: 53420 loss: 0.0034 lr: 0.02\n",
      "iteration: 53430 loss: 0.0030 lr: 0.02\n",
      "iteration: 53440 loss: 0.0045 lr: 0.02\n",
      "iteration: 53450 loss: 0.0036 lr: 0.02\n",
      "iteration: 53460 loss: 0.0029 lr: 0.02\n",
      "iteration: 53470 loss: 0.0038 lr: 0.02\n",
      "iteration: 53480 loss: 0.0042 lr: 0.02\n",
      "iteration: 53490 loss: 0.0040 lr: 0.02\n",
      "iteration: 53500 loss: 0.0044 lr: 0.02\n",
      "iteration: 53510 loss: 0.0036 lr: 0.02\n",
      "iteration: 53520 loss: 0.0042 lr: 0.02\n",
      "iteration: 53530 loss: 0.0045 lr: 0.02\n",
      "iteration: 53540 loss: 0.0048 lr: 0.02\n",
      "iteration: 53550 loss: 0.0036 lr: 0.02\n",
      "iteration: 53560 loss: 0.0043 lr: 0.02\n",
      "iteration: 53570 loss: 0.0040 lr: 0.02\n",
      "iteration: 53580 loss: 0.0056 lr: 0.02\n",
      "iteration: 53590 loss: 0.0043 lr: 0.02\n",
      "iteration: 53600 loss: 0.0045 lr: 0.02\n",
      "iteration: 53610 loss: 0.0036 lr: 0.02\n",
      "iteration: 53620 loss: 0.0042 lr: 0.02\n",
      "iteration: 53630 loss: 0.0047 lr: 0.02\n",
      "iteration: 53640 loss: 0.0041 lr: 0.02\n",
      "iteration: 53650 loss: 0.0039 lr: 0.02\n",
      "iteration: 53660 loss: 0.0031 lr: 0.02\n",
      "iteration: 53670 loss: 0.0034 lr: 0.02\n",
      "iteration: 53680 loss: 0.0034 lr: 0.02\n",
      "iteration: 53690 loss: 0.0025 lr: 0.02\n",
      "iteration: 53700 loss: 0.0034 lr: 0.02\n",
      "iteration: 53710 loss: 0.0028 lr: 0.02\n",
      "iteration: 53720 loss: 0.0036 lr: 0.02\n",
      "iteration: 53730 loss: 0.0038 lr: 0.02\n",
      "iteration: 53740 loss: 0.0029 lr: 0.02\n",
      "iteration: 53750 loss: 0.0036 lr: 0.02\n",
      "iteration: 53760 loss: 0.0048 lr: 0.02\n",
      "iteration: 53770 loss: 0.0037 lr: 0.02\n",
      "iteration: 53780 loss: 0.0033 lr: 0.02\n",
      "iteration: 53790 loss: 0.0033 lr: 0.02\n",
      "iteration: 53800 loss: 0.0032 lr: 0.02\n",
      "iteration: 53810 loss: 0.0036 lr: 0.02\n",
      "iteration: 53820 loss: 0.0046 lr: 0.02\n",
      "iteration: 53830 loss: 0.0050 lr: 0.02\n",
      "iteration: 53840 loss: 0.0033 lr: 0.02\n",
      "iteration: 53850 loss: 0.0034 lr: 0.02\n",
      "iteration: 53860 loss: 0.0030 lr: 0.02\n",
      "iteration: 53870 loss: 0.0037 lr: 0.02\n",
      "iteration: 53880 loss: 0.0030 lr: 0.02\n",
      "iteration: 53890 loss: 0.0032 lr: 0.02\n",
      "iteration: 53900 loss: 0.0038 lr: 0.02\n",
      "iteration: 53910 loss: 0.0035 lr: 0.02\n",
      "iteration: 53920 loss: 0.0029 lr: 0.02\n",
      "iteration: 53930 loss: 0.0035 lr: 0.02\n",
      "iteration: 53940 loss: 0.0036 lr: 0.02\n",
      "iteration: 53950 loss: 0.0049 lr: 0.02\n",
      "iteration: 53960 loss: 0.0032 lr: 0.02\n",
      "iteration: 53970 loss: 0.0037 lr: 0.02\n",
      "iteration: 53980 loss: 0.0048 lr: 0.02\n",
      "iteration: 53990 loss: 0.0041 lr: 0.02\n",
      "iteration: 54000 loss: 0.0044 lr: 0.02\n",
      "iteration: 54010 loss: 0.0039 lr: 0.02\n",
      "iteration: 54020 loss: 0.0036 lr: 0.02\n",
      "iteration: 54030 loss: 0.0033 lr: 0.02\n",
      "iteration: 54040 loss: 0.0033 lr: 0.02\n",
      "iteration: 54050 loss: 0.0035 lr: 0.02\n",
      "iteration: 54060 loss: 0.0032 lr: 0.02\n",
      "iteration: 54070 loss: 0.0039 lr: 0.02\n",
      "iteration: 54080 loss: 0.0039 lr: 0.02\n",
      "iteration: 54090 loss: 0.0035 lr: 0.02\n",
      "iteration: 54100 loss: 0.0042 lr: 0.02\n",
      "iteration: 54110 loss: 0.0032 lr: 0.02\n",
      "iteration: 54120 loss: 0.0030 lr: 0.02\n",
      "iteration: 54130 loss: 0.0033 lr: 0.02\n",
      "iteration: 54140 loss: 0.0043 lr: 0.02\n",
      "iteration: 54150 loss: 0.0037 lr: 0.02\n",
      "iteration: 54160 loss: 0.0040 lr: 0.02\n",
      "iteration: 54170 loss: 0.0048 lr: 0.02\n",
      "iteration: 54180 loss: 0.0028 lr: 0.02\n",
      "iteration: 54190 loss: 0.0040 lr: 0.02\n",
      "iteration: 54200 loss: 0.0036 lr: 0.02\n",
      "iteration: 54210 loss: 0.0034 lr: 0.02\n",
      "iteration: 54220 loss: 0.0032 lr: 0.02\n",
      "iteration: 54230 loss: 0.0043 lr: 0.02\n",
      "iteration: 54240 loss: 0.0054 lr: 0.02\n",
      "iteration: 54250 loss: 0.0037 lr: 0.02\n",
      "iteration: 54260 loss: 0.0040 lr: 0.02\n",
      "iteration: 54270 loss: 0.0030 lr: 0.02\n",
      "iteration: 54280 loss: 0.0041 lr: 0.02\n",
      "iteration: 54290 loss: 0.0036 lr: 0.02\n",
      "iteration: 54300 loss: 0.0036 lr: 0.02\n",
      "iteration: 54310 loss: 0.0044 lr: 0.02\n",
      "iteration: 54320 loss: 0.0053 lr: 0.02\n",
      "iteration: 54330 loss: 0.0036 lr: 0.02\n",
      "iteration: 54340 loss: 0.0030 lr: 0.02\n",
      "iteration: 54350 loss: 0.0042 lr: 0.02\n",
      "iteration: 54360 loss: 0.0037 lr: 0.02\n",
      "iteration: 54370 loss: 0.0037 lr: 0.02\n",
      "iteration: 54380 loss: 0.0032 lr: 0.02\n",
      "iteration: 54390 loss: 0.0037 lr: 0.02\n",
      "iteration: 54400 loss: 0.0035 lr: 0.02\n",
      "iteration: 54410 loss: 0.0040 lr: 0.02\n",
      "iteration: 54420 loss: 0.0040 lr: 0.02\n",
      "iteration: 54430 loss: 0.0033 lr: 0.02\n",
      "iteration: 54440 loss: 0.0029 lr: 0.02\n",
      "iteration: 54450 loss: 0.0032 lr: 0.02\n",
      "iteration: 54460 loss: 0.0034 lr: 0.02\n",
      "iteration: 54470 loss: 0.0039 lr: 0.02\n",
      "iteration: 54480 loss: 0.0033 lr: 0.02\n",
      "iteration: 54490 loss: 0.0055 lr: 0.02\n",
      "iteration: 54500 loss: 0.0037 lr: 0.02\n",
      "iteration: 54510 loss: 0.0046 lr: 0.02\n",
      "iteration: 54520 loss: 0.0035 lr: 0.02\n",
      "iteration: 54530 loss: 0.0036 lr: 0.02\n",
      "iteration: 54540 loss: 0.0034 lr: 0.02\n",
      "iteration: 54550 loss: 0.0028 lr: 0.02\n",
      "iteration: 54560 loss: 0.0038 lr: 0.02\n",
      "iteration: 54570 loss: 0.0026 lr: 0.02\n",
      "iteration: 54580 loss: 0.0024 lr: 0.02\n",
      "iteration: 54590 loss: 0.0034 lr: 0.02\n",
      "iteration: 54600 loss: 0.0037 lr: 0.02\n",
      "iteration: 54610 loss: 0.0034 lr: 0.02\n",
      "iteration: 54620 loss: 0.0037 lr: 0.02\n",
      "iteration: 54630 loss: 0.0042 lr: 0.02\n",
      "iteration: 54640 loss: 0.0034 lr: 0.02\n",
      "iteration: 54650 loss: 0.0033 lr: 0.02\n",
      "iteration: 54660 loss: 0.0036 lr: 0.02\n",
      "iteration: 54670 loss: 0.0039 lr: 0.02\n",
      "iteration: 54680 loss: 0.0049 lr: 0.02\n",
      "iteration: 54690 loss: 0.0038 lr: 0.02\n",
      "iteration: 54700 loss: 0.0042 lr: 0.02\n",
      "iteration: 54710 loss: 0.0041 lr: 0.02\n",
      "iteration: 54720 loss: 0.0032 lr: 0.02\n",
      "iteration: 54730 loss: 0.0034 lr: 0.02\n",
      "iteration: 54740 loss: 0.0035 lr: 0.02\n",
      "iteration: 54750 loss: 0.0031 lr: 0.02\n",
      "iteration: 54760 loss: 0.0038 lr: 0.02\n",
      "iteration: 54770 loss: 0.0039 lr: 0.02\n",
      "iteration: 54780 loss: 0.0049 lr: 0.02\n",
      "iteration: 54790 loss: 0.0040 lr: 0.02\n",
      "iteration: 54800 loss: 0.0045 lr: 0.02\n",
      "iteration: 54810 loss: 0.0032 lr: 0.02\n",
      "iteration: 54820 loss: 0.0039 lr: 0.02\n",
      "iteration: 54830 loss: 0.0034 lr: 0.02\n",
      "iteration: 54840 loss: 0.0031 lr: 0.02\n",
      "iteration: 54850 loss: 0.0034 lr: 0.02\n",
      "iteration: 54860 loss: 0.0039 lr: 0.02\n",
      "iteration: 54870 loss: 0.0048 lr: 0.02\n",
      "iteration: 54880 loss: 0.0035 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 54890 loss: 0.0043 lr: 0.02\n",
      "iteration: 54900 loss: 0.0031 lr: 0.02\n",
      "iteration: 54910 loss: 0.0034 lr: 0.02\n",
      "iteration: 54920 loss: 0.0032 lr: 0.02\n",
      "iteration: 54930 loss: 0.0036 lr: 0.02\n",
      "iteration: 54940 loss: 0.0043 lr: 0.02\n",
      "iteration: 54950 loss: 0.0040 lr: 0.02\n",
      "iteration: 54960 loss: 0.0036 lr: 0.02\n",
      "iteration: 54970 loss: 0.0031 lr: 0.02\n",
      "iteration: 54980 loss: 0.0051 lr: 0.02\n",
      "iteration: 54990 loss: 0.0045 lr: 0.02\n",
      "iteration: 55000 loss: 0.0030 lr: 0.02\n",
      "iteration: 55010 loss: 0.0039 lr: 0.02\n",
      "iteration: 55020 loss: 0.0043 lr: 0.02\n",
      "iteration: 55030 loss: 0.0032 lr: 0.02\n",
      "iteration: 55040 loss: 0.0048 lr: 0.02\n",
      "iteration: 55050 loss: 0.0032 lr: 0.02\n",
      "iteration: 55060 loss: 0.0035 lr: 0.02\n",
      "iteration: 55070 loss: 0.0036 lr: 0.02\n",
      "iteration: 55080 loss: 0.0039 lr: 0.02\n",
      "iteration: 55090 loss: 0.0032 lr: 0.02\n",
      "iteration: 55100 loss: 0.0032 lr: 0.02\n",
      "iteration: 55110 loss: 0.0034 lr: 0.02\n",
      "iteration: 55120 loss: 0.0028 lr: 0.02\n",
      "iteration: 55130 loss: 0.0037 lr: 0.02\n",
      "iteration: 55140 loss: 0.0034 lr: 0.02\n",
      "iteration: 55150 loss: 0.0046 lr: 0.02\n",
      "iteration: 55160 loss: 0.0034 lr: 0.02\n",
      "iteration: 55170 loss: 0.0030 lr: 0.02\n",
      "iteration: 55180 loss: 0.0036 lr: 0.02\n",
      "iteration: 55190 loss: 0.0045 lr: 0.02\n",
      "iteration: 55200 loss: 0.0040 lr: 0.02\n",
      "iteration: 55210 loss: 0.0035 lr: 0.02\n",
      "iteration: 55220 loss: 0.0040 lr: 0.02\n",
      "iteration: 55230 loss: 0.0041 lr: 0.02\n",
      "iteration: 55240 loss: 0.0040 lr: 0.02\n",
      "iteration: 55250 loss: 0.0039 lr: 0.02\n",
      "iteration: 55260 loss: 0.0041 lr: 0.02\n",
      "iteration: 55270 loss: 0.0048 lr: 0.02\n",
      "iteration: 55280 loss: 0.0035 lr: 0.02\n",
      "iteration: 55290 loss: 0.0031 lr: 0.02\n",
      "iteration: 55300 loss: 0.0025 lr: 0.02\n",
      "iteration: 55310 loss: 0.0039 lr: 0.02\n",
      "iteration: 55320 loss: 0.0036 lr: 0.02\n",
      "iteration: 55330 loss: 0.0044 lr: 0.02\n",
      "iteration: 55340 loss: 0.0041 lr: 0.02\n",
      "iteration: 55350 loss: 0.0039 lr: 0.02\n",
      "iteration: 55360 loss: 0.0040 lr: 0.02\n",
      "iteration: 55370 loss: 0.0038 lr: 0.02\n",
      "iteration: 55380 loss: 0.0029 lr: 0.02\n",
      "iteration: 55390 loss: 0.0033 lr: 0.02\n",
      "iteration: 55400 loss: 0.0036 lr: 0.02\n",
      "iteration: 55410 loss: 0.0037 lr: 0.02\n",
      "iteration: 55420 loss: 0.0042 lr: 0.02\n",
      "iteration: 55430 loss: 0.0033 lr: 0.02\n",
      "iteration: 55440 loss: 0.0036 lr: 0.02\n",
      "iteration: 55450 loss: 0.0046 lr: 0.02\n",
      "iteration: 55460 loss: 0.0035 lr: 0.02\n",
      "iteration: 55470 loss: 0.0034 lr: 0.02\n",
      "iteration: 55480 loss: 0.0043 lr: 0.02\n",
      "iteration: 55490 loss: 0.0055 lr: 0.02\n",
      "iteration: 55500 loss: 0.0033 lr: 0.02\n",
      "iteration: 55510 loss: 0.0034 lr: 0.02\n",
      "iteration: 55520 loss: 0.0037 lr: 0.02\n",
      "iteration: 55530 loss: 0.0039 lr: 0.02\n",
      "iteration: 55540 loss: 0.0036 lr: 0.02\n",
      "iteration: 55550 loss: 0.0029 lr: 0.02\n",
      "iteration: 55560 loss: 0.0049 lr: 0.02\n",
      "iteration: 55570 loss: 0.0033 lr: 0.02\n",
      "iteration: 55580 loss: 0.0028 lr: 0.02\n",
      "iteration: 55590 loss: 0.0026 lr: 0.02\n",
      "iteration: 55600 loss: 0.0037 lr: 0.02\n",
      "iteration: 55610 loss: 0.0029 lr: 0.02\n",
      "iteration: 55620 loss: 0.0034 lr: 0.02\n",
      "iteration: 55630 loss: 0.0040 lr: 0.02\n",
      "iteration: 55640 loss: 0.0045 lr: 0.02\n",
      "iteration: 55650 loss: 0.0040 lr: 0.02\n",
      "iteration: 55660 loss: 0.0038 lr: 0.02\n",
      "iteration: 55670 loss: 0.0038 lr: 0.02\n",
      "iteration: 55680 loss: 0.0042 lr: 0.02\n",
      "iteration: 55690 loss: 0.0030 lr: 0.02\n",
      "iteration: 55700 loss: 0.0038 lr: 0.02\n",
      "iteration: 55710 loss: 0.0035 lr: 0.02\n",
      "iteration: 55720 loss: 0.0051 lr: 0.02\n",
      "iteration: 55730 loss: 0.0035 lr: 0.02\n",
      "iteration: 55740 loss: 0.0038 lr: 0.02\n",
      "iteration: 55750 loss: 0.0038 lr: 0.02\n",
      "iteration: 55760 loss: 0.0036 lr: 0.02\n",
      "iteration: 55770 loss: 0.0032 lr: 0.02\n",
      "iteration: 55780 loss: 0.0038 lr: 0.02\n",
      "iteration: 55790 loss: 0.0032 lr: 0.02\n",
      "iteration: 55800 loss: 0.0031 lr: 0.02\n",
      "iteration: 55810 loss: 0.0037 lr: 0.02\n",
      "iteration: 55820 loss: 0.0037 lr: 0.02\n",
      "iteration: 55830 loss: 0.0046 lr: 0.02\n",
      "iteration: 55840 loss: 0.0031 lr: 0.02\n",
      "iteration: 55850 loss: 0.0041 lr: 0.02\n",
      "iteration: 55860 loss: 0.0054 lr: 0.02\n",
      "iteration: 55870 loss: 0.0048 lr: 0.02\n",
      "iteration: 55880 loss: 0.0040 lr: 0.02\n",
      "iteration: 55890 loss: 0.0046 lr: 0.02\n",
      "iteration: 55900 loss: 0.0034 lr: 0.02\n",
      "iteration: 55910 loss: 0.0041 lr: 0.02\n",
      "iteration: 55920 loss: 0.0040 lr: 0.02\n",
      "iteration: 55930 loss: 0.0036 lr: 0.02\n",
      "iteration: 55940 loss: 0.0036 lr: 0.02\n",
      "iteration: 55950 loss: 0.0046 lr: 0.02\n",
      "iteration: 55960 loss: 0.0028 lr: 0.02\n",
      "iteration: 55970 loss: 0.0027 lr: 0.02\n",
      "iteration: 55980 loss: 0.0029 lr: 0.02\n",
      "iteration: 55990 loss: 0.0040 lr: 0.02\n",
      "iteration: 56000 loss: 0.0043 lr: 0.02\n",
      "iteration: 56010 loss: 0.0041 lr: 0.02\n",
      "iteration: 56020 loss: 0.0049 lr: 0.02\n",
      "iteration: 56030 loss: 0.0044 lr: 0.02\n",
      "iteration: 56040 loss: 0.0041 lr: 0.02\n",
      "iteration: 56050 loss: 0.0042 lr: 0.02\n",
      "iteration: 56060 loss: 0.0041 lr: 0.02\n",
      "iteration: 56070 loss: 0.0030 lr: 0.02\n",
      "iteration: 56080 loss: 0.0028 lr: 0.02\n",
      "iteration: 56090 loss: 0.0038 lr: 0.02\n",
      "iteration: 56100 loss: 0.0039 lr: 0.02\n",
      "iteration: 56110 loss: 0.0052 lr: 0.02\n",
      "iteration: 56120 loss: 0.0048 lr: 0.02\n",
      "iteration: 56130 loss: 0.0041 lr: 0.02\n",
      "iteration: 56140 loss: 0.0052 lr: 0.02\n",
      "iteration: 56150 loss: 0.0049 lr: 0.02\n",
      "iteration: 56160 loss: 0.0046 lr: 0.02\n",
      "iteration: 56170 loss: 0.0031 lr: 0.02\n",
      "iteration: 56180 loss: 0.0034 lr: 0.02\n",
      "iteration: 56190 loss: 0.0026 lr: 0.02\n",
      "iteration: 56200 loss: 0.0046 lr: 0.02\n",
      "iteration: 56210 loss: 0.0035 lr: 0.02\n",
      "iteration: 56220 loss: 0.0047 lr: 0.02\n",
      "iteration: 56230 loss: 0.0032 lr: 0.02\n",
      "iteration: 56240 loss: 0.0038 lr: 0.02\n",
      "iteration: 56250 loss: 0.0035 lr: 0.02\n",
      "iteration: 56260 loss: 0.0036 lr: 0.02\n",
      "iteration: 56270 loss: 0.0038 lr: 0.02\n",
      "iteration: 56280 loss: 0.0033 lr: 0.02\n",
      "iteration: 56290 loss: 0.0038 lr: 0.02\n",
      "iteration: 56300 loss: 0.0039 lr: 0.02\n",
      "iteration: 56310 loss: 0.0039 lr: 0.02\n",
      "iteration: 56320 loss: 0.0043 lr: 0.02\n",
      "iteration: 56330 loss: 0.0039 lr: 0.02\n",
      "iteration: 56340 loss: 0.0030 lr: 0.02\n",
      "iteration: 56350 loss: 0.0033 lr: 0.02\n",
      "iteration: 56360 loss: 0.0031 lr: 0.02\n",
      "iteration: 56370 loss: 0.0035 lr: 0.02\n",
      "iteration: 56380 loss: 0.0035 lr: 0.02\n",
      "iteration: 56390 loss: 0.0034 lr: 0.02\n",
      "iteration: 56400 loss: 0.0038 lr: 0.02\n",
      "iteration: 56410 loss: 0.0033 lr: 0.02\n",
      "iteration: 56420 loss: 0.0030 lr: 0.02\n",
      "iteration: 56430 loss: 0.0037 lr: 0.02\n",
      "iteration: 56440 loss: 0.0038 lr: 0.02\n",
      "iteration: 56450 loss: 0.0037 lr: 0.02\n",
      "iteration: 56460 loss: 0.0031 lr: 0.02\n",
      "iteration: 56470 loss: 0.0031 lr: 0.02\n",
      "iteration: 56480 loss: 0.0038 lr: 0.02\n",
      "iteration: 56490 loss: 0.0040 lr: 0.02\n",
      "iteration: 56500 loss: 0.0034 lr: 0.02\n",
      "iteration: 56510 loss: 0.0038 lr: 0.02\n",
      "iteration: 56520 loss: 0.0043 lr: 0.02\n",
      "iteration: 56530 loss: 0.0031 lr: 0.02\n",
      "iteration: 56540 loss: 0.0034 lr: 0.02\n",
      "iteration: 56550 loss: 0.0043 lr: 0.02\n",
      "iteration: 56560 loss: 0.0035 lr: 0.02\n",
      "iteration: 56570 loss: 0.0032 lr: 0.02\n",
      "iteration: 56580 loss: 0.0051 lr: 0.02\n",
      "iteration: 56590 loss: 0.0038 lr: 0.02\n",
      "iteration: 56600 loss: 0.0049 lr: 0.02\n",
      "iteration: 56610 loss: 0.0038 lr: 0.02\n",
      "iteration: 56620 loss: 0.0057 lr: 0.02\n",
      "iteration: 56630 loss: 0.0033 lr: 0.02\n",
      "iteration: 56640 loss: 0.0041 lr: 0.02\n",
      "iteration: 56650 loss: 0.0032 lr: 0.02\n",
      "iteration: 56660 loss: 0.0035 lr: 0.02\n",
      "iteration: 56670 loss: 0.0040 lr: 0.02\n",
      "iteration: 56680 loss: 0.0050 lr: 0.02\n",
      "iteration: 56690 loss: 0.0044 lr: 0.02\n",
      "iteration: 56700 loss: 0.0029 lr: 0.02\n",
      "iteration: 56710 loss: 0.0031 lr: 0.02\n",
      "iteration: 56720 loss: 0.0039 lr: 0.02\n",
      "iteration: 56730 loss: 0.0037 lr: 0.02\n",
      "iteration: 56740 loss: 0.0043 lr: 0.02\n",
      "iteration: 56750 loss: 0.0058 lr: 0.02\n",
      "iteration: 56760 loss: 0.0038 lr: 0.02\n",
      "iteration: 56770 loss: 0.0038 lr: 0.02\n",
      "iteration: 56780 loss: 0.0035 lr: 0.02\n",
      "iteration: 56790 loss: 0.0040 lr: 0.02\n",
      "iteration: 56800 loss: 0.0035 lr: 0.02\n",
      "iteration: 56810 loss: 0.0038 lr: 0.02\n",
      "iteration: 56820 loss: 0.0040 lr: 0.02\n",
      "iteration: 56830 loss: 0.0038 lr: 0.02\n",
      "iteration: 56840 loss: 0.0026 lr: 0.02\n",
      "iteration: 56850 loss: 0.0043 lr: 0.02\n",
      "iteration: 56860 loss: 0.0033 lr: 0.02\n",
      "iteration: 56870 loss: 0.0042 lr: 0.02\n",
      "iteration: 56880 loss: 0.0031 lr: 0.02\n",
      "iteration: 56890 loss: 0.0030 lr: 0.02\n",
      "iteration: 56900 loss: 0.0031 lr: 0.02\n",
      "iteration: 56910 loss: 0.0037 lr: 0.02\n",
      "iteration: 56920 loss: 0.0042 lr: 0.02\n",
      "iteration: 56930 loss: 0.0028 lr: 0.02\n",
      "iteration: 56940 loss: 0.0031 lr: 0.02\n",
      "iteration: 56950 loss: 0.0052 lr: 0.02\n",
      "iteration: 56960 loss: 0.0043 lr: 0.02\n",
      "iteration: 56970 loss: 0.0059 lr: 0.02\n",
      "iteration: 56980 loss: 0.0050 lr: 0.02\n",
      "iteration: 56990 loss: 0.0037 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 57000 loss: 0.0046 lr: 0.02\n",
      "iteration: 57010 loss: 0.0039 lr: 0.02\n",
      "iteration: 57020 loss: 0.0045 lr: 0.02\n",
      "iteration: 57030 loss: 0.0037 lr: 0.02\n",
      "iteration: 57040 loss: 0.0029 lr: 0.02\n",
      "iteration: 57050 loss: 0.0057 lr: 0.02\n",
      "iteration: 57060 loss: 0.0047 lr: 0.02\n",
      "iteration: 57070 loss: 0.0042 lr: 0.02\n",
      "iteration: 57080 loss: 0.0037 lr: 0.02\n",
      "iteration: 57090 loss: 0.0033 lr: 0.02\n",
      "iteration: 57100 loss: 0.0040 lr: 0.02\n",
      "iteration: 57110 loss: 0.0033 lr: 0.02\n",
      "iteration: 57120 loss: 0.0033 lr: 0.02\n",
      "iteration: 57130 loss: 0.0031 lr: 0.02\n",
      "iteration: 57140 loss: 0.0038 lr: 0.02\n",
      "iteration: 57150 loss: 0.0051 lr: 0.02\n",
      "iteration: 57160 loss: 0.0038 lr: 0.02\n",
      "iteration: 57170 loss: 0.0042 lr: 0.02\n",
      "iteration: 57180 loss: 0.0056 lr: 0.02\n",
      "iteration: 57190 loss: 0.0036 lr: 0.02\n",
      "iteration: 57200 loss: 0.0029 lr: 0.02\n",
      "iteration: 57210 loss: 0.0046 lr: 0.02\n",
      "iteration: 57220 loss: 0.0038 lr: 0.02\n",
      "iteration: 57230 loss: 0.0041 lr: 0.02\n",
      "iteration: 57240 loss: 0.0040 lr: 0.02\n",
      "iteration: 57250 loss: 0.0030 lr: 0.02\n",
      "iteration: 57260 loss: 0.0037 lr: 0.02\n",
      "iteration: 57270 loss: 0.0040 lr: 0.02\n",
      "iteration: 57280 loss: 0.0042 lr: 0.02\n",
      "iteration: 57290 loss: 0.0035 lr: 0.02\n",
      "iteration: 57300 loss: 0.0028 lr: 0.02\n",
      "iteration: 57310 loss: 0.0031 lr: 0.02\n",
      "iteration: 57320 loss: 0.0037 lr: 0.02\n",
      "iteration: 57330 loss: 0.0040 lr: 0.02\n",
      "iteration: 57340 loss: 0.0033 lr: 0.02\n",
      "iteration: 57350 loss: 0.0043 lr: 0.02\n",
      "iteration: 57360 loss: 0.0033 lr: 0.02\n",
      "iteration: 57370 loss: 0.0033 lr: 0.02\n",
      "iteration: 57380 loss: 0.0034 lr: 0.02\n",
      "iteration: 57390 loss: 0.0035 lr: 0.02\n",
      "iteration: 57400 loss: 0.0031 lr: 0.02\n",
      "iteration: 57410 loss: 0.0051 lr: 0.02\n",
      "iteration: 57420 loss: 0.0037 lr: 0.02\n",
      "iteration: 57430 loss: 0.0039 lr: 0.02\n",
      "iteration: 57440 loss: 0.0038 lr: 0.02\n",
      "iteration: 57450 loss: 0.0039 lr: 0.02\n",
      "iteration: 57460 loss: 0.0043 lr: 0.02\n",
      "iteration: 57470 loss: 0.0041 lr: 0.02\n",
      "iteration: 57480 loss: 0.0025 lr: 0.02\n",
      "iteration: 57490 loss: 0.0046 lr: 0.02\n",
      "iteration: 57500 loss: 0.0049 lr: 0.02\n",
      "iteration: 57510 loss: 0.0029 lr: 0.02\n",
      "iteration: 57520 loss: 0.0037 lr: 0.02\n",
      "iteration: 57530 loss: 0.0041 lr: 0.02\n",
      "iteration: 57540 loss: 0.0037 lr: 0.02\n",
      "iteration: 57550 loss: 0.0039 lr: 0.02\n",
      "iteration: 57560 loss: 0.0034 lr: 0.02\n",
      "iteration: 57570 loss: 0.0051 lr: 0.02\n",
      "iteration: 57580 loss: 0.0029 lr: 0.02\n",
      "iteration: 57590 loss: 0.0043 lr: 0.02\n",
      "iteration: 57600 loss: 0.0040 lr: 0.02\n",
      "iteration: 57610 loss: 0.0034 lr: 0.02\n",
      "iteration: 57620 loss: 0.0040 lr: 0.02\n",
      "iteration: 57630 loss: 0.0044 lr: 0.02\n",
      "iteration: 57640 loss: 0.0035 lr: 0.02\n",
      "iteration: 57650 loss: 0.0031 lr: 0.02\n",
      "iteration: 57660 loss: 0.0031 lr: 0.02\n",
      "iteration: 57670 loss: 0.0037 lr: 0.02\n",
      "iteration: 57680 loss: 0.0038 lr: 0.02\n",
      "iteration: 57690 loss: 0.0031 lr: 0.02\n",
      "iteration: 57700 loss: 0.0034 lr: 0.02\n",
      "iteration: 57710 loss: 0.0036 lr: 0.02\n",
      "iteration: 57720 loss: 0.0030 lr: 0.02\n",
      "iteration: 57730 loss: 0.0030 lr: 0.02\n",
      "iteration: 57740 loss: 0.0040 lr: 0.02\n",
      "iteration: 57750 loss: 0.0031 lr: 0.02\n",
      "iteration: 57760 loss: 0.0040 lr: 0.02\n",
      "iteration: 57770 loss: 0.0029 lr: 0.02\n",
      "iteration: 57780 loss: 0.0029 lr: 0.02\n",
      "iteration: 57790 loss: 0.0024 lr: 0.02\n",
      "iteration: 57800 loss: 0.0040 lr: 0.02\n",
      "iteration: 57810 loss: 0.0031 lr: 0.02\n",
      "iteration: 57820 loss: 0.0036 lr: 0.02\n",
      "iteration: 57830 loss: 0.0039 lr: 0.02\n",
      "iteration: 57840 loss: 0.0034 lr: 0.02\n",
      "iteration: 57850 loss: 0.0045 lr: 0.02\n",
      "iteration: 57860 loss: 0.0044 lr: 0.02\n",
      "iteration: 57870 loss: 0.0041 lr: 0.02\n",
      "iteration: 57880 loss: 0.0032 lr: 0.02\n",
      "iteration: 57890 loss: 0.0032 lr: 0.02\n",
      "iteration: 57900 loss: 0.0040 lr: 0.02\n",
      "iteration: 57910 loss: 0.0035 lr: 0.02\n",
      "iteration: 57920 loss: 0.0039 lr: 0.02\n",
      "iteration: 57930 loss: 0.0046 lr: 0.02\n",
      "iteration: 57940 loss: 0.0046 lr: 0.02\n",
      "iteration: 57950 loss: 0.0032 lr: 0.02\n",
      "iteration: 57960 loss: 0.0034 lr: 0.02\n",
      "iteration: 57970 loss: 0.0028 lr: 0.02\n",
      "iteration: 57980 loss: 0.0029 lr: 0.02\n",
      "iteration: 57990 loss: 0.0031 lr: 0.02\n",
      "iteration: 58000 loss: 0.0035 lr: 0.02\n",
      "iteration: 58010 loss: 0.0031 lr: 0.02\n",
      "iteration: 58020 loss: 0.0033 lr: 0.02\n",
      "iteration: 58030 loss: 0.0036 lr: 0.02\n",
      "iteration: 58040 loss: 0.0031 lr: 0.02\n",
      "iteration: 58050 loss: 0.0035 lr: 0.02\n",
      "iteration: 58060 loss: 0.0040 lr: 0.02\n",
      "iteration: 58070 loss: 0.0033 lr: 0.02\n",
      "iteration: 58080 loss: 0.0049 lr: 0.02\n",
      "iteration: 58090 loss: 0.0037 lr: 0.02\n",
      "iteration: 58100 loss: 0.0025 lr: 0.02\n",
      "iteration: 58110 loss: 0.0046 lr: 0.02\n",
      "iteration: 58120 loss: 0.0033 lr: 0.02\n",
      "iteration: 58130 loss: 0.0045 lr: 0.02\n",
      "iteration: 58140 loss: 0.0034 lr: 0.02\n",
      "iteration: 58150 loss: 0.0041 lr: 0.02\n",
      "iteration: 58160 loss: 0.0042 lr: 0.02\n",
      "iteration: 58170 loss: 0.0031 lr: 0.02\n",
      "iteration: 58180 loss: 0.0036 lr: 0.02\n",
      "iteration: 58190 loss: 0.0037 lr: 0.02\n",
      "iteration: 58200 loss: 0.0033 lr: 0.02\n",
      "iteration: 58210 loss: 0.0045 lr: 0.02\n",
      "iteration: 58220 loss: 0.0032 lr: 0.02\n",
      "iteration: 58230 loss: 0.0040 lr: 0.02\n",
      "iteration: 58240 loss: 0.0030 lr: 0.02\n",
      "iteration: 58250 loss: 0.0050 lr: 0.02\n",
      "iteration: 58260 loss: 0.0039 lr: 0.02\n",
      "iteration: 58270 loss: 0.0042 lr: 0.02\n",
      "iteration: 58280 loss: 0.0033 lr: 0.02\n",
      "iteration: 58290 loss: 0.0045 lr: 0.02\n",
      "iteration: 58300 loss: 0.0032 lr: 0.02\n",
      "iteration: 58310 loss: 0.0036 lr: 0.02\n",
      "iteration: 58320 loss: 0.0036 lr: 0.02\n",
      "iteration: 58330 loss: 0.0040 lr: 0.02\n",
      "iteration: 58340 loss: 0.0041 lr: 0.02\n",
      "iteration: 58350 loss: 0.0043 lr: 0.02\n",
      "iteration: 58360 loss: 0.0046 lr: 0.02\n",
      "iteration: 58370 loss: 0.0037 lr: 0.02\n",
      "iteration: 58380 loss: 0.0040 lr: 0.02\n",
      "iteration: 58390 loss: 0.0044 lr: 0.02\n",
      "iteration: 58400 loss: 0.0033 lr: 0.02\n",
      "iteration: 58410 loss: 0.0038 lr: 0.02\n",
      "iteration: 58420 loss: 0.0043 lr: 0.02\n",
      "iteration: 58430 loss: 0.0045 lr: 0.02\n",
      "iteration: 58440 loss: 0.0029 lr: 0.02\n",
      "iteration: 58450 loss: 0.0044 lr: 0.02\n",
      "iteration: 58460 loss: 0.0049 lr: 0.02\n",
      "iteration: 58470 loss: 0.0034 lr: 0.02\n",
      "iteration: 58480 loss: 0.0035 lr: 0.02\n",
      "iteration: 58490 loss: 0.0049 lr: 0.02\n",
      "iteration: 58500 loss: 0.0041 lr: 0.02\n",
      "iteration: 58510 loss: 0.0029 lr: 0.02\n",
      "iteration: 58520 loss: 0.0032 lr: 0.02\n",
      "iteration: 58530 loss: 0.0042 lr: 0.02\n",
      "iteration: 58540 loss: 0.0038 lr: 0.02\n",
      "iteration: 58550 loss: 0.0029 lr: 0.02\n",
      "iteration: 58560 loss: 0.0033 lr: 0.02\n",
      "iteration: 58570 loss: 0.0034 lr: 0.02\n",
      "iteration: 58580 loss: 0.0024 lr: 0.02\n",
      "iteration: 58590 loss: 0.0038 lr: 0.02\n",
      "iteration: 58600 loss: 0.0043 lr: 0.02\n",
      "iteration: 58610 loss: 0.0031 lr: 0.02\n",
      "iteration: 58620 loss: 0.0032 lr: 0.02\n",
      "iteration: 58630 loss: 0.0035 lr: 0.02\n",
      "iteration: 58640 loss: 0.0030 lr: 0.02\n",
      "iteration: 58650 loss: 0.0044 lr: 0.02\n",
      "iteration: 58660 loss: 0.0042 lr: 0.02\n",
      "iteration: 58670 loss: 0.0041 lr: 0.02\n",
      "iteration: 58680 loss: 0.0036 lr: 0.02\n",
      "iteration: 58690 loss: 0.0036 lr: 0.02\n",
      "iteration: 58700 loss: 0.0036 lr: 0.02\n",
      "iteration: 58710 loss: 0.0029 lr: 0.02\n",
      "iteration: 58720 loss: 0.0037 lr: 0.02\n",
      "iteration: 58730 loss: 0.0034 lr: 0.02\n",
      "iteration: 58740 loss: 0.0032 lr: 0.02\n",
      "iteration: 58750 loss: 0.0051 lr: 0.02\n",
      "iteration: 58760 loss: 0.0047 lr: 0.02\n",
      "iteration: 58770 loss: 0.0046 lr: 0.02\n",
      "iteration: 58780 loss: 0.0039 lr: 0.02\n",
      "iteration: 58790 loss: 0.0041 lr: 0.02\n",
      "iteration: 58800 loss: 0.0030 lr: 0.02\n",
      "iteration: 58810 loss: 0.0039 lr: 0.02\n",
      "iteration: 58820 loss: 0.0033 lr: 0.02\n",
      "iteration: 58830 loss: 0.0041 lr: 0.02\n",
      "iteration: 58840 loss: 0.0035 lr: 0.02\n",
      "iteration: 58850 loss: 0.0044 lr: 0.02\n",
      "iteration: 58860 loss: 0.0054 lr: 0.02\n",
      "iteration: 58870 loss: 0.0039 lr: 0.02\n",
      "iteration: 58880 loss: 0.0037 lr: 0.02\n",
      "iteration: 58890 loss: 0.0029 lr: 0.02\n",
      "iteration: 58900 loss: 0.0038 lr: 0.02\n",
      "iteration: 58910 loss: 0.0032 lr: 0.02\n",
      "iteration: 58920 loss: 0.0041 lr: 0.02\n",
      "iteration: 58930 loss: 0.0047 lr: 0.02\n",
      "iteration: 58940 loss: 0.0041 lr: 0.02\n",
      "iteration: 58950 loss: 0.0030 lr: 0.02\n",
      "iteration: 58960 loss: 0.0033 lr: 0.02\n",
      "iteration: 58970 loss: 0.0037 lr: 0.02\n",
      "iteration: 58980 loss: 0.0042 lr: 0.02\n",
      "iteration: 58990 loss: 0.0046 lr: 0.02\n",
      "iteration: 59000 loss: 0.0033 lr: 0.02\n",
      "iteration: 59010 loss: 0.0031 lr: 0.02\n",
      "iteration: 59020 loss: 0.0044 lr: 0.02\n",
      "iteration: 59030 loss: 0.0033 lr: 0.02\n",
      "iteration: 59040 loss: 0.0036 lr: 0.02\n",
      "iteration: 59050 loss: 0.0036 lr: 0.02\n",
      "iteration: 59060 loss: 0.0038 lr: 0.02\n",
      "iteration: 59070 loss: 0.0040 lr: 0.02\n",
      "iteration: 59080 loss: 0.0034 lr: 0.02\n",
      "iteration: 59090 loss: 0.0042 lr: 0.02\n",
      "iteration: 59100 loss: 0.0031 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 59110 loss: 0.0035 lr: 0.02\n",
      "iteration: 59120 loss: 0.0033 lr: 0.02\n",
      "iteration: 59130 loss: 0.0041 lr: 0.02\n",
      "iteration: 59140 loss: 0.0030 lr: 0.02\n",
      "iteration: 59150 loss: 0.0031 lr: 0.02\n",
      "iteration: 59160 loss: 0.0045 lr: 0.02\n",
      "iteration: 59170 loss: 0.0031 lr: 0.02\n",
      "iteration: 59180 loss: 0.0035 lr: 0.02\n",
      "iteration: 59190 loss: 0.0042 lr: 0.02\n",
      "iteration: 59200 loss: 0.0031 lr: 0.02\n",
      "iteration: 59210 loss: 0.0035 lr: 0.02\n",
      "iteration: 59220 loss: 0.0044 lr: 0.02\n",
      "iteration: 59230 loss: 0.0033 lr: 0.02\n",
      "iteration: 59240 loss: 0.0041 lr: 0.02\n",
      "iteration: 59250 loss: 0.0034 lr: 0.02\n",
      "iteration: 59260 loss: 0.0039 lr: 0.02\n",
      "iteration: 59270 loss: 0.0033 lr: 0.02\n",
      "iteration: 59280 loss: 0.0032 lr: 0.02\n",
      "iteration: 59290 loss: 0.0030 lr: 0.02\n",
      "iteration: 59300 loss: 0.0036 lr: 0.02\n",
      "iteration: 59310 loss: 0.0027 lr: 0.02\n",
      "iteration: 59320 loss: 0.0036 lr: 0.02\n",
      "iteration: 59330 loss: 0.0032 lr: 0.02\n",
      "iteration: 59340 loss: 0.0035 lr: 0.02\n",
      "iteration: 59350 loss: 0.0045 lr: 0.02\n",
      "iteration: 59360 loss: 0.0033 lr: 0.02\n",
      "iteration: 59370 loss: 0.0039 lr: 0.02\n",
      "iteration: 59380 loss: 0.0035 lr: 0.02\n",
      "iteration: 59390 loss: 0.0036 lr: 0.02\n",
      "iteration: 59400 loss: 0.0040 lr: 0.02\n",
      "iteration: 59410 loss: 0.0039 lr: 0.02\n",
      "iteration: 59420 loss: 0.0039 lr: 0.02\n",
      "iteration: 59430 loss: 0.0039 lr: 0.02\n",
      "iteration: 59440 loss: 0.0034 lr: 0.02\n",
      "iteration: 59450 loss: 0.0036 lr: 0.02\n",
      "iteration: 59460 loss: 0.0028 lr: 0.02\n",
      "iteration: 59470 loss: 0.0057 lr: 0.02\n",
      "iteration: 59480 loss: 0.0027 lr: 0.02\n",
      "iteration: 59490 loss: 0.0041 lr: 0.02\n",
      "iteration: 59500 loss: 0.0051 lr: 0.02\n",
      "iteration: 59510 loss: 0.0036 lr: 0.02\n",
      "iteration: 59520 loss: 0.0041 lr: 0.02\n",
      "iteration: 59530 loss: 0.0035 lr: 0.02\n",
      "iteration: 59540 loss: 0.0033 lr: 0.02\n",
      "iteration: 59550 loss: 0.0047 lr: 0.02\n",
      "iteration: 59560 loss: 0.0038 lr: 0.02\n",
      "iteration: 59570 loss: 0.0034 lr: 0.02\n",
      "iteration: 59580 loss: 0.0035 lr: 0.02\n",
      "iteration: 59590 loss: 0.0042 lr: 0.02\n",
      "iteration: 59600 loss: 0.0039 lr: 0.02\n",
      "iteration: 59610 loss: 0.0038 lr: 0.02\n",
      "iteration: 59620 loss: 0.0041 lr: 0.02\n",
      "iteration: 59630 loss: 0.0026 lr: 0.02\n",
      "iteration: 59640 loss: 0.0044 lr: 0.02\n",
      "iteration: 59650 loss: 0.0039 lr: 0.02\n",
      "iteration: 59660 loss: 0.0026 lr: 0.02\n",
      "iteration: 59670 loss: 0.0027 lr: 0.02\n",
      "iteration: 59680 loss: 0.0036 lr: 0.02\n",
      "iteration: 59690 loss: 0.0029 lr: 0.02\n",
      "iteration: 59700 loss: 0.0034 lr: 0.02\n",
      "iteration: 59710 loss: 0.0034 lr: 0.02\n",
      "iteration: 59720 loss: 0.0041 lr: 0.02\n",
      "iteration: 59730 loss: 0.0048 lr: 0.02\n",
      "iteration: 59740 loss: 0.0052 lr: 0.02\n",
      "iteration: 59750 loss: 0.0039 lr: 0.02\n",
      "iteration: 59760 loss: 0.0037 lr: 0.02\n",
      "iteration: 59770 loss: 0.0040 lr: 0.02\n",
      "iteration: 59780 loss: 0.0046 lr: 0.02\n",
      "iteration: 59790 loss: 0.0037 lr: 0.02\n",
      "iteration: 59800 loss: 0.0032 lr: 0.02\n",
      "iteration: 59810 loss: 0.0042 lr: 0.02\n",
      "iteration: 59820 loss: 0.0048 lr: 0.02\n",
      "iteration: 59830 loss: 0.0036 lr: 0.02\n",
      "iteration: 59840 loss: 0.0026 lr: 0.02\n",
      "iteration: 59850 loss: 0.0042 lr: 0.02\n",
      "iteration: 59860 loss: 0.0044 lr: 0.02\n",
      "iteration: 59870 loss: 0.0034 lr: 0.02\n",
      "iteration: 59880 loss: 0.0041 lr: 0.02\n",
      "iteration: 59890 loss: 0.0035 lr: 0.02\n",
      "iteration: 59900 loss: 0.0034 lr: 0.02\n",
      "iteration: 59910 loss: 0.0042 lr: 0.02\n",
      "iteration: 59920 loss: 0.0039 lr: 0.02\n",
      "iteration: 59930 loss: 0.0036 lr: 0.02\n",
      "iteration: 59940 loss: 0.0040 lr: 0.02\n",
      "iteration: 59950 loss: 0.0040 lr: 0.02\n",
      "iteration: 59960 loss: 0.0034 lr: 0.02\n",
      "iteration: 59970 loss: 0.0027 lr: 0.02\n",
      "iteration: 59980 loss: 0.0042 lr: 0.02\n",
      "iteration: 59990 loss: 0.0043 lr: 0.02\n",
      "iteration: 60000 loss: 0.0041 lr: 0.02\n",
      "iteration: 60010 loss: 0.0041 lr: 0.02\n",
      "iteration: 60020 loss: 0.0038 lr: 0.02\n",
      "iteration: 60030 loss: 0.0040 lr: 0.02\n",
      "iteration: 60040 loss: 0.0045 lr: 0.02\n",
      "iteration: 60050 loss: 0.0032 lr: 0.02\n",
      "iteration: 60060 loss: 0.0042 lr: 0.02\n",
      "iteration: 60070 loss: 0.0036 lr: 0.02\n",
      "iteration: 60080 loss: 0.0031 lr: 0.02\n",
      "iteration: 60090 loss: 0.0037 lr: 0.02\n",
      "iteration: 60100 loss: 0.0030 lr: 0.02\n",
      "iteration: 60110 loss: 0.0026 lr: 0.02\n",
      "iteration: 60120 loss: 0.0030 lr: 0.02\n",
      "iteration: 60130 loss: 0.0038 lr: 0.02\n",
      "iteration: 60140 loss: 0.0036 lr: 0.02\n",
      "iteration: 60150 loss: 0.0035 lr: 0.02\n",
      "iteration: 60160 loss: 0.0025 lr: 0.02\n",
      "iteration: 60170 loss: 0.0043 lr: 0.02\n",
      "iteration: 60180 loss: 0.0044 lr: 0.02\n",
      "iteration: 60190 loss: 0.0037 lr: 0.02\n",
      "iteration: 60200 loss: 0.0042 lr: 0.02\n",
      "iteration: 60210 loss: 0.0031 lr: 0.02\n",
      "iteration: 60220 loss: 0.0043 lr: 0.02\n",
      "iteration: 60230 loss: 0.0044 lr: 0.02\n",
      "iteration: 60240 loss: 0.0037 lr: 0.02\n",
      "iteration: 60250 loss: 0.0033 lr: 0.02\n",
      "iteration: 60260 loss: 0.0042 lr: 0.02\n",
      "iteration: 60270 loss: 0.0036 lr: 0.02\n",
      "iteration: 60280 loss: 0.0044 lr: 0.02\n",
      "iteration: 60290 loss: 0.0037 lr: 0.02\n",
      "iteration: 60300 loss: 0.0034 lr: 0.02\n",
      "iteration: 60310 loss: 0.0046 lr: 0.02\n",
      "iteration: 60320 loss: 0.0029 lr: 0.02\n",
      "iteration: 60330 loss: 0.0033 lr: 0.02\n",
      "iteration: 60340 loss: 0.0032 lr: 0.02\n",
      "iteration: 60350 loss: 0.0035 lr: 0.02\n",
      "iteration: 60360 loss: 0.0041 lr: 0.02\n",
      "iteration: 60370 loss: 0.0044 lr: 0.02\n",
      "iteration: 60380 loss: 0.0037 lr: 0.02\n",
      "iteration: 60390 loss: 0.0035 lr: 0.02\n",
      "iteration: 60400 loss: 0.0032 lr: 0.02\n",
      "iteration: 60410 loss: 0.0032 lr: 0.02\n",
      "iteration: 60420 loss: 0.0037 lr: 0.02\n",
      "iteration: 60430 loss: 0.0031 lr: 0.02\n",
      "iteration: 60440 loss: 0.0039 lr: 0.02\n",
      "iteration: 60450 loss: 0.0036 lr: 0.02\n",
      "iteration: 60460 loss: 0.0033 lr: 0.02\n",
      "iteration: 60470 loss: 0.0036 lr: 0.02\n",
      "iteration: 60480 loss: 0.0033 lr: 0.02\n",
      "iteration: 60490 loss: 0.0040 lr: 0.02\n",
      "iteration: 60500 loss: 0.0029 lr: 0.02\n",
      "iteration: 60510 loss: 0.0038 lr: 0.02\n",
      "iteration: 60520 loss: 0.0033 lr: 0.02\n",
      "iteration: 60530 loss: 0.0053 lr: 0.02\n",
      "iteration: 60540 loss: 0.0040 lr: 0.02\n",
      "iteration: 60550 loss: 0.0031 lr: 0.02\n",
      "iteration: 60560 loss: 0.0033 lr: 0.02\n",
      "iteration: 60570 loss: 0.0040 lr: 0.02\n",
      "iteration: 60580 loss: 0.0034 lr: 0.02\n",
      "iteration: 60590 loss: 0.0035 lr: 0.02\n",
      "iteration: 60600 loss: 0.0037 lr: 0.02\n",
      "iteration: 60610 loss: 0.0033 lr: 0.02\n",
      "iteration: 60620 loss: 0.0033 lr: 0.02\n",
      "iteration: 60630 loss: 0.0034 lr: 0.02\n",
      "iteration: 60640 loss: 0.0043 lr: 0.02\n",
      "iteration: 60650 loss: 0.0040 lr: 0.02\n",
      "iteration: 60660 loss: 0.0031 lr: 0.02\n",
      "iteration: 60670 loss: 0.0050 lr: 0.02\n",
      "iteration: 60680 loss: 0.0037 lr: 0.02\n",
      "iteration: 60690 loss: 0.0040 lr: 0.02\n",
      "iteration: 60700 loss: 0.0042 lr: 0.02\n",
      "iteration: 60710 loss: 0.0043 lr: 0.02\n",
      "iteration: 60720 loss: 0.0039 lr: 0.02\n",
      "iteration: 60730 loss: 0.0038 lr: 0.02\n",
      "iteration: 60740 loss: 0.0032 lr: 0.02\n",
      "iteration: 60750 loss: 0.0031 lr: 0.02\n",
      "iteration: 60760 loss: 0.0045 lr: 0.02\n",
      "iteration: 60770 loss: 0.0026 lr: 0.02\n",
      "iteration: 60780 loss: 0.0034 lr: 0.02\n",
      "iteration: 60790 loss: 0.0035 lr: 0.02\n",
      "iteration: 60800 loss: 0.0035 lr: 0.02\n",
      "iteration: 60810 loss: 0.0048 lr: 0.02\n",
      "iteration: 60820 loss: 0.0042 lr: 0.02\n",
      "iteration: 60830 loss: 0.0045 lr: 0.02\n",
      "iteration: 60840 loss: 0.0030 lr: 0.02\n",
      "iteration: 60850 loss: 0.0036 lr: 0.02\n",
      "iteration: 60860 loss: 0.0029 lr: 0.02\n",
      "iteration: 60870 loss: 0.0037 lr: 0.02\n",
      "iteration: 60880 loss: 0.0065 lr: 0.02\n",
      "iteration: 60890 loss: 0.0049 lr: 0.02\n",
      "iteration: 60900 loss: 0.0046 lr: 0.02\n",
      "iteration: 60910 loss: 0.0033 lr: 0.02\n",
      "iteration: 60920 loss: 0.0034 lr: 0.02\n",
      "iteration: 60930 loss: 0.0042 lr: 0.02\n",
      "iteration: 60940 loss: 0.0038 lr: 0.02\n",
      "iteration: 60950 loss: 0.0036 lr: 0.02\n",
      "iteration: 60960 loss: 0.0038 lr: 0.02\n",
      "iteration: 60970 loss: 0.0048 lr: 0.02\n",
      "iteration: 60980 loss: 0.0032 lr: 0.02\n",
      "iteration: 60990 loss: 0.0032 lr: 0.02\n",
      "iteration: 61000 loss: 0.0044 lr: 0.02\n",
      "iteration: 61010 loss: 0.0037 lr: 0.02\n",
      "iteration: 61020 loss: 0.0034 lr: 0.02\n",
      "iteration: 61030 loss: 0.0036 lr: 0.02\n",
      "iteration: 61040 loss: 0.0033 lr: 0.02\n",
      "iteration: 61050 loss: 0.0029 lr: 0.02\n",
      "iteration: 61060 loss: 0.0038 lr: 0.02\n",
      "iteration: 61070 loss: 0.0030 lr: 0.02\n",
      "iteration: 61080 loss: 0.0031 lr: 0.02\n",
      "iteration: 61090 loss: 0.0042 lr: 0.02\n",
      "iteration: 61100 loss: 0.0043 lr: 0.02\n",
      "iteration: 61110 loss: 0.0039 lr: 0.02\n",
      "iteration: 61120 loss: 0.0035 lr: 0.02\n",
      "iteration: 61130 loss: 0.0028 lr: 0.02\n",
      "iteration: 61140 loss: 0.0038 lr: 0.02\n",
      "iteration: 61150 loss: 0.0031 lr: 0.02\n",
      "iteration: 61160 loss: 0.0048 lr: 0.02\n",
      "iteration: 61170 loss: 0.0023 lr: 0.02\n",
      "iteration: 61180 loss: 0.0031 lr: 0.02\n",
      "iteration: 61190 loss: 0.0038 lr: 0.02\n",
      "iteration: 61200 loss: 0.0046 lr: 0.02\n",
      "iteration: 61210 loss: 0.0048 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 61220 loss: 0.0035 lr: 0.02\n",
      "iteration: 61230 loss: 0.0030 lr: 0.02\n",
      "iteration: 61240 loss: 0.0024 lr: 0.02\n",
      "iteration: 61250 loss: 0.0028 lr: 0.02\n",
      "iteration: 61260 loss: 0.0052 lr: 0.02\n",
      "iteration: 61270 loss: 0.0033 lr: 0.02\n",
      "iteration: 61280 loss: 0.0039 lr: 0.02\n",
      "iteration: 61290 loss: 0.0035 lr: 0.02\n",
      "iteration: 61300 loss: 0.0031 lr: 0.02\n",
      "iteration: 61310 loss: 0.0032 lr: 0.02\n",
      "iteration: 61320 loss: 0.0044 lr: 0.02\n",
      "iteration: 61330 loss: 0.0032 lr: 0.02\n",
      "iteration: 61340 loss: 0.0037 lr: 0.02\n",
      "iteration: 61350 loss: 0.0044 lr: 0.02\n",
      "iteration: 61360 loss: 0.0047 lr: 0.02\n",
      "iteration: 61370 loss: 0.0040 lr: 0.02\n",
      "iteration: 61380 loss: 0.0048 lr: 0.02\n",
      "iteration: 61390 loss: 0.0040 lr: 0.02\n",
      "iteration: 61400 loss: 0.0040 lr: 0.02\n",
      "iteration: 61410 loss: 0.0032 lr: 0.02\n",
      "iteration: 61420 loss: 0.0038 lr: 0.02\n",
      "iteration: 61430 loss: 0.0028 lr: 0.02\n",
      "iteration: 61440 loss: 0.0031 lr: 0.02\n",
      "iteration: 61450 loss: 0.0031 lr: 0.02\n",
      "iteration: 61460 loss: 0.0030 lr: 0.02\n",
      "iteration: 61470 loss: 0.0039 lr: 0.02\n",
      "iteration: 61480 loss: 0.0042 lr: 0.02\n",
      "iteration: 61490 loss: 0.0042 lr: 0.02\n",
      "iteration: 61500 loss: 0.0043 lr: 0.02\n",
      "iteration: 61510 loss: 0.0040 lr: 0.02\n",
      "iteration: 61520 loss: 0.0032 lr: 0.02\n",
      "iteration: 61530 loss: 0.0040 lr: 0.02\n",
      "iteration: 61540 loss: 0.0031 lr: 0.02\n",
      "iteration: 61550 loss: 0.0032 lr: 0.02\n",
      "iteration: 61560 loss: 0.0035 lr: 0.02\n",
      "iteration: 61570 loss: 0.0037 lr: 0.02\n",
      "iteration: 61580 loss: 0.0027 lr: 0.02\n",
      "iteration: 61590 loss: 0.0032 lr: 0.02\n",
      "iteration: 61600 loss: 0.0035 lr: 0.02\n",
      "iteration: 61610 loss: 0.0039 lr: 0.02\n",
      "iteration: 61620 loss: 0.0024 lr: 0.02\n",
      "iteration: 61630 loss: 0.0057 lr: 0.02\n",
      "iteration: 61640 loss: 0.0038 lr: 0.02\n",
      "iteration: 61650 loss: 0.0061 lr: 0.02\n",
      "iteration: 61660 loss: 0.0041 lr: 0.02\n",
      "iteration: 61670 loss: 0.0042 lr: 0.02\n",
      "iteration: 61680 loss: 0.0034 lr: 0.02\n",
      "iteration: 61690 loss: 0.0059 lr: 0.02\n",
      "iteration: 61700 loss: 0.0042 lr: 0.02\n",
      "iteration: 61710 loss: 0.0030 lr: 0.02\n",
      "iteration: 61720 loss: 0.0033 lr: 0.02\n",
      "iteration: 61730 loss: 0.0036 lr: 0.02\n",
      "iteration: 61740 loss: 0.0032 lr: 0.02\n",
      "iteration: 61750 loss: 0.0030 lr: 0.02\n",
      "iteration: 61760 loss: 0.0056 lr: 0.02\n",
      "iteration: 61770 loss: 0.0038 lr: 0.02\n",
      "iteration: 61780 loss: 0.0040 lr: 0.02\n",
      "iteration: 61790 loss: 0.0033 lr: 0.02\n",
      "iteration: 61800 loss: 0.0041 lr: 0.02\n",
      "iteration: 61810 loss: 0.0029 lr: 0.02\n",
      "iteration: 61820 loss: 0.0036 lr: 0.02\n",
      "iteration: 61830 loss: 0.0030 lr: 0.02\n",
      "iteration: 61840 loss: 0.0029 lr: 0.02\n",
      "iteration: 61850 loss: 0.0041 lr: 0.02\n",
      "iteration: 61860 loss: 0.0039 lr: 0.02\n",
      "iteration: 61870 loss: 0.0031 lr: 0.02\n",
      "iteration: 61880 loss: 0.0042 lr: 0.02\n",
      "iteration: 61890 loss: 0.0030 lr: 0.02\n",
      "iteration: 61900 loss: 0.0032 lr: 0.02\n",
      "iteration: 61910 loss: 0.0039 lr: 0.02\n",
      "iteration: 61920 loss: 0.0031 lr: 0.02\n",
      "iteration: 61930 loss: 0.0041 lr: 0.02\n",
      "iteration: 61940 loss: 0.0028 lr: 0.02\n",
      "iteration: 61950 loss: 0.0025 lr: 0.02\n",
      "iteration: 61960 loss: 0.0040 lr: 0.02\n",
      "iteration: 61970 loss: 0.0043 lr: 0.02\n",
      "iteration: 61980 loss: 0.0034 lr: 0.02\n",
      "iteration: 61990 loss: 0.0041 lr: 0.02\n",
      "iteration: 62000 loss: 0.0037 lr: 0.02\n",
      "iteration: 62010 loss: 0.0043 lr: 0.02\n",
      "iteration: 62020 loss: 0.0031 lr: 0.02\n",
      "iteration: 62030 loss: 0.0025 lr: 0.02\n",
      "iteration: 62040 loss: 0.0031 lr: 0.02\n",
      "iteration: 62050 loss: 0.0036 lr: 0.02\n",
      "iteration: 62060 loss: 0.0030 lr: 0.02\n",
      "iteration: 62070 loss: 0.0039 lr: 0.02\n",
      "iteration: 62080 loss: 0.0035 lr: 0.02\n",
      "iteration: 62090 loss: 0.0033 lr: 0.02\n",
      "iteration: 62100 loss: 0.0040 lr: 0.02\n",
      "iteration: 62110 loss: 0.0029 lr: 0.02\n",
      "iteration: 62120 loss: 0.0043 lr: 0.02\n",
      "iteration: 62130 loss: 0.0034 lr: 0.02\n",
      "iteration: 62140 loss: 0.0041 lr: 0.02\n",
      "iteration: 62150 loss: 0.0035 lr: 0.02\n",
      "iteration: 62160 loss: 0.0032 lr: 0.02\n",
      "iteration: 62170 loss: 0.0035 lr: 0.02\n",
      "iteration: 62180 loss: 0.0039 lr: 0.02\n",
      "iteration: 62190 loss: 0.0034 lr: 0.02\n",
      "iteration: 62200 loss: 0.0034 lr: 0.02\n",
      "iteration: 62210 loss: 0.0036 lr: 0.02\n",
      "iteration: 62220 loss: 0.0039 lr: 0.02\n",
      "iteration: 62230 loss: 0.0035 lr: 0.02\n",
      "iteration: 62240 loss: 0.0038 lr: 0.02\n",
      "iteration: 62250 loss: 0.0036 lr: 0.02\n",
      "iteration: 62260 loss: 0.0030 lr: 0.02\n",
      "iteration: 62270 loss: 0.0037 lr: 0.02\n",
      "iteration: 62280 loss: 0.0039 lr: 0.02\n",
      "iteration: 62290 loss: 0.0035 lr: 0.02\n",
      "iteration: 62300 loss: 0.0032 lr: 0.02\n",
      "iteration: 62310 loss: 0.0030 lr: 0.02\n",
      "iteration: 62320 loss: 0.0045 lr: 0.02\n",
      "iteration: 62330 loss: 0.0034 lr: 0.02\n",
      "iteration: 62340 loss: 0.0034 lr: 0.02\n",
      "iteration: 62350 loss: 0.0036 lr: 0.02\n",
      "iteration: 62360 loss: 0.0035 lr: 0.02\n",
      "iteration: 62370 loss: 0.0043 lr: 0.02\n",
      "iteration: 62380 loss: 0.0035 lr: 0.02\n",
      "iteration: 62390 loss: 0.0037 lr: 0.02\n",
      "iteration: 62400 loss: 0.0035 lr: 0.02\n",
      "iteration: 62410 loss: 0.0032 lr: 0.02\n",
      "iteration: 62420 loss: 0.0031 lr: 0.02\n",
      "iteration: 62430 loss: 0.0040 lr: 0.02\n",
      "iteration: 62440 loss: 0.0040 lr: 0.02\n",
      "iteration: 62450 loss: 0.0032 lr: 0.02\n",
      "iteration: 62460 loss: 0.0032 lr: 0.02\n",
      "iteration: 62470 loss: 0.0043 lr: 0.02\n",
      "iteration: 62480 loss: 0.0039 lr: 0.02\n",
      "iteration: 62490 loss: 0.0042 lr: 0.02\n",
      "iteration: 62500 loss: 0.0040 lr: 0.02\n",
      "iteration: 62510 loss: 0.0046 lr: 0.02\n",
      "iteration: 62520 loss: 0.0041 lr: 0.02\n",
      "iteration: 62530 loss: 0.0030 lr: 0.02\n",
      "iteration: 62540 loss: 0.0047 lr: 0.02\n",
      "iteration: 62550 loss: 0.0039 lr: 0.02\n",
      "iteration: 62560 loss: 0.0035 lr: 0.02\n",
      "iteration: 62570 loss: 0.0033 lr: 0.02\n",
      "iteration: 62580 loss: 0.0034 lr: 0.02\n",
      "iteration: 62590 loss: 0.0043 lr: 0.02\n",
      "iteration: 62600 loss: 0.0035 lr: 0.02\n",
      "iteration: 62610 loss: 0.0037 lr: 0.02\n",
      "iteration: 62620 loss: 0.0032 lr: 0.02\n",
      "iteration: 62630 loss: 0.0025 lr: 0.02\n",
      "iteration: 62640 loss: 0.0040 lr: 0.02\n",
      "iteration: 62650 loss: 0.0027 lr: 0.02\n",
      "iteration: 62660 loss: 0.0028 lr: 0.02\n",
      "iteration: 62670 loss: 0.0040 lr: 0.02\n",
      "iteration: 62680 loss: 0.0028 lr: 0.02\n",
      "iteration: 62690 loss: 0.0047 lr: 0.02\n",
      "iteration: 62700 loss: 0.0050 lr: 0.02\n",
      "iteration: 62710 loss: 0.0039 lr: 0.02\n",
      "iteration: 62720 loss: 0.0035 lr: 0.02\n",
      "iteration: 62730 loss: 0.0048 lr: 0.02\n",
      "iteration: 62740 loss: 0.0043 lr: 0.02\n",
      "iteration: 62750 loss: 0.0036 lr: 0.02\n",
      "iteration: 62760 loss: 0.0032 lr: 0.02\n",
      "iteration: 62770 loss: 0.0032 lr: 0.02\n",
      "iteration: 62780 loss: 0.0039 lr: 0.02\n",
      "iteration: 62790 loss: 0.0032 lr: 0.02\n",
      "iteration: 62800 loss: 0.0046 lr: 0.02\n",
      "iteration: 62810 loss: 0.0033 lr: 0.02\n",
      "iteration: 62820 loss: 0.0031 lr: 0.02\n",
      "iteration: 62830 loss: 0.0042 lr: 0.02\n",
      "iteration: 62840 loss: 0.0042 lr: 0.02\n",
      "iteration: 62850 loss: 0.0041 lr: 0.02\n",
      "iteration: 62860 loss: 0.0031 lr: 0.02\n",
      "iteration: 62870 loss: 0.0028 lr: 0.02\n",
      "iteration: 62880 loss: 0.0033 lr: 0.02\n",
      "iteration: 62890 loss: 0.0036 lr: 0.02\n",
      "iteration: 62900 loss: 0.0050 lr: 0.02\n",
      "iteration: 62910 loss: 0.0034 lr: 0.02\n",
      "iteration: 62920 loss: 0.0035 lr: 0.02\n",
      "iteration: 62930 loss: 0.0039 lr: 0.02\n",
      "iteration: 62940 loss: 0.0037 lr: 0.02\n",
      "iteration: 62950 loss: 0.0029 lr: 0.02\n",
      "iteration: 62960 loss: 0.0036 lr: 0.02\n",
      "iteration: 62970 loss: 0.0028 lr: 0.02\n",
      "iteration: 62980 loss: 0.0030 lr: 0.02\n",
      "iteration: 62990 loss: 0.0045 lr: 0.02\n",
      "iteration: 63000 loss: 0.0039 lr: 0.02\n",
      "iteration: 63010 loss: 0.0043 lr: 0.02\n",
      "iteration: 63020 loss: 0.0035 lr: 0.02\n",
      "iteration: 63030 loss: 0.0039 lr: 0.02\n",
      "iteration: 63040 loss: 0.0038 lr: 0.02\n",
      "iteration: 63050 loss: 0.0039 lr: 0.02\n",
      "iteration: 63060 loss: 0.0032 lr: 0.02\n",
      "iteration: 63070 loss: 0.0030 lr: 0.02\n",
      "iteration: 63080 loss: 0.0038 lr: 0.02\n",
      "iteration: 63090 loss: 0.0037 lr: 0.02\n",
      "iteration: 63100 loss: 0.0028 lr: 0.02\n",
      "iteration: 63110 loss: 0.0046 lr: 0.02\n",
      "iteration: 63120 loss: 0.0034 lr: 0.02\n",
      "iteration: 63130 loss: 0.0040 lr: 0.02\n",
      "iteration: 63140 loss: 0.0036 lr: 0.02\n",
      "iteration: 63150 loss: 0.0029 lr: 0.02\n",
      "iteration: 63160 loss: 0.0045 lr: 0.02\n",
      "iteration: 63170 loss: 0.0023 lr: 0.02\n",
      "iteration: 63180 loss: 0.0037 lr: 0.02\n",
      "iteration: 63190 loss: 0.0036 lr: 0.02\n",
      "iteration: 63200 loss: 0.0032 lr: 0.02\n",
      "iteration: 63210 loss: 0.0036 lr: 0.02\n",
      "iteration: 63220 loss: 0.0038 lr: 0.02\n",
      "iteration: 63230 loss: 0.0031 lr: 0.02\n",
      "iteration: 63240 loss: 0.0038 lr: 0.02\n",
      "iteration: 63250 loss: 0.0034 lr: 0.02\n",
      "iteration: 63260 loss: 0.0038 lr: 0.02\n",
      "iteration: 63270 loss: 0.0029 lr: 0.02\n",
      "iteration: 63280 loss: 0.0033 lr: 0.02\n",
      "iteration: 63290 loss: 0.0029 lr: 0.02\n",
      "iteration: 63300 loss: 0.0028 lr: 0.02\n",
      "iteration: 63310 loss: 0.0030 lr: 0.02\n",
      "iteration: 63320 loss: 0.0035 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 63330 loss: 0.0039 lr: 0.02\n",
      "iteration: 63340 loss: 0.0040 lr: 0.02\n",
      "iteration: 63350 loss: 0.0033 lr: 0.02\n",
      "iteration: 63360 loss: 0.0027 lr: 0.02\n",
      "iteration: 63370 loss: 0.0034 lr: 0.02\n",
      "iteration: 63380 loss: 0.0046 lr: 0.02\n",
      "iteration: 63390 loss: 0.0035 lr: 0.02\n",
      "iteration: 63400 loss: 0.0044 lr: 0.02\n",
      "iteration: 63410 loss: 0.0034 lr: 0.02\n",
      "iteration: 63420 loss: 0.0034 lr: 0.02\n",
      "iteration: 63430 loss: 0.0033 lr: 0.02\n",
      "iteration: 63440 loss: 0.0035 lr: 0.02\n",
      "iteration: 63450 loss: 0.0031 lr: 0.02\n",
      "iteration: 63460 loss: 0.0028 lr: 0.02\n",
      "iteration: 63470 loss: 0.0025 lr: 0.02\n",
      "iteration: 63480 loss: 0.0030 lr: 0.02\n",
      "iteration: 63490 loss: 0.0033 lr: 0.02\n",
      "iteration: 63500 loss: 0.0026 lr: 0.02\n",
      "iteration: 63510 loss: 0.0042 lr: 0.02\n",
      "iteration: 63520 loss: 0.0043 lr: 0.02\n",
      "iteration: 63530 loss: 0.0035 lr: 0.02\n",
      "iteration: 63540 loss: 0.0046 lr: 0.02\n",
      "iteration: 63550 loss: 0.0033 lr: 0.02\n",
      "iteration: 63560 loss: 0.0033 lr: 0.02\n",
      "iteration: 63570 loss: 0.0057 lr: 0.02\n",
      "iteration: 63580 loss: 0.0033 lr: 0.02\n",
      "iteration: 63590 loss: 0.0034 lr: 0.02\n",
      "iteration: 63600 loss: 0.0041 lr: 0.02\n",
      "iteration: 63610 loss: 0.0029 lr: 0.02\n",
      "iteration: 63620 loss: 0.0038 lr: 0.02\n",
      "iteration: 63630 loss: 0.0034 lr: 0.02\n",
      "iteration: 63640 loss: 0.0033 lr: 0.02\n",
      "iteration: 63650 loss: 0.0038 lr: 0.02\n",
      "iteration: 63660 loss: 0.0038 lr: 0.02\n",
      "iteration: 63670 loss: 0.0029 lr: 0.02\n",
      "iteration: 63680 loss: 0.0027 lr: 0.02\n",
      "iteration: 63690 loss: 0.0028 lr: 0.02\n",
      "iteration: 63700 loss: 0.0036 lr: 0.02\n",
      "iteration: 63710 loss: 0.0039 lr: 0.02\n",
      "iteration: 63720 loss: 0.0028 lr: 0.02\n",
      "iteration: 63730 loss: 0.0033 lr: 0.02\n",
      "iteration: 63740 loss: 0.0042 lr: 0.02\n",
      "iteration: 63750 loss: 0.0046 lr: 0.02\n",
      "iteration: 63760 loss: 0.0026 lr: 0.02\n",
      "iteration: 63770 loss: 0.0031 lr: 0.02\n",
      "iteration: 63780 loss: 0.0038 lr: 0.02\n",
      "iteration: 63790 loss: 0.0037 lr: 0.02\n",
      "iteration: 63800 loss: 0.0045 lr: 0.02\n",
      "iteration: 63810 loss: 0.0021 lr: 0.02\n",
      "iteration: 63820 loss: 0.0036 lr: 0.02\n",
      "iteration: 63830 loss: 0.0046 lr: 0.02\n",
      "iteration: 63840 loss: 0.0041 lr: 0.02\n",
      "iteration: 63850 loss: 0.0035 lr: 0.02\n",
      "iteration: 63860 loss: 0.0030 lr: 0.02\n",
      "iteration: 63870 loss: 0.0038 lr: 0.02\n",
      "iteration: 63880 loss: 0.0035 lr: 0.02\n",
      "iteration: 63890 loss: 0.0036 lr: 0.02\n",
      "iteration: 63900 loss: 0.0033 lr: 0.02\n",
      "iteration: 63910 loss: 0.0043 lr: 0.02\n",
      "iteration: 63920 loss: 0.0027 lr: 0.02\n",
      "iteration: 63930 loss: 0.0039 lr: 0.02\n",
      "iteration: 63940 loss: 0.0042 lr: 0.02\n",
      "iteration: 63950 loss: 0.0035 lr: 0.02\n",
      "iteration: 63960 loss: 0.0055 lr: 0.02\n",
      "iteration: 63970 loss: 0.0036 lr: 0.02\n",
      "iteration: 63980 loss: 0.0036 lr: 0.02\n",
      "iteration: 63990 loss: 0.0039 lr: 0.02\n",
      "iteration: 64000 loss: 0.0035 lr: 0.02\n",
      "iteration: 64010 loss: 0.0039 lr: 0.02\n",
      "iteration: 64020 loss: 0.0032 lr: 0.02\n",
      "iteration: 64030 loss: 0.0027 lr: 0.02\n",
      "iteration: 64040 loss: 0.0035 lr: 0.02\n",
      "iteration: 64050 loss: 0.0033 lr: 0.02\n",
      "iteration: 64060 loss: 0.0037 lr: 0.02\n",
      "iteration: 64070 loss: 0.0034 lr: 0.02\n",
      "iteration: 64080 loss: 0.0040 lr: 0.02\n",
      "iteration: 64090 loss: 0.0034 lr: 0.02\n",
      "iteration: 64100 loss: 0.0032 lr: 0.02\n",
      "iteration: 64110 loss: 0.0038 lr: 0.02\n",
      "iteration: 64120 loss: 0.0035 lr: 0.02\n",
      "iteration: 64130 loss: 0.0038 lr: 0.02\n",
      "iteration: 64140 loss: 0.0051 lr: 0.02\n",
      "iteration: 64150 loss: 0.0039 lr: 0.02\n",
      "iteration: 64160 loss: 0.0029 lr: 0.02\n",
      "iteration: 64170 loss: 0.0031 lr: 0.02\n",
      "iteration: 64180 loss: 0.0038 lr: 0.02\n",
      "iteration: 64190 loss: 0.0034 lr: 0.02\n",
      "iteration: 64200 loss: 0.0035 lr: 0.02\n",
      "iteration: 64210 loss: 0.0025 lr: 0.02\n",
      "iteration: 64220 loss: 0.0032 lr: 0.02\n",
      "iteration: 64230 loss: 0.0043 lr: 0.02\n",
      "iteration: 64240 loss: 0.0053 lr: 0.02\n",
      "iteration: 64250 loss: 0.0035 lr: 0.02\n",
      "iteration: 64260 loss: 0.0038 lr: 0.02\n",
      "iteration: 64270 loss: 0.0032 lr: 0.02\n",
      "iteration: 64280 loss: 0.0037 lr: 0.02\n",
      "iteration: 64290 loss: 0.0040 lr: 0.02\n",
      "iteration: 64300 loss: 0.0039 lr: 0.02\n",
      "iteration: 64310 loss: 0.0036 lr: 0.02\n",
      "iteration: 64320 loss: 0.0048 lr: 0.02\n",
      "iteration: 64330 loss: 0.0033 lr: 0.02\n",
      "iteration: 64340 loss: 0.0032 lr: 0.02\n",
      "iteration: 64350 loss: 0.0038 lr: 0.02\n",
      "iteration: 64360 loss: 0.0038 lr: 0.02\n",
      "iteration: 64370 loss: 0.0028 lr: 0.02\n",
      "iteration: 64380 loss: 0.0028 lr: 0.02\n",
      "iteration: 64390 loss: 0.0039 lr: 0.02\n",
      "iteration: 64400 loss: 0.0050 lr: 0.02\n",
      "iteration: 64410 loss: 0.0037 lr: 0.02\n",
      "iteration: 64420 loss: 0.0041 lr: 0.02\n",
      "iteration: 64430 loss: 0.0036 lr: 0.02\n",
      "iteration: 64440 loss: 0.0029 lr: 0.02\n",
      "iteration: 64450 loss: 0.0039 lr: 0.02\n",
      "iteration: 64460 loss: 0.0037 lr: 0.02\n",
      "iteration: 64470 loss: 0.0041 lr: 0.02\n",
      "iteration: 64480 loss: 0.0044 lr: 0.02\n",
      "iteration: 64490 loss: 0.0034 lr: 0.02\n",
      "iteration: 64500 loss: 0.0033 lr: 0.02\n",
      "iteration: 64510 loss: 0.0033 lr: 0.02\n",
      "iteration: 64520 loss: 0.0037 lr: 0.02\n",
      "iteration: 64530 loss: 0.0039 lr: 0.02\n",
      "iteration: 64540 loss: 0.0025 lr: 0.02\n",
      "iteration: 64550 loss: 0.0033 lr: 0.02\n",
      "iteration: 64560 loss: 0.0045 lr: 0.02\n",
      "iteration: 64570 loss: 0.0033 lr: 0.02\n",
      "iteration: 64580 loss: 0.0036 lr: 0.02\n",
      "iteration: 64590 loss: 0.0038 lr: 0.02\n",
      "iteration: 64600 loss: 0.0026 lr: 0.02\n",
      "iteration: 64610 loss: 0.0030 lr: 0.02\n",
      "iteration: 64620 loss: 0.0028 lr: 0.02\n",
      "iteration: 64630 loss: 0.0042 lr: 0.02\n",
      "iteration: 64640 loss: 0.0033 lr: 0.02\n",
      "iteration: 64650 loss: 0.0039 lr: 0.02\n",
      "iteration: 64660 loss: 0.0035 lr: 0.02\n",
      "iteration: 64670 loss: 0.0044 lr: 0.02\n",
      "iteration: 64680 loss: 0.0030 lr: 0.02\n",
      "iteration: 64690 loss: 0.0033 lr: 0.02\n",
      "iteration: 64700 loss: 0.0041 lr: 0.02\n",
      "iteration: 64710 loss: 0.0045 lr: 0.02\n",
      "iteration: 64720 loss: 0.0030 lr: 0.02\n",
      "iteration: 64730 loss: 0.0035 lr: 0.02\n",
      "iteration: 64740 loss: 0.0038 lr: 0.02\n",
      "iteration: 64750 loss: 0.0031 lr: 0.02\n",
      "iteration: 64760 loss: 0.0033 lr: 0.02\n",
      "iteration: 64770 loss: 0.0035 lr: 0.02\n",
      "iteration: 64780 loss: 0.0038 lr: 0.02\n",
      "iteration: 64790 loss: 0.0032 lr: 0.02\n",
      "iteration: 64800 loss: 0.0031 lr: 0.02\n",
      "iteration: 64810 loss: 0.0029 lr: 0.02\n",
      "iteration: 64820 loss: 0.0038 lr: 0.02\n",
      "iteration: 64830 loss: 0.0037 lr: 0.02\n",
      "iteration: 64840 loss: 0.0023 lr: 0.02\n",
      "iteration: 64850 loss: 0.0031 lr: 0.02\n",
      "iteration: 64860 loss: 0.0038 lr: 0.02\n",
      "iteration: 64870 loss: 0.0036 lr: 0.02\n",
      "iteration: 64880 loss: 0.0028 lr: 0.02\n",
      "iteration: 64890 loss: 0.0041 lr: 0.02\n",
      "iteration: 64900 loss: 0.0039 lr: 0.02\n",
      "iteration: 64910 loss: 0.0033 lr: 0.02\n",
      "iteration: 64920 loss: 0.0035 lr: 0.02\n",
      "iteration: 64930 loss: 0.0037 lr: 0.02\n",
      "iteration: 64940 loss: 0.0040 lr: 0.02\n",
      "iteration: 64950 loss: 0.0043 lr: 0.02\n",
      "iteration: 64960 loss: 0.0046 lr: 0.02\n",
      "iteration: 64970 loss: 0.0036 lr: 0.02\n",
      "iteration: 64980 loss: 0.0034 lr: 0.02\n",
      "iteration: 64990 loss: 0.0033 lr: 0.02\n",
      "iteration: 65000 loss: 0.0024 lr: 0.02\n",
      "iteration: 65010 loss: 0.0026 lr: 0.02\n",
      "iteration: 65020 loss: 0.0043 lr: 0.02\n",
      "iteration: 65030 loss: 0.0038 lr: 0.02\n",
      "iteration: 65040 loss: 0.0034 lr: 0.02\n",
      "iteration: 65050 loss: 0.0033 lr: 0.02\n",
      "iteration: 65060 loss: 0.0050 lr: 0.02\n",
      "iteration: 65070 loss: 0.0039 lr: 0.02\n",
      "iteration: 65080 loss: 0.0039 lr: 0.02\n",
      "iteration: 65090 loss: 0.0031 lr: 0.02\n",
      "iteration: 65100 loss: 0.0025 lr: 0.02\n",
      "iteration: 65110 loss: 0.0048 lr: 0.02\n",
      "iteration: 65120 loss: 0.0041 lr: 0.02\n",
      "iteration: 65130 loss: 0.0035 lr: 0.02\n",
      "iteration: 65140 loss: 0.0042 lr: 0.02\n",
      "iteration: 65150 loss: 0.0055 lr: 0.02\n",
      "iteration: 65160 loss: 0.0032 lr: 0.02\n",
      "iteration: 65170 loss: 0.0035 lr: 0.02\n",
      "iteration: 65180 loss: 0.0038 lr: 0.02\n",
      "iteration: 65190 loss: 0.0037 lr: 0.02\n",
      "iteration: 65200 loss: 0.0038 lr: 0.02\n",
      "iteration: 65210 loss: 0.0034 lr: 0.02\n",
      "iteration: 65220 loss: 0.0033 lr: 0.02\n",
      "iteration: 65230 loss: 0.0033 lr: 0.02\n",
      "iteration: 65240 loss: 0.0029 lr: 0.02\n",
      "iteration: 65250 loss: 0.0029 lr: 0.02\n",
      "iteration: 65260 loss: 0.0040 lr: 0.02\n",
      "iteration: 65270 loss: 0.0042 lr: 0.02\n",
      "iteration: 65280 loss: 0.0026 lr: 0.02\n",
      "iteration: 65290 loss: 0.0044 lr: 0.02\n",
      "iteration: 65300 loss: 0.0046 lr: 0.02\n",
      "iteration: 65310 loss: 0.0031 lr: 0.02\n",
      "iteration: 65320 loss: 0.0040 lr: 0.02\n",
      "iteration: 65330 loss: 0.0041 lr: 0.02\n",
      "iteration: 65340 loss: 0.0027 lr: 0.02\n",
      "iteration: 65350 loss: 0.0035 lr: 0.02\n",
      "iteration: 65360 loss: 0.0037 lr: 0.02\n",
      "iteration: 65370 loss: 0.0034 lr: 0.02\n",
      "iteration: 65380 loss: 0.0032 lr: 0.02\n",
      "iteration: 65390 loss: 0.0030 lr: 0.02\n",
      "iteration: 65400 loss: 0.0027 lr: 0.02\n",
      "iteration: 65410 loss: 0.0041 lr: 0.02\n",
      "iteration: 65420 loss: 0.0045 lr: 0.02\n",
      "iteration: 65430 loss: 0.0041 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 65440 loss: 0.0034 lr: 0.02\n",
      "iteration: 65450 loss: 0.0042 lr: 0.02\n",
      "iteration: 65460 loss: 0.0027 lr: 0.02\n",
      "iteration: 65470 loss: 0.0046 lr: 0.02\n",
      "iteration: 65480 loss: 0.0028 lr: 0.02\n",
      "iteration: 65490 loss: 0.0031 lr: 0.02\n",
      "iteration: 65500 loss: 0.0031 lr: 0.02\n",
      "iteration: 65510 loss: 0.0034 lr: 0.02\n",
      "iteration: 65520 loss: 0.0047 lr: 0.02\n",
      "iteration: 65530 loss: 0.0037 lr: 0.02\n",
      "iteration: 65540 loss: 0.0031 lr: 0.02\n",
      "iteration: 65550 loss: 0.0041 lr: 0.02\n",
      "iteration: 65560 loss: 0.0030 lr: 0.02\n",
      "iteration: 65570 loss: 0.0036 lr: 0.02\n",
      "iteration: 65580 loss: 0.0031 lr: 0.02\n",
      "iteration: 65590 loss: 0.0033 lr: 0.02\n",
      "iteration: 65600 loss: 0.0028 lr: 0.02\n",
      "iteration: 65610 loss: 0.0029 lr: 0.02\n",
      "iteration: 65620 loss: 0.0045 lr: 0.02\n",
      "iteration: 65630 loss: 0.0037 lr: 0.02\n",
      "iteration: 65640 loss: 0.0031 lr: 0.02\n",
      "iteration: 65650 loss: 0.0045 lr: 0.02\n",
      "iteration: 65660 loss: 0.0028 lr: 0.02\n",
      "iteration: 65670 loss: 0.0034 lr: 0.02\n",
      "iteration: 65680 loss: 0.0033 lr: 0.02\n",
      "iteration: 65690 loss: 0.0033 lr: 0.02\n",
      "iteration: 65700 loss: 0.0023 lr: 0.02\n",
      "iteration: 65710 loss: 0.0034 lr: 0.02\n",
      "iteration: 65720 loss: 0.0035 lr: 0.02\n",
      "iteration: 65730 loss: 0.0034 lr: 0.02\n",
      "iteration: 65740 loss: 0.0027 lr: 0.02\n",
      "iteration: 65750 loss: 0.0031 lr: 0.02\n",
      "iteration: 65760 loss: 0.0032 lr: 0.02\n",
      "iteration: 65770 loss: 0.0025 lr: 0.02\n",
      "iteration: 65780 loss: 0.0038 lr: 0.02\n",
      "iteration: 65790 loss: 0.0027 lr: 0.02\n",
      "iteration: 65800 loss: 0.0032 lr: 0.02\n",
      "iteration: 65810 loss: 0.0034 lr: 0.02\n",
      "iteration: 65820 loss: 0.0041 lr: 0.02\n",
      "iteration: 65830 loss: 0.0033 lr: 0.02\n",
      "iteration: 65840 loss: 0.0050 lr: 0.02\n",
      "iteration: 65850 loss: 0.0033 lr: 0.02\n",
      "iteration: 65860 loss: 0.0037 lr: 0.02\n",
      "iteration: 65870 loss: 0.0036 lr: 0.02\n",
      "iteration: 65880 loss: 0.0026 lr: 0.02\n",
      "iteration: 65890 loss: 0.0037 lr: 0.02\n",
      "iteration: 65900 loss: 0.0031 lr: 0.02\n",
      "iteration: 65910 loss: 0.0041 lr: 0.02\n",
      "iteration: 65920 loss: 0.0032 lr: 0.02\n",
      "iteration: 65930 loss: 0.0041 lr: 0.02\n",
      "iteration: 65940 loss: 0.0031 lr: 0.02\n",
      "iteration: 65950 loss: 0.0031 lr: 0.02\n",
      "iteration: 65960 loss: 0.0031 lr: 0.02\n",
      "iteration: 65970 loss: 0.0032 lr: 0.02\n",
      "iteration: 65980 loss: 0.0034 lr: 0.02\n",
      "iteration: 65990 loss: 0.0026 lr: 0.02\n",
      "iteration: 66000 loss: 0.0026 lr: 0.02\n",
      "iteration: 66010 loss: 0.0032 lr: 0.02\n",
      "iteration: 66020 loss: 0.0038 lr: 0.02\n",
      "iteration: 66030 loss: 0.0025 lr: 0.02\n",
      "iteration: 66040 loss: 0.0034 lr: 0.02\n",
      "iteration: 66050 loss: 0.0027 lr: 0.02\n",
      "iteration: 66060 loss: 0.0027 lr: 0.02\n",
      "iteration: 66070 loss: 0.0043 lr: 0.02\n",
      "iteration: 66080 loss: 0.0035 lr: 0.02\n",
      "iteration: 66090 loss: 0.0047 lr: 0.02\n",
      "iteration: 66100 loss: 0.0039 lr: 0.02\n",
      "iteration: 66110 loss: 0.0029 lr: 0.02\n",
      "iteration: 66120 loss: 0.0040 lr: 0.02\n",
      "iteration: 66130 loss: 0.0027 lr: 0.02\n",
      "iteration: 66140 loss: 0.0041 lr: 0.02\n",
      "iteration: 66150 loss: 0.0031 lr: 0.02\n",
      "iteration: 66160 loss: 0.0029 lr: 0.02\n",
      "iteration: 66170 loss: 0.0036 lr: 0.02\n",
      "iteration: 66180 loss: 0.0039 lr: 0.02\n",
      "iteration: 66190 loss: 0.0033 lr: 0.02\n",
      "iteration: 66200 loss: 0.0038 lr: 0.02\n",
      "iteration: 66210 loss: 0.0044 lr: 0.02\n",
      "iteration: 66220 loss: 0.0041 lr: 0.02\n",
      "iteration: 66230 loss: 0.0039 lr: 0.02\n",
      "iteration: 66240 loss: 0.0035 lr: 0.02\n",
      "iteration: 66250 loss: 0.0026 lr: 0.02\n",
      "iteration: 66260 loss: 0.0049 lr: 0.02\n",
      "iteration: 66270 loss: 0.0038 lr: 0.02\n",
      "iteration: 66280 loss: 0.0049 lr: 0.02\n",
      "iteration: 66290 loss: 0.0045 lr: 0.02\n",
      "iteration: 66300 loss: 0.0035 lr: 0.02\n",
      "iteration: 66310 loss: 0.0035 lr: 0.02\n",
      "iteration: 66320 loss: 0.0031 lr: 0.02\n",
      "iteration: 66330 loss: 0.0029 lr: 0.02\n",
      "iteration: 66340 loss: 0.0035 lr: 0.02\n",
      "iteration: 66350 loss: 0.0028 lr: 0.02\n",
      "iteration: 66360 loss: 0.0030 lr: 0.02\n",
      "iteration: 66370 loss: 0.0031 lr: 0.02\n",
      "iteration: 66380 loss: 0.0041 lr: 0.02\n",
      "iteration: 66390 loss: 0.0033 lr: 0.02\n",
      "iteration: 66400 loss: 0.0028 lr: 0.02\n",
      "iteration: 66410 loss: 0.0027 lr: 0.02\n",
      "iteration: 66420 loss: 0.0035 lr: 0.02\n",
      "iteration: 66430 loss: 0.0028 lr: 0.02\n",
      "iteration: 66440 loss: 0.0042 lr: 0.02\n",
      "iteration: 66450 loss: 0.0031 lr: 0.02\n",
      "iteration: 66460 loss: 0.0031 lr: 0.02\n",
      "iteration: 66470 loss: 0.0039 lr: 0.02\n",
      "iteration: 66480 loss: 0.0035 lr: 0.02\n",
      "iteration: 66490 loss: 0.0027 lr: 0.02\n",
      "iteration: 66500 loss: 0.0035 lr: 0.02\n",
      "iteration: 66510 loss: 0.0030 lr: 0.02\n",
      "iteration: 66520 loss: 0.0031 lr: 0.02\n",
      "iteration: 66530 loss: 0.0027 lr: 0.02\n",
      "iteration: 66540 loss: 0.0033 lr: 0.02\n",
      "iteration: 66550 loss: 0.0051 lr: 0.02\n",
      "iteration: 66560 loss: 0.0034 lr: 0.02\n",
      "iteration: 66570 loss: 0.0038 lr: 0.02\n",
      "iteration: 66580 loss: 0.0028 lr: 0.02\n",
      "iteration: 66590 loss: 0.0038 lr: 0.02\n",
      "iteration: 66600 loss: 0.0039 lr: 0.02\n",
      "iteration: 66610 loss: 0.0037 lr: 0.02\n",
      "iteration: 66620 loss: 0.0038 lr: 0.02\n",
      "iteration: 66630 loss: 0.0051 lr: 0.02\n",
      "iteration: 66640 loss: 0.0036 lr: 0.02\n",
      "iteration: 66650 loss: 0.0038 lr: 0.02\n",
      "iteration: 66660 loss: 0.0029 lr: 0.02\n",
      "iteration: 66670 loss: 0.0038 lr: 0.02\n",
      "iteration: 66680 loss: 0.0036 lr: 0.02\n",
      "iteration: 66690 loss: 0.0024 lr: 0.02\n",
      "iteration: 66700 loss: 0.0033 lr: 0.02\n",
      "iteration: 66710 loss: 0.0043 lr: 0.02\n",
      "iteration: 66720 loss: 0.0040 lr: 0.02\n",
      "iteration: 66730 loss: 0.0027 lr: 0.02\n",
      "iteration: 66740 loss: 0.0028 lr: 0.02\n",
      "iteration: 66750 loss: 0.0033 lr: 0.02\n",
      "iteration: 66760 loss: 0.0042 lr: 0.02\n",
      "iteration: 66770 loss: 0.0050 lr: 0.02\n",
      "iteration: 66780 loss: 0.0040 lr: 0.02\n",
      "iteration: 66790 loss: 0.0036 lr: 0.02\n",
      "iteration: 66800 loss: 0.0023 lr: 0.02\n",
      "iteration: 66810 loss: 0.0029 lr: 0.02\n",
      "iteration: 66820 loss: 0.0029 lr: 0.02\n",
      "iteration: 66830 loss: 0.0031 lr: 0.02\n",
      "iteration: 66840 loss: 0.0030 lr: 0.02\n",
      "iteration: 66850 loss: 0.0043 lr: 0.02\n",
      "iteration: 66860 loss: 0.0040 lr: 0.02\n",
      "iteration: 66870 loss: 0.0037 lr: 0.02\n",
      "iteration: 66880 loss: 0.0038 lr: 0.02\n",
      "iteration: 66890 loss: 0.0046 lr: 0.02\n",
      "iteration: 66900 loss: 0.0028 lr: 0.02\n",
      "iteration: 66910 loss: 0.0035 lr: 0.02\n",
      "iteration: 66920 loss: 0.0035 lr: 0.02\n",
      "iteration: 66930 loss: 0.0040 lr: 0.02\n",
      "iteration: 66940 loss: 0.0027 lr: 0.02\n",
      "iteration: 66950 loss: 0.0041 lr: 0.02\n",
      "iteration: 66960 loss: 0.0042 lr: 0.02\n",
      "iteration: 66970 loss: 0.0034 lr: 0.02\n",
      "iteration: 66980 loss: 0.0042 lr: 0.02\n",
      "iteration: 66990 loss: 0.0046 lr: 0.02\n",
      "iteration: 67000 loss: 0.0041 lr: 0.02\n",
      "iteration: 67010 loss: 0.0037 lr: 0.02\n",
      "iteration: 67020 loss: 0.0032 lr: 0.02\n",
      "iteration: 67030 loss: 0.0036 lr: 0.02\n",
      "iteration: 67040 loss: 0.0028 lr: 0.02\n",
      "iteration: 67050 loss: 0.0032 lr: 0.02\n",
      "iteration: 67060 loss: 0.0037 lr: 0.02\n",
      "iteration: 67070 loss: 0.0041 lr: 0.02\n",
      "iteration: 67080 loss: 0.0029 lr: 0.02\n",
      "iteration: 67090 loss: 0.0037 lr: 0.02\n",
      "iteration: 67100 loss: 0.0033 lr: 0.02\n",
      "iteration: 67110 loss: 0.0071 lr: 0.02\n",
      "iteration: 67120 loss: 0.0040 lr: 0.02\n",
      "iteration: 67130 loss: 0.0048 lr: 0.02\n",
      "iteration: 67140 loss: 0.0053 lr: 0.02\n",
      "iteration: 67150 loss: 0.0050 lr: 0.02\n",
      "iteration: 67160 loss: 0.0044 lr: 0.02\n",
      "iteration: 67170 loss: 0.0036 lr: 0.02\n",
      "iteration: 67180 loss: 0.0036 lr: 0.02\n",
      "iteration: 67190 loss: 0.0051 lr: 0.02\n",
      "iteration: 67200 loss: 0.0039 lr: 0.02\n",
      "iteration: 67210 loss: 0.0051 lr: 0.02\n",
      "iteration: 67220 loss: 0.0027 lr: 0.02\n",
      "iteration: 67230 loss: 0.0030 lr: 0.02\n",
      "iteration: 67240 loss: 0.0037 lr: 0.02\n",
      "iteration: 67250 loss: 0.0030 lr: 0.02\n",
      "iteration: 67260 loss: 0.0033 lr: 0.02\n",
      "iteration: 67270 loss: 0.0029 lr: 0.02\n",
      "iteration: 67280 loss: 0.0063 lr: 0.02\n",
      "iteration: 67290 loss: 0.0028 lr: 0.02\n",
      "iteration: 67300 loss: 0.0030 lr: 0.02\n",
      "iteration: 67310 loss: 0.0047 lr: 0.02\n",
      "iteration: 67320 loss: 0.0036 lr: 0.02\n",
      "iteration: 67330 loss: 0.0035 lr: 0.02\n",
      "iteration: 67340 loss: 0.0037 lr: 0.02\n",
      "iteration: 67350 loss: 0.0031 lr: 0.02\n",
      "iteration: 67360 loss: 0.0037 lr: 0.02\n",
      "iteration: 67370 loss: 0.0033 lr: 0.02\n",
      "iteration: 67380 loss: 0.0034 lr: 0.02\n",
      "iteration: 67390 loss: 0.0033 lr: 0.02\n",
      "iteration: 67400 loss: 0.0039 lr: 0.02\n",
      "iteration: 67410 loss: 0.0031 lr: 0.02\n",
      "iteration: 67420 loss: 0.0031 lr: 0.02\n",
      "iteration: 67430 loss: 0.0040 lr: 0.02\n",
      "iteration: 67440 loss: 0.0037 lr: 0.02\n",
      "iteration: 67450 loss: 0.0039 lr: 0.02\n",
      "iteration: 67460 loss: 0.0034 lr: 0.02\n",
      "iteration: 67470 loss: 0.0037 lr: 0.02\n",
      "iteration: 67480 loss: 0.0030 lr: 0.02\n",
      "iteration: 67490 loss: 0.0033 lr: 0.02\n",
      "iteration: 67500 loss: 0.0046 lr: 0.02\n",
      "iteration: 67510 loss: 0.0036 lr: 0.02\n",
      "iteration: 67520 loss: 0.0038 lr: 0.02\n",
      "iteration: 67530 loss: 0.0037 lr: 0.02\n",
      "iteration: 67540 loss: 0.0035 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 67550 loss: 0.0032 lr: 0.02\n",
      "iteration: 67560 loss: 0.0039 lr: 0.02\n",
      "iteration: 67570 loss: 0.0041 lr: 0.02\n",
      "iteration: 67580 loss: 0.0036 lr: 0.02\n",
      "iteration: 67590 loss: 0.0038 lr: 0.02\n",
      "iteration: 67600 loss: 0.0037 lr: 0.02\n",
      "iteration: 67610 loss: 0.0034 lr: 0.02\n",
      "iteration: 67620 loss: 0.0027 lr: 0.02\n",
      "iteration: 67630 loss: 0.0030 lr: 0.02\n",
      "iteration: 67640 loss: 0.0032 lr: 0.02\n",
      "iteration: 67650 loss: 0.0032 lr: 0.02\n",
      "iteration: 67660 loss: 0.0024 lr: 0.02\n",
      "iteration: 67670 loss: 0.0031 lr: 0.02\n",
      "iteration: 67680 loss: 0.0028 lr: 0.02\n",
      "iteration: 67690 loss: 0.0033 lr: 0.02\n",
      "iteration: 67700 loss: 0.0047 lr: 0.02\n",
      "iteration: 67710 loss: 0.0032 lr: 0.02\n",
      "iteration: 67720 loss: 0.0040 lr: 0.02\n",
      "iteration: 67730 loss: 0.0030 lr: 0.02\n",
      "iteration: 67740 loss: 0.0042 lr: 0.02\n",
      "iteration: 67750 loss: 0.0042 lr: 0.02\n",
      "iteration: 67760 loss: 0.0029 lr: 0.02\n",
      "iteration: 67770 loss: 0.0030 lr: 0.02\n",
      "iteration: 67780 loss: 0.0033 lr: 0.02\n",
      "iteration: 67790 loss: 0.0037 lr: 0.02\n",
      "iteration: 67800 loss: 0.0031 lr: 0.02\n",
      "iteration: 67810 loss: 0.0030 lr: 0.02\n",
      "iteration: 67820 loss: 0.0029 lr: 0.02\n",
      "iteration: 67830 loss: 0.0030 lr: 0.02\n",
      "iteration: 67840 loss: 0.0034 lr: 0.02\n",
      "iteration: 67850 loss: 0.0031 lr: 0.02\n",
      "iteration: 67860 loss: 0.0039 lr: 0.02\n",
      "iteration: 67870 loss: 0.0039 lr: 0.02\n",
      "iteration: 67880 loss: 0.0051 lr: 0.02\n",
      "iteration: 67890 loss: 0.0030 lr: 0.02\n",
      "iteration: 67900 loss: 0.0026 lr: 0.02\n",
      "iteration: 67910 loss: 0.0029 lr: 0.02\n",
      "iteration: 67920 loss: 0.0035 lr: 0.02\n",
      "iteration: 67930 loss: 0.0033 lr: 0.02\n",
      "iteration: 67940 loss: 0.0037 lr: 0.02\n",
      "iteration: 67950 loss: 0.0032 lr: 0.02\n",
      "iteration: 67960 loss: 0.0033 lr: 0.02\n",
      "iteration: 67970 loss: 0.0032 lr: 0.02\n",
      "iteration: 67980 loss: 0.0034 lr: 0.02\n",
      "iteration: 67990 loss: 0.0029 lr: 0.02\n",
      "iteration: 68000 loss: 0.0030 lr: 0.02\n",
      "iteration: 68010 loss: 0.0037 lr: 0.02\n",
      "iteration: 68020 loss: 0.0032 lr: 0.02\n",
      "iteration: 68030 loss: 0.0039 lr: 0.02\n",
      "iteration: 68040 loss: 0.0039 lr: 0.02\n",
      "iteration: 68050 loss: 0.0031 lr: 0.02\n",
      "iteration: 68060 loss: 0.0026 lr: 0.02\n",
      "iteration: 68070 loss: 0.0035 lr: 0.02\n",
      "iteration: 68080 loss: 0.0035 lr: 0.02\n",
      "iteration: 68090 loss: 0.0028 lr: 0.02\n",
      "iteration: 68100 loss: 0.0026 lr: 0.02\n",
      "iteration: 68110 loss: 0.0035 lr: 0.02\n",
      "iteration: 68120 loss: 0.0042 lr: 0.02\n",
      "iteration: 68130 loss: 0.0038 lr: 0.02\n",
      "iteration: 68140 loss: 0.0033 lr: 0.02\n",
      "iteration: 68150 loss: 0.0032 lr: 0.02\n",
      "iteration: 68160 loss: 0.0034 lr: 0.02\n",
      "iteration: 68170 loss: 0.0025 lr: 0.02\n",
      "iteration: 68180 loss: 0.0032 lr: 0.02\n",
      "iteration: 68190 loss: 0.0047 lr: 0.02\n",
      "iteration: 68200 loss: 0.0029 lr: 0.02\n",
      "iteration: 68210 loss: 0.0035 lr: 0.02\n",
      "iteration: 68220 loss: 0.0043 lr: 0.02\n",
      "iteration: 68230 loss: 0.0032 lr: 0.02\n",
      "iteration: 68240 loss: 0.0034 lr: 0.02\n",
      "iteration: 68250 loss: 0.0034 lr: 0.02\n",
      "iteration: 68260 loss: 0.0034 lr: 0.02\n",
      "iteration: 68270 loss: 0.0042 lr: 0.02\n",
      "iteration: 68280 loss: 0.0031 lr: 0.02\n",
      "iteration: 68290 loss: 0.0036 lr: 0.02\n",
      "iteration: 68300 loss: 0.0038 lr: 0.02\n",
      "iteration: 68310 loss: 0.0027 lr: 0.02\n",
      "iteration: 68320 loss: 0.0025 lr: 0.02\n",
      "iteration: 68330 loss: 0.0030 lr: 0.02\n",
      "iteration: 68340 loss: 0.0032 lr: 0.02\n",
      "iteration: 68350 loss: 0.0034 lr: 0.02\n",
      "iteration: 68360 loss: 0.0029 lr: 0.02\n",
      "iteration: 68370 loss: 0.0035 lr: 0.02\n",
      "iteration: 68380 loss: 0.0031 lr: 0.02\n",
      "iteration: 68390 loss: 0.0030 lr: 0.02\n",
      "iteration: 68400 loss: 0.0037 lr: 0.02\n",
      "iteration: 68410 loss: 0.0034 lr: 0.02\n",
      "iteration: 68420 loss: 0.0053 lr: 0.02\n",
      "iteration: 68430 loss: 0.0027 lr: 0.02\n",
      "iteration: 68440 loss: 0.0030 lr: 0.02\n",
      "iteration: 68450 loss: 0.0032 lr: 0.02\n",
      "iteration: 68460 loss: 0.0028 lr: 0.02\n",
      "iteration: 68470 loss: 0.0034 lr: 0.02\n",
      "iteration: 68480 loss: 0.0045 lr: 0.02\n",
      "iteration: 68490 loss: 0.0036 lr: 0.02\n",
      "iteration: 68500 loss: 0.0036 lr: 0.02\n",
      "iteration: 68510 loss: 0.0044 lr: 0.02\n",
      "iteration: 68520 loss: 0.0037 lr: 0.02\n",
      "iteration: 68530 loss: 0.0033 lr: 0.02\n",
      "iteration: 68540 loss: 0.0034 lr: 0.02\n",
      "iteration: 68550 loss: 0.0047 lr: 0.02\n",
      "iteration: 68560 loss: 0.0036 lr: 0.02\n",
      "iteration: 68570 loss: 0.0034 lr: 0.02\n",
      "iteration: 68580 loss: 0.0039 lr: 0.02\n",
      "iteration: 68590 loss: 0.0029 lr: 0.02\n",
      "iteration: 68600 loss: 0.0031 lr: 0.02\n",
      "iteration: 68610 loss: 0.0035 lr: 0.02\n",
      "iteration: 68620 loss: 0.0047 lr: 0.02\n",
      "iteration: 68630 loss: 0.0033 lr: 0.02\n",
      "iteration: 68640 loss: 0.0041 lr: 0.02\n",
      "iteration: 68650 loss: 0.0032 lr: 0.02\n",
      "iteration: 68660 loss: 0.0050 lr: 0.02\n",
      "iteration: 68670 loss: 0.0035 lr: 0.02\n",
      "iteration: 68680 loss: 0.0034 lr: 0.02\n",
      "iteration: 68690 loss: 0.0034 lr: 0.02\n",
      "iteration: 68700 loss: 0.0038 lr: 0.02\n",
      "iteration: 68710 loss: 0.0036 lr: 0.02\n",
      "iteration: 68720 loss: 0.0031 lr: 0.02\n",
      "iteration: 68730 loss: 0.0032 lr: 0.02\n",
      "iteration: 68740 loss: 0.0028 lr: 0.02\n",
      "iteration: 68750 loss: 0.0045 lr: 0.02\n",
      "iteration: 68760 loss: 0.0046 lr: 0.02\n",
      "iteration: 68770 loss: 0.0037 lr: 0.02\n",
      "iteration: 68780 loss: 0.0029 lr: 0.02\n",
      "iteration: 68790 loss: 0.0025 lr: 0.02\n",
      "iteration: 68800 loss: 0.0040 lr: 0.02\n",
      "iteration: 68810 loss: 0.0034 lr: 0.02\n",
      "iteration: 68820 loss: 0.0028 lr: 0.02\n",
      "iteration: 68830 loss: 0.0047 lr: 0.02\n",
      "iteration: 68840 loss: 0.0041 lr: 0.02\n",
      "iteration: 68850 loss: 0.0025 lr: 0.02\n",
      "iteration: 68860 loss: 0.0037 lr: 0.02\n",
      "iteration: 68870 loss: 0.0025 lr: 0.02\n",
      "iteration: 68880 loss: 0.0033 lr: 0.02\n",
      "iteration: 68890 loss: 0.0026 lr: 0.02\n",
      "iteration: 68900 loss: 0.0032 lr: 0.02\n",
      "iteration: 68910 loss: 0.0035 lr: 0.02\n",
      "iteration: 68920 loss: 0.0028 lr: 0.02\n",
      "iteration: 68930 loss: 0.0026 lr: 0.02\n",
      "iteration: 68940 loss: 0.0035 lr: 0.02\n",
      "iteration: 68950 loss: 0.0031 lr: 0.02\n",
      "iteration: 68960 loss: 0.0032 lr: 0.02\n",
      "iteration: 68970 loss: 0.0030 lr: 0.02\n",
      "iteration: 68980 loss: 0.0033 lr: 0.02\n",
      "iteration: 68990 loss: 0.0028 lr: 0.02\n",
      "iteration: 69000 loss: 0.0041 lr: 0.02\n",
      "iteration: 69010 loss: 0.0037 lr: 0.02\n",
      "iteration: 69020 loss: 0.0041 lr: 0.02\n",
      "iteration: 69030 loss: 0.0038 lr: 0.02\n",
      "iteration: 69040 loss: 0.0038 lr: 0.02\n",
      "iteration: 69050 loss: 0.0040 lr: 0.02\n",
      "iteration: 69060 loss: 0.0043 lr: 0.02\n",
      "iteration: 69070 loss: 0.0037 lr: 0.02\n",
      "iteration: 69080 loss: 0.0033 lr: 0.02\n",
      "iteration: 69090 loss: 0.0049 lr: 0.02\n",
      "iteration: 69100 loss: 0.0027 lr: 0.02\n",
      "iteration: 69110 loss: 0.0028 lr: 0.02\n",
      "iteration: 69120 loss: 0.0034 lr: 0.02\n",
      "iteration: 69130 loss: 0.0025 lr: 0.02\n",
      "iteration: 69140 loss: 0.0041 lr: 0.02\n",
      "iteration: 69150 loss: 0.0032 lr: 0.02\n",
      "iteration: 69160 loss: 0.0029 lr: 0.02\n",
      "iteration: 69170 loss: 0.0036 lr: 0.02\n",
      "iteration: 69180 loss: 0.0046 lr: 0.02\n",
      "iteration: 69190 loss: 0.0029 lr: 0.02\n",
      "iteration: 69200 loss: 0.0033 lr: 0.02\n",
      "iteration: 69210 loss: 0.0039 lr: 0.02\n",
      "iteration: 69220 loss: 0.0041 lr: 0.02\n",
      "iteration: 69230 loss: 0.0038 lr: 0.02\n",
      "iteration: 69240 loss: 0.0028 lr: 0.02\n",
      "iteration: 69250 loss: 0.0038 lr: 0.02\n",
      "iteration: 69260 loss: 0.0039 lr: 0.02\n",
      "iteration: 69270 loss: 0.0032 lr: 0.02\n",
      "iteration: 69280 loss: 0.0036 lr: 0.02\n",
      "iteration: 69290 loss: 0.0038 lr: 0.02\n",
      "iteration: 69300 loss: 0.0030 lr: 0.02\n",
      "iteration: 69310 loss: 0.0038 lr: 0.02\n",
      "iteration: 69320 loss: 0.0032 lr: 0.02\n",
      "iteration: 69330 loss: 0.0031 lr: 0.02\n",
      "iteration: 69340 loss: 0.0034 lr: 0.02\n",
      "iteration: 69350 loss: 0.0049 lr: 0.02\n",
      "iteration: 69360 loss: 0.0036 lr: 0.02\n",
      "iteration: 69370 loss: 0.0033 lr: 0.02\n",
      "iteration: 69380 loss: 0.0026 lr: 0.02\n",
      "iteration: 69390 loss: 0.0037 lr: 0.02\n",
      "iteration: 69400 loss: 0.0050 lr: 0.02\n",
      "iteration: 69410 loss: 0.0038 lr: 0.02\n",
      "iteration: 69420 loss: 0.0039 lr: 0.02\n",
      "iteration: 69430 loss: 0.0031 lr: 0.02\n",
      "iteration: 69440 loss: 0.0037 lr: 0.02\n",
      "iteration: 69450 loss: 0.0035 lr: 0.02\n",
      "iteration: 69460 loss: 0.0030 lr: 0.02\n",
      "iteration: 69470 loss: 0.0046 lr: 0.02\n",
      "iteration: 69480 loss: 0.0025 lr: 0.02\n",
      "iteration: 69490 loss: 0.0030 lr: 0.02\n",
      "iteration: 69500 loss: 0.0037 lr: 0.02\n",
      "iteration: 69510 loss: 0.0035 lr: 0.02\n",
      "iteration: 69520 loss: 0.0032 lr: 0.02\n",
      "iteration: 69530 loss: 0.0036 lr: 0.02\n",
      "iteration: 69540 loss: 0.0032 lr: 0.02\n",
      "iteration: 69550 loss: 0.0056 lr: 0.02\n",
      "iteration: 69560 loss: 0.0038 lr: 0.02\n",
      "iteration: 69570 loss: 0.0044 lr: 0.02\n",
      "iteration: 69580 loss: 0.0028 lr: 0.02\n",
      "iteration: 69590 loss: 0.0047 lr: 0.02\n",
      "iteration: 69600 loss: 0.0029 lr: 0.02\n",
      "iteration: 69610 loss: 0.0033 lr: 0.02\n",
      "iteration: 69620 loss: 0.0032 lr: 0.02\n",
      "iteration: 69630 loss: 0.0034 lr: 0.02\n",
      "iteration: 69640 loss: 0.0029 lr: 0.02\n",
      "iteration: 69650 loss: 0.0033 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 69660 loss: 0.0033 lr: 0.02\n",
      "iteration: 69670 loss: 0.0025 lr: 0.02\n",
      "iteration: 69680 loss: 0.0033 lr: 0.02\n",
      "iteration: 69690 loss: 0.0033 lr: 0.02\n",
      "iteration: 69700 loss: 0.0034 lr: 0.02\n",
      "iteration: 69710 loss: 0.0041 lr: 0.02\n",
      "iteration: 69720 loss: 0.0033 lr: 0.02\n",
      "iteration: 69730 loss: 0.0031 lr: 0.02\n",
      "iteration: 69740 loss: 0.0034 lr: 0.02\n",
      "iteration: 69750 loss: 0.0028 lr: 0.02\n",
      "iteration: 69760 loss: 0.0028 lr: 0.02\n",
      "iteration: 69770 loss: 0.0035 lr: 0.02\n",
      "iteration: 69780 loss: 0.0039 lr: 0.02\n",
      "iteration: 69790 loss: 0.0026 lr: 0.02\n",
      "iteration: 69800 loss: 0.0040 lr: 0.02\n",
      "iteration: 69810 loss: 0.0034 lr: 0.02\n",
      "iteration: 69820 loss: 0.0036 lr: 0.02\n",
      "iteration: 69830 loss: 0.0034 lr: 0.02\n",
      "iteration: 69840 loss: 0.0026 lr: 0.02\n",
      "iteration: 69850 loss: 0.0047 lr: 0.02\n",
      "iteration: 69860 loss: 0.0039 lr: 0.02\n",
      "iteration: 69870 loss: 0.0031 lr: 0.02\n",
      "iteration: 69880 loss: 0.0031 lr: 0.02\n",
      "iteration: 69890 loss: 0.0037 lr: 0.02\n",
      "iteration: 69900 loss: 0.0037 lr: 0.02\n",
      "iteration: 69910 loss: 0.0030 lr: 0.02\n",
      "iteration: 69920 loss: 0.0035 lr: 0.02\n",
      "iteration: 69930 loss: 0.0034 lr: 0.02\n",
      "iteration: 69940 loss: 0.0027 lr: 0.02\n",
      "iteration: 69950 loss: 0.0035 lr: 0.02\n",
      "iteration: 69960 loss: 0.0034 lr: 0.02\n",
      "iteration: 69970 loss: 0.0030 lr: 0.02\n",
      "iteration: 69980 loss: 0.0043 lr: 0.02\n",
      "iteration: 69990 loss: 0.0037 lr: 0.02\n",
      "iteration: 70000 loss: 0.0032 lr: 0.02\n",
      "iteration: 70010 loss: 0.0036 lr: 0.02\n",
      "iteration: 70020 loss: 0.0029 lr: 0.02\n",
      "iteration: 70030 loss: 0.0035 lr: 0.02\n",
      "iteration: 70040 loss: 0.0031 lr: 0.02\n",
      "iteration: 70050 loss: 0.0031 lr: 0.02\n",
      "iteration: 70060 loss: 0.0041 lr: 0.02\n",
      "iteration: 70070 loss: 0.0033 lr: 0.02\n",
      "iteration: 70080 loss: 0.0036 lr: 0.02\n",
      "iteration: 70090 loss: 0.0033 lr: 0.02\n",
      "iteration: 70100 loss: 0.0032 lr: 0.02\n",
      "iteration: 70110 loss: 0.0029 lr: 0.02\n",
      "iteration: 70120 loss: 0.0055 lr: 0.02\n",
      "iteration: 70130 loss: 0.0035 lr: 0.02\n",
      "iteration: 70140 loss: 0.0029 lr: 0.02\n",
      "iteration: 70150 loss: 0.0028 lr: 0.02\n",
      "iteration: 70160 loss: 0.0032 lr: 0.02\n",
      "iteration: 70170 loss: 0.0027 lr: 0.02\n",
      "iteration: 70180 loss: 0.0038 lr: 0.02\n",
      "iteration: 70190 loss: 0.0032 lr: 0.02\n",
      "iteration: 70200 loss: 0.0030 lr: 0.02\n",
      "iteration: 70210 loss: 0.0052 lr: 0.02\n",
      "iteration: 70220 loss: 0.0031 lr: 0.02\n",
      "iteration: 70230 loss: 0.0027 lr: 0.02\n",
      "iteration: 70240 loss: 0.0047 lr: 0.02\n",
      "iteration: 70250 loss: 0.0037 lr: 0.02\n",
      "iteration: 70260 loss: 0.0036 lr: 0.02\n",
      "iteration: 70270 loss: 0.0033 lr: 0.02\n",
      "iteration: 70280 loss: 0.0037 lr: 0.02\n",
      "iteration: 70290 loss: 0.0040 lr: 0.02\n",
      "iteration: 70300 loss: 0.0039 lr: 0.02\n",
      "iteration: 70310 loss: 0.0040 lr: 0.02\n",
      "iteration: 70320 loss: 0.0032 lr: 0.02\n",
      "iteration: 70330 loss: 0.0042 lr: 0.02\n",
      "iteration: 70340 loss: 0.0037 lr: 0.02\n",
      "iteration: 70350 loss: 0.0041 lr: 0.02\n",
      "iteration: 70360 loss: 0.0032 lr: 0.02\n",
      "iteration: 70370 loss: 0.0031 lr: 0.02\n",
      "iteration: 70380 loss: 0.0047 lr: 0.02\n",
      "iteration: 70390 loss: 0.0041 lr: 0.02\n",
      "iteration: 70400 loss: 0.0030 lr: 0.02\n",
      "iteration: 70410 loss: 0.0043 lr: 0.02\n",
      "iteration: 70420 loss: 0.0031 lr: 0.02\n",
      "iteration: 70430 loss: 0.0031 lr: 0.02\n",
      "iteration: 70440 loss: 0.0035 lr: 0.02\n",
      "iteration: 70450 loss: 0.0048 lr: 0.02\n",
      "iteration: 70460 loss: 0.0039 lr: 0.02\n",
      "iteration: 70470 loss: 0.0034 lr: 0.02\n",
      "iteration: 70480 loss: 0.0037 lr: 0.02\n",
      "iteration: 70490 loss: 0.0037 lr: 0.02\n",
      "iteration: 70500 loss: 0.0041 lr: 0.02\n",
      "iteration: 70510 loss: 0.0036 lr: 0.02\n",
      "iteration: 70520 loss: 0.0057 lr: 0.02\n",
      "iteration: 70530 loss: 0.0041 lr: 0.02\n",
      "iteration: 70540 loss: 0.0034 lr: 0.02\n",
      "iteration: 70550 loss: 0.0034 lr: 0.02\n",
      "iteration: 70560 loss: 0.0039 lr: 0.02\n",
      "iteration: 70570 loss: 0.0027 lr: 0.02\n",
      "iteration: 70580 loss: 0.0031 lr: 0.02\n",
      "iteration: 70590 loss: 0.0030 lr: 0.02\n",
      "iteration: 70600 loss: 0.0026 lr: 0.02\n",
      "iteration: 70610 loss: 0.0025 lr: 0.02\n",
      "iteration: 70620 loss: 0.0030 lr: 0.02\n",
      "iteration: 70630 loss: 0.0036 lr: 0.02\n",
      "iteration: 70640 loss: 0.0033 lr: 0.02\n",
      "iteration: 70650 loss: 0.0032 lr: 0.02\n",
      "iteration: 70660 loss: 0.0029 lr: 0.02\n",
      "iteration: 70670 loss: 0.0038 lr: 0.02\n",
      "iteration: 70680 loss: 0.0031 lr: 0.02\n",
      "iteration: 70690 loss: 0.0024 lr: 0.02\n",
      "iteration: 70700 loss: 0.0038 lr: 0.02\n",
      "iteration: 70710 loss: 0.0034 lr: 0.02\n",
      "iteration: 70720 loss: 0.0030 lr: 0.02\n",
      "iteration: 70730 loss: 0.0031 lr: 0.02\n",
      "iteration: 70740 loss: 0.0052 lr: 0.02\n",
      "iteration: 70750 loss: 0.0033 lr: 0.02\n",
      "iteration: 70760 loss: 0.0024 lr: 0.02\n",
      "iteration: 70770 loss: 0.0042 lr: 0.02\n",
      "iteration: 70780 loss: 0.0033 lr: 0.02\n",
      "iteration: 70790 loss: 0.0032 lr: 0.02\n",
      "iteration: 70800 loss: 0.0045 lr: 0.02\n",
      "iteration: 70810 loss: 0.0030 lr: 0.02\n",
      "iteration: 70820 loss: 0.0032 lr: 0.02\n",
      "iteration: 70830 loss: 0.0037 lr: 0.02\n",
      "iteration: 70840 loss: 0.0027 lr: 0.02\n",
      "iteration: 70850 loss: 0.0026 lr: 0.02\n",
      "iteration: 70860 loss: 0.0038 lr: 0.02\n",
      "iteration: 70870 loss: 0.0024 lr: 0.02\n",
      "iteration: 70880 loss: 0.0032 lr: 0.02\n",
      "iteration: 70890 loss: 0.0030 lr: 0.02\n",
      "iteration: 70900 loss: 0.0035 lr: 0.02\n",
      "iteration: 70910 loss: 0.0030 lr: 0.02\n",
      "iteration: 70920 loss: 0.0034 lr: 0.02\n",
      "iteration: 70930 loss: 0.0034 lr: 0.02\n",
      "iteration: 70940 loss: 0.0030 lr: 0.02\n",
      "iteration: 70950 loss: 0.0032 lr: 0.02\n",
      "iteration: 70960 loss: 0.0030 lr: 0.02\n",
      "iteration: 70970 loss: 0.0028 lr: 0.02\n",
      "iteration: 70980 loss: 0.0025 lr: 0.02\n",
      "iteration: 70990 loss: 0.0041 lr: 0.02\n",
      "iteration: 71000 loss: 0.0032 lr: 0.02\n",
      "iteration: 71010 loss: 0.0033 lr: 0.02\n",
      "iteration: 71020 loss: 0.0031 lr: 0.02\n",
      "iteration: 71030 loss: 0.0036 lr: 0.02\n",
      "iteration: 71040 loss: 0.0026 lr: 0.02\n",
      "iteration: 71050 loss: 0.0024 lr: 0.02\n",
      "iteration: 71060 loss: 0.0028 lr: 0.02\n",
      "iteration: 71070 loss: 0.0044 lr: 0.02\n",
      "iteration: 71080 loss: 0.0047 lr: 0.02\n",
      "iteration: 71090 loss: 0.0037 lr: 0.02\n",
      "iteration: 71100 loss: 0.0033 lr: 0.02\n",
      "iteration: 71110 loss: 0.0031 lr: 0.02\n",
      "iteration: 71120 loss: 0.0031 lr: 0.02\n",
      "iteration: 71130 loss: 0.0024 lr: 0.02\n",
      "iteration: 71140 loss: 0.0035 lr: 0.02\n",
      "iteration: 71150 loss: 0.0029 lr: 0.02\n",
      "iteration: 71160 loss: 0.0034 lr: 0.02\n",
      "iteration: 71170 loss: 0.0036 lr: 0.02\n",
      "iteration: 71180 loss: 0.0029 lr: 0.02\n",
      "iteration: 71190 loss: 0.0032 lr: 0.02\n",
      "iteration: 71200 loss: 0.0034 lr: 0.02\n",
      "iteration: 71210 loss: 0.0029 lr: 0.02\n",
      "iteration: 71220 loss: 0.0034 lr: 0.02\n",
      "iteration: 71230 loss: 0.0033 lr: 0.02\n",
      "iteration: 71240 loss: 0.0038 lr: 0.02\n",
      "iteration: 71250 loss: 0.0038 lr: 0.02\n",
      "iteration: 71260 loss: 0.0042 lr: 0.02\n",
      "iteration: 71270 loss: 0.0035 lr: 0.02\n",
      "iteration: 71280 loss: 0.0046 lr: 0.02\n",
      "iteration: 71290 loss: 0.0032 lr: 0.02\n",
      "iteration: 71300 loss: 0.0034 lr: 0.02\n",
      "iteration: 71310 loss: 0.0048 lr: 0.02\n",
      "iteration: 71320 loss: 0.0040 lr: 0.02\n",
      "iteration: 71330 loss: 0.0030 lr: 0.02\n",
      "iteration: 71340 loss: 0.0037 lr: 0.02\n",
      "iteration: 71350 loss: 0.0035 lr: 0.02\n",
      "iteration: 71360 loss: 0.0029 lr: 0.02\n",
      "iteration: 71370 loss: 0.0045 lr: 0.02\n",
      "iteration: 71380 loss: 0.0030 lr: 0.02\n",
      "iteration: 71390 loss: 0.0029 lr: 0.02\n",
      "iteration: 71400 loss: 0.0032 lr: 0.02\n",
      "iteration: 71410 loss: 0.0033 lr: 0.02\n",
      "iteration: 71420 loss: 0.0028 lr: 0.02\n",
      "iteration: 71430 loss: 0.0035 lr: 0.02\n",
      "iteration: 71440 loss: 0.0032 lr: 0.02\n",
      "iteration: 71450 loss: 0.0042 lr: 0.02\n",
      "iteration: 71460 loss: 0.0033 lr: 0.02\n",
      "iteration: 71470 loss: 0.0027 lr: 0.02\n",
      "iteration: 71480 loss: 0.0037 lr: 0.02\n",
      "iteration: 71490 loss: 0.0033 lr: 0.02\n",
      "iteration: 71500 loss: 0.0041 lr: 0.02\n",
      "iteration: 71510 loss: 0.0052 lr: 0.02\n",
      "iteration: 71520 loss: 0.0045 lr: 0.02\n",
      "iteration: 71530 loss: 0.0037 lr: 0.02\n",
      "iteration: 71540 loss: 0.0028 lr: 0.02\n",
      "iteration: 71550 loss: 0.0028 lr: 0.02\n",
      "iteration: 71560 loss: 0.0047 lr: 0.02\n",
      "iteration: 71570 loss: 0.0029 lr: 0.02\n",
      "iteration: 71580 loss: 0.0028 lr: 0.02\n",
      "iteration: 71590 loss: 0.0033 lr: 0.02\n",
      "iteration: 71600 loss: 0.0027 lr: 0.02\n",
      "iteration: 71610 loss: 0.0038 lr: 0.02\n",
      "iteration: 71620 loss: 0.0031 lr: 0.02\n",
      "iteration: 71630 loss: 0.0028 lr: 0.02\n",
      "iteration: 71640 loss: 0.0033 lr: 0.02\n",
      "iteration: 71650 loss: 0.0035 lr: 0.02\n",
      "iteration: 71660 loss: 0.0047 lr: 0.02\n",
      "iteration: 71670 loss: 0.0048 lr: 0.02\n",
      "iteration: 71680 loss: 0.0032 lr: 0.02\n",
      "iteration: 71690 loss: 0.0034 lr: 0.02\n",
      "iteration: 71700 loss: 0.0033 lr: 0.02\n",
      "iteration: 71710 loss: 0.0026 lr: 0.02\n",
      "iteration: 71720 loss: 0.0036 lr: 0.02\n",
      "iteration: 71730 loss: 0.0034 lr: 0.02\n",
      "iteration: 71740 loss: 0.0040 lr: 0.02\n",
      "iteration: 71750 loss: 0.0032 lr: 0.02\n",
      "iteration: 71760 loss: 0.0032 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 71770 loss: 0.0025 lr: 0.02\n",
      "iteration: 71780 loss: 0.0040 lr: 0.02\n",
      "iteration: 71790 loss: 0.0027 lr: 0.02\n",
      "iteration: 71800 loss: 0.0027 lr: 0.02\n",
      "iteration: 71810 loss: 0.0033 lr: 0.02\n",
      "iteration: 71820 loss: 0.0028 lr: 0.02\n",
      "iteration: 71830 loss: 0.0033 lr: 0.02\n",
      "iteration: 71840 loss: 0.0034 lr: 0.02\n",
      "iteration: 71850 loss: 0.0027 lr: 0.02\n",
      "iteration: 71860 loss: 0.0032 lr: 0.02\n",
      "iteration: 71870 loss: 0.0029 lr: 0.02\n",
      "iteration: 71880 loss: 0.0027 lr: 0.02\n",
      "iteration: 71890 loss: 0.0038 lr: 0.02\n",
      "iteration: 71900 loss: 0.0030 lr: 0.02\n",
      "iteration: 71910 loss: 0.0030 lr: 0.02\n",
      "iteration: 71920 loss: 0.0034 lr: 0.02\n",
      "iteration: 71930 loss: 0.0033 lr: 0.02\n",
      "iteration: 71940 loss: 0.0036 lr: 0.02\n",
      "iteration: 71950 loss: 0.0040 lr: 0.02\n",
      "iteration: 71960 loss: 0.0030 lr: 0.02\n",
      "iteration: 71970 loss: 0.0037 lr: 0.02\n",
      "iteration: 71980 loss: 0.0034 lr: 0.02\n",
      "iteration: 71990 loss: 0.0039 lr: 0.02\n",
      "iteration: 72000 loss: 0.0036 lr: 0.02\n",
      "iteration: 72010 loss: 0.0040 lr: 0.02\n",
      "iteration: 72020 loss: 0.0047 lr: 0.02\n",
      "iteration: 72030 loss: 0.0028 lr: 0.02\n",
      "iteration: 72040 loss: 0.0036 lr: 0.02\n",
      "iteration: 72050 loss: 0.0033 lr: 0.02\n",
      "iteration: 72060 loss: 0.0037 lr: 0.02\n",
      "iteration: 72070 loss: 0.0032 lr: 0.02\n",
      "iteration: 72080 loss: 0.0032 lr: 0.02\n",
      "iteration: 72090 loss: 0.0037 lr: 0.02\n",
      "iteration: 72100 loss: 0.0023 lr: 0.02\n",
      "iteration: 72110 loss: 0.0032 lr: 0.02\n",
      "iteration: 72120 loss: 0.0037 lr: 0.02\n",
      "iteration: 72130 loss: 0.0029 lr: 0.02\n",
      "iteration: 72140 loss: 0.0029 lr: 0.02\n",
      "iteration: 72150 loss: 0.0041 lr: 0.02\n",
      "iteration: 72160 loss: 0.0036 lr: 0.02\n",
      "iteration: 72170 loss: 0.0034 lr: 0.02\n",
      "iteration: 72180 loss: 0.0034 lr: 0.02\n",
      "iteration: 72190 loss: 0.0025 lr: 0.02\n",
      "iteration: 72200 loss: 0.0034 lr: 0.02\n",
      "iteration: 72210 loss: 0.0031 lr: 0.02\n",
      "iteration: 72220 loss: 0.0029 lr: 0.02\n",
      "iteration: 72230 loss: 0.0034 lr: 0.02\n",
      "iteration: 72240 loss: 0.0036 lr: 0.02\n",
      "iteration: 72250 loss: 0.0025 lr: 0.02\n",
      "iteration: 72260 loss: 0.0028 lr: 0.02\n",
      "iteration: 72270 loss: 0.0032 lr: 0.02\n",
      "iteration: 72280 loss: 0.0040 lr: 0.02\n",
      "iteration: 72290 loss: 0.0037 lr: 0.02\n",
      "iteration: 72300 loss: 0.0040 lr: 0.02\n",
      "iteration: 72310 loss: 0.0039 lr: 0.02\n",
      "iteration: 72320 loss: 0.0042 lr: 0.02\n",
      "iteration: 72330 loss: 0.0036 lr: 0.02\n",
      "iteration: 72340 loss: 0.0029 lr: 0.02\n",
      "iteration: 72350 loss: 0.0040 lr: 0.02\n",
      "iteration: 72360 loss: 0.0041 lr: 0.02\n",
      "iteration: 72370 loss: 0.0039 lr: 0.02\n",
      "iteration: 72380 loss: 0.0044 lr: 0.02\n",
      "iteration: 72390 loss: 0.0034 lr: 0.02\n",
      "iteration: 72400 loss: 0.0039 lr: 0.02\n",
      "iteration: 72410 loss: 0.0042 lr: 0.02\n",
      "iteration: 72420 loss: 0.0032 lr: 0.02\n",
      "iteration: 72430 loss: 0.0033 lr: 0.02\n",
      "iteration: 72440 loss: 0.0051 lr: 0.02\n",
      "iteration: 72450 loss: 0.0030 lr: 0.02\n",
      "iteration: 72460 loss: 0.0040 lr: 0.02\n",
      "iteration: 72470 loss: 0.0038 lr: 0.02\n",
      "iteration: 72480 loss: 0.0034 lr: 0.02\n",
      "iteration: 72490 loss: 0.0030 lr: 0.02\n",
      "iteration: 72500 loss: 0.0031 lr: 0.02\n",
      "iteration: 72510 loss: 0.0029 lr: 0.02\n",
      "iteration: 72520 loss: 0.0035 lr: 0.02\n",
      "iteration: 72530 loss: 0.0030 lr: 0.02\n",
      "iteration: 72540 loss: 0.0029 lr: 0.02\n",
      "iteration: 72550 loss: 0.0033 lr: 0.02\n",
      "iteration: 72560 loss: 0.0038 lr: 0.02\n",
      "iteration: 72570 loss: 0.0025 lr: 0.02\n",
      "iteration: 72580 loss: 0.0028 lr: 0.02\n",
      "iteration: 72590 loss: 0.0024 lr: 0.02\n",
      "iteration: 72600 loss: 0.0027 lr: 0.02\n",
      "iteration: 72610 loss: 0.0036 lr: 0.02\n",
      "iteration: 72620 loss: 0.0038 lr: 0.02\n",
      "iteration: 72630 loss: 0.0041 lr: 0.02\n",
      "iteration: 72640 loss: 0.0035 lr: 0.02\n",
      "iteration: 72650 loss: 0.0034 lr: 0.02\n",
      "iteration: 72660 loss: 0.0028 lr: 0.02\n",
      "iteration: 72670 loss: 0.0030 lr: 0.02\n",
      "iteration: 72680 loss: 0.0026 lr: 0.02\n",
      "iteration: 72690 loss: 0.0027 lr: 0.02\n",
      "iteration: 72700 loss: 0.0034 lr: 0.02\n",
      "iteration: 72710 loss: 0.0035 lr: 0.02\n",
      "iteration: 72720 loss: 0.0027 lr: 0.02\n",
      "iteration: 72730 loss: 0.0030 lr: 0.02\n",
      "iteration: 72740 loss: 0.0032 lr: 0.02\n",
      "iteration: 72750 loss: 0.0029 lr: 0.02\n",
      "iteration: 72760 loss: 0.0034 lr: 0.02\n",
      "iteration: 72770 loss: 0.0034 lr: 0.02\n",
      "iteration: 72780 loss: 0.0038 lr: 0.02\n",
      "iteration: 72790 loss: 0.0038 lr: 0.02\n",
      "iteration: 72800 loss: 0.0040 lr: 0.02\n",
      "iteration: 72810 loss: 0.0037 lr: 0.02\n",
      "iteration: 72820 loss: 0.0041 lr: 0.02\n",
      "iteration: 72830 loss: 0.0050 lr: 0.02\n",
      "iteration: 72840 loss: 0.0025 lr: 0.02\n",
      "iteration: 72850 loss: 0.0040 lr: 0.02\n",
      "iteration: 72860 loss: 0.0044 lr: 0.02\n",
      "iteration: 72870 loss: 0.0038 lr: 0.02\n",
      "iteration: 72880 loss: 0.0034 lr: 0.02\n",
      "iteration: 72890 loss: 0.0032 lr: 0.02\n",
      "iteration: 72900 loss: 0.0042 lr: 0.02\n",
      "iteration: 72910 loss: 0.0038 lr: 0.02\n",
      "iteration: 72920 loss: 0.0034 lr: 0.02\n",
      "iteration: 72930 loss: 0.0046 lr: 0.02\n",
      "iteration: 72940 loss: 0.0041 lr: 0.02\n",
      "iteration: 72950 loss: 0.0032 lr: 0.02\n",
      "iteration: 72960 loss: 0.0029 lr: 0.02\n",
      "iteration: 72970 loss: 0.0039 lr: 0.02\n",
      "iteration: 72980 loss: 0.0029 lr: 0.02\n",
      "iteration: 72990 loss: 0.0030 lr: 0.02\n",
      "iteration: 73000 loss: 0.0037 lr: 0.02\n",
      "iteration: 73010 loss: 0.0025 lr: 0.02\n",
      "iteration: 73020 loss: 0.0031 lr: 0.02\n",
      "iteration: 73030 loss: 0.0030 lr: 0.02\n",
      "iteration: 73040 loss: 0.0030 lr: 0.02\n",
      "iteration: 73050 loss: 0.0036 lr: 0.02\n",
      "iteration: 73060 loss: 0.0033 lr: 0.02\n",
      "iteration: 73070 loss: 0.0026 lr: 0.02\n",
      "iteration: 73080 loss: 0.0041 lr: 0.02\n",
      "iteration: 73090 loss: 0.0039 lr: 0.02\n",
      "iteration: 73100 loss: 0.0026 lr: 0.02\n",
      "iteration: 73110 loss: 0.0040 lr: 0.02\n",
      "iteration: 73120 loss: 0.0037 lr: 0.02\n",
      "iteration: 73130 loss: 0.0036 lr: 0.02\n",
      "iteration: 73140 loss: 0.0034 lr: 0.02\n",
      "iteration: 73150 loss: 0.0048 lr: 0.02\n",
      "iteration: 73160 loss: 0.0036 lr: 0.02\n",
      "iteration: 73170 loss: 0.0032 lr: 0.02\n",
      "iteration: 73180 loss: 0.0035 lr: 0.02\n",
      "iteration: 73190 loss: 0.0031 lr: 0.02\n",
      "iteration: 73200 loss: 0.0036 lr: 0.02\n",
      "iteration: 73210 loss: 0.0031 lr: 0.02\n",
      "iteration: 73220 loss: 0.0030 lr: 0.02\n",
      "iteration: 73230 loss: 0.0032 lr: 0.02\n",
      "iteration: 73240 loss: 0.0035 lr: 0.02\n",
      "iteration: 73250 loss: 0.0033 lr: 0.02\n",
      "iteration: 73260 loss: 0.0028 lr: 0.02\n",
      "iteration: 73270 loss: 0.0028 lr: 0.02\n",
      "iteration: 73280 loss: 0.0036 lr: 0.02\n",
      "iteration: 73290 loss: 0.0043 lr: 0.02\n",
      "iteration: 73300 loss: 0.0035 lr: 0.02\n",
      "iteration: 73310 loss: 0.0029 lr: 0.02\n",
      "iteration: 73320 loss: 0.0036 lr: 0.02\n",
      "iteration: 73330 loss: 0.0034 lr: 0.02\n",
      "iteration: 73340 loss: 0.0026 lr: 0.02\n",
      "iteration: 73350 loss: 0.0028 lr: 0.02\n",
      "iteration: 73360 loss: 0.0028 lr: 0.02\n",
      "iteration: 73370 loss: 0.0035 lr: 0.02\n",
      "iteration: 73380 loss: 0.0030 lr: 0.02\n",
      "iteration: 73390 loss: 0.0026 lr: 0.02\n",
      "iteration: 73400 loss: 0.0038 lr: 0.02\n",
      "iteration: 73410 loss: 0.0040 lr: 0.02\n",
      "iteration: 73420 loss: 0.0036 lr: 0.02\n",
      "iteration: 73430 loss: 0.0023 lr: 0.02\n",
      "iteration: 73440 loss: 0.0033 lr: 0.02\n",
      "iteration: 73450 loss: 0.0033 lr: 0.02\n",
      "iteration: 73460 loss: 0.0028 lr: 0.02\n",
      "iteration: 73470 loss: 0.0025 lr: 0.02\n",
      "iteration: 73480 loss: 0.0035 lr: 0.02\n",
      "iteration: 73490 loss: 0.0044 lr: 0.02\n",
      "iteration: 73500 loss: 0.0033 lr: 0.02\n",
      "iteration: 73510 loss: 0.0047 lr: 0.02\n",
      "iteration: 73520 loss: 0.0036 lr: 0.02\n",
      "iteration: 73530 loss: 0.0043 lr: 0.02\n",
      "iteration: 73540 loss: 0.0040 lr: 0.02\n",
      "iteration: 73550 loss: 0.0043 lr: 0.02\n",
      "iteration: 73560 loss: 0.0028 lr: 0.02\n",
      "iteration: 73570 loss: 0.0037 lr: 0.02\n",
      "iteration: 73580 loss: 0.0033 lr: 0.02\n",
      "iteration: 73590 loss: 0.0035 lr: 0.02\n",
      "iteration: 73600 loss: 0.0034 lr: 0.02\n",
      "iteration: 73610 loss: 0.0032 lr: 0.02\n",
      "iteration: 73620 loss: 0.0023 lr: 0.02\n",
      "iteration: 73630 loss: 0.0027 lr: 0.02\n",
      "iteration: 73640 loss: 0.0031 lr: 0.02\n",
      "iteration: 73650 loss: 0.0035 lr: 0.02\n",
      "iteration: 73660 loss: 0.0028 lr: 0.02\n",
      "iteration: 73670 loss: 0.0038 lr: 0.02\n",
      "iteration: 73680 loss: 0.0028 lr: 0.02\n",
      "iteration: 73690 loss: 0.0024 lr: 0.02\n",
      "iteration: 73700 loss: 0.0030 lr: 0.02\n",
      "iteration: 73710 loss: 0.0028 lr: 0.02\n",
      "iteration: 73720 loss: 0.0033 lr: 0.02\n",
      "iteration: 73730 loss: 0.0036 lr: 0.02\n",
      "iteration: 73740 loss: 0.0035 lr: 0.02\n",
      "iteration: 73750 loss: 0.0037 lr: 0.02\n",
      "iteration: 73760 loss: 0.0037 lr: 0.02\n",
      "iteration: 73770 loss: 0.0036 lr: 0.02\n",
      "iteration: 73780 loss: 0.0031 lr: 0.02\n",
      "iteration: 73790 loss: 0.0029 lr: 0.02\n",
      "iteration: 73800 loss: 0.0028 lr: 0.02\n",
      "iteration: 73810 loss: 0.0040 lr: 0.02\n",
      "iteration: 73820 loss: 0.0036 lr: 0.02\n",
      "iteration: 73830 loss: 0.0035 lr: 0.02\n",
      "iteration: 73840 loss: 0.0030 lr: 0.02\n",
      "iteration: 73850 loss: 0.0038 lr: 0.02\n",
      "iteration: 73860 loss: 0.0028 lr: 0.02\n",
      "iteration: 73870 loss: 0.0025 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 73880 loss: 0.0023 lr: 0.02\n",
      "iteration: 73890 loss: 0.0032 lr: 0.02\n",
      "iteration: 73900 loss: 0.0032 lr: 0.02\n",
      "iteration: 73910 loss: 0.0031 lr: 0.02\n",
      "iteration: 73920 loss: 0.0034 lr: 0.02\n",
      "iteration: 73930 loss: 0.0036 lr: 0.02\n",
      "iteration: 73940 loss: 0.0030 lr: 0.02\n",
      "iteration: 73950 loss: 0.0027 lr: 0.02\n",
      "iteration: 73960 loss: 0.0033 lr: 0.02\n",
      "iteration: 73970 loss: 0.0038 lr: 0.02\n",
      "iteration: 73980 loss: 0.0027 lr: 0.02\n",
      "iteration: 73990 loss: 0.0042 lr: 0.02\n",
      "iteration: 74000 loss: 0.0030 lr: 0.02\n",
      "iteration: 74010 loss: 0.0035 lr: 0.02\n",
      "iteration: 74020 loss: 0.0032 lr: 0.02\n",
      "iteration: 74030 loss: 0.0034 lr: 0.02\n",
      "iteration: 74040 loss: 0.0047 lr: 0.02\n",
      "iteration: 74050 loss: 0.0033 lr: 0.02\n",
      "iteration: 74060 loss: 0.0035 lr: 0.02\n",
      "iteration: 74070 loss: 0.0037 lr: 0.02\n",
      "iteration: 74080 loss: 0.0033 lr: 0.02\n",
      "iteration: 74090 loss: 0.0028 lr: 0.02\n",
      "iteration: 74100 loss: 0.0042 lr: 0.02\n",
      "iteration: 74110 loss: 0.0027 lr: 0.02\n",
      "iteration: 74120 loss: 0.0035 lr: 0.02\n",
      "iteration: 74130 loss: 0.0026 lr: 0.02\n",
      "iteration: 74140 loss: 0.0029 lr: 0.02\n",
      "iteration: 74150 loss: 0.0039 lr: 0.02\n",
      "iteration: 74160 loss: 0.0033 lr: 0.02\n",
      "iteration: 74170 loss: 0.0039 lr: 0.02\n",
      "iteration: 74180 loss: 0.0029 lr: 0.02\n",
      "iteration: 74190 loss: 0.0028 lr: 0.02\n",
      "iteration: 74200 loss: 0.0039 lr: 0.02\n",
      "iteration: 74210 loss: 0.0032 lr: 0.02\n",
      "iteration: 74220 loss: 0.0024 lr: 0.02\n",
      "iteration: 74230 loss: 0.0026 lr: 0.02\n",
      "iteration: 74240 loss: 0.0023 lr: 0.02\n",
      "iteration: 74250 loss: 0.0032 lr: 0.02\n",
      "iteration: 74260 loss: 0.0028 lr: 0.02\n",
      "iteration: 74270 loss: 0.0028 lr: 0.02\n",
      "iteration: 74280 loss: 0.0045 lr: 0.02\n",
      "iteration: 74290 loss: 0.0031 lr: 0.02\n",
      "iteration: 74300 loss: 0.0040 lr: 0.02\n",
      "iteration: 74310 loss: 0.0032 lr: 0.02\n",
      "iteration: 74320 loss: 0.0031 lr: 0.02\n",
      "iteration: 74330 loss: 0.0038 lr: 0.02\n",
      "iteration: 74340 loss: 0.0038 lr: 0.02\n",
      "iteration: 74350 loss: 0.0038 lr: 0.02\n",
      "iteration: 74360 loss: 0.0040 lr: 0.02\n",
      "iteration: 74370 loss: 0.0029 lr: 0.02\n",
      "iteration: 74380 loss: 0.0026 lr: 0.02\n",
      "iteration: 74390 loss: 0.0033 lr: 0.02\n",
      "iteration: 74400 loss: 0.0041 lr: 0.02\n",
      "iteration: 74410 loss: 0.0040 lr: 0.02\n",
      "iteration: 74420 loss: 0.0032 lr: 0.02\n",
      "iteration: 74430 loss: 0.0034 lr: 0.02\n",
      "iteration: 74440 loss: 0.0033 lr: 0.02\n",
      "iteration: 74450 loss: 0.0038 lr: 0.02\n",
      "iteration: 74460 loss: 0.0023 lr: 0.02\n",
      "iteration: 74470 loss: 0.0031 lr: 0.02\n",
      "iteration: 74480 loss: 0.0032 lr: 0.02\n",
      "iteration: 74490 loss: 0.0026 lr: 0.02\n",
      "iteration: 74500 loss: 0.0040 lr: 0.02\n",
      "iteration: 74510 loss: 0.0032 lr: 0.02\n",
      "iteration: 74520 loss: 0.0026 lr: 0.02\n",
      "iteration: 74530 loss: 0.0023 lr: 0.02\n",
      "iteration: 74540 loss: 0.0037 lr: 0.02\n",
      "iteration: 74550 loss: 0.0034 lr: 0.02\n",
      "iteration: 74560 loss: 0.0031 lr: 0.02\n",
      "iteration: 74570 loss: 0.0033 lr: 0.02\n",
      "iteration: 74580 loss: 0.0037 lr: 0.02\n",
      "iteration: 74590 loss: 0.0027 lr: 0.02\n",
      "iteration: 74600 loss: 0.0030 lr: 0.02\n",
      "iteration: 74610 loss: 0.0039 lr: 0.02\n",
      "iteration: 74620 loss: 0.0031 lr: 0.02\n",
      "iteration: 74630 loss: 0.0036 lr: 0.02\n",
      "iteration: 74640 loss: 0.0033 lr: 0.02\n",
      "iteration: 74650 loss: 0.0030 lr: 0.02\n",
      "iteration: 74660 loss: 0.0039 lr: 0.02\n",
      "iteration: 74670 loss: 0.0039 lr: 0.02\n",
      "iteration: 74680 loss: 0.0036 lr: 0.02\n",
      "iteration: 74690 loss: 0.0033 lr: 0.02\n",
      "iteration: 74700 loss: 0.0029 lr: 0.02\n",
      "iteration: 74710 loss: 0.0031 lr: 0.02\n",
      "iteration: 74720 loss: 0.0033 lr: 0.02\n",
      "iteration: 74730 loss: 0.0038 lr: 0.02\n",
      "iteration: 74740 loss: 0.0041 lr: 0.02\n",
      "iteration: 74750 loss: 0.0028 lr: 0.02\n",
      "iteration: 74760 loss: 0.0029 lr: 0.02\n",
      "iteration: 74770 loss: 0.0029 lr: 0.02\n",
      "iteration: 74780 loss: 0.0044 lr: 0.02\n",
      "iteration: 74790 loss: 0.0038 lr: 0.02\n",
      "iteration: 74800 loss: 0.0032 lr: 0.02\n",
      "iteration: 74810 loss: 0.0032 lr: 0.02\n",
      "iteration: 74820 loss: 0.0029 lr: 0.02\n",
      "iteration: 74830 loss: 0.0032 lr: 0.02\n",
      "iteration: 74840 loss: 0.0031 lr: 0.02\n",
      "iteration: 74850 loss: 0.0040 lr: 0.02\n",
      "iteration: 74860 loss: 0.0039 lr: 0.02\n",
      "iteration: 74870 loss: 0.0030 lr: 0.02\n",
      "iteration: 74880 loss: 0.0053 lr: 0.02\n",
      "iteration: 74890 loss: 0.0033 lr: 0.02\n",
      "iteration: 74900 loss: 0.0037 lr: 0.02\n",
      "iteration: 74910 loss: 0.0034 lr: 0.02\n",
      "iteration: 74920 loss: 0.0028 lr: 0.02\n",
      "iteration: 74930 loss: 0.0034 lr: 0.02\n",
      "iteration: 74940 loss: 0.0033 lr: 0.02\n",
      "iteration: 74950 loss: 0.0030 lr: 0.02\n",
      "iteration: 74960 loss: 0.0031 lr: 0.02\n",
      "iteration: 74970 loss: 0.0030 lr: 0.02\n",
      "iteration: 74980 loss: 0.0034 lr: 0.02\n",
      "iteration: 74990 loss: 0.0023 lr: 0.02\n",
      "iteration: 75000 loss: 0.0024 lr: 0.02\n",
      "iteration: 75010 loss: 0.0031 lr: 0.02\n",
      "iteration: 75020 loss: 0.0027 lr: 0.02\n",
      "iteration: 75030 loss: 0.0026 lr: 0.02\n",
      "iteration: 75040 loss: 0.0032 lr: 0.02\n",
      "iteration: 75050 loss: 0.0024 lr: 0.02\n",
      "iteration: 75060 loss: 0.0035 lr: 0.02\n",
      "iteration: 75070 loss: 0.0041 lr: 0.02\n",
      "iteration: 75080 loss: 0.0032 lr: 0.02\n",
      "iteration: 75090 loss: 0.0038 lr: 0.02\n",
      "iteration: 75100 loss: 0.0035 lr: 0.02\n",
      "iteration: 75110 loss: 0.0046 lr: 0.02\n",
      "iteration: 75120 loss: 0.0042 lr: 0.02\n",
      "iteration: 75130 loss: 0.0030 lr: 0.02\n",
      "iteration: 75140 loss: 0.0032 lr: 0.02\n",
      "iteration: 75150 loss: 0.0035 lr: 0.02\n",
      "iteration: 75160 loss: 0.0034 lr: 0.02\n",
      "iteration: 75170 loss: 0.0030 lr: 0.02\n",
      "iteration: 75180 loss: 0.0029 lr: 0.02\n",
      "iteration: 75190 loss: 0.0035 lr: 0.02\n",
      "iteration: 75200 loss: 0.0027 lr: 0.02\n",
      "iteration: 75210 loss: 0.0037 lr: 0.02\n",
      "iteration: 75220 loss: 0.0033 lr: 0.02\n",
      "iteration: 75230 loss: 0.0037 lr: 0.02\n",
      "iteration: 75240 loss: 0.0043 lr: 0.02\n",
      "iteration: 75250 loss: 0.0028 lr: 0.02\n",
      "iteration: 75260 loss: 0.0036 lr: 0.02\n",
      "iteration: 75270 loss: 0.0037 lr: 0.02\n",
      "iteration: 75280 loss: 0.0030 lr: 0.02\n",
      "iteration: 75290 loss: 0.0033 lr: 0.02\n",
      "iteration: 75300 loss: 0.0032 lr: 0.02\n",
      "iteration: 75310 loss: 0.0037 lr: 0.02\n",
      "iteration: 75320 loss: 0.0031 lr: 0.02\n",
      "iteration: 75330 loss: 0.0029 lr: 0.02\n",
      "iteration: 75340 loss: 0.0049 lr: 0.02\n",
      "iteration: 75350 loss: 0.0029 lr: 0.02\n",
      "iteration: 75360 loss: 0.0022 lr: 0.02\n",
      "iteration: 75370 loss: 0.0036 lr: 0.02\n",
      "iteration: 75380 loss: 0.0031 lr: 0.02\n",
      "iteration: 75390 loss: 0.0034 lr: 0.02\n",
      "iteration: 75400 loss: 0.0029 lr: 0.02\n",
      "iteration: 75410 loss: 0.0032 lr: 0.02\n",
      "iteration: 75420 loss: 0.0023 lr: 0.02\n",
      "iteration: 75430 loss: 0.0029 lr: 0.02\n",
      "iteration: 75440 loss: 0.0029 lr: 0.02\n",
      "iteration: 75450 loss: 0.0028 lr: 0.02\n",
      "iteration: 75460 loss: 0.0023 lr: 0.02\n",
      "iteration: 75470 loss: 0.0027 lr: 0.02\n",
      "iteration: 75480 loss: 0.0037 lr: 0.02\n",
      "iteration: 75490 loss: 0.0035 lr: 0.02\n",
      "iteration: 75500 loss: 0.0028 lr: 0.02\n",
      "iteration: 75510 loss: 0.0032 lr: 0.02\n",
      "iteration: 75520 loss: 0.0033 lr: 0.02\n",
      "iteration: 75530 loss: 0.0044 lr: 0.02\n",
      "iteration: 75540 loss: 0.0025 lr: 0.02\n",
      "iteration: 75550 loss: 0.0025 lr: 0.02\n",
      "iteration: 75560 loss: 0.0027 lr: 0.02\n",
      "iteration: 75570 loss: 0.0038 lr: 0.02\n",
      "iteration: 75580 loss: 0.0034 lr: 0.02\n",
      "iteration: 75590 loss: 0.0041 lr: 0.02\n",
      "iteration: 75600 loss: 0.0031 lr: 0.02\n",
      "iteration: 75610 loss: 0.0034 lr: 0.02\n",
      "iteration: 75620 loss: 0.0031 lr: 0.02\n",
      "iteration: 75630 loss: 0.0037 lr: 0.02\n",
      "iteration: 75640 loss: 0.0036 lr: 0.02\n",
      "iteration: 75650 loss: 0.0031 lr: 0.02\n",
      "iteration: 75660 loss: 0.0030 lr: 0.02\n",
      "iteration: 75670 loss: 0.0034 lr: 0.02\n",
      "iteration: 75680 loss: 0.0044 lr: 0.02\n",
      "iteration: 75690 loss: 0.0034 lr: 0.02\n",
      "iteration: 75700 loss: 0.0051 lr: 0.02\n",
      "iteration: 75710 loss: 0.0025 lr: 0.02\n",
      "iteration: 75720 loss: 0.0029 lr: 0.02\n",
      "iteration: 75730 loss: 0.0029 lr: 0.02\n",
      "iteration: 75740 loss: 0.0042 lr: 0.02\n",
      "iteration: 75750 loss: 0.0028 lr: 0.02\n",
      "iteration: 75760 loss: 0.0031 lr: 0.02\n",
      "iteration: 75770 loss: 0.0049 lr: 0.02\n",
      "iteration: 75780 loss: 0.0039 lr: 0.02\n",
      "iteration: 75790 loss: 0.0036 lr: 0.02\n",
      "iteration: 75800 loss: 0.0039 lr: 0.02\n",
      "iteration: 75810 loss: 0.0026 lr: 0.02\n",
      "iteration: 75820 loss: 0.0037 lr: 0.02\n",
      "iteration: 75830 loss: 0.0030 lr: 0.02\n",
      "iteration: 75840 loss: 0.0035 lr: 0.02\n",
      "iteration: 75850 loss: 0.0047 lr: 0.02\n",
      "iteration: 75860 loss: 0.0026 lr: 0.02\n",
      "iteration: 75870 loss: 0.0038 lr: 0.02\n",
      "iteration: 75880 loss: 0.0038 lr: 0.02\n",
      "iteration: 75890 loss: 0.0031 lr: 0.02\n",
      "iteration: 75900 loss: 0.0035 lr: 0.02\n",
      "iteration: 75910 loss: 0.0026 lr: 0.02\n",
      "iteration: 75920 loss: 0.0030 lr: 0.02\n",
      "iteration: 75930 loss: 0.0036 lr: 0.02\n",
      "iteration: 75940 loss: 0.0029 lr: 0.02\n",
      "iteration: 75950 loss: 0.0026 lr: 0.02\n",
      "iteration: 75960 loss: 0.0030 lr: 0.02\n",
      "iteration: 75970 loss: 0.0035 lr: 0.02\n",
      "iteration: 75980 loss: 0.0031 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 75990 loss: 0.0040 lr: 0.02\n",
      "iteration: 76000 loss: 0.0031 lr: 0.02\n",
      "iteration: 76010 loss: 0.0034 lr: 0.02\n",
      "iteration: 76020 loss: 0.0036 lr: 0.02\n",
      "iteration: 76030 loss: 0.0028 lr: 0.02\n",
      "iteration: 76040 loss: 0.0034 lr: 0.02\n",
      "iteration: 76050 loss: 0.0030 lr: 0.02\n",
      "iteration: 76060 loss: 0.0027 lr: 0.02\n",
      "iteration: 76070 loss: 0.0027 lr: 0.02\n",
      "iteration: 76080 loss: 0.0028 lr: 0.02\n",
      "iteration: 76090 loss: 0.0031 lr: 0.02\n",
      "iteration: 76100 loss: 0.0031 lr: 0.02\n",
      "iteration: 76110 loss: 0.0029 lr: 0.02\n",
      "iteration: 76120 loss: 0.0027 lr: 0.02\n",
      "iteration: 76130 loss: 0.0031 lr: 0.02\n",
      "iteration: 76140 loss: 0.0031 lr: 0.02\n",
      "iteration: 76150 loss: 0.0035 lr: 0.02\n",
      "iteration: 76160 loss: 0.0031 lr: 0.02\n",
      "iteration: 76170 loss: 0.0049 lr: 0.02\n",
      "iteration: 76180 loss: 0.0043 lr: 0.02\n",
      "iteration: 76190 loss: 0.0037 lr: 0.02\n",
      "iteration: 76200 loss: 0.0025 lr: 0.02\n",
      "iteration: 76210 loss: 0.0028 lr: 0.02\n",
      "iteration: 76220 loss: 0.0027 lr: 0.02\n",
      "iteration: 76230 loss: 0.0034 lr: 0.02\n",
      "iteration: 76240 loss: 0.0027 lr: 0.02\n",
      "iteration: 76250 loss: 0.0030 lr: 0.02\n",
      "iteration: 76260 loss: 0.0027 lr: 0.02\n",
      "iteration: 76270 loss: 0.0033 lr: 0.02\n",
      "iteration: 76280 loss: 0.0031 lr: 0.02\n",
      "iteration: 76290 loss: 0.0028 lr: 0.02\n",
      "iteration: 76300 loss: 0.0025 lr: 0.02\n",
      "iteration: 76310 loss: 0.0029 lr: 0.02\n",
      "iteration: 76320 loss: 0.0044 lr: 0.02\n",
      "iteration: 76330 loss: 0.0027 lr: 0.02\n",
      "iteration: 76340 loss: 0.0038 lr: 0.02\n",
      "iteration: 76350 loss: 0.0036 lr: 0.02\n",
      "iteration: 76360 loss: 0.0030 lr: 0.02\n",
      "iteration: 76370 loss: 0.0036 lr: 0.02\n",
      "iteration: 76380 loss: 0.0029 lr: 0.02\n",
      "iteration: 76390 loss: 0.0038 lr: 0.02\n",
      "iteration: 76400 loss: 0.0031 lr: 0.02\n",
      "iteration: 76410 loss: 0.0027 lr: 0.02\n",
      "iteration: 76420 loss: 0.0024 lr: 0.02\n",
      "iteration: 76430 loss: 0.0030 lr: 0.02\n",
      "iteration: 76440 loss: 0.0029 lr: 0.02\n",
      "iteration: 76450 loss: 0.0041 lr: 0.02\n",
      "iteration: 76460 loss: 0.0032 lr: 0.02\n",
      "iteration: 76470 loss: 0.0034 lr: 0.02\n",
      "iteration: 76480 loss: 0.0041 lr: 0.02\n",
      "iteration: 76490 loss: 0.0032 lr: 0.02\n",
      "iteration: 76500 loss: 0.0035 lr: 0.02\n",
      "iteration: 76510 loss: 0.0030 lr: 0.02\n",
      "iteration: 76520 loss: 0.0035 lr: 0.02\n",
      "iteration: 76530 loss: 0.0032 lr: 0.02\n",
      "iteration: 76540 loss: 0.0038 lr: 0.02\n",
      "iteration: 76550 loss: 0.0034 lr: 0.02\n",
      "iteration: 76560 loss: 0.0025 lr: 0.02\n",
      "iteration: 76570 loss: 0.0034 lr: 0.02\n",
      "iteration: 76580 loss: 0.0037 lr: 0.02\n",
      "iteration: 76590 loss: 0.0025 lr: 0.02\n",
      "iteration: 76600 loss: 0.0036 lr: 0.02\n",
      "iteration: 76610 loss: 0.0029 lr: 0.02\n",
      "iteration: 76620 loss: 0.0030 lr: 0.02\n",
      "iteration: 76630 loss: 0.0028 lr: 0.02\n",
      "iteration: 76640 loss: 0.0034 lr: 0.02\n",
      "iteration: 76650 loss: 0.0028 lr: 0.02\n",
      "iteration: 76660 loss: 0.0029 lr: 0.02\n",
      "iteration: 76670 loss: 0.0033 lr: 0.02\n",
      "iteration: 76680 loss: 0.0033 lr: 0.02\n",
      "iteration: 76690 loss: 0.0031 lr: 0.02\n",
      "iteration: 76700 loss: 0.0025 lr: 0.02\n",
      "iteration: 76710 loss: 0.0038 lr: 0.02\n",
      "iteration: 76720 loss: 0.0041 lr: 0.02\n",
      "iteration: 76730 loss: 0.0031 lr: 0.02\n",
      "iteration: 76740 loss: 0.0030 lr: 0.02\n",
      "iteration: 76750 loss: 0.0051 lr: 0.02\n",
      "iteration: 76760 loss: 0.0038 lr: 0.02\n",
      "iteration: 76770 loss: 0.0031 lr: 0.02\n",
      "iteration: 76780 loss: 0.0036 lr: 0.02\n",
      "iteration: 76790 loss: 0.0033 lr: 0.02\n",
      "iteration: 76800 loss: 0.0035 lr: 0.02\n",
      "iteration: 76810 loss: 0.0036 lr: 0.02\n",
      "iteration: 76820 loss: 0.0024 lr: 0.02\n",
      "iteration: 76830 loss: 0.0035 lr: 0.02\n",
      "iteration: 76840 loss: 0.0033 lr: 0.02\n",
      "iteration: 76850 loss: 0.0023 lr: 0.02\n",
      "iteration: 76860 loss: 0.0033 lr: 0.02\n",
      "iteration: 76870 loss: 0.0038 lr: 0.02\n",
      "iteration: 76880 loss: 0.0038 lr: 0.02\n",
      "iteration: 76890 loss: 0.0036 lr: 0.02\n",
      "iteration: 76900 loss: 0.0040 lr: 0.02\n",
      "iteration: 76910 loss: 0.0038 lr: 0.02\n",
      "iteration: 76920 loss: 0.0040 lr: 0.02\n",
      "iteration: 76930 loss: 0.0030 lr: 0.02\n",
      "iteration: 76940 loss: 0.0024 lr: 0.02\n",
      "iteration: 76950 loss: 0.0029 lr: 0.02\n",
      "iteration: 76960 loss: 0.0039 lr: 0.02\n",
      "iteration: 76970 loss: 0.0034 lr: 0.02\n",
      "iteration: 76980 loss: 0.0035 lr: 0.02\n",
      "iteration: 76990 loss: 0.0035 lr: 0.02\n",
      "iteration: 77000 loss: 0.0034 lr: 0.02\n",
      "iteration: 77010 loss: 0.0032 lr: 0.02\n",
      "iteration: 77020 loss: 0.0029 lr: 0.02\n",
      "iteration: 77030 loss: 0.0036 lr: 0.02\n",
      "iteration: 77040 loss: 0.0041 lr: 0.02\n",
      "iteration: 77050 loss: 0.0045 lr: 0.02\n",
      "iteration: 77060 loss: 0.0018 lr: 0.02\n",
      "iteration: 77070 loss: 0.0042 lr: 0.02\n",
      "iteration: 77080 loss: 0.0037 lr: 0.02\n",
      "iteration: 77090 loss: 0.0028 lr: 0.02\n",
      "iteration: 77100 loss: 0.0029 lr: 0.02\n",
      "iteration: 77110 loss: 0.0031 lr: 0.02\n",
      "iteration: 77120 loss: 0.0027 lr: 0.02\n",
      "iteration: 77130 loss: 0.0029 lr: 0.02\n",
      "iteration: 77140 loss: 0.0028 lr: 0.02\n",
      "iteration: 77150 loss: 0.0037 lr: 0.02\n",
      "iteration: 77160 loss: 0.0027 lr: 0.02\n",
      "iteration: 77170 loss: 0.0037 lr: 0.02\n",
      "iteration: 77180 loss: 0.0037 lr: 0.02\n",
      "iteration: 77190 loss: 0.0041 lr: 0.02\n",
      "iteration: 77200 loss: 0.0041 lr: 0.02\n",
      "iteration: 77210 loss: 0.0034 lr: 0.02\n",
      "iteration: 77220 loss: 0.0031 lr: 0.02\n",
      "iteration: 77230 loss: 0.0033 lr: 0.02\n",
      "iteration: 77240 loss: 0.0033 lr: 0.02\n",
      "iteration: 77250 loss: 0.0024 lr: 0.02\n",
      "iteration: 77260 loss: 0.0038 lr: 0.02\n",
      "iteration: 77270 loss: 0.0019 lr: 0.02\n",
      "iteration: 77280 loss: 0.0031 lr: 0.02\n",
      "iteration: 77290 loss: 0.0030 lr: 0.02\n",
      "iteration: 77300 loss: 0.0034 lr: 0.02\n",
      "iteration: 77310 loss: 0.0027 lr: 0.02\n",
      "iteration: 77320 loss: 0.0045 lr: 0.02\n",
      "iteration: 77330 loss: 0.0029 lr: 0.02\n",
      "iteration: 77340 loss: 0.0039 lr: 0.02\n",
      "iteration: 77350 loss: 0.0037 lr: 0.02\n",
      "iteration: 77360 loss: 0.0029 lr: 0.02\n",
      "iteration: 77370 loss: 0.0031 lr: 0.02\n",
      "iteration: 77380 loss: 0.0037 lr: 0.02\n",
      "iteration: 77390 loss: 0.0032 lr: 0.02\n",
      "iteration: 77400 loss: 0.0027 lr: 0.02\n",
      "iteration: 77410 loss: 0.0027 lr: 0.02\n",
      "iteration: 77420 loss: 0.0038 lr: 0.02\n",
      "iteration: 77430 loss: 0.0023 lr: 0.02\n",
      "iteration: 77440 loss: 0.0034 lr: 0.02\n",
      "iteration: 77450 loss: 0.0027 lr: 0.02\n",
      "iteration: 77460 loss: 0.0024 lr: 0.02\n",
      "iteration: 77470 loss: 0.0035 lr: 0.02\n",
      "iteration: 77480 loss: 0.0023 lr: 0.02\n",
      "iteration: 77490 loss: 0.0042 lr: 0.02\n",
      "iteration: 77500 loss: 0.0028 lr: 0.02\n",
      "iteration: 77510 loss: 0.0039 lr: 0.02\n",
      "iteration: 77520 loss: 0.0030 lr: 0.02\n",
      "iteration: 77530 loss: 0.0027 lr: 0.02\n",
      "iteration: 77540 loss: 0.0037 lr: 0.02\n",
      "iteration: 77550 loss: 0.0053 lr: 0.02\n",
      "iteration: 77560 loss: 0.0028 lr: 0.02\n",
      "iteration: 77570 loss: 0.0024 lr: 0.02\n",
      "iteration: 77580 loss: 0.0039 lr: 0.02\n",
      "iteration: 77590 loss: 0.0031 lr: 0.02\n",
      "iteration: 77600 loss: 0.0032 lr: 0.02\n",
      "iteration: 77610 loss: 0.0049 lr: 0.02\n",
      "iteration: 77620 loss: 0.0025 lr: 0.02\n",
      "iteration: 77630 loss: 0.0028 lr: 0.02\n",
      "iteration: 77640 loss: 0.0026 lr: 0.02\n",
      "iteration: 77650 loss: 0.0028 lr: 0.02\n",
      "iteration: 77660 loss: 0.0023 lr: 0.02\n",
      "iteration: 77670 loss: 0.0032 lr: 0.02\n",
      "iteration: 77680 loss: 0.0028 lr: 0.02\n",
      "iteration: 77690 loss: 0.0030 lr: 0.02\n",
      "iteration: 77700 loss: 0.0044 lr: 0.02\n",
      "iteration: 77710 loss: 0.0027 lr: 0.02\n",
      "iteration: 77720 loss: 0.0029 lr: 0.02\n",
      "iteration: 77730 loss: 0.0026 lr: 0.02\n",
      "iteration: 77740 loss: 0.0038 lr: 0.02\n",
      "iteration: 77750 loss: 0.0033 lr: 0.02\n",
      "iteration: 77760 loss: 0.0030 lr: 0.02\n",
      "iteration: 77770 loss: 0.0034 lr: 0.02\n",
      "iteration: 77780 loss: 0.0029 lr: 0.02\n",
      "iteration: 77790 loss: 0.0030 lr: 0.02\n",
      "iteration: 77800 loss: 0.0041 lr: 0.02\n",
      "iteration: 77810 loss: 0.0039 lr: 0.02\n",
      "iteration: 77820 loss: 0.0033 lr: 0.02\n",
      "iteration: 77830 loss: 0.0023 lr: 0.02\n",
      "iteration: 77840 loss: 0.0034 lr: 0.02\n",
      "iteration: 77850 loss: 0.0045 lr: 0.02\n",
      "iteration: 77860 loss: 0.0038 lr: 0.02\n",
      "iteration: 77870 loss: 0.0035 lr: 0.02\n",
      "iteration: 77880 loss: 0.0042 lr: 0.02\n",
      "iteration: 77890 loss: 0.0039 lr: 0.02\n",
      "iteration: 77900 loss: 0.0034 lr: 0.02\n",
      "iteration: 77910 loss: 0.0028 lr: 0.02\n",
      "iteration: 77920 loss: 0.0024 lr: 0.02\n",
      "iteration: 77930 loss: 0.0034 lr: 0.02\n",
      "iteration: 77940 loss: 0.0027 lr: 0.02\n",
      "iteration: 77950 loss: 0.0041 lr: 0.02\n",
      "iteration: 77960 loss: 0.0041 lr: 0.02\n",
      "iteration: 77970 loss: 0.0035 lr: 0.02\n",
      "iteration: 77980 loss: 0.0047 lr: 0.02\n",
      "iteration: 77990 loss: 0.0023 lr: 0.02\n",
      "iteration: 78000 loss: 0.0036 lr: 0.02\n",
      "iteration: 78010 loss: 0.0045 lr: 0.02\n",
      "iteration: 78020 loss: 0.0030 lr: 0.02\n",
      "iteration: 78030 loss: 0.0033 lr: 0.02\n",
      "iteration: 78040 loss: 0.0043 lr: 0.02\n",
      "iteration: 78050 loss: 0.0034 lr: 0.02\n",
      "iteration: 78060 loss: 0.0027 lr: 0.02\n",
      "iteration: 78070 loss: 0.0029 lr: 0.02\n",
      "iteration: 78080 loss: 0.0038 lr: 0.02\n",
      "iteration: 78090 loss: 0.0030 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 78100 loss: 0.0039 lr: 0.02\n",
      "iteration: 78110 loss: 0.0029 lr: 0.02\n",
      "iteration: 78120 loss: 0.0037 lr: 0.02\n",
      "iteration: 78130 loss: 0.0035 lr: 0.02\n",
      "iteration: 78140 loss: 0.0026 lr: 0.02\n",
      "iteration: 78150 loss: 0.0029 lr: 0.02\n",
      "iteration: 78160 loss: 0.0036 lr: 0.02\n",
      "iteration: 78170 loss: 0.0024 lr: 0.02\n",
      "iteration: 78180 loss: 0.0038 lr: 0.02\n",
      "iteration: 78190 loss: 0.0028 lr: 0.02\n",
      "iteration: 78200 loss: 0.0024 lr: 0.02\n",
      "iteration: 78210 loss: 0.0026 lr: 0.02\n",
      "iteration: 78220 loss: 0.0035 lr: 0.02\n",
      "iteration: 78230 loss: 0.0035 lr: 0.02\n",
      "iteration: 78240 loss: 0.0031 lr: 0.02\n",
      "iteration: 78250 loss: 0.0028 lr: 0.02\n",
      "iteration: 78260 loss: 0.0035 lr: 0.02\n",
      "iteration: 78270 loss: 0.0050 lr: 0.02\n",
      "iteration: 78280 loss: 0.0033 lr: 0.02\n",
      "iteration: 78290 loss: 0.0035 lr: 0.02\n",
      "iteration: 78300 loss: 0.0026 lr: 0.02\n",
      "iteration: 78310 loss: 0.0034 lr: 0.02\n",
      "iteration: 78320 loss: 0.0030 lr: 0.02\n",
      "iteration: 78330 loss: 0.0032 lr: 0.02\n",
      "iteration: 78340 loss: 0.0030 lr: 0.02\n",
      "iteration: 78350 loss: 0.0040 lr: 0.02\n",
      "iteration: 78360 loss: 0.0035 lr: 0.02\n",
      "iteration: 78370 loss: 0.0031 lr: 0.02\n",
      "iteration: 78380 loss: 0.0033 lr: 0.02\n",
      "iteration: 78390 loss: 0.0037 lr: 0.02\n",
      "iteration: 78400 loss: 0.0034 lr: 0.02\n",
      "iteration: 78410 loss: 0.0031 lr: 0.02\n",
      "iteration: 78420 loss: 0.0035 lr: 0.02\n",
      "iteration: 78430 loss: 0.0034 lr: 0.02\n",
      "iteration: 78440 loss: 0.0030 lr: 0.02\n",
      "iteration: 78450 loss: 0.0034 lr: 0.02\n",
      "iteration: 78460 loss: 0.0034 lr: 0.02\n",
      "iteration: 78470 loss: 0.0033 lr: 0.02\n",
      "iteration: 78480 loss: 0.0037 lr: 0.02\n",
      "iteration: 78490 loss: 0.0036 lr: 0.02\n",
      "iteration: 78500 loss: 0.0027 lr: 0.02\n",
      "iteration: 78510 loss: 0.0028 lr: 0.02\n",
      "iteration: 78520 loss: 0.0028 lr: 0.02\n",
      "iteration: 78530 loss: 0.0042 lr: 0.02\n",
      "iteration: 78540 loss: 0.0038 lr: 0.02\n",
      "iteration: 78550 loss: 0.0031 lr: 0.02\n",
      "iteration: 78560 loss: 0.0027 lr: 0.02\n",
      "iteration: 78570 loss: 0.0026 lr: 0.02\n",
      "iteration: 78580 loss: 0.0035 lr: 0.02\n",
      "iteration: 78590 loss: 0.0040 lr: 0.02\n",
      "iteration: 78600 loss: 0.0028 lr: 0.02\n",
      "iteration: 78610 loss: 0.0028 lr: 0.02\n",
      "iteration: 78620 loss: 0.0023 lr: 0.02\n",
      "iteration: 78630 loss: 0.0033 lr: 0.02\n",
      "iteration: 78640 loss: 0.0031 lr: 0.02\n",
      "iteration: 78650 loss: 0.0033 lr: 0.02\n",
      "iteration: 78660 loss: 0.0031 lr: 0.02\n",
      "iteration: 78670 loss: 0.0025 lr: 0.02\n",
      "iteration: 78680 loss: 0.0046 lr: 0.02\n",
      "iteration: 78690 loss: 0.0034 lr: 0.02\n",
      "iteration: 78700 loss: 0.0028 lr: 0.02\n",
      "iteration: 78710 loss: 0.0031 lr: 0.02\n",
      "iteration: 78720 loss: 0.0034 lr: 0.02\n",
      "iteration: 78730 loss: 0.0036 lr: 0.02\n",
      "iteration: 78740 loss: 0.0034 lr: 0.02\n",
      "iteration: 78750 loss: 0.0041 lr: 0.02\n",
      "iteration: 78760 loss: 0.0028 lr: 0.02\n",
      "iteration: 78770 loss: 0.0034 lr: 0.02\n",
      "iteration: 78780 loss: 0.0032 lr: 0.02\n",
      "iteration: 78790 loss: 0.0037 lr: 0.02\n",
      "iteration: 78800 loss: 0.0029 lr: 0.02\n",
      "iteration: 78810 loss: 0.0036 lr: 0.02\n",
      "iteration: 78820 loss: 0.0038 lr: 0.02\n",
      "iteration: 78830 loss: 0.0029 lr: 0.02\n",
      "iteration: 78840 loss: 0.0041 lr: 0.02\n",
      "iteration: 78850 loss: 0.0041 lr: 0.02\n",
      "iteration: 78860 loss: 0.0048 lr: 0.02\n",
      "iteration: 78870 loss: 0.0044 lr: 0.02\n",
      "iteration: 78880 loss: 0.0034 lr: 0.02\n",
      "iteration: 78890 loss: 0.0034 lr: 0.02\n",
      "iteration: 78900 loss: 0.0027 lr: 0.02\n",
      "iteration: 78910 loss: 0.0045 lr: 0.02\n",
      "iteration: 78920 loss: 0.0030 lr: 0.02\n",
      "iteration: 78930 loss: 0.0026 lr: 0.02\n",
      "iteration: 78940 loss: 0.0038 lr: 0.02\n",
      "iteration: 78950 loss: 0.0032 lr: 0.02\n",
      "iteration: 78960 loss: 0.0027 lr: 0.02\n",
      "iteration: 78970 loss: 0.0040 lr: 0.02\n",
      "iteration: 78980 loss: 0.0044 lr: 0.02\n",
      "iteration: 78990 loss: 0.0028 lr: 0.02\n",
      "iteration: 79000 loss: 0.0031 lr: 0.02\n",
      "iteration: 79010 loss: 0.0033 lr: 0.02\n",
      "iteration: 79020 loss: 0.0034 lr: 0.02\n",
      "iteration: 79030 loss: 0.0032 lr: 0.02\n",
      "iteration: 79040 loss: 0.0033 lr: 0.02\n",
      "iteration: 79050 loss: 0.0026 lr: 0.02\n",
      "iteration: 79060 loss: 0.0033 lr: 0.02\n",
      "iteration: 79070 loss: 0.0031 lr: 0.02\n",
      "iteration: 79080 loss: 0.0043 lr: 0.02\n",
      "iteration: 79090 loss: 0.0032 lr: 0.02\n",
      "iteration: 79100 loss: 0.0029 lr: 0.02\n",
      "iteration: 79110 loss: 0.0030 lr: 0.02\n",
      "iteration: 79120 loss: 0.0034 lr: 0.02\n",
      "iteration: 79130 loss: 0.0029 lr: 0.02\n",
      "iteration: 79140 loss: 0.0024 lr: 0.02\n",
      "iteration: 79150 loss: 0.0028 lr: 0.02\n",
      "iteration: 79160 loss: 0.0033 lr: 0.02\n",
      "iteration: 79170 loss: 0.0025 lr: 0.02\n",
      "iteration: 79180 loss: 0.0037 lr: 0.02\n",
      "iteration: 79190 loss: 0.0028 lr: 0.02\n",
      "iteration: 79200 loss: 0.0030 lr: 0.02\n",
      "iteration: 79210 loss: 0.0026 lr: 0.02\n",
      "iteration: 79220 loss: 0.0026 lr: 0.02\n",
      "iteration: 79230 loss: 0.0032 lr: 0.02\n",
      "iteration: 79240 loss: 0.0023 lr: 0.02\n",
      "iteration: 79250 loss: 0.0033 lr: 0.02\n",
      "iteration: 79260 loss: 0.0034 lr: 0.02\n",
      "iteration: 79270 loss: 0.0033 lr: 0.02\n",
      "iteration: 79280 loss: 0.0030 lr: 0.02\n",
      "iteration: 79290 loss: 0.0029 lr: 0.02\n",
      "iteration: 79300 loss: 0.0032 lr: 0.02\n",
      "iteration: 79310 loss: 0.0036 lr: 0.02\n",
      "iteration: 79320 loss: 0.0033 lr: 0.02\n",
      "iteration: 79330 loss: 0.0031 lr: 0.02\n",
      "iteration: 79340 loss: 0.0032 lr: 0.02\n",
      "iteration: 79350 loss: 0.0030 lr: 0.02\n",
      "iteration: 79360 loss: 0.0024 lr: 0.02\n",
      "iteration: 79370 loss: 0.0028 lr: 0.02\n",
      "iteration: 79380 loss: 0.0025 lr: 0.02\n",
      "iteration: 79390 loss: 0.0029 lr: 0.02\n",
      "iteration: 79400 loss: 0.0038 lr: 0.02\n",
      "iteration: 79410 loss: 0.0026 lr: 0.02\n",
      "iteration: 79420 loss: 0.0033 lr: 0.02\n",
      "iteration: 79430 loss: 0.0037 lr: 0.02\n",
      "iteration: 79440 loss: 0.0033 lr: 0.02\n",
      "iteration: 79450 loss: 0.0027 lr: 0.02\n",
      "iteration: 79460 loss: 0.0030 lr: 0.02\n",
      "iteration: 79470 loss: 0.0038 lr: 0.02\n",
      "iteration: 79480 loss: 0.0026 lr: 0.02\n",
      "iteration: 79490 loss: 0.0032 lr: 0.02\n",
      "iteration: 79500 loss: 0.0038 lr: 0.02\n",
      "iteration: 79510 loss: 0.0027 lr: 0.02\n",
      "iteration: 79520 loss: 0.0041 lr: 0.02\n",
      "iteration: 79530 loss: 0.0036 lr: 0.02\n",
      "iteration: 79540 loss: 0.0039 lr: 0.02\n",
      "iteration: 79550 loss: 0.0028 lr: 0.02\n",
      "iteration: 79560 loss: 0.0034 lr: 0.02\n",
      "iteration: 79570 loss: 0.0036 lr: 0.02\n",
      "iteration: 79580 loss: 0.0041 lr: 0.02\n",
      "iteration: 79590 loss: 0.0041 lr: 0.02\n",
      "iteration: 79600 loss: 0.0032 lr: 0.02\n",
      "iteration: 79610 loss: 0.0033 lr: 0.02\n",
      "iteration: 79620 loss: 0.0024 lr: 0.02\n",
      "iteration: 79630 loss: 0.0026 lr: 0.02\n",
      "iteration: 79640 loss: 0.0036 lr: 0.02\n",
      "iteration: 79650 loss: 0.0026 lr: 0.02\n",
      "iteration: 79660 loss: 0.0032 lr: 0.02\n",
      "iteration: 79670 loss: 0.0029 lr: 0.02\n",
      "iteration: 79680 loss: 0.0036 lr: 0.02\n",
      "iteration: 79690 loss: 0.0036 lr: 0.02\n",
      "iteration: 79700 loss: 0.0026 lr: 0.02\n",
      "iteration: 79710 loss: 0.0026 lr: 0.02\n",
      "iteration: 79720 loss: 0.0034 lr: 0.02\n",
      "iteration: 79730 loss: 0.0031 lr: 0.02\n",
      "iteration: 79740 loss: 0.0034 lr: 0.02\n",
      "iteration: 79750 loss: 0.0026 lr: 0.02\n",
      "iteration: 79760 loss: 0.0026 lr: 0.02\n",
      "iteration: 79770 loss: 0.0026 lr: 0.02\n",
      "iteration: 79780 loss: 0.0035 lr: 0.02\n",
      "iteration: 79790 loss: 0.0032 lr: 0.02\n",
      "iteration: 79800 loss: 0.0029 lr: 0.02\n",
      "iteration: 79810 loss: 0.0031 lr: 0.02\n",
      "iteration: 79820 loss: 0.0030 lr: 0.02\n",
      "iteration: 79830 loss: 0.0025 lr: 0.02\n",
      "iteration: 79840 loss: 0.0032 lr: 0.02\n",
      "iteration: 79850 loss: 0.0032 lr: 0.02\n",
      "iteration: 79860 loss: 0.0029 lr: 0.02\n",
      "iteration: 79870 loss: 0.0036 lr: 0.02\n",
      "iteration: 79880 loss: 0.0030 lr: 0.02\n",
      "iteration: 79890 loss: 0.0032 lr: 0.02\n",
      "iteration: 79900 loss: 0.0037 lr: 0.02\n",
      "iteration: 79910 loss: 0.0037 lr: 0.02\n",
      "iteration: 79920 loss: 0.0034 lr: 0.02\n",
      "iteration: 79930 loss: 0.0029 lr: 0.02\n",
      "iteration: 79940 loss: 0.0034 lr: 0.02\n",
      "iteration: 79950 loss: 0.0033 lr: 0.02\n",
      "iteration: 79960 loss: 0.0033 lr: 0.02\n",
      "iteration: 79970 loss: 0.0029 lr: 0.02\n",
      "iteration: 79980 loss: 0.0034 lr: 0.02\n",
      "iteration: 79990 loss: 0.0040 lr: 0.02\n",
      "iteration: 80000 loss: 0.0036 lr: 0.02\n",
      "iteration: 80010 loss: 0.0028 lr: 0.02\n",
      "iteration: 80020 loss: 0.0037 lr: 0.02\n",
      "iteration: 80030 loss: 0.0028 lr: 0.02\n",
      "iteration: 80040 loss: 0.0025 lr: 0.02\n",
      "iteration: 80050 loss: 0.0032 lr: 0.02\n",
      "iteration: 80060 loss: 0.0027 lr: 0.02\n",
      "iteration: 80070 loss: 0.0034 lr: 0.02\n",
      "iteration: 80080 loss: 0.0032 lr: 0.02\n",
      "iteration: 80090 loss: 0.0028 lr: 0.02\n",
      "iteration: 80100 loss: 0.0025 lr: 0.02\n",
      "iteration: 80110 loss: 0.0024 lr: 0.02\n",
      "iteration: 80120 loss: 0.0037 lr: 0.02\n",
      "iteration: 80130 loss: 0.0030 lr: 0.02\n",
      "iteration: 80140 loss: 0.0034 lr: 0.02\n",
      "iteration: 80150 loss: 0.0018 lr: 0.02\n",
      "iteration: 80160 loss: 0.0020 lr: 0.02\n",
      "iteration: 80170 loss: 0.0036 lr: 0.02\n",
      "iteration: 80180 loss: 0.0027 lr: 0.02\n",
      "iteration: 80190 loss: 0.0024 lr: 0.02\n",
      "iteration: 80200 loss: 0.0038 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 80210 loss: 0.0031 lr: 0.02\n",
      "iteration: 80220 loss: 0.0027 lr: 0.02\n",
      "iteration: 80230 loss: 0.0033 lr: 0.02\n",
      "iteration: 80240 loss: 0.0024 lr: 0.02\n",
      "iteration: 80250 loss: 0.0034 lr: 0.02\n",
      "iteration: 80260 loss: 0.0036 lr: 0.02\n",
      "iteration: 80270 loss: 0.0027 lr: 0.02\n",
      "iteration: 80280 loss: 0.0023 lr: 0.02\n",
      "iteration: 80290 loss: 0.0036 lr: 0.02\n",
      "iteration: 80300 loss: 0.0031 lr: 0.02\n",
      "iteration: 80310 loss: 0.0035 lr: 0.02\n",
      "iteration: 80320 loss: 0.0023 lr: 0.02\n",
      "iteration: 80330 loss: 0.0032 lr: 0.02\n",
      "iteration: 80340 loss: 0.0038 lr: 0.02\n",
      "iteration: 80350 loss: 0.0028 lr: 0.02\n",
      "iteration: 80360 loss: 0.0027 lr: 0.02\n",
      "iteration: 80370 loss: 0.0031 lr: 0.02\n",
      "iteration: 80380 loss: 0.0024 lr: 0.02\n",
      "iteration: 80390 loss: 0.0030 lr: 0.02\n",
      "iteration: 80400 loss: 0.0039 lr: 0.02\n",
      "iteration: 80410 loss: 0.0032 lr: 0.02\n",
      "iteration: 80420 loss: 0.0027 lr: 0.02\n",
      "iteration: 80430 loss: 0.0026 lr: 0.02\n",
      "iteration: 80440 loss: 0.0030 lr: 0.02\n",
      "iteration: 80450 loss: 0.0025 lr: 0.02\n",
      "iteration: 80460 loss: 0.0026 lr: 0.02\n",
      "iteration: 80470 loss: 0.0034 lr: 0.02\n",
      "iteration: 80480 loss: 0.0030 lr: 0.02\n",
      "iteration: 80490 loss: 0.0028 lr: 0.02\n",
      "iteration: 80500 loss: 0.0027 lr: 0.02\n",
      "iteration: 80510 loss: 0.0041 lr: 0.02\n",
      "iteration: 80520 loss: 0.0040 lr: 0.02\n",
      "iteration: 80530 loss: 0.0031 lr: 0.02\n",
      "iteration: 80540 loss: 0.0030 lr: 0.02\n",
      "iteration: 80550 loss: 0.0039 lr: 0.02\n",
      "iteration: 80560 loss: 0.0040 lr: 0.02\n",
      "iteration: 80570 loss: 0.0028 lr: 0.02\n",
      "iteration: 80580 loss: 0.0042 lr: 0.02\n",
      "iteration: 80590 loss: 0.0030 lr: 0.02\n",
      "iteration: 80600 loss: 0.0025 lr: 0.02\n",
      "iteration: 80610 loss: 0.0029 lr: 0.02\n",
      "iteration: 80620 loss: 0.0032 lr: 0.02\n",
      "iteration: 80630 loss: 0.0036 lr: 0.02\n",
      "iteration: 80640 loss: 0.0027 lr: 0.02\n",
      "iteration: 80650 loss: 0.0039 lr: 0.02\n",
      "iteration: 80660 loss: 0.0039 lr: 0.02\n",
      "iteration: 80670 loss: 0.0035 lr: 0.02\n",
      "iteration: 80680 loss: 0.0026 lr: 0.02\n",
      "iteration: 80690 loss: 0.0029 lr: 0.02\n",
      "iteration: 80700 loss: 0.0034 lr: 0.02\n",
      "iteration: 80710 loss: 0.0028 lr: 0.02\n",
      "iteration: 80720 loss: 0.0031 lr: 0.02\n",
      "iteration: 80730 loss: 0.0033 lr: 0.02\n",
      "iteration: 80740 loss: 0.0029 lr: 0.02\n",
      "iteration: 80750 loss: 0.0039 lr: 0.02\n",
      "iteration: 80760 loss: 0.0024 lr: 0.02\n",
      "iteration: 80770 loss: 0.0053 lr: 0.02\n",
      "iteration: 80780 loss: 0.0035 lr: 0.02\n",
      "iteration: 80790 loss: 0.0041 lr: 0.02\n",
      "iteration: 80800 loss: 0.0035 lr: 0.02\n",
      "iteration: 80810 loss: 0.0033 lr: 0.02\n",
      "iteration: 80820 loss: 0.0031 lr: 0.02\n",
      "iteration: 80830 loss: 0.0036 lr: 0.02\n",
      "iteration: 80840 loss: 0.0033 lr: 0.02\n",
      "iteration: 80850 loss: 0.0027 lr: 0.02\n",
      "iteration: 80860 loss: 0.0033 lr: 0.02\n",
      "iteration: 80870 loss: 0.0027 lr: 0.02\n",
      "iteration: 80880 loss: 0.0030 lr: 0.02\n",
      "iteration: 80890 loss: 0.0024 lr: 0.02\n",
      "iteration: 80900 loss: 0.0030 lr: 0.02\n",
      "iteration: 80910 loss: 0.0033 lr: 0.02\n",
      "iteration: 80920 loss: 0.0031 lr: 0.02\n",
      "iteration: 80930 loss: 0.0032 lr: 0.02\n",
      "iteration: 80940 loss: 0.0024 lr: 0.02\n",
      "iteration: 80950 loss: 0.0033 lr: 0.02\n",
      "iteration: 80960 loss: 0.0028 lr: 0.02\n",
      "iteration: 80970 loss: 0.0033 lr: 0.02\n",
      "iteration: 80980 loss: 0.0036 lr: 0.02\n",
      "iteration: 80990 loss: 0.0031 lr: 0.02\n",
      "iteration: 81000 loss: 0.0034 lr: 0.02\n",
      "iteration: 81010 loss: 0.0031 lr: 0.02\n",
      "iteration: 81020 loss: 0.0036 lr: 0.02\n",
      "iteration: 81030 loss: 0.0037 lr: 0.02\n",
      "iteration: 81040 loss: 0.0039 lr: 0.02\n",
      "iteration: 81050 loss: 0.0036 lr: 0.02\n",
      "iteration: 81060 loss: 0.0031 lr: 0.02\n",
      "iteration: 81070 loss: 0.0036 lr: 0.02\n",
      "iteration: 81080 loss: 0.0036 lr: 0.02\n",
      "iteration: 81090 loss: 0.0036 lr: 0.02\n",
      "iteration: 81100 loss: 0.0045 lr: 0.02\n",
      "iteration: 81110 loss: 0.0034 lr: 0.02\n",
      "iteration: 81120 loss: 0.0031 lr: 0.02\n",
      "iteration: 81130 loss: 0.0027 lr: 0.02\n",
      "iteration: 81140 loss: 0.0042 lr: 0.02\n",
      "iteration: 81150 loss: 0.0030 lr: 0.02\n",
      "iteration: 81160 loss: 0.0049 lr: 0.02\n",
      "iteration: 81170 loss: 0.0041 lr: 0.02\n",
      "iteration: 81180 loss: 0.0028 lr: 0.02\n",
      "iteration: 81190 loss: 0.0037 lr: 0.02\n",
      "iteration: 81200 loss: 0.0025 lr: 0.02\n",
      "iteration: 81210 loss: 0.0031 lr: 0.02\n",
      "iteration: 81220 loss: 0.0032 lr: 0.02\n",
      "iteration: 81230 loss: 0.0046 lr: 0.02\n",
      "iteration: 81240 loss: 0.0032 lr: 0.02\n",
      "iteration: 81250 loss: 0.0036 lr: 0.02\n",
      "iteration: 81260 loss: 0.0029 lr: 0.02\n",
      "iteration: 81270 loss: 0.0036 lr: 0.02\n",
      "iteration: 81280 loss: 0.0022 lr: 0.02\n",
      "iteration: 81290 loss: 0.0032 lr: 0.02\n",
      "iteration: 81300 loss: 0.0044 lr: 0.02\n",
      "iteration: 81310 loss: 0.0039 lr: 0.02\n",
      "iteration: 81320 loss: 0.0040 lr: 0.02\n",
      "iteration: 81330 loss: 0.0027 lr: 0.02\n",
      "iteration: 81340 loss: 0.0031 lr: 0.02\n",
      "iteration: 81350 loss: 0.0039 lr: 0.02\n",
      "iteration: 81360 loss: 0.0042 lr: 0.02\n",
      "iteration: 81370 loss: 0.0034 lr: 0.02\n",
      "iteration: 81380 loss: 0.0029 lr: 0.02\n",
      "iteration: 81390 loss: 0.0036 lr: 0.02\n",
      "iteration: 81400 loss: 0.0043 lr: 0.02\n",
      "iteration: 81410 loss: 0.0028 lr: 0.02\n",
      "iteration: 81420 loss: 0.0026 lr: 0.02\n",
      "iteration: 81430 loss: 0.0025 lr: 0.02\n",
      "iteration: 81440 loss: 0.0041 lr: 0.02\n",
      "iteration: 81450 loss: 0.0028 lr: 0.02\n",
      "iteration: 81460 loss: 0.0047 lr: 0.02\n",
      "iteration: 81470 loss: 0.0039 lr: 0.02\n",
      "iteration: 81480 loss: 0.0032 lr: 0.02\n",
      "iteration: 81490 loss: 0.0034 lr: 0.02\n",
      "iteration: 81500 loss: 0.0036 lr: 0.02\n",
      "iteration: 81510 loss: 0.0029 lr: 0.02\n",
      "iteration: 81520 loss: 0.0038 lr: 0.02\n",
      "iteration: 81530 loss: 0.0023 lr: 0.02\n",
      "iteration: 81540 loss: 0.0037 lr: 0.02\n",
      "iteration: 81550 loss: 0.0042 lr: 0.02\n",
      "iteration: 81560 loss: 0.0032 lr: 0.02\n",
      "iteration: 81570 loss: 0.0032 lr: 0.02\n",
      "iteration: 81580 loss: 0.0042 lr: 0.02\n",
      "iteration: 81590 loss: 0.0032 lr: 0.02\n",
      "iteration: 81600 loss: 0.0033 lr: 0.02\n",
      "iteration: 81610 loss: 0.0026 lr: 0.02\n",
      "iteration: 81620 loss: 0.0030 lr: 0.02\n",
      "iteration: 81630 loss: 0.0029 lr: 0.02\n",
      "iteration: 81640 loss: 0.0025 lr: 0.02\n",
      "iteration: 81650 loss: 0.0035 lr: 0.02\n",
      "iteration: 81660 loss: 0.0027 lr: 0.02\n",
      "iteration: 81670 loss: 0.0037 lr: 0.02\n",
      "iteration: 81680 loss: 0.0027 lr: 0.02\n",
      "iteration: 81690 loss: 0.0030 lr: 0.02\n",
      "iteration: 81700 loss: 0.0023 lr: 0.02\n",
      "iteration: 81710 loss: 0.0027 lr: 0.02\n",
      "iteration: 81720 loss: 0.0024 lr: 0.02\n",
      "iteration: 81730 loss: 0.0030 lr: 0.02\n",
      "iteration: 81740 loss: 0.0038 lr: 0.02\n",
      "iteration: 81750 loss: 0.0026 lr: 0.02\n",
      "iteration: 81760 loss: 0.0030 lr: 0.02\n",
      "iteration: 81770 loss: 0.0032 lr: 0.02\n",
      "iteration: 81780 loss: 0.0027 lr: 0.02\n",
      "iteration: 81790 loss: 0.0035 lr: 0.02\n",
      "iteration: 81800 loss: 0.0026 lr: 0.02\n",
      "iteration: 81810 loss: 0.0025 lr: 0.02\n",
      "iteration: 81820 loss: 0.0029 lr: 0.02\n",
      "iteration: 81830 loss: 0.0037 lr: 0.02\n",
      "iteration: 81840 loss: 0.0032 lr: 0.02\n",
      "iteration: 81850 loss: 0.0029 lr: 0.02\n",
      "iteration: 81860 loss: 0.0034 lr: 0.02\n",
      "iteration: 81870 loss: 0.0031 lr: 0.02\n",
      "iteration: 81880 loss: 0.0028 lr: 0.02\n",
      "iteration: 81890 loss: 0.0042 lr: 0.02\n",
      "iteration: 81900 loss: 0.0025 lr: 0.02\n",
      "iteration: 81910 loss: 0.0032 lr: 0.02\n",
      "iteration: 81920 loss: 0.0025 lr: 0.02\n",
      "iteration: 81930 loss: 0.0029 lr: 0.02\n",
      "iteration: 81940 loss: 0.0034 lr: 0.02\n",
      "iteration: 81950 loss: 0.0026 lr: 0.02\n",
      "iteration: 81960 loss: 0.0032 lr: 0.02\n",
      "iteration: 81970 loss: 0.0031 lr: 0.02\n",
      "iteration: 81980 loss: 0.0027 lr: 0.02\n",
      "iteration: 81990 loss: 0.0036 lr: 0.02\n",
      "iteration: 82000 loss: 0.0029 lr: 0.02\n",
      "iteration: 82010 loss: 0.0026 lr: 0.02\n",
      "iteration: 82020 loss: 0.0035 lr: 0.02\n",
      "iteration: 82030 loss: 0.0037 lr: 0.02\n",
      "iteration: 82040 loss: 0.0034 lr: 0.02\n",
      "iteration: 82050 loss: 0.0030 lr: 0.02\n",
      "iteration: 82060 loss: 0.0029 lr: 0.02\n",
      "iteration: 82070 loss: 0.0039 lr: 0.02\n",
      "iteration: 82080 loss: 0.0034 lr: 0.02\n",
      "iteration: 82090 loss: 0.0036 lr: 0.02\n",
      "iteration: 82100 loss: 0.0021 lr: 0.02\n",
      "iteration: 82110 loss: 0.0026 lr: 0.02\n",
      "iteration: 82120 loss: 0.0025 lr: 0.02\n",
      "iteration: 82130 loss: 0.0033 lr: 0.02\n",
      "iteration: 82140 loss: 0.0043 lr: 0.02\n",
      "iteration: 82150 loss: 0.0043 lr: 0.02\n",
      "iteration: 82160 loss: 0.0044 lr: 0.02\n",
      "iteration: 82170 loss: 0.0023 lr: 0.02\n",
      "iteration: 82180 loss: 0.0044 lr: 0.02\n",
      "iteration: 82190 loss: 0.0033 lr: 0.02\n",
      "iteration: 82200 loss: 0.0035 lr: 0.02\n",
      "iteration: 82210 loss: 0.0043 lr: 0.02\n",
      "iteration: 82220 loss: 0.0038 lr: 0.02\n",
      "iteration: 82230 loss: 0.0031 lr: 0.02\n",
      "iteration: 82240 loss: 0.0030 lr: 0.02\n",
      "iteration: 82250 loss: 0.0048 lr: 0.02\n",
      "iteration: 82260 loss: 0.0035 lr: 0.02\n",
      "iteration: 82270 loss: 0.0037 lr: 0.02\n",
      "iteration: 82280 loss: 0.0034 lr: 0.02\n",
      "iteration: 82290 loss: 0.0032 lr: 0.02\n",
      "iteration: 82300 loss: 0.0026 lr: 0.02\n",
      "iteration: 82310 loss: 0.0033 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 82320 loss: 0.0029 lr: 0.02\n",
      "iteration: 82330 loss: 0.0031 lr: 0.02\n",
      "iteration: 82340 loss: 0.0027 lr: 0.02\n",
      "iteration: 82350 loss: 0.0033 lr: 0.02\n",
      "iteration: 82360 loss: 0.0036 lr: 0.02\n",
      "iteration: 82370 loss: 0.0031 lr: 0.02\n",
      "iteration: 82380 loss: 0.0032 lr: 0.02\n",
      "iteration: 82390 loss: 0.0033 lr: 0.02\n",
      "iteration: 82400 loss: 0.0034 lr: 0.02\n",
      "iteration: 82410 loss: 0.0040 lr: 0.02\n",
      "iteration: 82420 loss: 0.0032 lr: 0.02\n",
      "iteration: 82430 loss: 0.0026 lr: 0.02\n",
      "iteration: 82440 loss: 0.0028 lr: 0.02\n",
      "iteration: 82450 loss: 0.0041 lr: 0.02\n",
      "iteration: 82460 loss: 0.0034 lr: 0.02\n",
      "iteration: 82470 loss: 0.0031 lr: 0.02\n",
      "iteration: 82480 loss: 0.0029 lr: 0.02\n",
      "iteration: 82490 loss: 0.0034 lr: 0.02\n",
      "iteration: 82500 loss: 0.0036 lr: 0.02\n",
      "iteration: 82510 loss: 0.0037 lr: 0.02\n",
      "iteration: 82520 loss: 0.0027 lr: 0.02\n",
      "iteration: 82530 loss: 0.0037 lr: 0.02\n",
      "iteration: 82540 loss: 0.0032 lr: 0.02\n",
      "iteration: 82550 loss: 0.0033 lr: 0.02\n",
      "iteration: 82560 loss: 0.0034 lr: 0.02\n",
      "iteration: 82570 loss: 0.0031 lr: 0.02\n",
      "iteration: 82580 loss: 0.0039 lr: 0.02\n",
      "iteration: 82590 loss: 0.0023 lr: 0.02\n",
      "iteration: 82600 loss: 0.0024 lr: 0.02\n",
      "iteration: 82610 loss: 0.0028 lr: 0.02\n",
      "iteration: 82620 loss: 0.0032 lr: 0.02\n",
      "iteration: 82630 loss: 0.0035 lr: 0.02\n",
      "iteration: 82640 loss: 0.0032 lr: 0.02\n",
      "iteration: 82650 loss: 0.0031 lr: 0.02\n",
      "iteration: 82660 loss: 0.0038 lr: 0.02\n",
      "iteration: 82670 loss: 0.0029 lr: 0.02\n",
      "iteration: 82680 loss: 0.0033 lr: 0.02\n",
      "iteration: 82690 loss: 0.0024 lr: 0.02\n",
      "iteration: 82700 loss: 0.0025 lr: 0.02\n",
      "iteration: 82710 loss: 0.0039 lr: 0.02\n",
      "iteration: 82720 loss: 0.0026 lr: 0.02\n",
      "iteration: 82730 loss: 0.0030 lr: 0.02\n",
      "iteration: 82740 loss: 0.0033 lr: 0.02\n",
      "iteration: 82750 loss: 0.0046 lr: 0.02\n",
      "iteration: 82760 loss: 0.0033 lr: 0.02\n",
      "iteration: 82770 loss: 0.0036 lr: 0.02\n",
      "iteration: 82780 loss: 0.0027 lr: 0.02\n",
      "iteration: 82790 loss: 0.0029 lr: 0.02\n",
      "iteration: 82800 loss: 0.0036 lr: 0.02\n",
      "iteration: 82810 loss: 0.0045 lr: 0.02\n",
      "iteration: 82820 loss: 0.0034 lr: 0.02\n",
      "iteration: 82830 loss: 0.0030 lr: 0.02\n",
      "iteration: 82840 loss: 0.0037 lr: 0.02\n",
      "iteration: 82850 loss: 0.0037 lr: 0.02\n",
      "iteration: 82860 loss: 0.0036 lr: 0.02\n",
      "iteration: 82870 loss: 0.0040 lr: 0.02\n",
      "iteration: 82880 loss: 0.0026 lr: 0.02\n",
      "iteration: 82890 loss: 0.0046 lr: 0.02\n",
      "iteration: 82900 loss: 0.0033 lr: 0.02\n",
      "iteration: 82910 loss: 0.0031 lr: 0.02\n",
      "iteration: 82920 loss: 0.0036 lr: 0.02\n",
      "iteration: 82930 loss: 0.0039 lr: 0.02\n",
      "iteration: 82940 loss: 0.0030 lr: 0.02\n",
      "iteration: 82950 loss: 0.0027 lr: 0.02\n",
      "iteration: 82960 loss: 0.0032 lr: 0.02\n",
      "iteration: 82970 loss: 0.0025 lr: 0.02\n",
      "iteration: 82980 loss: 0.0029 lr: 0.02\n",
      "iteration: 82990 loss: 0.0033 lr: 0.02\n",
      "iteration: 83000 loss: 0.0042 lr: 0.02\n",
      "iteration: 83010 loss: 0.0027 lr: 0.02\n",
      "iteration: 83020 loss: 0.0033 lr: 0.02\n",
      "iteration: 83030 loss: 0.0043 lr: 0.02\n",
      "iteration: 83040 loss: 0.0034 lr: 0.02\n",
      "iteration: 83050 loss: 0.0036 lr: 0.02\n",
      "iteration: 83060 loss: 0.0031 lr: 0.02\n",
      "iteration: 83070 loss: 0.0027 lr: 0.02\n",
      "iteration: 83080 loss: 0.0029 lr: 0.02\n",
      "iteration: 83090 loss: 0.0042 lr: 0.02\n",
      "iteration: 83100 loss: 0.0038 lr: 0.02\n",
      "iteration: 83110 loss: 0.0030 lr: 0.02\n",
      "iteration: 83120 loss: 0.0030 lr: 0.02\n",
      "iteration: 83130 loss: 0.0027 lr: 0.02\n",
      "iteration: 83140 loss: 0.0029 lr: 0.02\n",
      "iteration: 83150 loss: 0.0022 lr: 0.02\n",
      "iteration: 83160 loss: 0.0029 lr: 0.02\n",
      "iteration: 83170 loss: 0.0029 lr: 0.02\n",
      "iteration: 83180 loss: 0.0043 lr: 0.02\n",
      "iteration: 83190 loss: 0.0029 lr: 0.02\n",
      "iteration: 83200 loss: 0.0029 lr: 0.02\n",
      "iteration: 83210 loss: 0.0027 lr: 0.02\n",
      "iteration: 83220 loss: 0.0028 lr: 0.02\n",
      "iteration: 83230 loss: 0.0033 lr: 0.02\n",
      "iteration: 83240 loss: 0.0033 lr: 0.02\n",
      "iteration: 83250 loss: 0.0020 lr: 0.02\n",
      "iteration: 83260 loss: 0.0028 lr: 0.02\n",
      "iteration: 83270 loss: 0.0033 lr: 0.02\n",
      "iteration: 83280 loss: 0.0041 lr: 0.02\n",
      "iteration: 83290 loss: 0.0025 lr: 0.02\n",
      "iteration: 83300 loss: 0.0032 lr: 0.02\n",
      "iteration: 83310 loss: 0.0037 lr: 0.02\n",
      "iteration: 83320 loss: 0.0044 lr: 0.02\n",
      "iteration: 83330 loss: 0.0028 lr: 0.02\n",
      "iteration: 83340 loss: 0.0038 lr: 0.02\n",
      "iteration: 83350 loss: 0.0033 lr: 0.02\n",
      "iteration: 83360 loss: 0.0036 lr: 0.02\n",
      "iteration: 83370 loss: 0.0026 lr: 0.02\n",
      "iteration: 83380 loss: 0.0036 lr: 0.02\n",
      "iteration: 83390 loss: 0.0022 lr: 0.02\n",
      "iteration: 83400 loss: 0.0033 lr: 0.02\n",
      "iteration: 83410 loss: 0.0036 lr: 0.02\n",
      "iteration: 83420 loss: 0.0033 lr: 0.02\n",
      "iteration: 83430 loss: 0.0040 lr: 0.02\n",
      "iteration: 83440 loss: 0.0031 lr: 0.02\n",
      "iteration: 83450 loss: 0.0024 lr: 0.02\n",
      "iteration: 83460 loss: 0.0031 lr: 0.02\n",
      "iteration: 83470 loss: 0.0028 lr: 0.02\n",
      "iteration: 83480 loss: 0.0029 lr: 0.02\n",
      "iteration: 83490 loss: 0.0032 lr: 0.02\n",
      "iteration: 83500 loss: 0.0044 lr: 0.02\n",
      "iteration: 83510 loss: 0.0025 lr: 0.02\n",
      "iteration: 83520 loss: 0.0030 lr: 0.02\n",
      "iteration: 83530 loss: 0.0052 lr: 0.02\n",
      "iteration: 83540 loss: 0.0035 lr: 0.02\n",
      "iteration: 83550 loss: 0.0038 lr: 0.02\n",
      "iteration: 83560 loss: 0.0023 lr: 0.02\n",
      "iteration: 83570 loss: 0.0027 lr: 0.02\n",
      "iteration: 83580 loss: 0.0038 lr: 0.02\n",
      "iteration: 83590 loss: 0.0044 lr: 0.02\n",
      "iteration: 83600 loss: 0.0028 lr: 0.02\n",
      "iteration: 83610 loss: 0.0028 lr: 0.02\n",
      "iteration: 83620 loss: 0.0022 lr: 0.02\n",
      "iteration: 83630 loss: 0.0046 lr: 0.02\n",
      "iteration: 83640 loss: 0.0032 lr: 0.02\n",
      "iteration: 83650 loss: 0.0030 lr: 0.02\n",
      "iteration: 83660 loss: 0.0046 lr: 0.02\n",
      "iteration: 83670 loss: 0.0034 lr: 0.02\n",
      "iteration: 83680 loss: 0.0039 lr: 0.02\n",
      "iteration: 83690 loss: 0.0028 lr: 0.02\n",
      "iteration: 83700 loss: 0.0033 lr: 0.02\n",
      "iteration: 83710 loss: 0.0027 lr: 0.02\n",
      "iteration: 83720 loss: 0.0035 lr: 0.02\n",
      "iteration: 83730 loss: 0.0026 lr: 0.02\n",
      "iteration: 83740 loss: 0.0036 lr: 0.02\n",
      "iteration: 83750 loss: 0.0045 lr: 0.02\n",
      "iteration: 83760 loss: 0.0022 lr: 0.02\n",
      "iteration: 83770 loss: 0.0036 lr: 0.02\n",
      "iteration: 83780 loss: 0.0033 lr: 0.02\n",
      "iteration: 83790 loss: 0.0034 lr: 0.02\n",
      "iteration: 83800 loss: 0.0023 lr: 0.02\n",
      "iteration: 83810 loss: 0.0031 lr: 0.02\n",
      "iteration: 83820 loss: 0.0034 lr: 0.02\n",
      "iteration: 83830 loss: 0.0039 lr: 0.02\n",
      "iteration: 83840 loss: 0.0034 lr: 0.02\n",
      "iteration: 83850 loss: 0.0045 lr: 0.02\n",
      "iteration: 83860 loss: 0.0037 lr: 0.02\n",
      "iteration: 83870 loss: 0.0045 lr: 0.02\n",
      "iteration: 83880 loss: 0.0036 lr: 0.02\n",
      "iteration: 83890 loss: 0.0037 lr: 0.02\n",
      "iteration: 83900 loss: 0.0030 lr: 0.02\n",
      "iteration: 83910 loss: 0.0029 lr: 0.02\n",
      "iteration: 83920 loss: 0.0030 lr: 0.02\n",
      "iteration: 83930 loss: 0.0037 lr: 0.02\n",
      "iteration: 83940 loss: 0.0029 lr: 0.02\n",
      "iteration: 83950 loss: 0.0026 lr: 0.02\n",
      "iteration: 83960 loss: 0.0031 lr: 0.02\n",
      "iteration: 83970 loss: 0.0026 lr: 0.02\n",
      "iteration: 83980 loss: 0.0028 lr: 0.02\n",
      "iteration: 83990 loss: 0.0037 lr: 0.02\n",
      "iteration: 84000 loss: 0.0041 lr: 0.02\n",
      "iteration: 84010 loss: 0.0031 lr: 0.02\n",
      "iteration: 84020 loss: 0.0043 lr: 0.02\n",
      "iteration: 84030 loss: 0.0026 lr: 0.02\n",
      "iteration: 84040 loss: 0.0028 lr: 0.02\n",
      "iteration: 84050 loss: 0.0029 lr: 0.02\n",
      "iteration: 84060 loss: 0.0034 lr: 0.02\n",
      "iteration: 84070 loss: 0.0035 lr: 0.02\n",
      "iteration: 84080 loss: 0.0036 lr: 0.02\n",
      "iteration: 84090 loss: 0.0036 lr: 0.02\n",
      "iteration: 84100 loss: 0.0038 lr: 0.02\n",
      "iteration: 84110 loss: 0.0037 lr: 0.02\n",
      "iteration: 84120 loss: 0.0035 lr: 0.02\n",
      "iteration: 84130 loss: 0.0034 lr: 0.02\n",
      "iteration: 84140 loss: 0.0038 lr: 0.02\n",
      "iteration: 84150 loss: 0.0028 lr: 0.02\n",
      "iteration: 84160 loss: 0.0027 lr: 0.02\n",
      "iteration: 84170 loss: 0.0029 lr: 0.02\n",
      "iteration: 84180 loss: 0.0028 lr: 0.02\n",
      "iteration: 84190 loss: 0.0029 lr: 0.02\n",
      "iteration: 84200 loss: 0.0035 lr: 0.02\n",
      "iteration: 84210 loss: 0.0032 lr: 0.02\n",
      "iteration: 84220 loss: 0.0026 lr: 0.02\n",
      "iteration: 84230 loss: 0.0034 lr: 0.02\n",
      "iteration: 84240 loss: 0.0036 lr: 0.02\n",
      "iteration: 84250 loss: 0.0035 lr: 0.02\n",
      "iteration: 84260 loss: 0.0038 lr: 0.02\n",
      "iteration: 84270 loss: 0.0035 lr: 0.02\n",
      "iteration: 84280 loss: 0.0033 lr: 0.02\n",
      "iteration: 84290 loss: 0.0033 lr: 0.02\n",
      "iteration: 84300 loss: 0.0031 lr: 0.02\n",
      "iteration: 84310 loss: 0.0027 lr: 0.02\n",
      "iteration: 84320 loss: 0.0055 lr: 0.02\n",
      "iteration: 84330 loss: 0.0044 lr: 0.02\n",
      "iteration: 84340 loss: 0.0028 lr: 0.02\n",
      "iteration: 84350 loss: 0.0034 lr: 0.02\n",
      "iteration: 84360 loss: 0.0036 lr: 0.02\n",
      "iteration: 84370 loss: 0.0034 lr: 0.02\n",
      "iteration: 84380 loss: 0.0028 lr: 0.02\n",
      "iteration: 84390 loss: 0.0038 lr: 0.02\n",
      "iteration: 84400 loss: 0.0026 lr: 0.02\n",
      "iteration: 84410 loss: 0.0035 lr: 0.02\n",
      "iteration: 84420 loss: 0.0026 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 84430 loss: 0.0039 lr: 0.02\n",
      "iteration: 84440 loss: 0.0043 lr: 0.02\n",
      "iteration: 84450 loss: 0.0045 lr: 0.02\n",
      "iteration: 84460 loss: 0.0042 lr: 0.02\n",
      "iteration: 84470 loss: 0.0036 lr: 0.02\n",
      "iteration: 84480 loss: 0.0029 lr: 0.02\n",
      "iteration: 84490 loss: 0.0040 lr: 0.02\n",
      "iteration: 84500 loss: 0.0026 lr: 0.02\n",
      "iteration: 84510 loss: 0.0024 lr: 0.02\n",
      "iteration: 84520 loss: 0.0029 lr: 0.02\n",
      "iteration: 84530 loss: 0.0027 lr: 0.02\n",
      "iteration: 84540 loss: 0.0030 lr: 0.02\n",
      "iteration: 84550 loss: 0.0033 lr: 0.02\n",
      "iteration: 84560 loss: 0.0033 lr: 0.02\n",
      "iteration: 84570 loss: 0.0038 lr: 0.02\n",
      "iteration: 84580 loss: 0.0026 lr: 0.02\n",
      "iteration: 84590 loss: 0.0027 lr: 0.02\n",
      "iteration: 84600 loss: 0.0037 lr: 0.02\n",
      "iteration: 84610 loss: 0.0032 lr: 0.02\n",
      "iteration: 84620 loss: 0.0035 lr: 0.02\n",
      "iteration: 84630 loss: 0.0028 lr: 0.02\n",
      "iteration: 84640 loss: 0.0032 lr: 0.02\n",
      "iteration: 84650 loss: 0.0034 lr: 0.02\n",
      "iteration: 84660 loss: 0.0030 lr: 0.02\n",
      "iteration: 84670 loss: 0.0047 lr: 0.02\n",
      "iteration: 84680 loss: 0.0029 lr: 0.02\n",
      "iteration: 84690 loss: 0.0034 lr: 0.02\n",
      "iteration: 84700 loss: 0.0023 lr: 0.02\n",
      "iteration: 84710 loss: 0.0029 lr: 0.02\n",
      "iteration: 84720 loss: 0.0024 lr: 0.02\n",
      "iteration: 84730 loss: 0.0029 lr: 0.02\n",
      "iteration: 84740 loss: 0.0024 lr: 0.02\n",
      "iteration: 84750 loss: 0.0035 lr: 0.02\n",
      "iteration: 84760 loss: 0.0028 lr: 0.02\n",
      "iteration: 84770 loss: 0.0027 lr: 0.02\n",
      "iteration: 84780 loss: 0.0030 lr: 0.02\n",
      "iteration: 84790 loss: 0.0029 lr: 0.02\n",
      "iteration: 84800 loss: 0.0024 lr: 0.02\n",
      "iteration: 84810 loss: 0.0026 lr: 0.02\n",
      "iteration: 84820 loss: 0.0027 lr: 0.02\n",
      "iteration: 84830 loss: 0.0036 lr: 0.02\n",
      "iteration: 84840 loss: 0.0032 lr: 0.02\n",
      "iteration: 84850 loss: 0.0034 lr: 0.02\n",
      "iteration: 84860 loss: 0.0032 lr: 0.02\n",
      "iteration: 84870 loss: 0.0029 lr: 0.02\n",
      "iteration: 84880 loss: 0.0030 lr: 0.02\n",
      "iteration: 84890 loss: 0.0029 lr: 0.02\n",
      "iteration: 84900 loss: 0.0044 lr: 0.02\n",
      "iteration: 84910 loss: 0.0030 lr: 0.02\n",
      "iteration: 84920 loss: 0.0026 lr: 0.02\n",
      "iteration: 84930 loss: 0.0021 lr: 0.02\n",
      "iteration: 84940 loss: 0.0036 lr: 0.02\n",
      "iteration: 84950 loss: 0.0022 lr: 0.02\n",
      "iteration: 84960 loss: 0.0029 lr: 0.02\n",
      "iteration: 84970 loss: 0.0029 lr: 0.02\n",
      "iteration: 84980 loss: 0.0025 lr: 0.02\n",
      "iteration: 84990 loss: 0.0031 lr: 0.02\n",
      "iteration: 85000 loss: 0.0037 lr: 0.02\n",
      "iteration: 85010 loss: 0.0029 lr: 0.02\n",
      "iteration: 85020 loss: 0.0037 lr: 0.02\n",
      "iteration: 85030 loss: 0.0031 lr: 0.02\n",
      "iteration: 85040 loss: 0.0035 lr: 0.02\n",
      "iteration: 85050 loss: 0.0028 lr: 0.02\n",
      "iteration: 85060 loss: 0.0033 lr: 0.02\n",
      "iteration: 85070 loss: 0.0026 lr: 0.02\n",
      "iteration: 85080 loss: 0.0030 lr: 0.02\n",
      "iteration: 85090 loss: 0.0029 lr: 0.02\n",
      "iteration: 85100 loss: 0.0027 lr: 0.02\n",
      "iteration: 85110 loss: 0.0031 lr: 0.02\n",
      "iteration: 85120 loss: 0.0038 lr: 0.02\n",
      "iteration: 85130 loss: 0.0044 lr: 0.02\n",
      "iteration: 85140 loss: 0.0026 lr: 0.02\n",
      "iteration: 85150 loss: 0.0026 lr: 0.02\n",
      "iteration: 85160 loss: 0.0024 lr: 0.02\n",
      "iteration: 85170 loss: 0.0031 lr: 0.02\n",
      "iteration: 85180 loss: 0.0034 lr: 0.02\n",
      "iteration: 85190 loss: 0.0029 lr: 0.02\n",
      "iteration: 85200 loss: 0.0038 lr: 0.02\n",
      "iteration: 85210 loss: 0.0029 lr: 0.02\n",
      "iteration: 85220 loss: 0.0022 lr: 0.02\n",
      "iteration: 85230 loss: 0.0022 lr: 0.02\n",
      "iteration: 85240 loss: 0.0026 lr: 0.02\n",
      "iteration: 85250 loss: 0.0034 lr: 0.02\n",
      "iteration: 85260 loss: 0.0031 lr: 0.02\n",
      "iteration: 85270 loss: 0.0024 lr: 0.02\n",
      "iteration: 85280 loss: 0.0034 lr: 0.02\n",
      "iteration: 85290 loss: 0.0028 lr: 0.02\n",
      "iteration: 85300 loss: 0.0035 lr: 0.02\n",
      "iteration: 85310 loss: 0.0033 lr: 0.02\n",
      "iteration: 85320 loss: 0.0040 lr: 0.02\n",
      "iteration: 85330 loss: 0.0027 lr: 0.02\n",
      "iteration: 85340 loss: 0.0032 lr: 0.02\n",
      "iteration: 85350 loss: 0.0030 lr: 0.02\n",
      "iteration: 85360 loss: 0.0025 lr: 0.02\n",
      "iteration: 85370 loss: 0.0024 lr: 0.02\n",
      "iteration: 85380 loss: 0.0018 lr: 0.02\n",
      "iteration: 85390 loss: 0.0027 lr: 0.02\n",
      "iteration: 85400 loss: 0.0045 lr: 0.02\n",
      "iteration: 85410 loss: 0.0040 lr: 0.02\n",
      "iteration: 85420 loss: 0.0026 lr: 0.02\n",
      "iteration: 85430 loss: 0.0031 lr: 0.02\n",
      "iteration: 85440 loss: 0.0032 lr: 0.02\n",
      "iteration: 85450 loss: 0.0042 lr: 0.02\n",
      "iteration: 85460 loss: 0.0030 lr: 0.02\n",
      "iteration: 85470 loss: 0.0038 lr: 0.02\n",
      "iteration: 85480 loss: 0.0030 lr: 0.02\n",
      "iteration: 85490 loss: 0.0027 lr: 0.02\n",
      "iteration: 85500 loss: 0.0025 lr: 0.02\n",
      "iteration: 85510 loss: 0.0034 lr: 0.02\n",
      "iteration: 85520 loss: 0.0034 lr: 0.02\n",
      "iteration: 85530 loss: 0.0028 lr: 0.02\n",
      "iteration: 85540 loss: 0.0027 lr: 0.02\n",
      "iteration: 85550 loss: 0.0028 lr: 0.02\n",
      "iteration: 85560 loss: 0.0028 lr: 0.02\n",
      "iteration: 85570 loss: 0.0032 lr: 0.02\n",
      "iteration: 85580 loss: 0.0036 lr: 0.02\n",
      "iteration: 85590 loss: 0.0028 lr: 0.02\n",
      "iteration: 85600 loss: 0.0034 lr: 0.02\n",
      "iteration: 85610 loss: 0.0042 lr: 0.02\n",
      "iteration: 85620 loss: 0.0039 lr: 0.02\n",
      "iteration: 85630 loss: 0.0033 lr: 0.02\n",
      "iteration: 85640 loss: 0.0035 lr: 0.02\n",
      "iteration: 85650 loss: 0.0034 lr: 0.02\n",
      "iteration: 85660 loss: 0.0031 lr: 0.02\n",
      "iteration: 85670 loss: 0.0027 lr: 0.02\n",
      "iteration: 85680 loss: 0.0025 lr: 0.02\n",
      "iteration: 85690 loss: 0.0040 lr: 0.02\n",
      "iteration: 85700 loss: 0.0035 lr: 0.02\n",
      "iteration: 85710 loss: 0.0031 lr: 0.02\n",
      "iteration: 85720 loss: 0.0036 lr: 0.02\n",
      "iteration: 85730 loss: 0.0039 lr: 0.02\n",
      "iteration: 85740 loss: 0.0030 lr: 0.02\n",
      "iteration: 85750 loss: 0.0028 lr: 0.02\n",
      "iteration: 85760 loss: 0.0022 lr: 0.02\n",
      "iteration: 85770 loss: 0.0028 lr: 0.02\n",
      "iteration: 85780 loss: 0.0025 lr: 0.02\n",
      "iteration: 85790 loss: 0.0024 lr: 0.02\n",
      "iteration: 85800 loss: 0.0024 lr: 0.02\n",
      "iteration: 85810 loss: 0.0035 lr: 0.02\n",
      "iteration: 85820 loss: 0.0024 lr: 0.02\n",
      "iteration: 85830 loss: 0.0030 lr: 0.02\n",
      "iteration: 85840 loss: 0.0038 lr: 0.02\n",
      "iteration: 85850 loss: 0.0028 lr: 0.02\n",
      "iteration: 85860 loss: 0.0032 lr: 0.02\n",
      "iteration: 85870 loss: 0.0028 lr: 0.02\n",
      "iteration: 85880 loss: 0.0030 lr: 0.02\n",
      "iteration: 85890 loss: 0.0032 lr: 0.02\n",
      "iteration: 85900 loss: 0.0039 lr: 0.02\n",
      "iteration: 85910 loss: 0.0042 lr: 0.02\n",
      "iteration: 85920 loss: 0.0034 lr: 0.02\n",
      "iteration: 85930 loss: 0.0026 lr: 0.02\n",
      "iteration: 85940 loss: 0.0034 lr: 0.02\n",
      "iteration: 85950 loss: 0.0027 lr: 0.02\n",
      "iteration: 85960 loss: 0.0033 lr: 0.02\n",
      "iteration: 85970 loss: 0.0035 lr: 0.02\n",
      "iteration: 85980 loss: 0.0029 lr: 0.02\n",
      "iteration: 85990 loss: 0.0030 lr: 0.02\n",
      "iteration: 86000 loss: 0.0032 lr: 0.02\n",
      "iteration: 86010 loss: 0.0025 lr: 0.02\n",
      "iteration: 86020 loss: 0.0033 lr: 0.02\n",
      "iteration: 86030 loss: 0.0026 lr: 0.02\n",
      "iteration: 86040 loss: 0.0030 lr: 0.02\n",
      "iteration: 86050 loss: 0.0043 lr: 0.02\n",
      "iteration: 86060 loss: 0.0029 lr: 0.02\n",
      "iteration: 86070 loss: 0.0036 lr: 0.02\n",
      "iteration: 86080 loss: 0.0038 lr: 0.02\n",
      "iteration: 86090 loss: 0.0027 lr: 0.02\n",
      "iteration: 86100 loss: 0.0030 lr: 0.02\n",
      "iteration: 86110 loss: 0.0028 lr: 0.02\n",
      "iteration: 86120 loss: 0.0032 lr: 0.02\n",
      "iteration: 86130 loss: 0.0036 lr: 0.02\n",
      "iteration: 86140 loss: 0.0042 lr: 0.02\n",
      "iteration: 86150 loss: 0.0032 lr: 0.02\n",
      "iteration: 86160 loss: 0.0034 lr: 0.02\n",
      "iteration: 86170 loss: 0.0039 lr: 0.02\n",
      "iteration: 86180 loss: 0.0029 lr: 0.02\n",
      "iteration: 86190 loss: 0.0032 lr: 0.02\n",
      "iteration: 86200 loss: 0.0028 lr: 0.02\n",
      "iteration: 86210 loss: 0.0025 lr: 0.02\n",
      "iteration: 86220 loss: 0.0033 lr: 0.02\n",
      "iteration: 86230 loss: 0.0030 lr: 0.02\n",
      "iteration: 86240 loss: 0.0031 lr: 0.02\n",
      "iteration: 86250 loss: 0.0039 lr: 0.02\n",
      "iteration: 86260 loss: 0.0031 lr: 0.02\n",
      "iteration: 86270 loss: 0.0035 lr: 0.02\n",
      "iteration: 86280 loss: 0.0031 lr: 0.02\n",
      "iteration: 86290 loss: 0.0036 lr: 0.02\n",
      "iteration: 86300 loss: 0.0034 lr: 0.02\n",
      "iteration: 86310 loss: 0.0028 lr: 0.02\n",
      "iteration: 86320 loss: 0.0026 lr: 0.02\n",
      "iteration: 86330 loss: 0.0039 lr: 0.02\n",
      "iteration: 86340 loss: 0.0032 lr: 0.02\n",
      "iteration: 86350 loss: 0.0028 lr: 0.02\n",
      "iteration: 86360 loss: 0.0025 lr: 0.02\n",
      "iteration: 86370 loss: 0.0032 lr: 0.02\n",
      "iteration: 86380 loss: 0.0032 lr: 0.02\n",
      "iteration: 86390 loss: 0.0031 lr: 0.02\n",
      "iteration: 86400 loss: 0.0036 lr: 0.02\n",
      "iteration: 86410 loss: 0.0024 lr: 0.02\n",
      "iteration: 86420 loss: 0.0033 lr: 0.02\n",
      "iteration: 86430 loss: 0.0026 lr: 0.02\n",
      "iteration: 86440 loss: 0.0035 lr: 0.02\n",
      "iteration: 86450 loss: 0.0025 lr: 0.02\n",
      "iteration: 86460 loss: 0.0036 lr: 0.02\n",
      "iteration: 86470 loss: 0.0028 lr: 0.02\n",
      "iteration: 86480 loss: 0.0035 lr: 0.02\n",
      "iteration: 86490 loss: 0.0035 lr: 0.02\n",
      "iteration: 86500 loss: 0.0030 lr: 0.02\n",
      "iteration: 86510 loss: 0.0031 lr: 0.02\n",
      "iteration: 86520 loss: 0.0032 lr: 0.02\n",
      "iteration: 86530 loss: 0.0032 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 86540 loss: 0.0036 lr: 0.02\n",
      "iteration: 86550 loss: 0.0031 lr: 0.02\n",
      "iteration: 86560 loss: 0.0025 lr: 0.02\n",
      "iteration: 86570 loss: 0.0035 lr: 0.02\n",
      "iteration: 86580 loss: 0.0027 lr: 0.02\n",
      "iteration: 86590 loss: 0.0024 lr: 0.02\n",
      "iteration: 86600 loss: 0.0035 lr: 0.02\n",
      "iteration: 86610 loss: 0.0026 lr: 0.02\n",
      "iteration: 86620 loss: 0.0033 lr: 0.02\n",
      "iteration: 86630 loss: 0.0035 lr: 0.02\n",
      "iteration: 86640 loss: 0.0019 lr: 0.02\n",
      "iteration: 86650 loss: 0.0035 lr: 0.02\n",
      "iteration: 86660 loss: 0.0028 lr: 0.02\n",
      "iteration: 86670 loss: 0.0028 lr: 0.02\n",
      "iteration: 86680 loss: 0.0033 lr: 0.02\n",
      "iteration: 86690 loss: 0.0025 lr: 0.02\n",
      "iteration: 86700 loss: 0.0035 lr: 0.02\n",
      "iteration: 86710 loss: 0.0034 lr: 0.02\n",
      "iteration: 86720 loss: 0.0024 lr: 0.02\n",
      "iteration: 86730 loss: 0.0029 lr: 0.02\n",
      "iteration: 86740 loss: 0.0026 lr: 0.02\n",
      "iteration: 86750 loss: 0.0026 lr: 0.02\n",
      "iteration: 86760 loss: 0.0028 lr: 0.02\n",
      "iteration: 86770 loss: 0.0034 lr: 0.02\n",
      "iteration: 86780 loss: 0.0027 lr: 0.02\n",
      "iteration: 86790 loss: 0.0026 lr: 0.02\n",
      "iteration: 86800 loss: 0.0020 lr: 0.02\n",
      "iteration: 86810 loss: 0.0036 lr: 0.02\n",
      "iteration: 86820 loss: 0.0027 lr: 0.02\n",
      "iteration: 86830 loss: 0.0021 lr: 0.02\n",
      "iteration: 86840 loss: 0.0030 lr: 0.02\n",
      "iteration: 86850 loss: 0.0030 lr: 0.02\n",
      "iteration: 86860 loss: 0.0035 lr: 0.02\n",
      "iteration: 86870 loss: 0.0028 lr: 0.02\n",
      "iteration: 86880 loss: 0.0030 lr: 0.02\n",
      "iteration: 86890 loss: 0.0033 lr: 0.02\n",
      "iteration: 86900 loss: 0.0025 lr: 0.02\n",
      "iteration: 86910 loss: 0.0033 lr: 0.02\n",
      "iteration: 86920 loss: 0.0031 lr: 0.02\n",
      "iteration: 86930 loss: 0.0029 lr: 0.02\n",
      "iteration: 86940 loss: 0.0028 lr: 0.02\n",
      "iteration: 86950 loss: 0.0031 lr: 0.02\n",
      "iteration: 86960 loss: 0.0025 lr: 0.02\n",
      "iteration: 86970 loss: 0.0032 lr: 0.02\n",
      "iteration: 86980 loss: 0.0041 lr: 0.02\n",
      "iteration: 86990 loss: 0.0030 lr: 0.02\n",
      "iteration: 87000 loss: 0.0034 lr: 0.02\n",
      "iteration: 87010 loss: 0.0035 lr: 0.02\n",
      "iteration: 87020 loss: 0.0037 lr: 0.02\n",
      "iteration: 87030 loss: 0.0028 lr: 0.02\n",
      "iteration: 87040 loss: 0.0040 lr: 0.02\n",
      "iteration: 87050 loss: 0.0024 lr: 0.02\n",
      "iteration: 87060 loss: 0.0033 lr: 0.02\n",
      "iteration: 87070 loss: 0.0028 lr: 0.02\n",
      "iteration: 87080 loss: 0.0032 lr: 0.02\n",
      "iteration: 87090 loss: 0.0023 lr: 0.02\n",
      "iteration: 87100 loss: 0.0028 lr: 0.02\n",
      "iteration: 87110 loss: 0.0024 lr: 0.02\n",
      "iteration: 87120 loss: 0.0025 lr: 0.02\n",
      "iteration: 87130 loss: 0.0027 lr: 0.02\n",
      "iteration: 87140 loss: 0.0046 lr: 0.02\n",
      "iteration: 87150 loss: 0.0039 lr: 0.02\n",
      "iteration: 87160 loss: 0.0033 lr: 0.02\n",
      "iteration: 87170 loss: 0.0038 lr: 0.02\n",
      "iteration: 87180 loss: 0.0035 lr: 0.02\n",
      "iteration: 87190 loss: 0.0033 lr: 0.02\n",
      "iteration: 87200 loss: 0.0033 lr: 0.02\n",
      "iteration: 87210 loss: 0.0033 lr: 0.02\n",
      "iteration: 87220 loss: 0.0040 lr: 0.02\n",
      "iteration: 87230 loss: 0.0026 lr: 0.02\n",
      "iteration: 87240 loss: 0.0033 lr: 0.02\n",
      "iteration: 87250 loss: 0.0034 lr: 0.02\n",
      "iteration: 87260 loss: 0.0034 lr: 0.02\n",
      "iteration: 87270 loss: 0.0026 lr: 0.02\n",
      "iteration: 87280 loss: 0.0025 lr: 0.02\n",
      "iteration: 87290 loss: 0.0041 lr: 0.02\n",
      "iteration: 87300 loss: 0.0034 lr: 0.02\n",
      "iteration: 87310 loss: 0.0034 lr: 0.02\n",
      "iteration: 87320 loss: 0.0028 lr: 0.02\n",
      "iteration: 87330 loss: 0.0031 lr: 0.02\n",
      "iteration: 87340 loss: 0.0033 lr: 0.02\n",
      "iteration: 87350 loss: 0.0028 lr: 0.02\n",
      "iteration: 87360 loss: 0.0022 lr: 0.02\n",
      "iteration: 87370 loss: 0.0024 lr: 0.02\n",
      "iteration: 87380 loss: 0.0026 lr: 0.02\n",
      "iteration: 87390 loss: 0.0030 lr: 0.02\n",
      "iteration: 87400 loss: 0.0036 lr: 0.02\n",
      "iteration: 87410 loss: 0.0023 lr: 0.02\n",
      "iteration: 87420 loss: 0.0031 lr: 0.02\n",
      "iteration: 87430 loss: 0.0032 lr: 0.02\n",
      "iteration: 87440 loss: 0.0031 lr: 0.02\n",
      "iteration: 87450 loss: 0.0045 lr: 0.02\n",
      "iteration: 87460 loss: 0.0043 lr: 0.02\n",
      "iteration: 87470 loss: 0.0026 lr: 0.02\n",
      "iteration: 87480 loss: 0.0039 lr: 0.02\n",
      "iteration: 87490 loss: 0.0035 lr: 0.02\n",
      "iteration: 87500 loss: 0.0033 lr: 0.02\n",
      "iteration: 87510 loss: 0.0029 lr: 0.02\n",
      "iteration: 87520 loss: 0.0028 lr: 0.02\n",
      "iteration: 87530 loss: 0.0025 lr: 0.02\n",
      "iteration: 87540 loss: 0.0033 lr: 0.02\n",
      "iteration: 87550 loss: 0.0028 lr: 0.02\n",
      "iteration: 87560 loss: 0.0040 lr: 0.02\n",
      "iteration: 87570 loss: 0.0031 lr: 0.02\n",
      "iteration: 87580 loss: 0.0034 lr: 0.02\n",
      "iteration: 87590 loss: 0.0039 lr: 0.02\n",
      "iteration: 87600 loss: 0.0030 lr: 0.02\n",
      "iteration: 87610 loss: 0.0028 lr: 0.02\n",
      "iteration: 87620 loss: 0.0027 lr: 0.02\n",
      "iteration: 87630 loss: 0.0039 lr: 0.02\n",
      "iteration: 87640 loss: 0.0028 lr: 0.02\n",
      "iteration: 87650 loss: 0.0043 lr: 0.02\n",
      "iteration: 87660 loss: 0.0029 lr: 0.02\n",
      "iteration: 87670 loss: 0.0036 lr: 0.02\n",
      "iteration: 87680 loss: 0.0036 lr: 0.02\n",
      "iteration: 87690 loss: 0.0034 lr: 0.02\n",
      "iteration: 87700 loss: 0.0039 lr: 0.02\n",
      "iteration: 87710 loss: 0.0024 lr: 0.02\n",
      "iteration: 87720 loss: 0.0030 lr: 0.02\n",
      "iteration: 87730 loss: 0.0029 lr: 0.02\n",
      "iteration: 87740 loss: 0.0021 lr: 0.02\n",
      "iteration: 87750 loss: 0.0038 lr: 0.02\n",
      "iteration: 87760 loss: 0.0029 lr: 0.02\n",
      "iteration: 87770 loss: 0.0023 lr: 0.02\n",
      "iteration: 87780 loss: 0.0034 lr: 0.02\n",
      "iteration: 87790 loss: 0.0029 lr: 0.02\n",
      "iteration: 87800 loss: 0.0037 lr: 0.02\n",
      "iteration: 87810 loss: 0.0042 lr: 0.02\n",
      "iteration: 87820 loss: 0.0030 lr: 0.02\n",
      "iteration: 87830 loss: 0.0030 lr: 0.02\n",
      "iteration: 87840 loss: 0.0029 lr: 0.02\n",
      "iteration: 87850 loss: 0.0034 lr: 0.02\n",
      "iteration: 87860 loss: 0.0025 lr: 0.02\n",
      "iteration: 87870 loss: 0.0027 lr: 0.02\n",
      "iteration: 87880 loss: 0.0028 lr: 0.02\n",
      "iteration: 87890 loss: 0.0025 lr: 0.02\n",
      "iteration: 87900 loss: 0.0026 lr: 0.02\n",
      "iteration: 87910 loss: 0.0027 lr: 0.02\n",
      "iteration: 87920 loss: 0.0027 lr: 0.02\n",
      "iteration: 87930 loss: 0.0029 lr: 0.02\n",
      "iteration: 87940 loss: 0.0031 lr: 0.02\n",
      "iteration: 87950 loss: 0.0031 lr: 0.02\n",
      "iteration: 87960 loss: 0.0031 lr: 0.02\n",
      "iteration: 87970 loss: 0.0040 lr: 0.02\n",
      "iteration: 87980 loss: 0.0033 lr: 0.02\n",
      "iteration: 87990 loss: 0.0033 lr: 0.02\n",
      "iteration: 88000 loss: 0.0031 lr: 0.02\n",
      "iteration: 88010 loss: 0.0031 lr: 0.02\n",
      "iteration: 88020 loss: 0.0043 lr: 0.02\n",
      "iteration: 88030 loss: 0.0046 lr: 0.02\n",
      "iteration: 88040 loss: 0.0035 lr: 0.02\n",
      "iteration: 88050 loss: 0.0041 lr: 0.02\n",
      "iteration: 88060 loss: 0.0034 lr: 0.02\n",
      "iteration: 88070 loss: 0.0044 lr: 0.02\n",
      "iteration: 88080 loss: 0.0042 lr: 0.02\n",
      "iteration: 88090 loss: 0.0049 lr: 0.02\n",
      "iteration: 88100 loss: 0.0037 lr: 0.02\n",
      "iteration: 88110 loss: 0.0037 lr: 0.02\n",
      "iteration: 88120 loss: 0.0030 lr: 0.02\n",
      "iteration: 88130 loss: 0.0040 lr: 0.02\n",
      "iteration: 88140 loss: 0.0035 lr: 0.02\n",
      "iteration: 88150 loss: 0.0036 lr: 0.02\n",
      "iteration: 88160 loss: 0.0034 lr: 0.02\n",
      "iteration: 88170 loss: 0.0034 lr: 0.02\n",
      "iteration: 88180 loss: 0.0028 lr: 0.02\n",
      "iteration: 88190 loss: 0.0042 lr: 0.02\n",
      "iteration: 88200 loss: 0.0024 lr: 0.02\n",
      "iteration: 88210 loss: 0.0034 lr: 0.02\n",
      "iteration: 88220 loss: 0.0025 lr: 0.02\n",
      "iteration: 88230 loss: 0.0029 lr: 0.02\n",
      "iteration: 88240 loss: 0.0037 lr: 0.02\n",
      "iteration: 88250 loss: 0.0023 lr: 0.02\n",
      "iteration: 88260 loss: 0.0030 lr: 0.02\n",
      "iteration: 88270 loss: 0.0025 lr: 0.02\n",
      "iteration: 88280 loss: 0.0021 lr: 0.02\n",
      "iteration: 88290 loss: 0.0021 lr: 0.02\n",
      "iteration: 88300 loss: 0.0028 lr: 0.02\n",
      "iteration: 88310 loss: 0.0033 lr: 0.02\n",
      "iteration: 88320 loss: 0.0036 lr: 0.02\n",
      "iteration: 88330 loss: 0.0028 lr: 0.02\n",
      "iteration: 88340 loss: 0.0030 lr: 0.02\n",
      "iteration: 88350 loss: 0.0034 lr: 0.02\n",
      "iteration: 88360 loss: 0.0035 lr: 0.02\n",
      "iteration: 88370 loss: 0.0044 lr: 0.02\n",
      "iteration: 88380 loss: 0.0027 lr: 0.02\n",
      "iteration: 88390 loss: 0.0031 lr: 0.02\n",
      "iteration: 88400 loss: 0.0024 lr: 0.02\n",
      "iteration: 88410 loss: 0.0030 lr: 0.02\n",
      "iteration: 88420 loss: 0.0031 lr: 0.02\n",
      "iteration: 88430 loss: 0.0025 lr: 0.02\n",
      "iteration: 88440 loss: 0.0024 lr: 0.02\n",
      "iteration: 88450 loss: 0.0029 lr: 0.02\n",
      "iteration: 88460 loss: 0.0055 lr: 0.02\n",
      "iteration: 88470 loss: 0.0035 lr: 0.02\n",
      "iteration: 88480 loss: 0.0031 lr: 0.02\n",
      "iteration: 88490 loss: 0.0027 lr: 0.02\n",
      "iteration: 88500 loss: 0.0025 lr: 0.02\n",
      "iteration: 88510 loss: 0.0043 lr: 0.02\n",
      "iteration: 88520 loss: 0.0034 lr: 0.02\n",
      "iteration: 88530 loss: 0.0024 lr: 0.02\n",
      "iteration: 88540 loss: 0.0023 lr: 0.02\n",
      "iteration: 88550 loss: 0.0042 lr: 0.02\n",
      "iteration: 88560 loss: 0.0025 lr: 0.02\n",
      "iteration: 88570 loss: 0.0026 lr: 0.02\n",
      "iteration: 88580 loss: 0.0041 lr: 0.02\n",
      "iteration: 88590 loss: 0.0046 lr: 0.02\n",
      "iteration: 88600 loss: 0.0032 lr: 0.02\n",
      "iteration: 88610 loss: 0.0041 lr: 0.02\n",
      "iteration: 88620 loss: 0.0031 lr: 0.02\n",
      "iteration: 88630 loss: 0.0040 lr: 0.02\n",
      "iteration: 88640 loss: 0.0032 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 88650 loss: 0.0033 lr: 0.02\n",
      "iteration: 88660 loss: 0.0026 lr: 0.02\n",
      "iteration: 88670 loss: 0.0029 lr: 0.02\n",
      "iteration: 88680 loss: 0.0031 lr: 0.02\n",
      "iteration: 88690 loss: 0.0026 lr: 0.02\n",
      "iteration: 88700 loss: 0.0029 lr: 0.02\n",
      "iteration: 88710 loss: 0.0032 lr: 0.02\n",
      "iteration: 88720 loss: 0.0033 lr: 0.02\n",
      "iteration: 88730 loss: 0.0028 lr: 0.02\n",
      "iteration: 88740 loss: 0.0029 lr: 0.02\n",
      "iteration: 88750 loss: 0.0044 lr: 0.02\n",
      "iteration: 88760 loss: 0.0032 lr: 0.02\n",
      "iteration: 88770 loss: 0.0030 lr: 0.02\n",
      "iteration: 88780 loss: 0.0039 lr: 0.02\n",
      "iteration: 88790 loss: 0.0029 lr: 0.02\n",
      "iteration: 88800 loss: 0.0028 lr: 0.02\n",
      "iteration: 88810 loss: 0.0030 lr: 0.02\n",
      "iteration: 88820 loss: 0.0030 lr: 0.02\n",
      "iteration: 88830 loss: 0.0034 lr: 0.02\n",
      "iteration: 88840 loss: 0.0032 lr: 0.02\n",
      "iteration: 88850 loss: 0.0024 lr: 0.02\n",
      "iteration: 88860 loss: 0.0034 lr: 0.02\n",
      "iteration: 88870 loss: 0.0037 lr: 0.02\n",
      "iteration: 88880 loss: 0.0032 lr: 0.02\n",
      "iteration: 88890 loss: 0.0044 lr: 0.02\n",
      "iteration: 88900 loss: 0.0030 lr: 0.02\n",
      "iteration: 88910 loss: 0.0035 lr: 0.02\n",
      "iteration: 88920 loss: 0.0034 lr: 0.02\n",
      "iteration: 88930 loss: 0.0029 lr: 0.02\n",
      "iteration: 88940 loss: 0.0023 lr: 0.02\n",
      "iteration: 88950 loss: 0.0026 lr: 0.02\n",
      "iteration: 88960 loss: 0.0027 lr: 0.02\n",
      "iteration: 88970 loss: 0.0034 lr: 0.02\n",
      "iteration: 88980 loss: 0.0034 lr: 0.02\n",
      "iteration: 88990 loss: 0.0035 lr: 0.02\n",
      "iteration: 89000 loss: 0.0030 lr: 0.02\n",
      "iteration: 89010 loss: 0.0031 lr: 0.02\n",
      "iteration: 89020 loss: 0.0030 lr: 0.02\n",
      "iteration: 89030 loss: 0.0034 lr: 0.02\n",
      "iteration: 89040 loss: 0.0030 lr: 0.02\n",
      "iteration: 89050 loss: 0.0028 lr: 0.02\n",
      "iteration: 89060 loss: 0.0045 lr: 0.02\n",
      "iteration: 89070 loss: 0.0036 lr: 0.02\n",
      "iteration: 89080 loss: 0.0030 lr: 0.02\n",
      "iteration: 89090 loss: 0.0038 lr: 0.02\n",
      "iteration: 89100 loss: 0.0033 lr: 0.02\n",
      "iteration: 89110 loss: 0.0034 lr: 0.02\n",
      "iteration: 89120 loss: 0.0036 lr: 0.02\n",
      "iteration: 89130 loss: 0.0025 lr: 0.02\n",
      "iteration: 89140 loss: 0.0028 lr: 0.02\n",
      "iteration: 89150 loss: 0.0028 lr: 0.02\n",
      "iteration: 89160 loss: 0.0033 lr: 0.02\n",
      "iteration: 89170 loss: 0.0028 lr: 0.02\n",
      "iteration: 89180 loss: 0.0025 lr: 0.02\n",
      "iteration: 89190 loss: 0.0035 lr: 0.02\n",
      "iteration: 89200 loss: 0.0039 lr: 0.02\n",
      "iteration: 89210 loss: 0.0032 lr: 0.02\n",
      "iteration: 89220 loss: 0.0032 lr: 0.02\n",
      "iteration: 89230 loss: 0.0028 lr: 0.02\n",
      "iteration: 89240 loss: 0.0038 lr: 0.02\n",
      "iteration: 89250 loss: 0.0036 lr: 0.02\n",
      "iteration: 89260 loss: 0.0031 lr: 0.02\n",
      "iteration: 89270 loss: 0.0027 lr: 0.02\n",
      "iteration: 89280 loss: 0.0029 lr: 0.02\n",
      "iteration: 89290 loss: 0.0022 lr: 0.02\n",
      "iteration: 89300 loss: 0.0035 lr: 0.02\n",
      "iteration: 89310 loss: 0.0026 lr: 0.02\n",
      "iteration: 89320 loss: 0.0027 lr: 0.02\n",
      "iteration: 89330 loss: 0.0037 lr: 0.02\n",
      "iteration: 89340 loss: 0.0042 lr: 0.02\n",
      "iteration: 89350 loss: 0.0037 lr: 0.02\n",
      "iteration: 89360 loss: 0.0026 lr: 0.02\n",
      "iteration: 89370 loss: 0.0032 lr: 0.02\n",
      "iteration: 89380 loss: 0.0026 lr: 0.02\n",
      "iteration: 89390 loss: 0.0038 lr: 0.02\n",
      "iteration: 89400 loss: 0.0029 lr: 0.02\n",
      "iteration: 89410 loss: 0.0033 lr: 0.02\n",
      "iteration: 89420 loss: 0.0025 lr: 0.02\n",
      "iteration: 89430 loss: 0.0031 lr: 0.02\n",
      "iteration: 89440 loss: 0.0037 lr: 0.02\n",
      "iteration: 89450 loss: 0.0028 lr: 0.02\n",
      "iteration: 89460 loss: 0.0024 lr: 0.02\n",
      "iteration: 89470 loss: 0.0036 lr: 0.02\n",
      "iteration: 89480 loss: 0.0028 lr: 0.02\n",
      "iteration: 89490 loss: 0.0030 lr: 0.02\n",
      "iteration: 89500 loss: 0.0030 lr: 0.02\n",
      "iteration: 89510 loss: 0.0041 lr: 0.02\n",
      "iteration: 89520 loss: 0.0041 lr: 0.02\n",
      "iteration: 89530 loss: 0.0025 lr: 0.02\n",
      "iteration: 89540 loss: 0.0022 lr: 0.02\n",
      "iteration: 89550 loss: 0.0028 lr: 0.02\n",
      "iteration: 89560 loss: 0.0021 lr: 0.02\n",
      "iteration: 89570 loss: 0.0027 lr: 0.02\n",
      "iteration: 89580 loss: 0.0030 lr: 0.02\n",
      "iteration: 89590 loss: 0.0034 lr: 0.02\n",
      "iteration: 89600 loss: 0.0023 lr: 0.02\n",
      "iteration: 89610 loss: 0.0033 lr: 0.02\n",
      "iteration: 89620 loss: 0.0040 lr: 0.02\n",
      "iteration: 89630 loss: 0.0030 lr: 0.02\n",
      "iteration: 89640 loss: 0.0033 lr: 0.02\n",
      "iteration: 89650 loss: 0.0032 lr: 0.02\n",
      "iteration: 89660 loss: 0.0028 lr: 0.02\n",
      "iteration: 89670 loss: 0.0028 lr: 0.02\n",
      "iteration: 89680 loss: 0.0035 lr: 0.02\n",
      "iteration: 89690 loss: 0.0026 lr: 0.02\n",
      "iteration: 89700 loss: 0.0023 lr: 0.02\n",
      "iteration: 89710 loss: 0.0030 lr: 0.02\n",
      "iteration: 89720 loss: 0.0032 lr: 0.02\n",
      "iteration: 89730 loss: 0.0031 lr: 0.02\n",
      "iteration: 89740 loss: 0.0025 lr: 0.02\n",
      "iteration: 89750 loss: 0.0033 lr: 0.02\n",
      "iteration: 89760 loss: 0.0028 lr: 0.02\n",
      "iteration: 89770 loss: 0.0036 lr: 0.02\n",
      "iteration: 89780 loss: 0.0033 lr: 0.02\n",
      "iteration: 89790 loss: 0.0036 lr: 0.02\n",
      "iteration: 89800 loss: 0.0026 lr: 0.02\n",
      "iteration: 89810 loss: 0.0037 lr: 0.02\n",
      "iteration: 89820 loss: 0.0028 lr: 0.02\n",
      "iteration: 89830 loss: 0.0034 lr: 0.02\n",
      "iteration: 89840 loss: 0.0025 lr: 0.02\n",
      "iteration: 89850 loss: 0.0038 lr: 0.02\n",
      "iteration: 89860 loss: 0.0028 lr: 0.02\n",
      "iteration: 89870 loss: 0.0027 lr: 0.02\n",
      "iteration: 89880 loss: 0.0030 lr: 0.02\n",
      "iteration: 89890 loss: 0.0027 lr: 0.02\n",
      "iteration: 89900 loss: 0.0027 lr: 0.02\n",
      "iteration: 89910 loss: 0.0030 lr: 0.02\n",
      "iteration: 89920 loss: 0.0039 lr: 0.02\n",
      "iteration: 89930 loss: 0.0033 lr: 0.02\n",
      "iteration: 89940 loss: 0.0030 lr: 0.02\n",
      "iteration: 89950 loss: 0.0025 lr: 0.02\n",
      "iteration: 89960 loss: 0.0026 lr: 0.02\n",
      "iteration: 89970 loss: 0.0025 lr: 0.02\n",
      "iteration: 89980 loss: 0.0024 lr: 0.02\n",
      "iteration: 89990 loss: 0.0037 lr: 0.02\n",
      "iteration: 90000 loss: 0.0029 lr: 0.02\n",
      "iteration: 90010 loss: 0.0028 lr: 0.02\n",
      "iteration: 90020 loss: 0.0031 lr: 0.02\n",
      "iteration: 90030 loss: 0.0036 lr: 0.02\n",
      "iteration: 90040 loss: 0.0034 lr: 0.02\n",
      "iteration: 90050 loss: 0.0027 lr: 0.02\n",
      "iteration: 90060 loss: 0.0039 lr: 0.02\n",
      "iteration: 90070 loss: 0.0034 lr: 0.02\n",
      "iteration: 90080 loss: 0.0027 lr: 0.02\n",
      "iteration: 90090 loss: 0.0034 lr: 0.02\n",
      "iteration: 90100 loss: 0.0028 lr: 0.02\n",
      "iteration: 90110 loss: 0.0026 lr: 0.02\n",
      "iteration: 90120 loss: 0.0031 lr: 0.02\n",
      "iteration: 90130 loss: 0.0027 lr: 0.02\n",
      "iteration: 90140 loss: 0.0029 lr: 0.02\n",
      "iteration: 90150 loss: 0.0038 lr: 0.02\n",
      "iteration: 90160 loss: 0.0034 lr: 0.02\n",
      "iteration: 90170 loss: 0.0035 lr: 0.02\n",
      "iteration: 90180 loss: 0.0045 lr: 0.02\n",
      "iteration: 90190 loss: 0.0031 lr: 0.02\n",
      "iteration: 90200 loss: 0.0027 lr: 0.02\n",
      "iteration: 90210 loss: 0.0035 lr: 0.02\n",
      "iteration: 90220 loss: 0.0035 lr: 0.02\n",
      "iteration: 90230 loss: 0.0028 lr: 0.02\n",
      "iteration: 90240 loss: 0.0029 lr: 0.02\n",
      "iteration: 90250 loss: 0.0032 lr: 0.02\n",
      "iteration: 90260 loss: 0.0029 lr: 0.02\n",
      "iteration: 90270 loss: 0.0041 lr: 0.02\n",
      "iteration: 90280 loss: 0.0026 lr: 0.02\n",
      "iteration: 90290 loss: 0.0033 lr: 0.02\n",
      "iteration: 90300 loss: 0.0031 lr: 0.02\n",
      "iteration: 90310 loss: 0.0039 lr: 0.02\n",
      "iteration: 90320 loss: 0.0033 lr: 0.02\n",
      "iteration: 90330 loss: 0.0033 lr: 0.02\n",
      "iteration: 90340 loss: 0.0026 lr: 0.02\n",
      "iteration: 90350 loss: 0.0030 lr: 0.02\n",
      "iteration: 90360 loss: 0.0034 lr: 0.02\n",
      "iteration: 90370 loss: 0.0066 lr: 0.02\n",
      "iteration: 90380 loss: 0.0037 lr: 0.02\n",
      "iteration: 90390 loss: 0.0040 lr: 0.02\n",
      "iteration: 90400 loss: 0.0026 lr: 0.02\n",
      "iteration: 90410 loss: 0.0039 lr: 0.02\n",
      "iteration: 90420 loss: 0.0034 lr: 0.02\n",
      "iteration: 90430 loss: 0.0032 lr: 0.02\n",
      "iteration: 90440 loss: 0.0028 lr: 0.02\n",
      "iteration: 90450 loss: 0.0035 lr: 0.02\n",
      "iteration: 90460 loss: 0.0031 lr: 0.02\n",
      "iteration: 90470 loss: 0.0029 lr: 0.02\n",
      "iteration: 90480 loss: 0.0022 lr: 0.02\n",
      "iteration: 90490 loss: 0.0031 lr: 0.02\n",
      "iteration: 90500 loss: 0.0028 lr: 0.02\n",
      "iteration: 90510 loss: 0.0031 lr: 0.02\n",
      "iteration: 90520 loss: 0.0039 lr: 0.02\n",
      "iteration: 90530 loss: 0.0030 lr: 0.02\n",
      "iteration: 90540 loss: 0.0029 lr: 0.02\n",
      "iteration: 90550 loss: 0.0025 lr: 0.02\n",
      "iteration: 90560 loss: 0.0033 lr: 0.02\n",
      "iteration: 90570 loss: 0.0033 lr: 0.02\n",
      "iteration: 90580 loss: 0.0024 lr: 0.02\n",
      "iteration: 90590 loss: 0.0036 lr: 0.02\n",
      "iteration: 90600 loss: 0.0032 lr: 0.02\n",
      "iteration: 90610 loss: 0.0033 lr: 0.02\n",
      "iteration: 90620 loss: 0.0030 lr: 0.02\n",
      "iteration: 90630 loss: 0.0035 lr: 0.02\n",
      "iteration: 90640 loss: 0.0032 lr: 0.02\n",
      "iteration: 90650 loss: 0.0030 lr: 0.02\n",
      "iteration: 90660 loss: 0.0026 lr: 0.02\n",
      "iteration: 90670 loss: 0.0032 lr: 0.02\n",
      "iteration: 90680 loss: 0.0032 lr: 0.02\n",
      "iteration: 90690 loss: 0.0033 lr: 0.02\n",
      "iteration: 90700 loss: 0.0030 lr: 0.02\n",
      "iteration: 90710 loss: 0.0023 lr: 0.02\n",
      "iteration: 90720 loss: 0.0035 lr: 0.02\n",
      "iteration: 90730 loss: 0.0033 lr: 0.02\n",
      "iteration: 90740 loss: 0.0031 lr: 0.02\n",
      "iteration: 90750 loss: 0.0036 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 90760 loss: 0.0023 lr: 0.02\n",
      "iteration: 90770 loss: 0.0039 lr: 0.02\n",
      "iteration: 90780 loss: 0.0027 lr: 0.02\n",
      "iteration: 90790 loss: 0.0031 lr: 0.02\n",
      "iteration: 90800 loss: 0.0030 lr: 0.02\n",
      "iteration: 90810 loss: 0.0027 lr: 0.02\n",
      "iteration: 90820 loss: 0.0023 lr: 0.02\n",
      "iteration: 90830 loss: 0.0028 lr: 0.02\n",
      "iteration: 90840 loss: 0.0035 lr: 0.02\n",
      "iteration: 90850 loss: 0.0021 lr: 0.02\n",
      "iteration: 90860 loss: 0.0039 lr: 0.02\n",
      "iteration: 90870 loss: 0.0035 lr: 0.02\n",
      "iteration: 90880 loss: 0.0031 lr: 0.02\n",
      "iteration: 90890 loss: 0.0052 lr: 0.02\n",
      "iteration: 90900 loss: 0.0023 lr: 0.02\n",
      "iteration: 90910 loss: 0.0041 lr: 0.02\n",
      "iteration: 90920 loss: 0.0025 lr: 0.02\n",
      "iteration: 90930 loss: 0.0036 lr: 0.02\n",
      "iteration: 90940 loss: 0.0045 lr: 0.02\n",
      "iteration: 90950 loss: 0.0038 lr: 0.02\n",
      "iteration: 90960 loss: 0.0027 lr: 0.02\n",
      "iteration: 90970 loss: 0.0039 lr: 0.02\n",
      "iteration: 90980 loss: 0.0029 lr: 0.02\n",
      "iteration: 90990 loss: 0.0036 lr: 0.02\n",
      "iteration: 91000 loss: 0.0047 lr: 0.02\n",
      "iteration: 91010 loss: 0.0028 lr: 0.02\n",
      "iteration: 91020 loss: 0.0033 lr: 0.02\n",
      "iteration: 91030 loss: 0.0039 lr: 0.02\n",
      "iteration: 91040 loss: 0.0032 lr: 0.02\n",
      "iteration: 91050 loss: 0.0034 lr: 0.02\n",
      "iteration: 91060 loss: 0.0028 lr: 0.02\n",
      "iteration: 91070 loss: 0.0022 lr: 0.02\n",
      "iteration: 91080 loss: 0.0026 lr: 0.02\n",
      "iteration: 91090 loss: 0.0029 lr: 0.02\n",
      "iteration: 91100 loss: 0.0030 lr: 0.02\n",
      "iteration: 91110 loss: 0.0034 lr: 0.02\n",
      "iteration: 91120 loss: 0.0034 lr: 0.02\n",
      "iteration: 91130 loss: 0.0037 lr: 0.02\n",
      "iteration: 91140 loss: 0.0028 lr: 0.02\n",
      "iteration: 91150 loss: 0.0038 lr: 0.02\n",
      "iteration: 91160 loss: 0.0033 lr: 0.02\n",
      "iteration: 91170 loss: 0.0028 lr: 0.02\n",
      "iteration: 91180 loss: 0.0026 lr: 0.02\n",
      "iteration: 91190 loss: 0.0025 lr: 0.02\n",
      "iteration: 91200 loss: 0.0027 lr: 0.02\n",
      "iteration: 91210 loss: 0.0031 lr: 0.02\n",
      "iteration: 91220 loss: 0.0026 lr: 0.02\n",
      "iteration: 91230 loss: 0.0031 lr: 0.02\n",
      "iteration: 91240 loss: 0.0027 lr: 0.02\n",
      "iteration: 91250 loss: 0.0029 lr: 0.02\n",
      "iteration: 91260 loss: 0.0033 lr: 0.02\n",
      "iteration: 91270 loss: 0.0037 lr: 0.02\n",
      "iteration: 91280 loss: 0.0027 lr: 0.02\n",
      "iteration: 91290 loss: 0.0036 lr: 0.02\n",
      "iteration: 91300 loss: 0.0030 lr: 0.02\n",
      "iteration: 91310 loss: 0.0034 lr: 0.02\n",
      "iteration: 91320 loss: 0.0029 lr: 0.02\n",
      "iteration: 91330 loss: 0.0032 lr: 0.02\n",
      "iteration: 91340 loss: 0.0031 lr: 0.02\n",
      "iteration: 91350 loss: 0.0031 lr: 0.02\n",
      "iteration: 91360 loss: 0.0027 lr: 0.02\n",
      "iteration: 91370 loss: 0.0033 lr: 0.02\n",
      "iteration: 91380 loss: 0.0035 lr: 0.02\n",
      "iteration: 91390 loss: 0.0024 lr: 0.02\n",
      "iteration: 91400 loss: 0.0033 lr: 0.02\n",
      "iteration: 91410 loss: 0.0028 lr: 0.02\n",
      "iteration: 91420 loss: 0.0035 lr: 0.02\n",
      "iteration: 91430 loss: 0.0049 lr: 0.02\n",
      "iteration: 91440 loss: 0.0031 lr: 0.02\n",
      "iteration: 91450 loss: 0.0030 lr: 0.02\n",
      "iteration: 91460 loss: 0.0038 lr: 0.02\n",
      "iteration: 91470 loss: 0.0032 lr: 0.02\n",
      "iteration: 91480 loss: 0.0036 lr: 0.02\n",
      "iteration: 91490 loss: 0.0025 lr: 0.02\n",
      "iteration: 91500 loss: 0.0045 lr: 0.02\n",
      "iteration: 91510 loss: 0.0032 lr: 0.02\n",
      "iteration: 91520 loss: 0.0030 lr: 0.02\n",
      "iteration: 91530 loss: 0.0031 lr: 0.02\n",
      "iteration: 91540 loss: 0.0027 lr: 0.02\n",
      "iteration: 91550 loss: 0.0032 lr: 0.02\n",
      "iteration: 91560 loss: 0.0029 lr: 0.02\n",
      "iteration: 91570 loss: 0.0031 lr: 0.02\n",
      "iteration: 91580 loss: 0.0027 lr: 0.02\n",
      "iteration: 91590 loss: 0.0042 lr: 0.02\n",
      "iteration: 91600 loss: 0.0024 lr: 0.02\n",
      "iteration: 91610 loss: 0.0039 lr: 0.02\n",
      "iteration: 91620 loss: 0.0029 lr: 0.02\n",
      "iteration: 91630 loss: 0.0032 lr: 0.02\n",
      "iteration: 91640 loss: 0.0040 lr: 0.02\n",
      "iteration: 91650 loss: 0.0060 lr: 0.02\n",
      "iteration: 91660 loss: 0.0029 lr: 0.02\n",
      "iteration: 91670 loss: 0.0039 lr: 0.02\n",
      "iteration: 91680 loss: 0.0025 lr: 0.02\n",
      "iteration: 91690 loss: 0.0029 lr: 0.02\n",
      "iteration: 91700 loss: 0.0034 lr: 0.02\n",
      "iteration: 91710 loss: 0.0045 lr: 0.02\n",
      "iteration: 91720 loss: 0.0028 lr: 0.02\n",
      "iteration: 91730 loss: 0.0027 lr: 0.02\n",
      "iteration: 91740 loss: 0.0031 lr: 0.02\n",
      "iteration: 91750 loss: 0.0026 lr: 0.02\n",
      "iteration: 91760 loss: 0.0032 lr: 0.02\n",
      "iteration: 91770 loss: 0.0024 lr: 0.02\n",
      "iteration: 91780 loss: 0.0029 lr: 0.02\n",
      "iteration: 91790 loss: 0.0029 lr: 0.02\n",
      "iteration: 91800 loss: 0.0028 lr: 0.02\n",
      "iteration: 91810 loss: 0.0037 lr: 0.02\n",
      "iteration: 91820 loss: 0.0027 lr: 0.02\n",
      "iteration: 91830 loss: 0.0027 lr: 0.02\n",
      "iteration: 91840 loss: 0.0026 lr: 0.02\n",
      "iteration: 91850 loss: 0.0025 lr: 0.02\n",
      "iteration: 91860 loss: 0.0023 lr: 0.02\n",
      "iteration: 91870 loss: 0.0030 lr: 0.02\n",
      "iteration: 91880 loss: 0.0030 lr: 0.02\n",
      "iteration: 91890 loss: 0.0035 lr: 0.02\n",
      "iteration: 91900 loss: 0.0045 lr: 0.02\n",
      "iteration: 91910 loss: 0.0037 lr: 0.02\n",
      "iteration: 91920 loss: 0.0031 lr: 0.02\n",
      "iteration: 91930 loss: 0.0042 lr: 0.02\n",
      "iteration: 91940 loss: 0.0028 lr: 0.02\n",
      "iteration: 91950 loss: 0.0037 lr: 0.02\n",
      "iteration: 91960 loss: 0.0036 lr: 0.02\n",
      "iteration: 91970 loss: 0.0033 lr: 0.02\n",
      "iteration: 91980 loss: 0.0032 lr: 0.02\n",
      "iteration: 91990 loss: 0.0033 lr: 0.02\n",
      "iteration: 92000 loss: 0.0032 lr: 0.02\n",
      "iteration: 92010 loss: 0.0028 lr: 0.02\n",
      "iteration: 92020 loss: 0.0036 lr: 0.02\n",
      "iteration: 92030 loss: 0.0029 lr: 0.02\n",
      "iteration: 92040 loss: 0.0033 lr: 0.02\n",
      "iteration: 92050 loss: 0.0027 lr: 0.02\n",
      "iteration: 92060 loss: 0.0038 lr: 0.02\n",
      "iteration: 92070 loss: 0.0036 lr: 0.02\n",
      "iteration: 92080 loss: 0.0028 lr: 0.02\n",
      "iteration: 92090 loss: 0.0030 lr: 0.02\n",
      "iteration: 92100 loss: 0.0026 lr: 0.02\n",
      "iteration: 92110 loss: 0.0030 lr: 0.02\n",
      "iteration: 92120 loss: 0.0032 lr: 0.02\n",
      "iteration: 92130 loss: 0.0034 lr: 0.02\n",
      "iteration: 92140 loss: 0.0033 lr: 0.02\n",
      "iteration: 92150 loss: 0.0034 lr: 0.02\n",
      "iteration: 92160 loss: 0.0026 lr: 0.02\n",
      "iteration: 92170 loss: 0.0028 lr: 0.02\n",
      "iteration: 92180 loss: 0.0038 lr: 0.02\n",
      "iteration: 92190 loss: 0.0048 lr: 0.02\n",
      "iteration: 92200 loss: 0.0028 lr: 0.02\n",
      "iteration: 92210 loss: 0.0042 lr: 0.02\n",
      "iteration: 92220 loss: 0.0028 lr: 0.02\n",
      "iteration: 92230 loss: 0.0024 lr: 0.02\n",
      "iteration: 92240 loss: 0.0021 lr: 0.02\n",
      "iteration: 92250 loss: 0.0025 lr: 0.02\n",
      "iteration: 92260 loss: 0.0034 lr: 0.02\n",
      "iteration: 92270 loss: 0.0037 lr: 0.02\n",
      "iteration: 92280 loss: 0.0023 lr: 0.02\n",
      "iteration: 92290 loss: 0.0033 lr: 0.02\n",
      "iteration: 92300 loss: 0.0030 lr: 0.02\n",
      "iteration: 92310 loss: 0.0027 lr: 0.02\n",
      "iteration: 92320 loss: 0.0030 lr: 0.02\n",
      "iteration: 92330 loss: 0.0035 lr: 0.02\n",
      "iteration: 92340 loss: 0.0027 lr: 0.02\n",
      "iteration: 92350 loss: 0.0028 lr: 0.02\n",
      "iteration: 92360 loss: 0.0030 lr: 0.02\n",
      "iteration: 92370 loss: 0.0021 lr: 0.02\n",
      "iteration: 92380 loss: 0.0031 lr: 0.02\n",
      "iteration: 92390 loss: 0.0027 lr: 0.02\n",
      "iteration: 92400 loss: 0.0026 lr: 0.02\n",
      "iteration: 92410 loss: 0.0041 lr: 0.02\n",
      "iteration: 92420 loss: 0.0033 lr: 0.02\n",
      "iteration: 92430 loss: 0.0023 lr: 0.02\n",
      "iteration: 92440 loss: 0.0030 lr: 0.02\n",
      "iteration: 92450 loss: 0.0032 lr: 0.02\n",
      "iteration: 92460 loss: 0.0033 lr: 0.02\n",
      "iteration: 92470 loss: 0.0029 lr: 0.02\n",
      "iteration: 92480 loss: 0.0034 lr: 0.02\n",
      "iteration: 92490 loss: 0.0035 lr: 0.02\n",
      "iteration: 92500 loss: 0.0031 lr: 0.02\n",
      "iteration: 92510 loss: 0.0026 lr: 0.02\n",
      "iteration: 92520 loss: 0.0024 lr: 0.02\n",
      "iteration: 92530 loss: 0.0029 lr: 0.02\n",
      "iteration: 92540 loss: 0.0035 lr: 0.02\n",
      "iteration: 92550 loss: 0.0027 lr: 0.02\n",
      "iteration: 92560 loss: 0.0030 lr: 0.02\n",
      "iteration: 92570 loss: 0.0029 lr: 0.02\n",
      "iteration: 92580 loss: 0.0023 lr: 0.02\n",
      "iteration: 92590 loss: 0.0025 lr: 0.02\n",
      "iteration: 92600 loss: 0.0029 lr: 0.02\n",
      "iteration: 92610 loss: 0.0030 lr: 0.02\n",
      "iteration: 92620 loss: 0.0038 lr: 0.02\n",
      "iteration: 92630 loss: 0.0035 lr: 0.02\n",
      "iteration: 92640 loss: 0.0032 lr: 0.02\n",
      "iteration: 92650 loss: 0.0021 lr: 0.02\n",
      "iteration: 92660 loss: 0.0027 lr: 0.02\n",
      "iteration: 92670 loss: 0.0030 lr: 0.02\n",
      "iteration: 92680 loss: 0.0030 lr: 0.02\n",
      "iteration: 92690 loss: 0.0030 lr: 0.02\n",
      "iteration: 92700 loss: 0.0025 lr: 0.02\n",
      "iteration: 92710 loss: 0.0029 lr: 0.02\n",
      "iteration: 92720 loss: 0.0025 lr: 0.02\n",
      "iteration: 92730 loss: 0.0047 lr: 0.02\n",
      "iteration: 92740 loss: 0.0039 lr: 0.02\n",
      "iteration: 92750 loss: 0.0031 lr: 0.02\n",
      "iteration: 92760 loss: 0.0024 lr: 0.02\n",
      "iteration: 92770 loss: 0.0037 lr: 0.02\n",
      "iteration: 92780 loss: 0.0039 lr: 0.02\n",
      "iteration: 92790 loss: 0.0030 lr: 0.02\n",
      "iteration: 92800 loss: 0.0031 lr: 0.02\n",
      "iteration: 92810 loss: 0.0029 lr: 0.02\n",
      "iteration: 92820 loss: 0.0032 lr: 0.02\n",
      "iteration: 92830 loss: 0.0031 lr: 0.02\n",
      "iteration: 92840 loss: 0.0031 lr: 0.02\n",
      "iteration: 92850 loss: 0.0032 lr: 0.02\n",
      "iteration: 92860 loss: 0.0029 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 92870 loss: 0.0038 lr: 0.02\n",
      "iteration: 92880 loss: 0.0043 lr: 0.02\n",
      "iteration: 92890 loss: 0.0025 lr: 0.02\n",
      "iteration: 92900 loss: 0.0040 lr: 0.02\n",
      "iteration: 92910 loss: 0.0023 lr: 0.02\n",
      "iteration: 92920 loss: 0.0024 lr: 0.02\n",
      "iteration: 92930 loss: 0.0039 lr: 0.02\n",
      "iteration: 92940 loss: 0.0026 lr: 0.02\n",
      "iteration: 92950 loss: 0.0027 lr: 0.02\n",
      "iteration: 92960 loss: 0.0027 lr: 0.02\n",
      "iteration: 92970 loss: 0.0031 lr: 0.02\n",
      "iteration: 92980 loss: 0.0031 lr: 0.02\n",
      "iteration: 92990 loss: 0.0031 lr: 0.02\n",
      "iteration: 93000 loss: 0.0036 lr: 0.02\n",
      "iteration: 93010 loss: 0.0024 lr: 0.02\n",
      "iteration: 93020 loss: 0.0031 lr: 0.02\n",
      "iteration: 93030 loss: 0.0026 lr: 0.02\n",
      "iteration: 93040 loss: 0.0032 lr: 0.02\n",
      "iteration: 93050 loss: 0.0040 lr: 0.02\n",
      "iteration: 93060 loss: 0.0029 lr: 0.02\n",
      "iteration: 93070 loss: 0.0023 lr: 0.02\n",
      "iteration: 93080 loss: 0.0041 lr: 0.02\n",
      "iteration: 93090 loss: 0.0033 lr: 0.02\n",
      "iteration: 93100 loss: 0.0029 lr: 0.02\n",
      "iteration: 93110 loss: 0.0028 lr: 0.02\n",
      "iteration: 93120 loss: 0.0035 lr: 0.02\n",
      "iteration: 93130 loss: 0.0032 lr: 0.02\n",
      "iteration: 93140 loss: 0.0031 lr: 0.02\n",
      "iteration: 93150 loss: 0.0025 lr: 0.02\n",
      "iteration: 93160 loss: 0.0032 lr: 0.02\n",
      "iteration: 93170 loss: 0.0029 lr: 0.02\n",
      "iteration: 93180 loss: 0.0040 lr: 0.02\n",
      "iteration: 93190 loss: 0.0022 lr: 0.02\n",
      "iteration: 93200 loss: 0.0027 lr: 0.02\n",
      "iteration: 93210 loss: 0.0039 lr: 0.02\n",
      "iteration: 93220 loss: 0.0031 lr: 0.02\n",
      "iteration: 93230 loss: 0.0031 lr: 0.02\n",
      "iteration: 93240 loss: 0.0039 lr: 0.02\n",
      "iteration: 93250 loss: 0.0033 lr: 0.02\n",
      "iteration: 93260 loss: 0.0026 lr: 0.02\n",
      "iteration: 93270 loss: 0.0036 lr: 0.02\n",
      "iteration: 93280 loss: 0.0025 lr: 0.02\n",
      "iteration: 93290 loss: 0.0033 lr: 0.02\n",
      "iteration: 93300 loss: 0.0024 lr: 0.02\n",
      "iteration: 93310 loss: 0.0024 lr: 0.02\n",
      "iteration: 93320 loss: 0.0023 lr: 0.02\n",
      "iteration: 93330 loss: 0.0040 lr: 0.02\n",
      "iteration: 93340 loss: 0.0027 lr: 0.02\n",
      "iteration: 93350 loss: 0.0028 lr: 0.02\n",
      "iteration: 93360 loss: 0.0028 lr: 0.02\n",
      "iteration: 93370 loss: 0.0033 lr: 0.02\n",
      "iteration: 93380 loss: 0.0039 lr: 0.02\n",
      "iteration: 93390 loss: 0.0040 lr: 0.02\n",
      "iteration: 93400 loss: 0.0031 lr: 0.02\n",
      "iteration: 93410 loss: 0.0030 lr: 0.02\n",
      "iteration: 93420 loss: 0.0034 lr: 0.02\n",
      "iteration: 93430 loss: 0.0027 lr: 0.02\n",
      "iteration: 93440 loss: 0.0026 lr: 0.02\n",
      "iteration: 93450 loss: 0.0025 lr: 0.02\n",
      "iteration: 93460 loss: 0.0034 lr: 0.02\n",
      "iteration: 93470 loss: 0.0030 lr: 0.02\n",
      "iteration: 93480 loss: 0.0028 lr: 0.02\n",
      "iteration: 93490 loss: 0.0035 lr: 0.02\n",
      "iteration: 93500 loss: 0.0023 lr: 0.02\n",
      "iteration: 93510 loss: 0.0027 lr: 0.02\n",
      "iteration: 93520 loss: 0.0026 lr: 0.02\n",
      "iteration: 93530 loss: 0.0041 lr: 0.02\n",
      "iteration: 93540 loss: 0.0031 lr: 0.02\n",
      "iteration: 93550 loss: 0.0027 lr: 0.02\n",
      "iteration: 93560 loss: 0.0024 lr: 0.02\n",
      "iteration: 93570 loss: 0.0027 lr: 0.02\n",
      "iteration: 93580 loss: 0.0033 lr: 0.02\n",
      "iteration: 93590 loss: 0.0025 lr: 0.02\n",
      "iteration: 93600 loss: 0.0034 lr: 0.02\n",
      "iteration: 93610 loss: 0.0032 lr: 0.02\n",
      "iteration: 93620 loss: 0.0028 lr: 0.02\n",
      "iteration: 93630 loss: 0.0031 lr: 0.02\n",
      "iteration: 93640 loss: 0.0036 lr: 0.02\n",
      "iteration: 93650 loss: 0.0036 lr: 0.02\n",
      "iteration: 93660 loss: 0.0029 lr: 0.02\n",
      "iteration: 93670 loss: 0.0032 lr: 0.02\n",
      "iteration: 93680 loss: 0.0031 lr: 0.02\n",
      "iteration: 93690 loss: 0.0026 lr: 0.02\n",
      "iteration: 93700 loss: 0.0025 lr: 0.02\n",
      "iteration: 93710 loss: 0.0022 lr: 0.02\n",
      "iteration: 93720 loss: 0.0032 lr: 0.02\n",
      "iteration: 93730 loss: 0.0030 lr: 0.02\n",
      "iteration: 93740 loss: 0.0034 lr: 0.02\n",
      "iteration: 93750 loss: 0.0026 lr: 0.02\n",
      "iteration: 93760 loss: 0.0022 lr: 0.02\n",
      "iteration: 93770 loss: 0.0032 lr: 0.02\n",
      "iteration: 93780 loss: 0.0032 lr: 0.02\n",
      "iteration: 93790 loss: 0.0027 lr: 0.02\n",
      "iteration: 93800 loss: 0.0028 lr: 0.02\n",
      "iteration: 93810 loss: 0.0025 lr: 0.02\n",
      "iteration: 93820 loss: 0.0027 lr: 0.02\n",
      "iteration: 93830 loss: 0.0031 lr: 0.02\n",
      "iteration: 93840 loss: 0.0029 lr: 0.02\n",
      "iteration: 93850 loss: 0.0025 lr: 0.02\n",
      "iteration: 93860 loss: 0.0028 lr: 0.02\n",
      "iteration: 93870 loss: 0.0031 lr: 0.02\n",
      "iteration: 93880 loss: 0.0024 lr: 0.02\n",
      "iteration: 93890 loss: 0.0033 lr: 0.02\n",
      "iteration: 93900 loss: 0.0024 lr: 0.02\n",
      "iteration: 93910 loss: 0.0038 lr: 0.02\n",
      "iteration: 93920 loss: 0.0035 lr: 0.02\n",
      "iteration: 93930 loss: 0.0031 lr: 0.02\n",
      "iteration: 93940 loss: 0.0031 lr: 0.02\n",
      "iteration: 93950 loss: 0.0037 lr: 0.02\n",
      "iteration: 93960 loss: 0.0027 lr: 0.02\n",
      "iteration: 93970 loss: 0.0041 lr: 0.02\n",
      "iteration: 93980 loss: 0.0032 lr: 0.02\n",
      "iteration: 93990 loss: 0.0032 lr: 0.02\n",
      "iteration: 94000 loss: 0.0032 lr: 0.02\n",
      "iteration: 94010 loss: 0.0028 lr: 0.02\n",
      "iteration: 94020 loss: 0.0040 lr: 0.02\n",
      "iteration: 94030 loss: 0.0026 lr: 0.02\n",
      "iteration: 94040 loss: 0.0040 lr: 0.02\n",
      "iteration: 94050 loss: 0.0025 lr: 0.02\n",
      "iteration: 94060 loss: 0.0030 lr: 0.02\n",
      "iteration: 94070 loss: 0.0028 lr: 0.02\n",
      "iteration: 94080 loss: 0.0036 lr: 0.02\n",
      "iteration: 94090 loss: 0.0026 lr: 0.02\n",
      "iteration: 94100 loss: 0.0032 lr: 0.02\n",
      "iteration: 94110 loss: 0.0026 lr: 0.02\n",
      "iteration: 94120 loss: 0.0030 lr: 0.02\n",
      "iteration: 94130 loss: 0.0042 lr: 0.02\n",
      "iteration: 94140 loss: 0.0034 lr: 0.02\n",
      "iteration: 94150 loss: 0.0026 lr: 0.02\n",
      "iteration: 94160 loss: 0.0025 lr: 0.02\n",
      "iteration: 94170 loss: 0.0024 lr: 0.02\n",
      "iteration: 94180 loss: 0.0035 lr: 0.02\n",
      "iteration: 94190 loss: 0.0029 lr: 0.02\n",
      "iteration: 94200 loss: 0.0031 lr: 0.02\n",
      "iteration: 94210 loss: 0.0029 lr: 0.02\n",
      "iteration: 94220 loss: 0.0035 lr: 0.02\n",
      "iteration: 94230 loss: 0.0026 lr: 0.02\n",
      "iteration: 94240 loss: 0.0033 lr: 0.02\n",
      "iteration: 94250 loss: 0.0032 lr: 0.02\n",
      "iteration: 94260 loss: 0.0029 lr: 0.02\n",
      "iteration: 94270 loss: 0.0031 lr: 0.02\n",
      "iteration: 94280 loss: 0.0031 lr: 0.02\n",
      "iteration: 94290 loss: 0.0049 lr: 0.02\n",
      "iteration: 94300 loss: 0.0025 lr: 0.02\n",
      "iteration: 94310 loss: 0.0031 lr: 0.02\n",
      "iteration: 94320 loss: 0.0027 lr: 0.02\n",
      "iteration: 94330 loss: 0.0034 lr: 0.02\n",
      "iteration: 94340 loss: 0.0035 lr: 0.02\n",
      "iteration: 94350 loss: 0.0025 lr: 0.02\n",
      "iteration: 94360 loss: 0.0031 lr: 0.02\n",
      "iteration: 94370 loss: 0.0031 lr: 0.02\n",
      "iteration: 94380 loss: 0.0028 lr: 0.02\n",
      "iteration: 94390 loss: 0.0031 lr: 0.02\n",
      "iteration: 94400 loss: 0.0030 lr: 0.02\n",
      "iteration: 94410 loss: 0.0021 lr: 0.02\n",
      "iteration: 94420 loss: 0.0037 lr: 0.02\n",
      "iteration: 94430 loss: 0.0026 lr: 0.02\n",
      "iteration: 94440 loss: 0.0028 lr: 0.02\n",
      "iteration: 94450 loss: 0.0039 lr: 0.02\n",
      "iteration: 94460 loss: 0.0028 lr: 0.02\n",
      "iteration: 94470 loss: 0.0040 lr: 0.02\n",
      "iteration: 94480 loss: 0.0030 lr: 0.02\n",
      "iteration: 94490 loss: 0.0038 lr: 0.02\n",
      "iteration: 94500 loss: 0.0025 lr: 0.02\n",
      "iteration: 94510 loss: 0.0024 lr: 0.02\n",
      "iteration: 94520 loss: 0.0032 lr: 0.02\n",
      "iteration: 94530 loss: 0.0040 lr: 0.02\n",
      "iteration: 94540 loss: 0.0032 lr: 0.02\n",
      "iteration: 94550 loss: 0.0039 lr: 0.02\n",
      "iteration: 94560 loss: 0.0039 lr: 0.02\n",
      "iteration: 94570 loss: 0.0026 lr: 0.02\n",
      "iteration: 94580 loss: 0.0021 lr: 0.02\n",
      "iteration: 94590 loss: 0.0030 lr: 0.02\n",
      "iteration: 94600 loss: 0.0032 lr: 0.02\n",
      "iteration: 94610 loss: 0.0031 lr: 0.02\n",
      "iteration: 94620 loss: 0.0033 lr: 0.02\n",
      "iteration: 94630 loss: 0.0024 lr: 0.02\n",
      "iteration: 94640 loss: 0.0032 lr: 0.02\n",
      "iteration: 94650 loss: 0.0031 lr: 0.02\n",
      "iteration: 94660 loss: 0.0032 lr: 0.02\n",
      "iteration: 94670 loss: 0.0024 lr: 0.02\n",
      "iteration: 94680 loss: 0.0026 lr: 0.02\n",
      "iteration: 94690 loss: 0.0036 lr: 0.02\n",
      "iteration: 94700 loss: 0.0019 lr: 0.02\n",
      "iteration: 94710 loss: 0.0024 lr: 0.02\n",
      "iteration: 94720 loss: 0.0028 lr: 0.02\n",
      "iteration: 94730 loss: 0.0029 lr: 0.02\n",
      "iteration: 94740 loss: 0.0034 lr: 0.02\n",
      "iteration: 94750 loss: 0.0035 lr: 0.02\n",
      "iteration: 94760 loss: 0.0042 lr: 0.02\n",
      "iteration: 94770 loss: 0.0023 lr: 0.02\n",
      "iteration: 94780 loss: 0.0030 lr: 0.02\n",
      "iteration: 94790 loss: 0.0038 lr: 0.02\n",
      "iteration: 94800 loss: 0.0027 lr: 0.02\n",
      "iteration: 94810 loss: 0.0020 lr: 0.02\n",
      "iteration: 94820 loss: 0.0035 lr: 0.02\n",
      "iteration: 94830 loss: 0.0027 lr: 0.02\n",
      "iteration: 94840 loss: 0.0023 lr: 0.02\n",
      "iteration: 94850 loss: 0.0026 lr: 0.02\n",
      "iteration: 94860 loss: 0.0023 lr: 0.02\n",
      "iteration: 94870 loss: 0.0036 lr: 0.02\n",
      "iteration: 94880 loss: 0.0030 lr: 0.02\n",
      "iteration: 94890 loss: 0.0026 lr: 0.02\n",
      "iteration: 94900 loss: 0.0026 lr: 0.02\n",
      "iteration: 94910 loss: 0.0029 lr: 0.02\n",
      "iteration: 94920 loss: 0.0033 lr: 0.02\n",
      "iteration: 94930 loss: 0.0027 lr: 0.02\n",
      "iteration: 94940 loss: 0.0030 lr: 0.02\n",
      "iteration: 94950 loss: 0.0029 lr: 0.02\n",
      "iteration: 94960 loss: 0.0033 lr: 0.02\n",
      "iteration: 94970 loss: 0.0031 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 94980 loss: 0.0036 lr: 0.02\n",
      "iteration: 94990 loss: 0.0049 lr: 0.02\n",
      "iteration: 95000 loss: 0.0026 lr: 0.02\n",
      "iteration: 95010 loss: 0.0037 lr: 0.02\n",
      "iteration: 95020 loss: 0.0036 lr: 0.02\n",
      "iteration: 95030 loss: 0.0020 lr: 0.02\n",
      "iteration: 95040 loss: 0.0026 lr: 0.02\n",
      "iteration: 95050 loss: 0.0026 lr: 0.02\n",
      "iteration: 95060 loss: 0.0031 lr: 0.02\n",
      "iteration: 95070 loss: 0.0028 lr: 0.02\n",
      "iteration: 95080 loss: 0.0033 lr: 0.02\n",
      "iteration: 95090 loss: 0.0026 lr: 0.02\n",
      "iteration: 95100 loss: 0.0033 lr: 0.02\n",
      "iteration: 95110 loss: 0.0026 lr: 0.02\n",
      "iteration: 95120 loss: 0.0030 lr: 0.02\n",
      "iteration: 95130 loss: 0.0024 lr: 0.02\n",
      "iteration: 95140 loss: 0.0026 lr: 0.02\n",
      "iteration: 95150 loss: 0.0028 lr: 0.02\n",
      "iteration: 95160 loss: 0.0027 lr: 0.02\n",
      "iteration: 95170 loss: 0.0021 lr: 0.02\n",
      "iteration: 95180 loss: 0.0028 lr: 0.02\n",
      "iteration: 95190 loss: 0.0027 lr: 0.02\n",
      "iteration: 95200 loss: 0.0023 lr: 0.02\n",
      "iteration: 95210 loss: 0.0035 lr: 0.02\n",
      "iteration: 95220 loss: 0.0025 lr: 0.02\n",
      "iteration: 95230 loss: 0.0032 lr: 0.02\n",
      "iteration: 95240 loss: 0.0030 lr: 0.02\n",
      "iteration: 95250 loss: 0.0028 lr: 0.02\n",
      "iteration: 95260 loss: 0.0023 lr: 0.02\n",
      "iteration: 95270 loss: 0.0025 lr: 0.02\n",
      "iteration: 95280 loss: 0.0026 lr: 0.02\n",
      "iteration: 95290 loss: 0.0044 lr: 0.02\n",
      "iteration: 95300 loss: 0.0031 lr: 0.02\n",
      "iteration: 95310 loss: 0.0039 lr: 0.02\n",
      "iteration: 95320 loss: 0.0033 lr: 0.02\n",
      "iteration: 95330 loss: 0.0032 lr: 0.02\n",
      "iteration: 95340 loss: 0.0028 lr: 0.02\n",
      "iteration: 95350 loss: 0.0032 lr: 0.02\n",
      "iteration: 95360 loss: 0.0028 lr: 0.02\n",
      "iteration: 95370 loss: 0.0037 lr: 0.02\n",
      "iteration: 95380 loss: 0.0038 lr: 0.02\n",
      "iteration: 95390 loss: 0.0021 lr: 0.02\n",
      "iteration: 95400 loss: 0.0030 lr: 0.02\n",
      "iteration: 95410 loss: 0.0028 lr: 0.02\n",
      "iteration: 95420 loss: 0.0037 lr: 0.02\n",
      "iteration: 95430 loss: 0.0025 lr: 0.02\n",
      "iteration: 95440 loss: 0.0025 lr: 0.02\n",
      "iteration: 95450 loss: 0.0036 lr: 0.02\n",
      "iteration: 95460 loss: 0.0028 lr: 0.02\n",
      "iteration: 95470 loss: 0.0034 lr: 0.02\n",
      "iteration: 95480 loss: 0.0031 lr: 0.02\n",
      "iteration: 95490 loss: 0.0031 lr: 0.02\n",
      "iteration: 95500 loss: 0.0032 lr: 0.02\n",
      "iteration: 95510 loss: 0.0027 lr: 0.02\n",
      "iteration: 95520 loss: 0.0029 lr: 0.02\n",
      "iteration: 95530 loss: 0.0029 lr: 0.02\n",
      "iteration: 95540 loss: 0.0026 lr: 0.02\n",
      "iteration: 95550 loss: 0.0033 lr: 0.02\n",
      "iteration: 95560 loss: 0.0035 lr: 0.02\n",
      "iteration: 95570 loss: 0.0034 lr: 0.02\n",
      "iteration: 95580 loss: 0.0042 lr: 0.02\n",
      "iteration: 95590 loss: 0.0036 lr: 0.02\n",
      "iteration: 95600 loss: 0.0030 lr: 0.02\n",
      "iteration: 95610 loss: 0.0028 lr: 0.02\n",
      "iteration: 95620 loss: 0.0033 lr: 0.02\n",
      "iteration: 95630 loss: 0.0042 lr: 0.02\n",
      "iteration: 95640 loss: 0.0029 lr: 0.02\n",
      "iteration: 95650 loss: 0.0050 lr: 0.02\n",
      "iteration: 95660 loss: 0.0040 lr: 0.02\n",
      "iteration: 95670 loss: 0.0038 lr: 0.02\n",
      "iteration: 95680 loss: 0.0040 lr: 0.02\n",
      "iteration: 95690 loss: 0.0037 lr: 0.02\n",
      "iteration: 95700 loss: 0.0029 lr: 0.02\n",
      "iteration: 95710 loss: 0.0033 lr: 0.02\n",
      "iteration: 95720 loss: 0.0026 lr: 0.02\n",
      "iteration: 95730 loss: 0.0026 lr: 0.02\n",
      "iteration: 95740 loss: 0.0028 lr: 0.02\n",
      "iteration: 95750 loss: 0.0038 lr: 0.02\n",
      "iteration: 95760 loss: 0.0028 lr: 0.02\n",
      "iteration: 95770 loss: 0.0029 lr: 0.02\n",
      "iteration: 95780 loss: 0.0025 lr: 0.02\n",
      "iteration: 95790 loss: 0.0028 lr: 0.02\n",
      "iteration: 95800 loss: 0.0025 lr: 0.02\n",
      "iteration: 95810 loss: 0.0042 lr: 0.02\n",
      "iteration: 95820 loss: 0.0022 lr: 0.02\n",
      "iteration: 95830 loss: 0.0023 lr: 0.02\n",
      "iteration: 95840 loss: 0.0027 lr: 0.02\n",
      "iteration: 95850 loss: 0.0037 lr: 0.02\n",
      "iteration: 95860 loss: 0.0031 lr: 0.02\n",
      "iteration: 95870 loss: 0.0028 lr: 0.02\n",
      "iteration: 95880 loss: 0.0034 lr: 0.02\n",
      "iteration: 95890 loss: 0.0042 lr: 0.02\n",
      "iteration: 95900 loss: 0.0026 lr: 0.02\n",
      "iteration: 95910 loss: 0.0035 lr: 0.02\n",
      "iteration: 95920 loss: 0.0028 lr: 0.02\n",
      "iteration: 95930 loss: 0.0026 lr: 0.02\n",
      "iteration: 95940 loss: 0.0032 lr: 0.02\n",
      "iteration: 95950 loss: 0.0025 lr: 0.02\n",
      "iteration: 95960 loss: 0.0035 lr: 0.02\n",
      "iteration: 95970 loss: 0.0031 lr: 0.02\n",
      "iteration: 95980 loss: 0.0034 lr: 0.02\n",
      "iteration: 95990 loss: 0.0036 lr: 0.02\n",
      "iteration: 96000 loss: 0.0032 lr: 0.02\n",
      "iteration: 96010 loss: 0.0034 lr: 0.02\n",
      "iteration: 96020 loss: 0.0023 lr: 0.02\n",
      "iteration: 96030 loss: 0.0028 lr: 0.02\n",
      "iteration: 96040 loss: 0.0034 lr: 0.02\n",
      "iteration: 96050 loss: 0.0035 lr: 0.02\n",
      "iteration: 96060 loss: 0.0042 lr: 0.02\n",
      "iteration: 96070 loss: 0.0021 lr: 0.02\n",
      "iteration: 96080 loss: 0.0029 lr: 0.02\n",
      "iteration: 96090 loss: 0.0033 lr: 0.02\n",
      "iteration: 96100 loss: 0.0035 lr: 0.02\n",
      "iteration: 96110 loss: 0.0030 lr: 0.02\n",
      "iteration: 96120 loss: 0.0032 lr: 0.02\n",
      "iteration: 96130 loss: 0.0041 lr: 0.02\n",
      "iteration: 96140 loss: 0.0028 lr: 0.02\n",
      "iteration: 96150 loss: 0.0037 lr: 0.02\n",
      "iteration: 96160 loss: 0.0026 lr: 0.02\n",
      "iteration: 96170 loss: 0.0026 lr: 0.02\n",
      "iteration: 96180 loss: 0.0032 lr: 0.02\n",
      "iteration: 96190 loss: 0.0027 lr: 0.02\n",
      "iteration: 96200 loss: 0.0030 lr: 0.02\n",
      "iteration: 96210 loss: 0.0028 lr: 0.02\n",
      "iteration: 96220 loss: 0.0037 lr: 0.02\n",
      "iteration: 96230 loss: 0.0036 lr: 0.02\n",
      "iteration: 96240 loss: 0.0044 lr: 0.02\n",
      "iteration: 96250 loss: 0.0030 lr: 0.02\n",
      "iteration: 96260 loss: 0.0025 lr: 0.02\n",
      "iteration: 96270 loss: 0.0037 lr: 0.02\n",
      "iteration: 96280 loss: 0.0037 lr: 0.02\n",
      "iteration: 96290 loss: 0.0028 lr: 0.02\n",
      "iteration: 96300 loss: 0.0031 lr: 0.02\n",
      "iteration: 96310 loss: 0.0023 lr: 0.02\n",
      "iteration: 96320 loss: 0.0025 lr: 0.02\n",
      "iteration: 96330 loss: 0.0021 lr: 0.02\n",
      "iteration: 96340 loss: 0.0028 lr: 0.02\n",
      "iteration: 96350 loss: 0.0035 lr: 0.02\n",
      "iteration: 96360 loss: 0.0025 lr: 0.02\n",
      "iteration: 96370 loss: 0.0031 lr: 0.02\n",
      "iteration: 96380 loss: 0.0023 lr: 0.02\n",
      "iteration: 96390 loss: 0.0038 lr: 0.02\n",
      "iteration: 96400 loss: 0.0033 lr: 0.02\n",
      "iteration: 96410 loss: 0.0033 lr: 0.02\n",
      "iteration: 96420 loss: 0.0026 lr: 0.02\n",
      "iteration: 96430 loss: 0.0038 lr: 0.02\n",
      "iteration: 96440 loss: 0.0033 lr: 0.02\n",
      "iteration: 96450 loss: 0.0028 lr: 0.02\n",
      "iteration: 96460 loss: 0.0022 lr: 0.02\n",
      "iteration: 96470 loss: 0.0024 lr: 0.02\n",
      "iteration: 96480 loss: 0.0026 lr: 0.02\n",
      "iteration: 96490 loss: 0.0034 lr: 0.02\n",
      "iteration: 96500 loss: 0.0027 lr: 0.02\n",
      "iteration: 96510 loss: 0.0032 lr: 0.02\n",
      "iteration: 96520 loss: 0.0029 lr: 0.02\n",
      "iteration: 96530 loss: 0.0028 lr: 0.02\n",
      "iteration: 96540 loss: 0.0034 lr: 0.02\n",
      "iteration: 96550 loss: 0.0020 lr: 0.02\n",
      "iteration: 96560 loss: 0.0036 lr: 0.02\n",
      "iteration: 96570 loss: 0.0038 lr: 0.02\n",
      "iteration: 96580 loss: 0.0029 lr: 0.02\n",
      "iteration: 96590 loss: 0.0023 lr: 0.02\n",
      "iteration: 96600 loss: 0.0042 lr: 0.02\n",
      "iteration: 96610 loss: 0.0021 lr: 0.02\n",
      "iteration: 96620 loss: 0.0030 lr: 0.02\n",
      "iteration: 96630 loss: 0.0032 lr: 0.02\n",
      "iteration: 96640 loss: 0.0028 lr: 0.02\n",
      "iteration: 96650 loss: 0.0038 lr: 0.02\n",
      "iteration: 96660 loss: 0.0030 lr: 0.02\n",
      "iteration: 96670 loss: 0.0024 lr: 0.02\n",
      "iteration: 96680 loss: 0.0023 lr: 0.02\n",
      "iteration: 96690 loss: 0.0030 lr: 0.02\n",
      "iteration: 96700 loss: 0.0032 lr: 0.02\n",
      "iteration: 96710 loss: 0.0032 lr: 0.02\n",
      "iteration: 96720 loss: 0.0046 lr: 0.02\n",
      "iteration: 96730 loss: 0.0033 lr: 0.02\n",
      "iteration: 96740 loss: 0.0031 lr: 0.02\n",
      "iteration: 96750 loss: 0.0042 lr: 0.02\n",
      "iteration: 96760 loss: 0.0024 lr: 0.02\n",
      "iteration: 96770 loss: 0.0021 lr: 0.02\n",
      "iteration: 96780 loss: 0.0045 lr: 0.02\n",
      "iteration: 96790 loss: 0.0025 lr: 0.02\n",
      "iteration: 96800 loss: 0.0024 lr: 0.02\n",
      "iteration: 96810 loss: 0.0025 lr: 0.02\n",
      "iteration: 96820 loss: 0.0030 lr: 0.02\n",
      "iteration: 96830 loss: 0.0028 lr: 0.02\n",
      "iteration: 96840 loss: 0.0028 lr: 0.02\n",
      "iteration: 96850 loss: 0.0033 lr: 0.02\n",
      "iteration: 96860 loss: 0.0027 lr: 0.02\n",
      "iteration: 96870 loss: 0.0023 lr: 0.02\n",
      "iteration: 96880 loss: 0.0023 lr: 0.02\n",
      "iteration: 96890 loss: 0.0022 lr: 0.02\n",
      "iteration: 96900 loss: 0.0027 lr: 0.02\n",
      "iteration: 96910 loss: 0.0027 lr: 0.02\n",
      "iteration: 96920 loss: 0.0028 lr: 0.02\n",
      "iteration: 96930 loss: 0.0054 lr: 0.02\n",
      "iteration: 96940 loss: 0.0040 lr: 0.02\n",
      "iteration: 96950 loss: 0.0030 lr: 0.02\n",
      "iteration: 96960 loss: 0.0027 lr: 0.02\n",
      "iteration: 96970 loss: 0.0033 lr: 0.02\n",
      "iteration: 96980 loss: 0.0023 lr: 0.02\n",
      "iteration: 96990 loss: 0.0023 lr: 0.02\n",
      "iteration: 97000 loss: 0.0025 lr: 0.02\n",
      "iteration: 97010 loss: 0.0032 lr: 0.02\n",
      "iteration: 97020 loss: 0.0027 lr: 0.02\n",
      "iteration: 97030 loss: 0.0028 lr: 0.02\n",
      "iteration: 97040 loss: 0.0024 lr: 0.02\n",
      "iteration: 97050 loss: 0.0024 lr: 0.02\n",
      "iteration: 97060 loss: 0.0034 lr: 0.02\n",
      "iteration: 97070 loss: 0.0029 lr: 0.02\n",
      "iteration: 97080 loss: 0.0028 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 97090 loss: 0.0034 lr: 0.02\n",
      "iteration: 97100 loss: 0.0032 lr: 0.02\n",
      "iteration: 97110 loss: 0.0032 lr: 0.02\n",
      "iteration: 97120 loss: 0.0021 lr: 0.02\n",
      "iteration: 97130 loss: 0.0036 lr: 0.02\n",
      "iteration: 97140 loss: 0.0022 lr: 0.02\n",
      "iteration: 97150 loss: 0.0023 lr: 0.02\n",
      "iteration: 97160 loss: 0.0027 lr: 0.02\n",
      "iteration: 97170 loss: 0.0028 lr: 0.02\n",
      "iteration: 97180 loss: 0.0017 lr: 0.02\n",
      "iteration: 97190 loss: 0.0028 lr: 0.02\n",
      "iteration: 97200 loss: 0.0023 lr: 0.02\n",
      "iteration: 97210 loss: 0.0031 lr: 0.02\n",
      "iteration: 97220 loss: 0.0029 lr: 0.02\n",
      "iteration: 97230 loss: 0.0031 lr: 0.02\n",
      "iteration: 97240 loss: 0.0029 lr: 0.02\n",
      "iteration: 97250 loss: 0.0025 lr: 0.02\n",
      "iteration: 97260 loss: 0.0025 lr: 0.02\n",
      "iteration: 97270 loss: 0.0022 lr: 0.02\n",
      "iteration: 97280 loss: 0.0030 lr: 0.02\n",
      "iteration: 97290 loss: 0.0030 lr: 0.02\n",
      "iteration: 97300 loss: 0.0031 lr: 0.02\n",
      "iteration: 97310 loss: 0.0027 lr: 0.02\n",
      "iteration: 97320 loss: 0.0027 lr: 0.02\n",
      "iteration: 97330 loss: 0.0030 lr: 0.02\n",
      "iteration: 97340 loss: 0.0049 lr: 0.02\n",
      "iteration: 97350 loss: 0.0035 lr: 0.02\n",
      "iteration: 97360 loss: 0.0035 lr: 0.02\n",
      "iteration: 97370 loss: 0.0027 lr: 0.02\n",
      "iteration: 97380 loss: 0.0023 lr: 0.02\n",
      "iteration: 97390 loss: 0.0028 lr: 0.02\n",
      "iteration: 97400 loss: 0.0027 lr: 0.02\n",
      "iteration: 97410 loss: 0.0029 lr: 0.02\n",
      "iteration: 97420 loss: 0.0027 lr: 0.02\n",
      "iteration: 97430 loss: 0.0023 lr: 0.02\n",
      "iteration: 97440 loss: 0.0031 lr: 0.02\n",
      "iteration: 97450 loss: 0.0034 lr: 0.02\n",
      "iteration: 97460 loss: 0.0024 lr: 0.02\n",
      "iteration: 97470 loss: 0.0028 lr: 0.02\n",
      "iteration: 97480 loss: 0.0028 lr: 0.02\n",
      "iteration: 97490 loss: 0.0028 lr: 0.02\n",
      "iteration: 97500 loss: 0.0025 lr: 0.02\n",
      "iteration: 97510 loss: 0.0029 lr: 0.02\n",
      "iteration: 97520 loss: 0.0025 lr: 0.02\n",
      "iteration: 97530 loss: 0.0036 lr: 0.02\n",
      "iteration: 97540 loss: 0.0029 lr: 0.02\n",
      "iteration: 97550 loss: 0.0021 lr: 0.02\n",
      "iteration: 97560 loss: 0.0025 lr: 0.02\n",
      "iteration: 97570 loss: 0.0030 lr: 0.02\n",
      "iteration: 97580 loss: 0.0030 lr: 0.02\n",
      "iteration: 97590 loss: 0.0028 lr: 0.02\n",
      "iteration: 97600 loss: 0.0019 lr: 0.02\n",
      "iteration: 97610 loss: 0.0025 lr: 0.02\n",
      "iteration: 97620 loss: 0.0027 lr: 0.02\n",
      "iteration: 97630 loss: 0.0025 lr: 0.02\n",
      "iteration: 97640 loss: 0.0031 lr: 0.02\n",
      "iteration: 97650 loss: 0.0022 lr: 0.02\n",
      "iteration: 97660 loss: 0.0031 lr: 0.02\n",
      "iteration: 97670 loss: 0.0027 lr: 0.02\n",
      "iteration: 97680 loss: 0.0035 lr: 0.02\n",
      "iteration: 97690 loss: 0.0033 lr: 0.02\n",
      "iteration: 97700 loss: 0.0028 lr: 0.02\n",
      "iteration: 97710 loss: 0.0025 lr: 0.02\n",
      "iteration: 97720 loss: 0.0040 lr: 0.02\n",
      "iteration: 97730 loss: 0.0037 lr: 0.02\n",
      "iteration: 97740 loss: 0.0029 lr: 0.02\n",
      "iteration: 97750 loss: 0.0030 lr: 0.02\n",
      "iteration: 97760 loss: 0.0027 lr: 0.02\n",
      "iteration: 97770 loss: 0.0028 lr: 0.02\n",
      "iteration: 97780 loss: 0.0026 lr: 0.02\n",
      "iteration: 97790 loss: 0.0022 lr: 0.02\n",
      "iteration: 97800 loss: 0.0028 lr: 0.02\n",
      "iteration: 97810 loss: 0.0032 lr: 0.02\n",
      "iteration: 97820 loss: 0.0030 lr: 0.02\n",
      "iteration: 97830 loss: 0.0032 lr: 0.02\n",
      "iteration: 97840 loss: 0.0033 lr: 0.02\n",
      "iteration: 97850 loss: 0.0029 lr: 0.02\n",
      "iteration: 97860 loss: 0.0032 lr: 0.02\n",
      "iteration: 97870 loss: 0.0022 lr: 0.02\n",
      "iteration: 97880 loss: 0.0036 lr: 0.02\n",
      "iteration: 97890 loss: 0.0037 lr: 0.02\n",
      "iteration: 97900 loss: 0.0027 lr: 0.02\n",
      "iteration: 97910 loss: 0.0038 lr: 0.02\n",
      "iteration: 97920 loss: 0.0030 lr: 0.02\n",
      "iteration: 97930 loss: 0.0038 lr: 0.02\n",
      "iteration: 97940 loss: 0.0041 lr: 0.02\n",
      "iteration: 97950 loss: 0.0028 lr: 0.02\n",
      "iteration: 97960 loss: 0.0028 lr: 0.02\n",
      "iteration: 97970 loss: 0.0034 lr: 0.02\n",
      "iteration: 97980 loss: 0.0026 lr: 0.02\n",
      "iteration: 97990 loss: 0.0028 lr: 0.02\n",
      "iteration: 98000 loss: 0.0034 lr: 0.02\n",
      "iteration: 98010 loss: 0.0032 lr: 0.02\n",
      "iteration: 98020 loss: 0.0024 lr: 0.02\n",
      "iteration: 98030 loss: 0.0031 lr: 0.02\n",
      "iteration: 98040 loss: 0.0034 lr: 0.02\n",
      "iteration: 98050 loss: 0.0033 lr: 0.02\n",
      "iteration: 98060 loss: 0.0031 lr: 0.02\n",
      "iteration: 98070 loss: 0.0029 lr: 0.02\n",
      "iteration: 98080 loss: 0.0039 lr: 0.02\n",
      "iteration: 98090 loss: 0.0023 lr: 0.02\n",
      "iteration: 98100 loss: 0.0028 lr: 0.02\n",
      "iteration: 98110 loss: 0.0026 lr: 0.02\n",
      "iteration: 98120 loss: 0.0031 lr: 0.02\n",
      "iteration: 98130 loss: 0.0028 lr: 0.02\n",
      "iteration: 98140 loss: 0.0026 lr: 0.02\n",
      "iteration: 98150 loss: 0.0042 lr: 0.02\n",
      "iteration: 98160 loss: 0.0025 lr: 0.02\n",
      "iteration: 98170 loss: 0.0024 lr: 0.02\n",
      "iteration: 98180 loss: 0.0031 lr: 0.02\n",
      "iteration: 98190 loss: 0.0033 lr: 0.02\n",
      "iteration: 98200 loss: 0.0025 lr: 0.02\n",
      "iteration: 98210 loss: 0.0035 lr: 0.02\n",
      "iteration: 98220 loss: 0.0029 lr: 0.02\n",
      "iteration: 98230 loss: 0.0025 lr: 0.02\n",
      "iteration: 98240 loss: 0.0027 lr: 0.02\n",
      "iteration: 98250 loss: 0.0031 lr: 0.02\n",
      "iteration: 98260 loss: 0.0028 lr: 0.02\n",
      "iteration: 98270 loss: 0.0018 lr: 0.02\n",
      "iteration: 98280 loss: 0.0029 lr: 0.02\n",
      "iteration: 98290 loss: 0.0032 lr: 0.02\n",
      "iteration: 98300 loss: 0.0043 lr: 0.02\n",
      "iteration: 98310 loss: 0.0027 lr: 0.02\n",
      "iteration: 98320 loss: 0.0022 lr: 0.02\n",
      "iteration: 98330 loss: 0.0026 lr: 0.02\n",
      "iteration: 98340 loss: 0.0032 lr: 0.02\n",
      "iteration: 98350 loss: 0.0026 lr: 0.02\n",
      "iteration: 98360 loss: 0.0020 lr: 0.02\n",
      "iteration: 98370 loss: 0.0023 lr: 0.02\n",
      "iteration: 98380 loss: 0.0030 lr: 0.02\n",
      "iteration: 98390 loss: 0.0029 lr: 0.02\n",
      "iteration: 98400 loss: 0.0026 lr: 0.02\n",
      "iteration: 98410 loss: 0.0023 lr: 0.02\n",
      "iteration: 98420 loss: 0.0024 lr: 0.02\n",
      "iteration: 98430 loss: 0.0023 lr: 0.02\n",
      "iteration: 98440 loss: 0.0026 lr: 0.02\n",
      "iteration: 98450 loss: 0.0022 lr: 0.02\n",
      "iteration: 98460 loss: 0.0021 lr: 0.02\n",
      "iteration: 98470 loss: 0.0034 lr: 0.02\n",
      "iteration: 98480 loss: 0.0030 lr: 0.02\n",
      "iteration: 98490 loss: 0.0032 lr: 0.02\n",
      "iteration: 98500 loss: 0.0031 lr: 0.02\n",
      "iteration: 98510 loss: 0.0024 lr: 0.02\n",
      "iteration: 98520 loss: 0.0037 lr: 0.02\n",
      "iteration: 98530 loss: 0.0034 lr: 0.02\n",
      "iteration: 98540 loss: 0.0030 lr: 0.02\n",
      "iteration: 98550 loss: 0.0022 lr: 0.02\n",
      "iteration: 98560 loss: 0.0026 lr: 0.02\n",
      "iteration: 98570 loss: 0.0034 lr: 0.02\n",
      "iteration: 98580 loss: 0.0032 lr: 0.02\n",
      "iteration: 98590 loss: 0.0029 lr: 0.02\n",
      "iteration: 98600 loss: 0.0039 lr: 0.02\n",
      "iteration: 98610 loss: 0.0024 lr: 0.02\n",
      "iteration: 98620 loss: 0.0046 lr: 0.02\n",
      "iteration: 98630 loss: 0.0031 lr: 0.02\n",
      "iteration: 98640 loss: 0.0041 lr: 0.02\n",
      "iteration: 98650 loss: 0.0029 lr: 0.02\n",
      "iteration: 98660 loss: 0.0025 lr: 0.02\n",
      "iteration: 98670 loss: 0.0028 lr: 0.02\n",
      "iteration: 98680 loss: 0.0026 lr: 0.02\n",
      "iteration: 98690 loss: 0.0031 lr: 0.02\n",
      "iteration: 98700 loss: 0.0029 lr: 0.02\n",
      "iteration: 98710 loss: 0.0030 lr: 0.02\n",
      "iteration: 98720 loss: 0.0028 lr: 0.02\n",
      "iteration: 98730 loss: 0.0033 lr: 0.02\n",
      "iteration: 98740 loss: 0.0027 lr: 0.02\n",
      "iteration: 98750 loss: 0.0029 lr: 0.02\n",
      "iteration: 98760 loss: 0.0024 lr: 0.02\n",
      "iteration: 98770 loss: 0.0034 lr: 0.02\n",
      "iteration: 98780 loss: 0.0034 lr: 0.02\n",
      "iteration: 98790 loss: 0.0032 lr: 0.02\n",
      "iteration: 98800 loss: 0.0038 lr: 0.02\n",
      "iteration: 98810 loss: 0.0039 lr: 0.02\n",
      "iteration: 98820 loss: 0.0029 lr: 0.02\n",
      "iteration: 98830 loss: 0.0029 lr: 0.02\n",
      "iteration: 98840 loss: 0.0023 lr: 0.02\n",
      "iteration: 98850 loss: 0.0026 lr: 0.02\n",
      "iteration: 98860 loss: 0.0033 lr: 0.02\n",
      "iteration: 98870 loss: 0.0027 lr: 0.02\n",
      "iteration: 98880 loss: 0.0022 lr: 0.02\n",
      "iteration: 98890 loss: 0.0022 lr: 0.02\n",
      "iteration: 98900 loss: 0.0023 lr: 0.02\n",
      "iteration: 98910 loss: 0.0027 lr: 0.02\n",
      "iteration: 98920 loss: 0.0026 lr: 0.02\n",
      "iteration: 98930 loss: 0.0020 lr: 0.02\n",
      "iteration: 98940 loss: 0.0024 lr: 0.02\n",
      "iteration: 98950 loss: 0.0025 lr: 0.02\n",
      "iteration: 98960 loss: 0.0027 lr: 0.02\n",
      "iteration: 98970 loss: 0.0024 lr: 0.02\n",
      "iteration: 98980 loss: 0.0021 lr: 0.02\n",
      "iteration: 98990 loss: 0.0038 lr: 0.02\n",
      "iteration: 99000 loss: 0.0026 lr: 0.02\n",
      "iteration: 99010 loss: 0.0038 lr: 0.02\n",
      "iteration: 99020 loss: 0.0025 lr: 0.02\n",
      "iteration: 99030 loss: 0.0041 lr: 0.02\n",
      "iteration: 99040 loss: 0.0027 lr: 0.02\n",
      "iteration: 99050 loss: 0.0035 lr: 0.02\n",
      "iteration: 99060 loss: 0.0026 lr: 0.02\n",
      "iteration: 99070 loss: 0.0035 lr: 0.02\n",
      "iteration: 99080 loss: 0.0028 lr: 0.02\n",
      "iteration: 99090 loss: 0.0030 lr: 0.02\n",
      "iteration: 99100 loss: 0.0030 lr: 0.02\n",
      "iteration: 99110 loss: 0.0030 lr: 0.02\n",
      "iteration: 99120 loss: 0.0024 lr: 0.02\n",
      "iteration: 99130 loss: 0.0036 lr: 0.02\n",
      "iteration: 99140 loss: 0.0024 lr: 0.02\n",
      "iteration: 99150 loss: 0.0022 lr: 0.02\n",
      "iteration: 99160 loss: 0.0028 lr: 0.02\n",
      "iteration: 99170 loss: 0.0028 lr: 0.02\n",
      "iteration: 99180 loss: 0.0028 lr: 0.02\n",
      "iteration: 99190 loss: 0.0026 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 99200 loss: 0.0037 lr: 0.02\n",
      "iteration: 99210 loss: 0.0028 lr: 0.02\n",
      "iteration: 99220 loss: 0.0025 lr: 0.02\n",
      "iteration: 99230 loss: 0.0024 lr: 0.02\n",
      "iteration: 99240 loss: 0.0028 lr: 0.02\n",
      "iteration: 99250 loss: 0.0029 lr: 0.02\n",
      "iteration: 99260 loss: 0.0026 lr: 0.02\n",
      "iteration: 99270 loss: 0.0032 lr: 0.02\n",
      "iteration: 99280 loss: 0.0030 lr: 0.02\n",
      "iteration: 99290 loss: 0.0027 lr: 0.02\n",
      "iteration: 99300 loss: 0.0027 lr: 0.02\n",
      "iteration: 99310 loss: 0.0030 lr: 0.02\n",
      "iteration: 99320 loss: 0.0039 lr: 0.02\n",
      "iteration: 99330 loss: 0.0035 lr: 0.02\n",
      "iteration: 99340 loss: 0.0031 lr: 0.02\n",
      "iteration: 99350 loss: 0.0037 lr: 0.02\n",
      "iteration: 99360 loss: 0.0041 lr: 0.02\n",
      "iteration: 99370 loss: 0.0032 lr: 0.02\n",
      "iteration: 99380 loss: 0.0028 lr: 0.02\n",
      "iteration: 99390 loss: 0.0030 lr: 0.02\n",
      "iteration: 99400 loss: 0.0034 lr: 0.02\n",
      "iteration: 99410 loss: 0.0030 lr: 0.02\n",
      "iteration: 99420 loss: 0.0026 lr: 0.02\n",
      "iteration: 99430 loss: 0.0048 lr: 0.02\n",
      "iteration: 99440 loss: 0.0027 lr: 0.02\n",
      "iteration: 99450 loss: 0.0031 lr: 0.02\n",
      "iteration: 99460 loss: 0.0028 lr: 0.02\n",
      "iteration: 99470 loss: 0.0023 lr: 0.02\n",
      "iteration: 99480 loss: 0.0024 lr: 0.02\n",
      "iteration: 99490 loss: 0.0028 lr: 0.02\n",
      "iteration: 99500 loss: 0.0024 lr: 0.02\n",
      "iteration: 99510 loss: 0.0030 lr: 0.02\n",
      "iteration: 99520 loss: 0.0032 lr: 0.02\n",
      "iteration: 99530 loss: 0.0033 lr: 0.02\n",
      "iteration: 99540 loss: 0.0025 lr: 0.02\n",
      "iteration: 99550 loss: 0.0028 lr: 0.02\n",
      "iteration: 99560 loss: 0.0027 lr: 0.02\n",
      "iteration: 99570 loss: 0.0024 lr: 0.02\n",
      "iteration: 99580 loss: 0.0033 lr: 0.02\n",
      "iteration: 99590 loss: 0.0031 lr: 0.02\n",
      "iteration: 99600 loss: 0.0033 lr: 0.02\n",
      "iteration: 99610 loss: 0.0028 lr: 0.02\n",
      "iteration: 99620 loss: 0.0033 lr: 0.02\n",
      "iteration: 99630 loss: 0.0036 lr: 0.02\n",
      "iteration: 99640 loss: 0.0031 lr: 0.02\n",
      "iteration: 99650 loss: 0.0033 lr: 0.02\n",
      "iteration: 99660 loss: 0.0044 lr: 0.02\n",
      "iteration: 99670 loss: 0.0030 lr: 0.02\n",
      "iteration: 99680 loss: 0.0033 lr: 0.02\n",
      "iteration: 99690 loss: 0.0029 lr: 0.02\n",
      "iteration: 99700 loss: 0.0028 lr: 0.02\n",
      "iteration: 99710 loss: 0.0034 lr: 0.02\n",
      "iteration: 99720 loss: 0.0039 lr: 0.02\n",
      "iteration: 99730 loss: 0.0032 lr: 0.02\n",
      "iteration: 99740 loss: 0.0037 lr: 0.02\n",
      "iteration: 99750 loss: 0.0050 lr: 0.02\n",
      "iteration: 99760 loss: 0.0034 lr: 0.02\n",
      "iteration: 99770 loss: 0.0034 lr: 0.02\n",
      "iteration: 99780 loss: 0.0038 lr: 0.02\n",
      "iteration: 99790 loss: 0.0031 lr: 0.02\n",
      "iteration: 99800 loss: 0.0030 lr: 0.02\n",
      "iteration: 99810 loss: 0.0036 lr: 0.02\n",
      "iteration: 99820 loss: 0.0035 lr: 0.02\n",
      "iteration: 99830 loss: 0.0037 lr: 0.02\n",
      "iteration: 99840 loss: 0.0026 lr: 0.02\n",
      "iteration: 99850 loss: 0.0032 lr: 0.02\n",
      "iteration: 99860 loss: 0.0044 lr: 0.02\n",
      "iteration: 99870 loss: 0.0046 lr: 0.02\n",
      "iteration: 99880 loss: 0.0032 lr: 0.02\n",
      "iteration: 99890 loss: 0.0036 lr: 0.02\n",
      "iteration: 99900 loss: 0.0027 lr: 0.02\n",
      "iteration: 99910 loss: 0.0027 lr: 0.02\n",
      "iteration: 99920 loss: 0.0031 lr: 0.02\n",
      "iteration: 99930 loss: 0.0038 lr: 0.02\n",
      "iteration: 99940 loss: 0.0026 lr: 0.02\n",
      "iteration: 99950 loss: 0.0034 lr: 0.02\n",
      "iteration: 99960 loss: 0.0026 lr: 0.02\n",
      "iteration: 99970 loss: 0.0026 lr: 0.02\n",
      "iteration: 99980 loss: 0.0025 lr: 0.02\n",
      "iteration: 99990 loss: 0.0030 lr: 0.02\n",
      "iteration: 100000 loss: 0.0030 lr: 0.02\n",
      "iteration: 100010 loss: 0.0024 lr: 0.02\n",
      "iteration: 100020 loss: 0.0025 lr: 0.02\n",
      "iteration: 100030 loss: 0.0026 lr: 0.02\n",
      "iteration: 100040 loss: 0.0029 lr: 0.02\n",
      "iteration: 100050 loss: 0.0027 lr: 0.02\n",
      "iteration: 100060 loss: 0.0028 lr: 0.02\n",
      "iteration: 100070 loss: 0.0027 lr: 0.02\n",
      "iteration: 100080 loss: 0.0030 lr: 0.02\n",
      "iteration: 100090 loss: 0.0033 lr: 0.02\n",
      "iteration: 100100 loss: 0.0041 lr: 0.02\n",
      "iteration: 100110 loss: 0.0026 lr: 0.02\n",
      "iteration: 100120 loss: 0.0030 lr: 0.02\n",
      "iteration: 100130 loss: 0.0033 lr: 0.02\n",
      "iteration: 100140 loss: 0.0029 lr: 0.02\n",
      "iteration: 100150 loss: 0.0029 lr: 0.02\n",
      "iteration: 100160 loss: 0.0037 lr: 0.02\n",
      "iteration: 100170 loss: 0.0036 lr: 0.02\n",
      "iteration: 100180 loss: 0.0029 lr: 0.02\n",
      "iteration: 100190 loss: 0.0028 lr: 0.02\n",
      "iteration: 100200 loss: 0.0028 lr: 0.02\n",
      "iteration: 100210 loss: 0.0031 lr: 0.02\n",
      "iteration: 100220 loss: 0.0026 lr: 0.02\n",
      "iteration: 100230 loss: 0.0038 lr: 0.02\n",
      "iteration: 100240 loss: 0.0030 lr: 0.02\n",
      "iteration: 100250 loss: 0.0029 lr: 0.02\n",
      "iteration: 100260 loss: 0.0038 lr: 0.02\n",
      "iteration: 100270 loss: 0.0026 lr: 0.02\n",
      "iteration: 100280 loss: 0.0030 lr: 0.02\n",
      "iteration: 100290 loss: 0.0026 lr: 0.02\n",
      "iteration: 100300 loss: 0.0031 lr: 0.02\n",
      "iteration: 100310 loss: 0.0028 lr: 0.02\n",
      "iteration: 100320 loss: 0.0022 lr: 0.02\n",
      "iteration: 100330 loss: 0.0032 lr: 0.02\n",
      "iteration: 100340 loss: 0.0041 lr: 0.02\n",
      "iteration: 100350 loss: 0.0025 lr: 0.02\n",
      "iteration: 100360 loss: 0.0028 lr: 0.02\n",
      "iteration: 100370 loss: 0.0024 lr: 0.02\n",
      "iteration: 100380 loss: 0.0030 lr: 0.02\n",
      "iteration: 100390 loss: 0.0029 lr: 0.02\n",
      "iteration: 100400 loss: 0.0032 lr: 0.02\n",
      "iteration: 100410 loss: 0.0036 lr: 0.02\n",
      "iteration: 100420 loss: 0.0034 lr: 0.02\n",
      "iteration: 100430 loss: 0.0023 lr: 0.02\n",
      "iteration: 100440 loss: 0.0039 lr: 0.02\n",
      "iteration: 100450 loss: 0.0030 lr: 0.02\n",
      "iteration: 100460 loss: 0.0030 lr: 0.02\n",
      "iteration: 100470 loss: 0.0030 lr: 0.02\n",
      "iteration: 100480 loss: 0.0042 lr: 0.02\n",
      "iteration: 100490 loss: 0.0025 lr: 0.02\n",
      "iteration: 100500 loss: 0.0036 lr: 0.02\n",
      "iteration: 100510 loss: 0.0049 lr: 0.02\n",
      "iteration: 100520 loss: 0.0026 lr: 0.02\n",
      "iteration: 100530 loss: 0.0032 lr: 0.02\n",
      "iteration: 100540 loss: 0.0038 lr: 0.02\n",
      "iteration: 100550 loss: 0.0033 lr: 0.02\n",
      "iteration: 100560 loss: 0.0023 lr: 0.02\n",
      "iteration: 100570 loss: 0.0027 lr: 0.02\n",
      "iteration: 100580 loss: 0.0025 lr: 0.02\n",
      "iteration: 100590 loss: 0.0034 lr: 0.02\n",
      "iteration: 100600 loss: 0.0023 lr: 0.02\n",
      "iteration: 100610 loss: 0.0036 lr: 0.02\n",
      "iteration: 100620 loss: 0.0025 lr: 0.02\n",
      "iteration: 100630 loss: 0.0029 lr: 0.02\n",
      "iteration: 100640 loss: 0.0021 lr: 0.02\n",
      "iteration: 100650 loss: 0.0034 lr: 0.02\n",
      "iteration: 100660 loss: 0.0033 lr: 0.02\n",
      "iteration: 100670 loss: 0.0034 lr: 0.02\n",
      "iteration: 100680 loss: 0.0031 lr: 0.02\n",
      "iteration: 100690 loss: 0.0021 lr: 0.02\n",
      "iteration: 100700 loss: 0.0021 lr: 0.02\n",
      "iteration: 100710 loss: 0.0028 lr: 0.02\n",
      "iteration: 100720 loss: 0.0025 lr: 0.02\n",
      "iteration: 100730 loss: 0.0024 lr: 0.02\n",
      "iteration: 100740 loss: 0.0033 lr: 0.02\n",
      "iteration: 100750 loss: 0.0033 lr: 0.02\n",
      "iteration: 100760 loss: 0.0029 lr: 0.02\n",
      "iteration: 100770 loss: 0.0033 lr: 0.02\n",
      "iteration: 100780 loss: 0.0034 lr: 0.02\n",
      "iteration: 100790 loss: 0.0026 lr: 0.02\n",
      "iteration: 100800 loss: 0.0034 lr: 0.02\n",
      "iteration: 100810 loss: 0.0033 lr: 0.02\n",
      "iteration: 100820 loss: 0.0034 lr: 0.02\n",
      "iteration: 100830 loss: 0.0035 lr: 0.02\n",
      "iteration: 100840 loss: 0.0037 lr: 0.02\n",
      "iteration: 100850 loss: 0.0029 lr: 0.02\n",
      "iteration: 100860 loss: 0.0033 lr: 0.02\n",
      "iteration: 100870 loss: 0.0032 lr: 0.02\n",
      "iteration: 100880 loss: 0.0042 lr: 0.02\n",
      "iteration: 100890 loss: 0.0031 lr: 0.02\n",
      "iteration: 100900 loss: 0.0031 lr: 0.02\n",
      "iteration: 100910 loss: 0.0022 lr: 0.02\n",
      "iteration: 100920 loss: 0.0031 lr: 0.02\n",
      "iteration: 100930 loss: 0.0025 lr: 0.02\n",
      "iteration: 100940 loss: 0.0031 lr: 0.02\n",
      "iteration: 100950 loss: 0.0033 lr: 0.02\n",
      "iteration: 100960 loss: 0.0034 lr: 0.02\n",
      "iteration: 100970 loss: 0.0023 lr: 0.02\n",
      "iteration: 100980 loss: 0.0017 lr: 0.02\n",
      "iteration: 100990 loss: 0.0030 lr: 0.02\n",
      "iteration: 101000 loss: 0.0032 lr: 0.02\n",
      "iteration: 101010 loss: 0.0030 lr: 0.02\n",
      "iteration: 101020 loss: 0.0039 lr: 0.02\n",
      "iteration: 101030 loss: 0.0027 lr: 0.02\n",
      "iteration: 101040 loss: 0.0020 lr: 0.02\n",
      "iteration: 101050 loss: 0.0025 lr: 0.02\n",
      "iteration: 101060 loss: 0.0023 lr: 0.02\n",
      "iteration: 101070 loss: 0.0026 lr: 0.02\n",
      "iteration: 101080 loss: 0.0027 lr: 0.02\n",
      "iteration: 101090 loss: 0.0033 lr: 0.02\n",
      "iteration: 101100 loss: 0.0022 lr: 0.02\n",
      "iteration: 101110 loss: 0.0034 lr: 0.02\n",
      "iteration: 101120 loss: 0.0022 lr: 0.02\n",
      "iteration: 101130 loss: 0.0026 lr: 0.02\n",
      "iteration: 101140 loss: 0.0031 lr: 0.02\n",
      "iteration: 101150 loss: 0.0025 lr: 0.02\n",
      "iteration: 101160 loss: 0.0028 lr: 0.02\n",
      "iteration: 101170 loss: 0.0022 lr: 0.02\n",
      "iteration: 101180 loss: 0.0031 lr: 0.02\n",
      "iteration: 101190 loss: 0.0028 lr: 0.02\n",
      "iteration: 101200 loss: 0.0036 lr: 0.02\n",
      "iteration: 101210 loss: 0.0028 lr: 0.02\n",
      "iteration: 101220 loss: 0.0038 lr: 0.02\n",
      "iteration: 101230 loss: 0.0034 lr: 0.02\n",
      "iteration: 101240 loss: 0.0031 lr: 0.02\n",
      "iteration: 101250 loss: 0.0032 lr: 0.02\n",
      "iteration: 101260 loss: 0.0026 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 101270 loss: 0.0033 lr: 0.02\n",
      "iteration: 101280 loss: 0.0031 lr: 0.02\n",
      "iteration: 101290 loss: 0.0030 lr: 0.02\n",
      "iteration: 101300 loss: 0.0029 lr: 0.02\n",
      "iteration: 101310 loss: 0.0024 lr: 0.02\n",
      "iteration: 101320 loss: 0.0031 lr: 0.02\n",
      "iteration: 101330 loss: 0.0026 lr: 0.02\n",
      "iteration: 101340 loss: 0.0035 lr: 0.02\n",
      "iteration: 101350 loss: 0.0024 lr: 0.02\n",
      "iteration: 101360 loss: 0.0025 lr: 0.02\n",
      "iteration: 101370 loss: 0.0033 lr: 0.02\n",
      "iteration: 101380 loss: 0.0023 lr: 0.02\n",
      "iteration: 101390 loss: 0.0033 lr: 0.02\n",
      "iteration: 101400 loss: 0.0034 lr: 0.02\n",
      "iteration: 101410 loss: 0.0023 lr: 0.02\n",
      "iteration: 101420 loss: 0.0031 lr: 0.02\n",
      "iteration: 101430 loss: 0.0030 lr: 0.02\n",
      "iteration: 101440 loss: 0.0029 lr: 0.02\n",
      "iteration: 101450 loss: 0.0024 lr: 0.02\n",
      "iteration: 101460 loss: 0.0028 lr: 0.02\n",
      "iteration: 101470 loss: 0.0022 lr: 0.02\n",
      "iteration: 101480 loss: 0.0031 lr: 0.02\n",
      "iteration: 101490 loss: 0.0022 lr: 0.02\n",
      "iteration: 101500 loss: 0.0020 lr: 0.02\n",
      "iteration: 101510 loss: 0.0028 lr: 0.02\n",
      "iteration: 101520 loss: 0.0035 lr: 0.02\n",
      "iteration: 101530 loss: 0.0026 lr: 0.02\n",
      "iteration: 101540 loss: 0.0031 lr: 0.02\n",
      "iteration: 101550 loss: 0.0025 lr: 0.02\n",
      "iteration: 101560 loss: 0.0029 lr: 0.02\n",
      "iteration: 101570 loss: 0.0029 lr: 0.02\n",
      "iteration: 101580 loss: 0.0035 lr: 0.02\n",
      "iteration: 101590 loss: 0.0029 lr: 0.02\n",
      "iteration: 101600 loss: 0.0029 lr: 0.02\n",
      "iteration: 101610 loss: 0.0027 lr: 0.02\n",
      "iteration: 101620 loss: 0.0028 lr: 0.02\n",
      "iteration: 101630 loss: 0.0024 lr: 0.02\n",
      "iteration: 101640 loss: 0.0034 lr: 0.02\n",
      "iteration: 101650 loss: 0.0026 lr: 0.02\n",
      "iteration: 101660 loss: 0.0025 lr: 0.02\n",
      "iteration: 101670 loss: 0.0026 lr: 0.02\n",
      "iteration: 101680 loss: 0.0022 lr: 0.02\n",
      "iteration: 101690 loss: 0.0028 lr: 0.02\n",
      "iteration: 101700 loss: 0.0029 lr: 0.02\n",
      "iteration: 101710 loss: 0.0029 lr: 0.02\n",
      "iteration: 101720 loss: 0.0028 lr: 0.02\n",
      "iteration: 101730 loss: 0.0040 lr: 0.02\n",
      "iteration: 101740 loss: 0.0022 lr: 0.02\n",
      "iteration: 101750 loss: 0.0023 lr: 0.02\n",
      "iteration: 101760 loss: 0.0025 lr: 0.02\n",
      "iteration: 101770 loss: 0.0034 lr: 0.02\n",
      "iteration: 101780 loss: 0.0028 lr: 0.02\n",
      "iteration: 101790 loss: 0.0028 lr: 0.02\n",
      "iteration: 101800 loss: 0.0038 lr: 0.02\n",
      "iteration: 101810 loss: 0.0022 lr: 0.02\n",
      "iteration: 101820 loss: 0.0043 lr: 0.02\n",
      "iteration: 101830 loss: 0.0031 lr: 0.02\n",
      "iteration: 101840 loss: 0.0034 lr: 0.02\n",
      "iteration: 101850 loss: 0.0026 lr: 0.02\n",
      "iteration: 101860 loss: 0.0028 lr: 0.02\n",
      "iteration: 101870 loss: 0.0022 lr: 0.02\n",
      "iteration: 101880 loss: 0.0026 lr: 0.02\n",
      "iteration: 101890 loss: 0.0031 lr: 0.02\n",
      "iteration: 101900 loss: 0.0033 lr: 0.02\n",
      "iteration: 101910 loss: 0.0035 lr: 0.02\n",
      "iteration: 101920 loss: 0.0028 lr: 0.02\n",
      "iteration: 101930 loss: 0.0041 lr: 0.02\n",
      "iteration: 101940 loss: 0.0026 lr: 0.02\n",
      "iteration: 101950 loss: 0.0025 lr: 0.02\n",
      "iteration: 101960 loss: 0.0037 lr: 0.02\n",
      "iteration: 101970 loss: 0.0030 lr: 0.02\n",
      "iteration: 101980 loss: 0.0024 lr: 0.02\n",
      "iteration: 101990 loss: 0.0026 lr: 0.02\n",
      "iteration: 102000 loss: 0.0025 lr: 0.02\n",
      "iteration: 102010 loss: 0.0022 lr: 0.02\n",
      "iteration: 102020 loss: 0.0043 lr: 0.02\n",
      "iteration: 102030 loss: 0.0028 lr: 0.02\n",
      "iteration: 102040 loss: 0.0023 lr: 0.02\n",
      "iteration: 102050 loss: 0.0024 lr: 0.02\n",
      "iteration: 102060 loss: 0.0028 lr: 0.02\n",
      "iteration: 102070 loss: 0.0027 lr: 0.02\n",
      "iteration: 102080 loss: 0.0032 lr: 0.02\n",
      "iteration: 102090 loss: 0.0027 lr: 0.02\n",
      "iteration: 102100 loss: 0.0034 lr: 0.02\n",
      "iteration: 102110 loss: 0.0036 lr: 0.02\n",
      "iteration: 102120 loss: 0.0029 lr: 0.02\n",
      "iteration: 102130 loss: 0.0027 lr: 0.02\n",
      "iteration: 102140 loss: 0.0026 lr: 0.02\n",
      "iteration: 102150 loss: 0.0051 lr: 0.02\n",
      "iteration: 102160 loss: 0.0045 lr: 0.02\n",
      "iteration: 102170 loss: 0.0030 lr: 0.02\n",
      "iteration: 102180 loss: 0.0037 lr: 0.02\n",
      "iteration: 102190 loss: 0.0026 lr: 0.02\n",
      "iteration: 102200 loss: 0.0033 lr: 0.02\n",
      "iteration: 102210 loss: 0.0036 lr: 0.02\n",
      "iteration: 102220 loss: 0.0032 lr: 0.02\n",
      "iteration: 102230 loss: 0.0028 lr: 0.02\n",
      "iteration: 102240 loss: 0.0028 lr: 0.02\n",
      "iteration: 102250 loss: 0.0028 lr: 0.02\n",
      "iteration: 102260 loss: 0.0028 lr: 0.02\n",
      "iteration: 102270 loss: 0.0030 lr: 0.02\n",
      "iteration: 102280 loss: 0.0034 lr: 0.02\n",
      "iteration: 102290 loss: 0.0036 lr: 0.02\n",
      "iteration: 102300 loss: 0.0031 lr: 0.02\n",
      "iteration: 102310 loss: 0.0033 lr: 0.02\n",
      "iteration: 102320 loss: 0.0036 lr: 0.02\n",
      "iteration: 102330 loss: 0.0034 lr: 0.02\n",
      "iteration: 102340 loss: 0.0029 lr: 0.02\n",
      "iteration: 102350 loss: 0.0024 lr: 0.02\n",
      "iteration: 102360 loss: 0.0025 lr: 0.02\n",
      "iteration: 102370 loss: 0.0035 lr: 0.02\n",
      "iteration: 102380 loss: 0.0045 lr: 0.02\n",
      "iteration: 102390 loss: 0.0028 lr: 0.02\n",
      "iteration: 102400 loss: 0.0029 lr: 0.02\n",
      "iteration: 102410 loss: 0.0028 lr: 0.02\n",
      "iteration: 102420 loss: 0.0027 lr: 0.02\n",
      "iteration: 102430 loss: 0.0030 lr: 0.02\n",
      "iteration: 102440 loss: 0.0038 lr: 0.02\n",
      "iteration: 102450 loss: 0.0034 lr: 0.02\n",
      "iteration: 102460 loss: 0.0030 lr: 0.02\n",
      "iteration: 102470 loss: 0.0032 lr: 0.02\n",
      "iteration: 102480 loss: 0.0036 lr: 0.02\n",
      "iteration: 102490 loss: 0.0044 lr: 0.02\n",
      "iteration: 102500 loss: 0.0030 lr: 0.02\n",
      "iteration: 102510 loss: 0.0032 lr: 0.02\n",
      "iteration: 102520 loss: 0.0034 lr: 0.02\n",
      "iteration: 102530 loss: 0.0027 lr: 0.02\n",
      "iteration: 102540 loss: 0.0039 lr: 0.02\n",
      "iteration: 102550 loss: 0.0032 lr: 0.02\n",
      "iteration: 102560 loss: 0.0034 lr: 0.02\n",
      "iteration: 102570 loss: 0.0033 lr: 0.02\n",
      "iteration: 102580 loss: 0.0028 lr: 0.02\n",
      "iteration: 102590 loss: 0.0031 lr: 0.02\n",
      "iteration: 102600 loss: 0.0032 lr: 0.02\n",
      "iteration: 102610 loss: 0.0027 lr: 0.02\n",
      "iteration: 102620 loss: 0.0019 lr: 0.02\n",
      "iteration: 102630 loss: 0.0045 lr: 0.02\n",
      "iteration: 102640 loss: 0.0034 lr: 0.02\n",
      "iteration: 102650 loss: 0.0024 lr: 0.02\n",
      "iteration: 102660 loss: 0.0029 lr: 0.02\n",
      "iteration: 102670 loss: 0.0035 lr: 0.02\n",
      "iteration: 102680 loss: 0.0022 lr: 0.02\n",
      "iteration: 102690 loss: 0.0029 lr: 0.02\n",
      "iteration: 102700 loss: 0.0033 lr: 0.02\n",
      "iteration: 102710 loss: 0.0034 lr: 0.02\n",
      "iteration: 102720 loss: 0.0025 lr: 0.02\n",
      "iteration: 102730 loss: 0.0032 lr: 0.02\n",
      "iteration: 102740 loss: 0.0022 lr: 0.02\n",
      "iteration: 102750 loss: 0.0026 lr: 0.02\n",
      "iteration: 102760 loss: 0.0027 lr: 0.02\n",
      "iteration: 102770 loss: 0.0026 lr: 0.02\n",
      "iteration: 102780 loss: 0.0032 lr: 0.02\n",
      "iteration: 102790 loss: 0.0019 lr: 0.02\n",
      "iteration: 102800 loss: 0.0022 lr: 0.02\n",
      "iteration: 102810 loss: 0.0025 lr: 0.02\n",
      "iteration: 102820 loss: 0.0023 lr: 0.02\n",
      "iteration: 102830 loss: 0.0035 lr: 0.02\n",
      "iteration: 102840 loss: 0.0030 lr: 0.02\n",
      "iteration: 102850 loss: 0.0027 lr: 0.02\n",
      "iteration: 102860 loss: 0.0028 lr: 0.02\n",
      "iteration: 102870 loss: 0.0031 lr: 0.02\n",
      "iteration: 102880 loss: 0.0019 lr: 0.02\n",
      "iteration: 102890 loss: 0.0031 lr: 0.02\n",
      "iteration: 102900 loss: 0.0036 lr: 0.02\n",
      "iteration: 102910 loss: 0.0024 lr: 0.02\n",
      "iteration: 102920 loss: 0.0028 lr: 0.02\n",
      "iteration: 102930 loss: 0.0035 lr: 0.02\n",
      "iteration: 102940 loss: 0.0029 lr: 0.02\n",
      "iteration: 102950 loss: 0.0028 lr: 0.02\n",
      "iteration: 102960 loss: 0.0023 lr: 0.02\n",
      "iteration: 102970 loss: 0.0033 lr: 0.02\n",
      "iteration: 102980 loss: 0.0028 lr: 0.02\n",
      "iteration: 102990 loss: 0.0032 lr: 0.02\n",
      "iteration: 103000 loss: 0.0025 lr: 0.02\n",
      "iteration: 103010 loss: 0.0030 lr: 0.02\n",
      "iteration: 103020 loss: 0.0025 lr: 0.02\n",
      "iteration: 103030 loss: 0.0020 lr: 0.02\n",
      "iteration: 103040 loss: 0.0038 lr: 0.02\n",
      "iteration: 103050 loss: 0.0039 lr: 0.02\n",
      "iteration: 103060 loss: 0.0034 lr: 0.02\n",
      "iteration: 103070 loss: 0.0027 lr: 0.02\n",
      "iteration: 103080 loss: 0.0033 lr: 0.02\n",
      "iteration: 103090 loss: 0.0026 lr: 0.02\n",
      "iteration: 103100 loss: 0.0051 lr: 0.02\n",
      "iteration: 103110 loss: 0.0037 lr: 0.02\n",
      "iteration: 103120 loss: 0.0042 lr: 0.02\n",
      "iteration: 103130 loss: 0.0025 lr: 0.02\n",
      "iteration: 103140 loss: 0.0038 lr: 0.02\n",
      "iteration: 103150 loss: 0.0029 lr: 0.02\n",
      "iteration: 103160 loss: 0.0032 lr: 0.02\n",
      "iteration: 103170 loss: 0.0042 lr: 0.02\n",
      "iteration: 103180 loss: 0.0026 lr: 0.02\n",
      "iteration: 103190 loss: 0.0029 lr: 0.02\n",
      "iteration: 103200 loss: 0.0034 lr: 0.02\n",
      "iteration: 103210 loss: 0.0037 lr: 0.02\n",
      "iteration: 103220 loss: 0.0029 lr: 0.02\n",
      "iteration: 103230 loss: 0.0038 lr: 0.02\n",
      "iteration: 103240 loss: 0.0036 lr: 0.02\n",
      "iteration: 103250 loss: 0.0035 lr: 0.02\n",
      "iteration: 103260 loss: 0.0038 lr: 0.02\n",
      "iteration: 103270 loss: 0.0032 lr: 0.02\n",
      "iteration: 103280 loss: 0.0037 lr: 0.02\n",
      "iteration: 103290 loss: 0.0028 lr: 0.02\n",
      "iteration: 103300 loss: 0.0041 lr: 0.02\n",
      "iteration: 103310 loss: 0.0028 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 103320 loss: 0.0028 lr: 0.02\n",
      "iteration: 103330 loss: 0.0019 lr: 0.02\n",
      "iteration: 103340 loss: 0.0049 lr: 0.02\n",
      "iteration: 103350 loss: 0.0039 lr: 0.02\n",
      "iteration: 103360 loss: 0.0034 lr: 0.02\n",
      "iteration: 103370 loss: 0.0032 lr: 0.02\n",
      "iteration: 103380 loss: 0.0029 lr: 0.02\n",
      "iteration: 103390 loss: 0.0029 lr: 0.02\n",
      "iteration: 103400 loss: 0.0053 lr: 0.02\n",
      "iteration: 103410 loss: 0.0024 lr: 0.02\n",
      "iteration: 103420 loss: 0.0028 lr: 0.02\n",
      "iteration: 103430 loss: 0.0025 lr: 0.02\n",
      "iteration: 103440 loss: 0.0024 lr: 0.02\n",
      "iteration: 103450 loss: 0.0035 lr: 0.02\n",
      "iteration: 103460 loss: 0.0030 lr: 0.02\n",
      "iteration: 103470 loss: 0.0038 lr: 0.02\n",
      "iteration: 103480 loss: 0.0040 lr: 0.02\n",
      "iteration: 103490 loss: 0.0033 lr: 0.02\n",
      "iteration: 103500 loss: 0.0031 lr: 0.02\n",
      "iteration: 103510 loss: 0.0032 lr: 0.02\n",
      "iteration: 103520 loss: 0.0022 lr: 0.02\n",
      "iteration: 103530 loss: 0.0028 lr: 0.02\n",
      "iteration: 103540 loss: 0.0043 lr: 0.02\n",
      "iteration: 103550 loss: 0.0030 lr: 0.02\n",
      "iteration: 103560 loss: 0.0028 lr: 0.02\n",
      "iteration: 103570 loss: 0.0023 lr: 0.02\n",
      "iteration: 103580 loss: 0.0023 lr: 0.02\n",
      "iteration: 103590 loss: 0.0022 lr: 0.02\n",
      "iteration: 103600 loss: 0.0028 lr: 0.02\n",
      "iteration: 103610 loss: 0.0026 lr: 0.02\n",
      "iteration: 103620 loss: 0.0030 lr: 0.02\n",
      "iteration: 103630 loss: 0.0028 lr: 0.02\n",
      "iteration: 103640 loss: 0.0027 lr: 0.02\n",
      "iteration: 103650 loss: 0.0027 lr: 0.02\n",
      "iteration: 103660 loss: 0.0030 lr: 0.02\n",
      "iteration: 103670 loss: 0.0037 lr: 0.02\n",
      "iteration: 103680 loss: 0.0022 lr: 0.02\n",
      "iteration: 103690 loss: 0.0027 lr: 0.02\n",
      "iteration: 103700 loss: 0.0029 lr: 0.02\n",
      "iteration: 103710 loss: 0.0033 lr: 0.02\n",
      "iteration: 103720 loss: 0.0019 lr: 0.02\n",
      "iteration: 103730 loss: 0.0025 lr: 0.02\n",
      "iteration: 103740 loss: 0.0025 lr: 0.02\n",
      "iteration: 103750 loss: 0.0024 lr: 0.02\n",
      "iteration: 103760 loss: 0.0027 lr: 0.02\n",
      "iteration: 103770 loss: 0.0029 lr: 0.02\n",
      "iteration: 103780 loss: 0.0030 lr: 0.02\n",
      "iteration: 103790 loss: 0.0038 lr: 0.02\n",
      "iteration: 103800 loss: 0.0029 lr: 0.02\n",
      "iteration: 103810 loss: 0.0030 lr: 0.02\n",
      "iteration: 103820 loss: 0.0029 lr: 0.02\n",
      "iteration: 103830 loss: 0.0023 lr: 0.02\n",
      "iteration: 103840 loss: 0.0036 lr: 0.02\n",
      "iteration: 103850 loss: 0.0036 lr: 0.02\n",
      "iteration: 103860 loss: 0.0028 lr: 0.02\n",
      "iteration: 103870 loss: 0.0035 lr: 0.02\n",
      "iteration: 103880 loss: 0.0027 lr: 0.02\n",
      "iteration: 103890 loss: 0.0031 lr: 0.02\n",
      "iteration: 103900 loss: 0.0031 lr: 0.02\n",
      "iteration: 103910 loss: 0.0025 lr: 0.02\n",
      "iteration: 103920 loss: 0.0027 lr: 0.02\n",
      "iteration: 103930 loss: 0.0030 lr: 0.02\n",
      "iteration: 103940 loss: 0.0025 lr: 0.02\n",
      "iteration: 103950 loss: 0.0028 lr: 0.02\n",
      "iteration: 103960 loss: 0.0025 lr: 0.02\n",
      "iteration: 103970 loss: 0.0029 lr: 0.02\n",
      "iteration: 103980 loss: 0.0028 lr: 0.02\n",
      "iteration: 103990 loss: 0.0025 lr: 0.02\n",
      "iteration: 104000 loss: 0.0031 lr: 0.02\n",
      "iteration: 104010 loss: 0.0033 lr: 0.02\n",
      "iteration: 104020 loss: 0.0032 lr: 0.02\n",
      "iteration: 104030 loss: 0.0027 lr: 0.02\n",
      "iteration: 104040 loss: 0.0026 lr: 0.02\n",
      "iteration: 104050 loss: 0.0025 lr: 0.02\n",
      "iteration: 104060 loss: 0.0022 lr: 0.02\n",
      "iteration: 104070 loss: 0.0025 lr: 0.02\n",
      "iteration: 104080 loss: 0.0025 lr: 0.02\n",
      "iteration: 104090 loss: 0.0032 lr: 0.02\n",
      "iteration: 104100 loss: 0.0032 lr: 0.02\n",
      "iteration: 104110 loss: 0.0028 lr: 0.02\n",
      "iteration: 104120 loss: 0.0036 lr: 0.02\n",
      "iteration: 104130 loss: 0.0040 lr: 0.02\n",
      "iteration: 104140 loss: 0.0029 lr: 0.02\n",
      "iteration: 104150 loss: 0.0034 lr: 0.02\n",
      "iteration: 104160 loss: 0.0024 lr: 0.02\n",
      "iteration: 104170 loss: 0.0022 lr: 0.02\n",
      "iteration: 104180 loss: 0.0027 lr: 0.02\n",
      "iteration: 104190 loss: 0.0036 lr: 0.02\n",
      "iteration: 104200 loss: 0.0024 lr: 0.02\n",
      "iteration: 104210 loss: 0.0020 lr: 0.02\n",
      "iteration: 104220 loss: 0.0035 lr: 0.02\n",
      "iteration: 104230 loss: 0.0028 lr: 0.02\n",
      "iteration: 104240 loss: 0.0032 lr: 0.02\n",
      "iteration: 104250 loss: 0.0031 lr: 0.02\n",
      "iteration: 104260 loss: 0.0038 lr: 0.02\n",
      "iteration: 104270 loss: 0.0024 lr: 0.02\n",
      "iteration: 104280 loss: 0.0034 lr: 0.02\n",
      "iteration: 104290 loss: 0.0031 lr: 0.02\n",
      "iteration: 104300 loss: 0.0024 lr: 0.02\n",
      "iteration: 104310 loss: 0.0028 lr: 0.02\n",
      "iteration: 104320 loss: 0.0026 lr: 0.02\n",
      "iteration: 104330 loss: 0.0028 lr: 0.02\n",
      "iteration: 104340 loss: 0.0023 lr: 0.02\n",
      "iteration: 104350 loss: 0.0027 lr: 0.02\n",
      "iteration: 104360 loss: 0.0032 lr: 0.02\n",
      "iteration: 104370 loss: 0.0024 lr: 0.02\n",
      "iteration: 104380 loss: 0.0026 lr: 0.02\n",
      "iteration: 104390 loss: 0.0027 lr: 0.02\n",
      "iteration: 104400 loss: 0.0027 lr: 0.02\n",
      "iteration: 104410 loss: 0.0035 lr: 0.02\n",
      "iteration: 104420 loss: 0.0026 lr: 0.02\n",
      "iteration: 104430 loss: 0.0024 lr: 0.02\n",
      "iteration: 104440 loss: 0.0026 lr: 0.02\n",
      "iteration: 104450 loss: 0.0035 lr: 0.02\n",
      "iteration: 104460 loss: 0.0027 lr: 0.02\n",
      "iteration: 104470 loss: 0.0030 lr: 0.02\n",
      "iteration: 104480 loss: 0.0024 lr: 0.02\n",
      "iteration: 104490 loss: 0.0024 lr: 0.02\n",
      "iteration: 104500 loss: 0.0033 lr: 0.02\n",
      "iteration: 104510 loss: 0.0032 lr: 0.02\n",
      "iteration: 104520 loss: 0.0027 lr: 0.02\n",
      "iteration: 104530 loss: 0.0032 lr: 0.02\n",
      "iteration: 104540 loss: 0.0032 lr: 0.02\n",
      "iteration: 104550 loss: 0.0043 lr: 0.02\n",
      "iteration: 104560 loss: 0.0025 lr: 0.02\n",
      "iteration: 104570 loss: 0.0024 lr: 0.02\n",
      "iteration: 104580 loss: 0.0039 lr: 0.02\n",
      "iteration: 104590 loss: 0.0031 lr: 0.02\n",
      "iteration: 104600 loss: 0.0027 lr: 0.02\n",
      "iteration: 104610 loss: 0.0028 lr: 0.02\n",
      "iteration: 104620 loss: 0.0029 lr: 0.02\n",
      "iteration: 104630 loss: 0.0024 lr: 0.02\n",
      "iteration: 104640 loss: 0.0024 lr: 0.02\n",
      "iteration: 104650 loss: 0.0025 lr: 0.02\n",
      "iteration: 104660 loss: 0.0024 lr: 0.02\n",
      "iteration: 104670 loss: 0.0037 lr: 0.02\n",
      "iteration: 104680 loss: 0.0031 lr: 0.02\n",
      "iteration: 104690 loss: 0.0030 lr: 0.02\n",
      "iteration: 104700 loss: 0.0031 lr: 0.02\n",
      "iteration: 104710 loss: 0.0030 lr: 0.02\n",
      "iteration: 104720 loss: 0.0029 lr: 0.02\n",
      "iteration: 104730 loss: 0.0030 lr: 0.02\n",
      "iteration: 104740 loss: 0.0027 lr: 0.02\n",
      "iteration: 104750 loss: 0.0027 lr: 0.02\n",
      "iteration: 104760 loss: 0.0035 lr: 0.02\n",
      "iteration: 104770 loss: 0.0036 lr: 0.02\n",
      "iteration: 104780 loss: 0.0026 lr: 0.02\n",
      "iteration: 104790 loss: 0.0031 lr: 0.02\n",
      "iteration: 104800 loss: 0.0028 lr: 0.02\n",
      "iteration: 104810 loss: 0.0032 lr: 0.02\n",
      "iteration: 104820 loss: 0.0025 lr: 0.02\n",
      "iteration: 104830 loss: 0.0022 lr: 0.02\n",
      "iteration: 104840 loss: 0.0039 lr: 0.02\n",
      "iteration: 104850 loss: 0.0027 lr: 0.02\n",
      "iteration: 104860 loss: 0.0036 lr: 0.02\n",
      "iteration: 104870 loss: 0.0030 lr: 0.02\n",
      "iteration: 104880 loss: 0.0028 lr: 0.02\n",
      "iteration: 104890 loss: 0.0031 lr: 0.02\n",
      "iteration: 104900 loss: 0.0026 lr: 0.02\n",
      "iteration: 104910 loss: 0.0032 lr: 0.02\n",
      "iteration: 104920 loss: 0.0040 lr: 0.02\n",
      "iteration: 104930 loss: 0.0036 lr: 0.02\n",
      "iteration: 104940 loss: 0.0023 lr: 0.02\n",
      "iteration: 104950 loss: 0.0028 lr: 0.02\n",
      "iteration: 104960 loss: 0.0025 lr: 0.02\n",
      "iteration: 104970 loss: 0.0029 lr: 0.02\n",
      "iteration: 104980 loss: 0.0024 lr: 0.02\n",
      "iteration: 104990 loss: 0.0029 lr: 0.02\n",
      "iteration: 105000 loss: 0.0020 lr: 0.02\n",
      "iteration: 105010 loss: 0.0024 lr: 0.02\n",
      "iteration: 105020 loss: 0.0035 lr: 0.02\n",
      "iteration: 105030 loss: 0.0039 lr: 0.02\n",
      "iteration: 105040 loss: 0.0030 lr: 0.02\n",
      "iteration: 105050 loss: 0.0033 lr: 0.02\n",
      "iteration: 105060 loss: 0.0022 lr: 0.02\n",
      "iteration: 105070 loss: 0.0023 lr: 0.02\n",
      "iteration: 105080 loss: 0.0038 lr: 0.02\n",
      "iteration: 105090 loss: 0.0025 lr: 0.02\n",
      "iteration: 105100 loss: 0.0029 lr: 0.02\n",
      "iteration: 105110 loss: 0.0034 lr: 0.02\n",
      "iteration: 105120 loss: 0.0038 lr: 0.02\n",
      "iteration: 105130 loss: 0.0023 lr: 0.02\n",
      "iteration: 105140 loss: 0.0024 lr: 0.02\n",
      "iteration: 105150 loss: 0.0028 lr: 0.02\n",
      "iteration: 105160 loss: 0.0028 lr: 0.02\n",
      "iteration: 105170 loss: 0.0029 lr: 0.02\n",
      "iteration: 105180 loss: 0.0028 lr: 0.02\n",
      "iteration: 105190 loss: 0.0035 lr: 0.02\n",
      "iteration: 105200 loss: 0.0026 lr: 0.02\n",
      "iteration: 105210 loss: 0.0027 lr: 0.02\n",
      "iteration: 105220 loss: 0.0033 lr: 0.02\n",
      "iteration: 105230 loss: 0.0025 lr: 0.02\n",
      "iteration: 105240 loss: 0.0020 lr: 0.02\n",
      "iteration: 105250 loss: 0.0039 lr: 0.02\n",
      "iteration: 105260 loss: 0.0033 lr: 0.02\n",
      "iteration: 105270 loss: 0.0027 lr: 0.02\n",
      "iteration: 105280 loss: 0.0028 lr: 0.02\n",
      "iteration: 105290 loss: 0.0026 lr: 0.02\n",
      "iteration: 105300 loss: 0.0030 lr: 0.02\n",
      "iteration: 105310 loss: 0.0027 lr: 0.02\n",
      "iteration: 105320 loss: 0.0030 lr: 0.02\n",
      "iteration: 105330 loss: 0.0026 lr: 0.02\n",
      "iteration: 105340 loss: 0.0018 lr: 0.02\n",
      "iteration: 105350 loss: 0.0035 lr: 0.02\n",
      "iteration: 105360 loss: 0.0027 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 105370 loss: 0.0027 lr: 0.02\n",
      "iteration: 105380 loss: 0.0033 lr: 0.02\n",
      "iteration: 105390 loss: 0.0024 lr: 0.02\n",
      "iteration: 105400 loss: 0.0031 lr: 0.02\n",
      "iteration: 105410 loss: 0.0024 lr: 0.02\n",
      "iteration: 105420 loss: 0.0024 lr: 0.02\n",
      "iteration: 105430 loss: 0.0031 lr: 0.02\n",
      "iteration: 105440 loss: 0.0026 lr: 0.02\n",
      "iteration: 105450 loss: 0.0037 lr: 0.02\n",
      "iteration: 105460 loss: 0.0023 lr: 0.02\n",
      "iteration: 105470 loss: 0.0034 lr: 0.02\n",
      "iteration: 105480 loss: 0.0033 lr: 0.02\n",
      "iteration: 105490 loss: 0.0033 lr: 0.02\n",
      "iteration: 105500 loss: 0.0026 lr: 0.02\n",
      "iteration: 105510 loss: 0.0028 lr: 0.02\n",
      "iteration: 105520 loss: 0.0024 lr: 0.02\n",
      "iteration: 105530 loss: 0.0024 lr: 0.02\n",
      "iteration: 105540 loss: 0.0036 lr: 0.02\n",
      "iteration: 105550 loss: 0.0029 lr: 0.02\n",
      "iteration: 105560 loss: 0.0028 lr: 0.02\n",
      "iteration: 105570 loss: 0.0032 lr: 0.02\n",
      "iteration: 105580 loss: 0.0036 lr: 0.02\n",
      "iteration: 105590 loss: 0.0029 lr: 0.02\n",
      "iteration: 105600 loss: 0.0029 lr: 0.02\n",
      "iteration: 105610 loss: 0.0028 lr: 0.02\n",
      "iteration: 105620 loss: 0.0026 lr: 0.02\n",
      "iteration: 105630 loss: 0.0032 lr: 0.02\n",
      "iteration: 105640 loss: 0.0029 lr: 0.02\n",
      "iteration: 105650 loss: 0.0028 lr: 0.02\n",
      "iteration: 105660 loss: 0.0028 lr: 0.02\n",
      "iteration: 105670 loss: 0.0028 lr: 0.02\n",
      "iteration: 105680 loss: 0.0023 lr: 0.02\n",
      "iteration: 105690 loss: 0.0025 lr: 0.02\n",
      "iteration: 105700 loss: 0.0028 lr: 0.02\n",
      "iteration: 105710 loss: 0.0036 lr: 0.02\n",
      "iteration: 105720 loss: 0.0034 lr: 0.02\n",
      "iteration: 105730 loss: 0.0030 lr: 0.02\n",
      "iteration: 105740 loss: 0.0031 lr: 0.02\n",
      "iteration: 105750 loss: 0.0030 lr: 0.02\n",
      "iteration: 105760 loss: 0.0028 lr: 0.02\n",
      "iteration: 105770 loss: 0.0026 lr: 0.02\n",
      "iteration: 105780 loss: 0.0032 lr: 0.02\n",
      "iteration: 105790 loss: 0.0026 lr: 0.02\n",
      "iteration: 105800 loss: 0.0024 lr: 0.02\n",
      "iteration: 105810 loss: 0.0038 lr: 0.02\n",
      "iteration: 105820 loss: 0.0026 lr: 0.02\n",
      "iteration: 105830 loss: 0.0031 lr: 0.02\n",
      "iteration: 105840 loss: 0.0034 lr: 0.02\n",
      "iteration: 105850 loss: 0.0028 lr: 0.02\n",
      "iteration: 105860 loss: 0.0032 lr: 0.02\n",
      "iteration: 105870 loss: 0.0029 lr: 0.02\n",
      "iteration: 105880 loss: 0.0038 lr: 0.02\n",
      "iteration: 105890 loss: 0.0033 lr: 0.02\n",
      "iteration: 105900 loss: 0.0028 lr: 0.02\n",
      "iteration: 105910 loss: 0.0024 lr: 0.02\n",
      "iteration: 105920 loss: 0.0031 lr: 0.02\n",
      "iteration: 105930 loss: 0.0028 lr: 0.02\n",
      "iteration: 105940 loss: 0.0035 lr: 0.02\n",
      "iteration: 105950 loss: 0.0019 lr: 0.02\n",
      "iteration: 105960 loss: 0.0033 lr: 0.02\n",
      "iteration: 105970 loss: 0.0032 lr: 0.02\n",
      "iteration: 105980 loss: 0.0027 lr: 0.02\n",
      "iteration: 105990 loss: 0.0037 lr: 0.02\n",
      "iteration: 106000 loss: 0.0028 lr: 0.02\n",
      "iteration: 106010 loss: 0.0027 lr: 0.02\n",
      "iteration: 106020 loss: 0.0029 lr: 0.02\n",
      "iteration: 106030 loss: 0.0030 lr: 0.02\n",
      "iteration: 106040 loss: 0.0023 lr: 0.02\n",
      "iteration: 106050 loss: 0.0036 lr: 0.02\n",
      "iteration: 106060 loss: 0.0039 lr: 0.02\n",
      "iteration: 106070 loss: 0.0033 lr: 0.02\n",
      "iteration: 106080 loss: 0.0034 lr: 0.02\n",
      "iteration: 106090 loss: 0.0022 lr: 0.02\n",
      "iteration: 106100 loss: 0.0032 lr: 0.02\n",
      "iteration: 106110 loss: 0.0035 lr: 0.02\n",
      "iteration: 106120 loss: 0.0031 lr: 0.02\n",
      "iteration: 106130 loss: 0.0022 lr: 0.02\n",
      "iteration: 106140 loss: 0.0028 lr: 0.02\n",
      "iteration: 106150 loss: 0.0028 lr: 0.02\n",
      "iteration: 106160 loss: 0.0028 lr: 0.02\n",
      "iteration: 106170 loss: 0.0030 lr: 0.02\n",
      "iteration: 106180 loss: 0.0025 lr: 0.02\n",
      "iteration: 106190 loss: 0.0029 lr: 0.02\n",
      "iteration: 106200 loss: 0.0031 lr: 0.02\n",
      "iteration: 106210 loss: 0.0021 lr: 0.02\n",
      "iteration: 106220 loss: 0.0035 lr: 0.02\n",
      "iteration: 106230 loss: 0.0027 lr: 0.02\n",
      "iteration: 106240 loss: 0.0029 lr: 0.02\n",
      "iteration: 106250 loss: 0.0026 lr: 0.02\n",
      "iteration: 106260 loss: 0.0023 lr: 0.02\n",
      "iteration: 106270 loss: 0.0024 lr: 0.02\n",
      "iteration: 106280 loss: 0.0028 lr: 0.02\n",
      "iteration: 106290 loss: 0.0029 lr: 0.02\n",
      "iteration: 106300 loss: 0.0029 lr: 0.02\n",
      "iteration: 106310 loss: 0.0023 lr: 0.02\n",
      "iteration: 106320 loss: 0.0026 lr: 0.02\n",
      "iteration: 106330 loss: 0.0027 lr: 0.02\n",
      "iteration: 106340 loss: 0.0029 lr: 0.02\n",
      "iteration: 106350 loss: 0.0029 lr: 0.02\n",
      "iteration: 106360 loss: 0.0023 lr: 0.02\n",
      "iteration: 106370 loss: 0.0026 lr: 0.02\n",
      "iteration: 106380 loss: 0.0028 lr: 0.02\n",
      "iteration: 106390 loss: 0.0030 lr: 0.02\n",
      "iteration: 106400 loss: 0.0030 lr: 0.02\n",
      "iteration: 106410 loss: 0.0028 lr: 0.02\n",
      "iteration: 106420 loss: 0.0028 lr: 0.02\n",
      "iteration: 106430 loss: 0.0027 lr: 0.02\n",
      "iteration: 106440 loss: 0.0031 lr: 0.02\n",
      "iteration: 106450 loss: 0.0024 lr: 0.02\n",
      "iteration: 106460 loss: 0.0024 lr: 0.02\n",
      "iteration: 106470 loss: 0.0028 lr: 0.02\n",
      "iteration: 106480 loss: 0.0050 lr: 0.02\n",
      "iteration: 106490 loss: 0.0029 lr: 0.02\n",
      "iteration: 106500 loss: 0.0025 lr: 0.02\n",
      "iteration: 106510 loss: 0.0026 lr: 0.02\n",
      "iteration: 106520 loss: 0.0025 lr: 0.02\n",
      "iteration: 106530 loss: 0.0031 lr: 0.02\n",
      "iteration: 106540 loss: 0.0022 lr: 0.02\n",
      "iteration: 106550 loss: 0.0031 lr: 0.02\n",
      "iteration: 106560 loss: 0.0028 lr: 0.02\n",
      "iteration: 106570 loss: 0.0021 lr: 0.02\n",
      "iteration: 106580 loss: 0.0031 lr: 0.02\n",
      "iteration: 106590 loss: 0.0040 lr: 0.02\n",
      "iteration: 106600 loss: 0.0029 lr: 0.02\n",
      "iteration: 106610 loss: 0.0033 lr: 0.02\n",
      "iteration: 106620 loss: 0.0028 lr: 0.02\n",
      "iteration: 106630 loss: 0.0023 lr: 0.02\n",
      "iteration: 106640 loss: 0.0034 lr: 0.02\n",
      "iteration: 106650 loss: 0.0025 lr: 0.02\n",
      "iteration: 106660 loss: 0.0028 lr: 0.02\n",
      "iteration: 106670 loss: 0.0031 lr: 0.02\n",
      "iteration: 106680 loss: 0.0031 lr: 0.02\n",
      "iteration: 106690 loss: 0.0026 lr: 0.02\n",
      "iteration: 106700 loss: 0.0027 lr: 0.02\n",
      "iteration: 106710 loss: 0.0030 lr: 0.02\n",
      "iteration: 106720 loss: 0.0031 lr: 0.02\n",
      "iteration: 106730 loss: 0.0032 lr: 0.02\n",
      "iteration: 106740 loss: 0.0031 lr: 0.02\n",
      "iteration: 106750 loss: 0.0036 lr: 0.02\n",
      "iteration: 106760 loss: 0.0028 lr: 0.02\n",
      "iteration: 106770 loss: 0.0023 lr: 0.02\n",
      "iteration: 106780 loss: 0.0030 lr: 0.02\n",
      "iteration: 106790 loss: 0.0028 lr: 0.02\n",
      "iteration: 106800 loss: 0.0028 lr: 0.02\n",
      "iteration: 106810 loss: 0.0029 lr: 0.02\n",
      "iteration: 106820 loss: 0.0032 lr: 0.02\n",
      "iteration: 106830 loss: 0.0030 lr: 0.02\n",
      "iteration: 106840 loss: 0.0026 lr: 0.02\n",
      "iteration: 106850 loss: 0.0025 lr: 0.02\n",
      "iteration: 106860 loss: 0.0027 lr: 0.02\n",
      "iteration: 106870 loss: 0.0027 lr: 0.02\n",
      "iteration: 106880 loss: 0.0036 lr: 0.02\n",
      "iteration: 106890 loss: 0.0026 lr: 0.02\n",
      "iteration: 106900 loss: 0.0036 lr: 0.02\n",
      "iteration: 106910 loss: 0.0042 lr: 0.02\n",
      "iteration: 106920 loss: 0.0030 lr: 0.02\n",
      "iteration: 106930 loss: 0.0031 lr: 0.02\n",
      "iteration: 106940 loss: 0.0025 lr: 0.02\n",
      "iteration: 106950 loss: 0.0027 lr: 0.02\n",
      "iteration: 106960 loss: 0.0024 lr: 0.02\n",
      "iteration: 106970 loss: 0.0024 lr: 0.02\n",
      "iteration: 106980 loss: 0.0034 lr: 0.02\n",
      "iteration: 106990 loss: 0.0042 lr: 0.02\n",
      "iteration: 107000 loss: 0.0033 lr: 0.02\n",
      "iteration: 107010 loss: 0.0028 lr: 0.02\n",
      "iteration: 107020 loss: 0.0028 lr: 0.02\n",
      "iteration: 107030 loss: 0.0021 lr: 0.02\n",
      "iteration: 107040 loss: 0.0028 lr: 0.02\n",
      "iteration: 107050 loss: 0.0025 lr: 0.02\n",
      "iteration: 107060 loss: 0.0021 lr: 0.02\n",
      "iteration: 107070 loss: 0.0025 lr: 0.02\n",
      "iteration: 107080 loss: 0.0028 lr: 0.02\n",
      "iteration: 107090 loss: 0.0031 lr: 0.02\n",
      "iteration: 107100 loss: 0.0025 lr: 0.02\n",
      "iteration: 107110 loss: 0.0030 lr: 0.02\n",
      "iteration: 107120 loss: 0.0025 lr: 0.02\n",
      "iteration: 107130 loss: 0.0019 lr: 0.02\n",
      "iteration: 107140 loss: 0.0035 lr: 0.02\n",
      "iteration: 107150 loss: 0.0029 lr: 0.02\n",
      "iteration: 107160 loss: 0.0040 lr: 0.02\n",
      "iteration: 107170 loss: 0.0031 lr: 0.02\n",
      "iteration: 107180 loss: 0.0034 lr: 0.02\n",
      "iteration: 107190 loss: 0.0035 lr: 0.02\n",
      "iteration: 107200 loss: 0.0025 lr: 0.02\n",
      "iteration: 107210 loss: 0.0033 lr: 0.02\n",
      "iteration: 107220 loss: 0.0029 lr: 0.02\n",
      "iteration: 107230 loss: 0.0035 lr: 0.02\n",
      "iteration: 107240 loss: 0.0025 lr: 0.02\n",
      "iteration: 107250 loss: 0.0028 lr: 0.02\n",
      "iteration: 107260 loss: 0.0025 lr: 0.02\n",
      "iteration: 107270 loss: 0.0027 lr: 0.02\n",
      "iteration: 107280 loss: 0.0036 lr: 0.02\n",
      "iteration: 107290 loss: 0.0028 lr: 0.02\n",
      "iteration: 107300 loss: 0.0034 lr: 0.02\n",
      "iteration: 107310 loss: 0.0029 lr: 0.02\n",
      "iteration: 107320 loss: 0.0034 lr: 0.02\n",
      "iteration: 107330 loss: 0.0023 lr: 0.02\n",
      "iteration: 107340 loss: 0.0026 lr: 0.02\n",
      "iteration: 107350 loss: 0.0029 lr: 0.02\n",
      "iteration: 107360 loss: 0.0020 lr: 0.02\n",
      "iteration: 107370 loss: 0.0030 lr: 0.02\n",
      "iteration: 107380 loss: 0.0029 lr: 0.02\n",
      "iteration: 107390 loss: 0.0028 lr: 0.02\n",
      "iteration: 107400 loss: 0.0024 lr: 0.02\n",
      "iteration: 107410 loss: 0.0026 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 107420 loss: 0.0026 lr: 0.02\n",
      "iteration: 107430 loss: 0.0027 lr: 0.02\n",
      "iteration: 107440 loss: 0.0023 lr: 0.02\n",
      "iteration: 107450 loss: 0.0028 lr: 0.02\n",
      "iteration: 107460 loss: 0.0035 lr: 0.02\n",
      "iteration: 107470 loss: 0.0026 lr: 0.02\n",
      "iteration: 107480 loss: 0.0035 lr: 0.02\n",
      "iteration: 107490 loss: 0.0023 lr: 0.02\n",
      "iteration: 107500 loss: 0.0031 lr: 0.02\n",
      "iteration: 107510 loss: 0.0030 lr: 0.02\n",
      "iteration: 107520 loss: 0.0031 lr: 0.02\n",
      "iteration: 107530 loss: 0.0031 lr: 0.02\n",
      "iteration: 107540 loss: 0.0027 lr: 0.02\n",
      "iteration: 107550 loss: 0.0028 lr: 0.02\n",
      "iteration: 107560 loss: 0.0030 lr: 0.02\n",
      "iteration: 107570 loss: 0.0033 lr: 0.02\n",
      "iteration: 107580 loss: 0.0026 lr: 0.02\n",
      "iteration: 107590 loss: 0.0029 lr: 0.02\n",
      "iteration: 107600 loss: 0.0039 lr: 0.02\n",
      "iteration: 107610 loss: 0.0030 lr: 0.02\n",
      "iteration: 107620 loss: 0.0033 lr: 0.02\n",
      "iteration: 107630 loss: 0.0030 lr: 0.02\n",
      "iteration: 107640 loss: 0.0030 lr: 0.02\n",
      "iteration: 107650 loss: 0.0031 lr: 0.02\n",
      "iteration: 107660 loss: 0.0026 lr: 0.02\n",
      "iteration: 107670 loss: 0.0025 lr: 0.02\n",
      "iteration: 107680 loss: 0.0025 lr: 0.02\n",
      "iteration: 107690 loss: 0.0024 lr: 0.02\n",
      "iteration: 107700 loss: 0.0028 lr: 0.02\n",
      "iteration: 107710 loss: 0.0031 lr: 0.02\n",
      "iteration: 107720 loss: 0.0030 lr: 0.02\n",
      "iteration: 107730 loss: 0.0024 lr: 0.02\n",
      "iteration: 107740 loss: 0.0029 lr: 0.02\n",
      "iteration: 107750 loss: 0.0029 lr: 0.02\n",
      "iteration: 107760 loss: 0.0028 lr: 0.02\n",
      "iteration: 107770 loss: 0.0035 lr: 0.02\n",
      "iteration: 107780 loss: 0.0024 lr: 0.02\n",
      "iteration: 107790 loss: 0.0033 lr: 0.02\n",
      "iteration: 107800 loss: 0.0034 lr: 0.02\n",
      "iteration: 107810 loss: 0.0035 lr: 0.02\n",
      "iteration: 107820 loss: 0.0027 lr: 0.02\n",
      "iteration: 107830 loss: 0.0034 lr: 0.02\n",
      "iteration: 107840 loss: 0.0027 lr: 0.02\n",
      "iteration: 107850 loss: 0.0027 lr: 0.02\n",
      "iteration: 107860 loss: 0.0025 lr: 0.02\n",
      "iteration: 107870 loss: 0.0028 lr: 0.02\n",
      "iteration: 107880 loss: 0.0037 lr: 0.02\n",
      "iteration: 107890 loss: 0.0026 lr: 0.02\n",
      "iteration: 107900 loss: 0.0035 lr: 0.02\n",
      "iteration: 107910 loss: 0.0036 lr: 0.02\n",
      "iteration: 107920 loss: 0.0031 lr: 0.02\n",
      "iteration: 107930 loss: 0.0030 lr: 0.02\n",
      "iteration: 107940 loss: 0.0024 lr: 0.02\n",
      "iteration: 107950 loss: 0.0046 lr: 0.02\n",
      "iteration: 107960 loss: 0.0041 lr: 0.02\n",
      "iteration: 107970 loss: 0.0039 lr: 0.02\n",
      "iteration: 107980 loss: 0.0035 lr: 0.02\n",
      "iteration: 107990 loss: 0.0022 lr: 0.02\n",
      "iteration: 108000 loss: 0.0024 lr: 0.02\n",
      "iteration: 108010 loss: 0.0030 lr: 0.02\n",
      "iteration: 108020 loss: 0.0041 lr: 0.02\n",
      "iteration: 108030 loss: 0.0028 lr: 0.02\n",
      "iteration: 108040 loss: 0.0025 lr: 0.02\n",
      "iteration: 108050 loss: 0.0029 lr: 0.02\n",
      "iteration: 108060 loss: 0.0029 lr: 0.02\n",
      "iteration: 108070 loss: 0.0028 lr: 0.02\n",
      "iteration: 108080 loss: 0.0041 lr: 0.02\n",
      "iteration: 108090 loss: 0.0031 lr: 0.02\n",
      "iteration: 108100 loss: 0.0025 lr: 0.02\n",
      "iteration: 108110 loss: 0.0027 lr: 0.02\n",
      "iteration: 108120 loss: 0.0034 lr: 0.02\n",
      "iteration: 108130 loss: 0.0024 lr: 0.02\n",
      "iteration: 108140 loss: 0.0026 lr: 0.02\n",
      "iteration: 108150 loss: 0.0023 lr: 0.02\n",
      "iteration: 108160 loss: 0.0030 lr: 0.02\n",
      "iteration: 108170 loss: 0.0030 lr: 0.02\n",
      "iteration: 108180 loss: 0.0029 lr: 0.02\n",
      "iteration: 108190 loss: 0.0037 lr: 0.02\n",
      "iteration: 108200 loss: 0.0027 lr: 0.02\n",
      "iteration: 108210 loss: 0.0035 lr: 0.02\n",
      "iteration: 108220 loss: 0.0034 lr: 0.02\n",
      "iteration: 108230 loss: 0.0043 lr: 0.02\n",
      "iteration: 108240 loss: 0.0025 lr: 0.02\n",
      "iteration: 108250 loss: 0.0027 lr: 0.02\n",
      "iteration: 108260 loss: 0.0027 lr: 0.02\n",
      "iteration: 108270 loss: 0.0026 lr: 0.02\n",
      "iteration: 108280 loss: 0.0037 lr: 0.02\n",
      "iteration: 108290 loss: 0.0036 lr: 0.02\n",
      "iteration: 108300 loss: 0.0033 lr: 0.02\n",
      "iteration: 108310 loss: 0.0037 lr: 0.02\n",
      "iteration: 108320 loss: 0.0026 lr: 0.02\n",
      "iteration: 108330 loss: 0.0029 lr: 0.02\n",
      "iteration: 108340 loss: 0.0034 lr: 0.02\n",
      "iteration: 108350 loss: 0.0035 lr: 0.02\n",
      "iteration: 108360 loss: 0.0022 lr: 0.02\n",
      "iteration: 108370 loss: 0.0036 lr: 0.02\n",
      "iteration: 108380 loss: 0.0021 lr: 0.02\n",
      "iteration: 108390 loss: 0.0032 lr: 0.02\n",
      "iteration: 108400 loss: 0.0028 lr: 0.02\n",
      "iteration: 108410 loss: 0.0031 lr: 0.02\n",
      "iteration: 108420 loss: 0.0027 lr: 0.02\n",
      "iteration: 108430 loss: 0.0031 lr: 0.02\n",
      "iteration: 108440 loss: 0.0027 lr: 0.02\n",
      "iteration: 108450 loss: 0.0024 lr: 0.02\n",
      "iteration: 108460 loss: 0.0021 lr: 0.02\n",
      "iteration: 108470 loss: 0.0024 lr: 0.02\n",
      "iteration: 108480 loss: 0.0027 lr: 0.02\n",
      "iteration: 108490 loss: 0.0033 lr: 0.02\n",
      "iteration: 108500 loss: 0.0025 lr: 0.02\n",
      "iteration: 108510 loss: 0.0028 lr: 0.02\n",
      "iteration: 108520 loss: 0.0043 lr: 0.02\n",
      "iteration: 108530 loss: 0.0024 lr: 0.02\n",
      "iteration: 108540 loss: 0.0028 lr: 0.02\n",
      "iteration: 108550 loss: 0.0027 lr: 0.02\n",
      "iteration: 108560 loss: 0.0029 lr: 0.02\n",
      "iteration: 108570 loss: 0.0024 lr: 0.02\n",
      "iteration: 108580 loss: 0.0040 lr: 0.02\n",
      "iteration: 108590 loss: 0.0028 lr: 0.02\n",
      "iteration: 108600 loss: 0.0030 lr: 0.02\n",
      "iteration: 108610 loss: 0.0025 lr: 0.02\n",
      "iteration: 108620 loss: 0.0034 lr: 0.02\n",
      "iteration: 108630 loss: 0.0040 lr: 0.02\n",
      "iteration: 108640 loss: 0.0033 lr: 0.02\n",
      "iteration: 108650 loss: 0.0034 lr: 0.02\n",
      "iteration: 108660 loss: 0.0028 lr: 0.02\n",
      "iteration: 108670 loss: 0.0028 lr: 0.02\n",
      "iteration: 108680 loss: 0.0031 lr: 0.02\n",
      "iteration: 108690 loss: 0.0036 lr: 0.02\n",
      "iteration: 108700 loss: 0.0031 lr: 0.02\n",
      "iteration: 108710 loss: 0.0025 lr: 0.02\n",
      "iteration: 108720 loss: 0.0033 lr: 0.02\n",
      "iteration: 108730 loss: 0.0030 lr: 0.02\n",
      "iteration: 108740 loss: 0.0025 lr: 0.02\n",
      "iteration: 108750 loss: 0.0030 lr: 0.02\n",
      "iteration: 108760 loss: 0.0033 lr: 0.02\n",
      "iteration: 108770 loss: 0.0026 lr: 0.02\n",
      "iteration: 108780 loss: 0.0035 lr: 0.02\n",
      "iteration: 108790 loss: 0.0034 lr: 0.02\n",
      "iteration: 108800 loss: 0.0036 lr: 0.02\n",
      "iteration: 108810 loss: 0.0039 lr: 0.02\n",
      "iteration: 108820 loss: 0.0026 lr: 0.02\n",
      "iteration: 108830 loss: 0.0033 lr: 0.02\n",
      "iteration: 108840 loss: 0.0029 lr: 0.02\n",
      "iteration: 108850 loss: 0.0021 lr: 0.02\n",
      "iteration: 108860 loss: 0.0035 lr: 0.02\n",
      "iteration: 108870 loss: 0.0025 lr: 0.02\n",
      "iteration: 108880 loss: 0.0023 lr: 0.02\n",
      "iteration: 108890 loss: 0.0019 lr: 0.02\n",
      "iteration: 108900 loss: 0.0035 lr: 0.02\n",
      "iteration: 108910 loss: 0.0031 lr: 0.02\n",
      "iteration: 108920 loss: 0.0036 lr: 0.02\n",
      "iteration: 108930 loss: 0.0026 lr: 0.02\n",
      "iteration: 108940 loss: 0.0033 lr: 0.02\n",
      "iteration: 108950 loss: 0.0029 lr: 0.02\n",
      "iteration: 108960 loss: 0.0029 lr: 0.02\n",
      "iteration: 108970 loss: 0.0034 lr: 0.02\n",
      "iteration: 108980 loss: 0.0028 lr: 0.02\n",
      "iteration: 108990 loss: 0.0030 lr: 0.02\n",
      "iteration: 109000 loss: 0.0031 lr: 0.02\n",
      "iteration: 109010 loss: 0.0025 lr: 0.02\n",
      "iteration: 109020 loss: 0.0032 lr: 0.02\n",
      "iteration: 109030 loss: 0.0033 lr: 0.02\n",
      "iteration: 109040 loss: 0.0029 lr: 0.02\n",
      "iteration: 109050 loss: 0.0032 lr: 0.02\n",
      "iteration: 109060 loss: 0.0031 lr: 0.02\n",
      "iteration: 109070 loss: 0.0028 lr: 0.02\n",
      "iteration: 109080 loss: 0.0032 lr: 0.02\n",
      "iteration: 109090 loss: 0.0024 lr: 0.02\n",
      "iteration: 109100 loss: 0.0029 lr: 0.02\n",
      "iteration: 109110 loss: 0.0030 lr: 0.02\n",
      "iteration: 109120 loss: 0.0031 lr: 0.02\n",
      "iteration: 109130 loss: 0.0033 lr: 0.02\n",
      "iteration: 109140 loss: 0.0020 lr: 0.02\n",
      "iteration: 109150 loss: 0.0029 lr: 0.02\n",
      "iteration: 109160 loss: 0.0022 lr: 0.02\n",
      "iteration: 109170 loss: 0.0036 lr: 0.02\n",
      "iteration: 109180 loss: 0.0026 lr: 0.02\n",
      "iteration: 109190 loss: 0.0026 lr: 0.02\n",
      "iteration: 109200 loss: 0.0026 lr: 0.02\n",
      "iteration: 109210 loss: 0.0023 lr: 0.02\n",
      "iteration: 109220 loss: 0.0032 lr: 0.02\n",
      "iteration: 109230 loss: 0.0034 lr: 0.02\n",
      "iteration: 109240 loss: 0.0024 lr: 0.02\n",
      "iteration: 109250 loss: 0.0031 lr: 0.02\n",
      "iteration: 109260 loss: 0.0029 lr: 0.02\n",
      "iteration: 109270 loss: 0.0027 lr: 0.02\n",
      "iteration: 109280 loss: 0.0034 lr: 0.02\n",
      "iteration: 109290 loss: 0.0026 lr: 0.02\n",
      "iteration: 109300 loss: 0.0024 lr: 0.02\n",
      "iteration: 109310 loss: 0.0028 lr: 0.02\n",
      "iteration: 109320 loss: 0.0040 lr: 0.02\n",
      "iteration: 109330 loss: 0.0033 lr: 0.02\n",
      "iteration: 109340 loss: 0.0022 lr: 0.02\n",
      "iteration: 109350 loss: 0.0032 lr: 0.02\n",
      "iteration: 109360 loss: 0.0020 lr: 0.02\n",
      "iteration: 109370 loss: 0.0032 lr: 0.02\n",
      "iteration: 109380 loss: 0.0025 lr: 0.02\n",
      "iteration: 109390 loss: 0.0028 lr: 0.02\n",
      "iteration: 109400 loss: 0.0022 lr: 0.02\n",
      "iteration: 109410 loss: 0.0029 lr: 0.02\n",
      "iteration: 109420 loss: 0.0022 lr: 0.02\n",
      "iteration: 109430 loss: 0.0023 lr: 0.02\n",
      "iteration: 109440 loss: 0.0022 lr: 0.02\n",
      "iteration: 109450 loss: 0.0027 lr: 0.02\n",
      "iteration: 109460 loss: 0.0026 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 109470 loss: 0.0021 lr: 0.02\n",
      "iteration: 109480 loss: 0.0029 lr: 0.02\n",
      "iteration: 109490 loss: 0.0029 lr: 0.02\n",
      "iteration: 109500 loss: 0.0028 lr: 0.02\n",
      "iteration: 109510 loss: 0.0028 lr: 0.02\n",
      "iteration: 109520 loss: 0.0021 lr: 0.02\n",
      "iteration: 109530 loss: 0.0026 lr: 0.02\n",
      "iteration: 109540 loss: 0.0026 lr: 0.02\n",
      "iteration: 109550 loss: 0.0024 lr: 0.02\n",
      "iteration: 109560 loss: 0.0028 lr: 0.02\n",
      "iteration: 109570 loss: 0.0025 lr: 0.02\n",
      "iteration: 109580 loss: 0.0026 lr: 0.02\n",
      "iteration: 109590 loss: 0.0025 lr: 0.02\n",
      "iteration: 109600 loss: 0.0027 lr: 0.02\n",
      "iteration: 109610 loss: 0.0025 lr: 0.02\n",
      "iteration: 109620 loss: 0.0028 lr: 0.02\n",
      "iteration: 109630 loss: 0.0028 lr: 0.02\n",
      "iteration: 109640 loss: 0.0024 lr: 0.02\n",
      "iteration: 109650 loss: 0.0029 lr: 0.02\n",
      "iteration: 109660 loss: 0.0026 lr: 0.02\n",
      "iteration: 109670 loss: 0.0022 lr: 0.02\n",
      "iteration: 109680 loss: 0.0029 lr: 0.02\n",
      "iteration: 109690 loss: 0.0024 lr: 0.02\n",
      "iteration: 109700 loss: 0.0021 lr: 0.02\n",
      "iteration: 109710 loss: 0.0031 lr: 0.02\n",
      "iteration: 109720 loss: 0.0031 lr: 0.02\n",
      "iteration: 109730 loss: 0.0030 lr: 0.02\n",
      "iteration: 109740 loss: 0.0035 lr: 0.02\n",
      "iteration: 109750 loss: 0.0030 lr: 0.02\n",
      "iteration: 109760 loss: 0.0035 lr: 0.02\n",
      "iteration: 109770 loss: 0.0021 lr: 0.02\n",
      "iteration: 109780 loss: 0.0025 lr: 0.02\n",
      "iteration: 109790 loss: 0.0026 lr: 0.02\n",
      "iteration: 109800 loss: 0.0031 lr: 0.02\n",
      "iteration: 109810 loss: 0.0027 lr: 0.02\n",
      "iteration: 109820 loss: 0.0036 lr: 0.02\n",
      "iteration: 109830 loss: 0.0028 lr: 0.02\n",
      "iteration: 109840 loss: 0.0031 lr: 0.02\n",
      "iteration: 109850 loss: 0.0034 lr: 0.02\n",
      "iteration: 109860 loss: 0.0025 lr: 0.02\n",
      "iteration: 109870 loss: 0.0028 lr: 0.02\n",
      "iteration: 109880 loss: 0.0027 lr: 0.02\n",
      "iteration: 109890 loss: 0.0032 lr: 0.02\n",
      "iteration: 109900 loss: 0.0025 lr: 0.02\n",
      "iteration: 109910 loss: 0.0026 lr: 0.02\n",
      "iteration: 109920 loss: 0.0034 lr: 0.02\n",
      "iteration: 109930 loss: 0.0024 lr: 0.02\n",
      "iteration: 109940 loss: 0.0021 lr: 0.02\n",
      "iteration: 109950 loss: 0.0031 lr: 0.02\n",
      "iteration: 109960 loss: 0.0031 lr: 0.02\n",
      "iteration: 109970 loss: 0.0028 lr: 0.02\n",
      "iteration: 109980 loss: 0.0031 lr: 0.02\n",
      "iteration: 109990 loss: 0.0025 lr: 0.02\n",
      "iteration: 110000 loss: 0.0025 lr: 0.02\n",
      "iteration: 110010 loss: 0.0027 lr: 0.02\n",
      "iteration: 110020 loss: 0.0026 lr: 0.02\n",
      "iteration: 110030 loss: 0.0022 lr: 0.02\n",
      "iteration: 110040 loss: 0.0025 lr: 0.02\n",
      "iteration: 110050 loss: 0.0023 lr: 0.02\n",
      "iteration: 110060 loss: 0.0023 lr: 0.02\n",
      "iteration: 110070 loss: 0.0021 lr: 0.02\n",
      "iteration: 110080 loss: 0.0034 lr: 0.02\n",
      "iteration: 110090 loss: 0.0029 lr: 0.02\n",
      "iteration: 110100 loss: 0.0040 lr: 0.02\n",
      "iteration: 110110 loss: 0.0031 lr: 0.02\n",
      "iteration: 110120 loss: 0.0034 lr: 0.02\n",
      "iteration: 110130 loss: 0.0029 lr: 0.02\n",
      "iteration: 110140 loss: 0.0030 lr: 0.02\n",
      "iteration: 110150 loss: 0.0030 lr: 0.02\n",
      "iteration: 110160 loss: 0.0041 lr: 0.02\n",
      "iteration: 110170 loss: 0.0043 lr: 0.02\n",
      "iteration: 110180 loss: 0.0030 lr: 0.02\n",
      "iteration: 110190 loss: 0.0040 lr: 0.02\n",
      "iteration: 110200 loss: 0.0035 lr: 0.02\n",
      "iteration: 110210 loss: 0.0035 lr: 0.02\n",
      "iteration: 110220 loss: 0.0039 lr: 0.02\n",
      "iteration: 110230 loss: 0.0021 lr: 0.02\n",
      "iteration: 110240 loss: 0.0030 lr: 0.02\n",
      "iteration: 110250 loss: 0.0032 lr: 0.02\n",
      "iteration: 110260 loss: 0.0035 lr: 0.02\n",
      "iteration: 110270 loss: 0.0030 lr: 0.02\n",
      "iteration: 110280 loss: 0.0025 lr: 0.02\n",
      "iteration: 110290 loss: 0.0025 lr: 0.02\n",
      "iteration: 110300 loss: 0.0025 lr: 0.02\n",
      "iteration: 110310 loss: 0.0025 lr: 0.02\n",
      "iteration: 110320 loss: 0.0023 lr: 0.02\n",
      "iteration: 110330 loss: 0.0040 lr: 0.02\n",
      "iteration: 110340 loss: 0.0024 lr: 0.02\n",
      "iteration: 110350 loss: 0.0024 lr: 0.02\n",
      "iteration: 110360 loss: 0.0018 lr: 0.02\n",
      "iteration: 110370 loss: 0.0032 lr: 0.02\n",
      "iteration: 110380 loss: 0.0024 lr: 0.02\n",
      "iteration: 110390 loss: 0.0026 lr: 0.02\n",
      "iteration: 110400 loss: 0.0027 lr: 0.02\n",
      "iteration: 110410 loss: 0.0028 lr: 0.02\n",
      "iteration: 110420 loss: 0.0025 lr: 0.02\n",
      "iteration: 110430 loss: 0.0040 lr: 0.02\n",
      "iteration: 110440 loss: 0.0027 lr: 0.02\n",
      "iteration: 110450 loss: 0.0028 lr: 0.02\n",
      "iteration: 110460 loss: 0.0032 lr: 0.02\n",
      "iteration: 110470 loss: 0.0026 lr: 0.02\n",
      "iteration: 110480 loss: 0.0039 lr: 0.02\n",
      "iteration: 110490 loss: 0.0025 lr: 0.02\n",
      "iteration: 110500 loss: 0.0026 lr: 0.02\n",
      "iteration: 110510 loss: 0.0025 lr: 0.02\n",
      "iteration: 110520 loss: 0.0034 lr: 0.02\n",
      "iteration: 110530 loss: 0.0029 lr: 0.02\n",
      "iteration: 110540 loss: 0.0028 lr: 0.02\n",
      "iteration: 110550 loss: 0.0030 lr: 0.02\n",
      "iteration: 110560 loss: 0.0024 lr: 0.02\n",
      "iteration: 110570 loss: 0.0029 lr: 0.02\n",
      "iteration: 110580 loss: 0.0026 lr: 0.02\n",
      "iteration: 110590 loss: 0.0028 lr: 0.02\n",
      "iteration: 110600 loss: 0.0036 lr: 0.02\n",
      "iteration: 110610 loss: 0.0047 lr: 0.02\n",
      "iteration: 110620 loss: 0.0029 lr: 0.02\n",
      "iteration: 110630 loss: 0.0026 lr: 0.02\n",
      "iteration: 110640 loss: 0.0028 lr: 0.02\n",
      "iteration: 110650 loss: 0.0033 lr: 0.02\n",
      "iteration: 110660 loss: 0.0045 lr: 0.02\n",
      "iteration: 110670 loss: 0.0025 lr: 0.02\n",
      "iteration: 110680 loss: 0.0020 lr: 0.02\n",
      "iteration: 110690 loss: 0.0027 lr: 0.02\n",
      "iteration: 110700 loss: 0.0031 lr: 0.02\n",
      "iteration: 110710 loss: 0.0030 lr: 0.02\n",
      "iteration: 110720 loss: 0.0036 lr: 0.02\n",
      "iteration: 110730 loss: 0.0028 lr: 0.02\n",
      "iteration: 110740 loss: 0.0026 lr: 0.02\n",
      "iteration: 110750 loss: 0.0026 lr: 0.02\n",
      "iteration: 110760 loss: 0.0033 lr: 0.02\n",
      "iteration: 110770 loss: 0.0035 lr: 0.02\n",
      "iteration: 110780 loss: 0.0031 lr: 0.02\n",
      "iteration: 110790 loss: 0.0035 lr: 0.02\n",
      "iteration: 110800 loss: 0.0031 lr: 0.02\n",
      "iteration: 110810 loss: 0.0035 lr: 0.02\n",
      "iteration: 110820 loss: 0.0029 lr: 0.02\n",
      "iteration: 110830 loss: 0.0026 lr: 0.02\n",
      "iteration: 110840 loss: 0.0030 lr: 0.02\n",
      "iteration: 110850 loss: 0.0035 lr: 0.02\n",
      "iteration: 110860 loss: 0.0026 lr: 0.02\n",
      "iteration: 110870 loss: 0.0022 lr: 0.02\n",
      "iteration: 110880 loss: 0.0027 lr: 0.02\n",
      "iteration: 110890 loss: 0.0024 lr: 0.02\n",
      "iteration: 110900 loss: 0.0023 lr: 0.02\n",
      "iteration: 110910 loss: 0.0038 lr: 0.02\n",
      "iteration: 110920 loss: 0.0025 lr: 0.02\n",
      "iteration: 110930 loss: 0.0034 lr: 0.02\n",
      "iteration: 110940 loss: 0.0024 lr: 0.02\n",
      "iteration: 110950 loss: 0.0025 lr: 0.02\n",
      "iteration: 110960 loss: 0.0023 lr: 0.02\n",
      "iteration: 110970 loss: 0.0032 lr: 0.02\n",
      "iteration: 110980 loss: 0.0029 lr: 0.02\n",
      "iteration: 110990 loss: 0.0025 lr: 0.02\n",
      "iteration: 111000 loss: 0.0032 lr: 0.02\n",
      "iteration: 111010 loss: 0.0031 lr: 0.02\n",
      "iteration: 111020 loss: 0.0028 lr: 0.02\n",
      "iteration: 111030 loss: 0.0030 lr: 0.02\n",
      "iteration: 111040 loss: 0.0024 lr: 0.02\n",
      "iteration: 111050 loss: 0.0036 lr: 0.02\n",
      "iteration: 111060 loss: 0.0029 lr: 0.02\n",
      "iteration: 111070 loss: 0.0037 lr: 0.02\n",
      "iteration: 111080 loss: 0.0039 lr: 0.02\n",
      "iteration: 111090 loss: 0.0032 lr: 0.02\n",
      "iteration: 111100 loss: 0.0023 lr: 0.02\n",
      "iteration: 111110 loss: 0.0028 lr: 0.02\n",
      "iteration: 111120 loss: 0.0029 lr: 0.02\n",
      "iteration: 111130 loss: 0.0027 lr: 0.02\n",
      "iteration: 111140 loss: 0.0027 lr: 0.02\n",
      "iteration: 111150 loss: 0.0025 lr: 0.02\n",
      "iteration: 111160 loss: 0.0036 lr: 0.02\n",
      "iteration: 111170 loss: 0.0025 lr: 0.02\n",
      "iteration: 111180 loss: 0.0030 lr: 0.02\n",
      "iteration: 111190 loss: 0.0021 lr: 0.02\n",
      "iteration: 111200 loss: 0.0028 lr: 0.02\n",
      "iteration: 111210 loss: 0.0032 lr: 0.02\n",
      "iteration: 111220 loss: 0.0021 lr: 0.02\n",
      "iteration: 111230 loss: 0.0033 lr: 0.02\n",
      "iteration: 111240 loss: 0.0023 lr: 0.02\n",
      "iteration: 111250 loss: 0.0023 lr: 0.02\n",
      "iteration: 111260 loss: 0.0023 lr: 0.02\n",
      "iteration: 111270 loss: 0.0029 lr: 0.02\n",
      "iteration: 111280 loss: 0.0023 lr: 0.02\n",
      "iteration: 111290 loss: 0.0022 lr: 0.02\n",
      "iteration: 111300 loss: 0.0026 lr: 0.02\n",
      "iteration: 111310 loss: 0.0024 lr: 0.02\n",
      "iteration: 111320 loss: 0.0040 lr: 0.02\n",
      "iteration: 111330 loss: 0.0031 lr: 0.02\n",
      "iteration: 111340 loss: 0.0024 lr: 0.02\n",
      "iteration: 111350 loss: 0.0031 lr: 0.02\n",
      "iteration: 111360 loss: 0.0025 lr: 0.02\n",
      "iteration: 111370 loss: 0.0029 lr: 0.02\n",
      "iteration: 111380 loss: 0.0033 lr: 0.02\n",
      "iteration: 111390 loss: 0.0021 lr: 0.02\n",
      "iteration: 111400 loss: 0.0031 lr: 0.02\n",
      "iteration: 111410 loss: 0.0021 lr: 0.02\n",
      "iteration: 111420 loss: 0.0029 lr: 0.02\n",
      "iteration: 111430 loss: 0.0039 lr: 0.02\n",
      "iteration: 111440 loss: 0.0021 lr: 0.02\n",
      "iteration: 111450 loss: 0.0027 lr: 0.02\n",
      "iteration: 111460 loss: 0.0027 lr: 0.02\n",
      "iteration: 111470 loss: 0.0029 lr: 0.02\n",
      "iteration: 111480 loss: 0.0026 lr: 0.02\n",
      "iteration: 111490 loss: 0.0028 lr: 0.02\n",
      "iteration: 111500 loss: 0.0023 lr: 0.02\n",
      "iteration: 111510 loss: 0.0021 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 111520 loss: 0.0028 lr: 0.02\n",
      "iteration: 111530 loss: 0.0026 lr: 0.02\n",
      "iteration: 111540 loss: 0.0020 lr: 0.02\n",
      "iteration: 111550 loss: 0.0024 lr: 0.02\n",
      "iteration: 111560 loss: 0.0027 lr: 0.02\n",
      "iteration: 111570 loss: 0.0028 lr: 0.02\n",
      "iteration: 111580 loss: 0.0025 lr: 0.02\n",
      "iteration: 111590 loss: 0.0026 lr: 0.02\n",
      "iteration: 111600 loss: 0.0028 lr: 0.02\n",
      "iteration: 111610 loss: 0.0033 lr: 0.02\n",
      "iteration: 111620 loss: 0.0033 lr: 0.02\n",
      "iteration: 111630 loss: 0.0025 lr: 0.02\n",
      "iteration: 111640 loss: 0.0025 lr: 0.02\n",
      "iteration: 111650 loss: 0.0033 lr: 0.02\n",
      "iteration: 111660 loss: 0.0027 lr: 0.02\n",
      "iteration: 111670 loss: 0.0028 lr: 0.02\n",
      "iteration: 111680 loss: 0.0029 lr: 0.02\n",
      "iteration: 111690 loss: 0.0028 lr: 0.02\n",
      "iteration: 111700 loss: 0.0026 lr: 0.02\n",
      "iteration: 111710 loss: 0.0033 lr: 0.02\n",
      "iteration: 111720 loss: 0.0031 lr: 0.02\n",
      "iteration: 111730 loss: 0.0029 lr: 0.02\n",
      "iteration: 111740 loss: 0.0021 lr: 0.02\n",
      "iteration: 111750 loss: 0.0021 lr: 0.02\n",
      "iteration: 111760 loss: 0.0022 lr: 0.02\n",
      "iteration: 111770 loss: 0.0029 lr: 0.02\n",
      "iteration: 111780 loss: 0.0024 lr: 0.02\n",
      "iteration: 111790 loss: 0.0022 lr: 0.02\n",
      "iteration: 111800 loss: 0.0030 lr: 0.02\n",
      "iteration: 111810 loss: 0.0030 lr: 0.02\n",
      "iteration: 111820 loss: 0.0025 lr: 0.02\n",
      "iteration: 111830 loss: 0.0024 lr: 0.02\n",
      "iteration: 111840 loss: 0.0030 lr: 0.02\n",
      "iteration: 111850 loss: 0.0023 lr: 0.02\n",
      "iteration: 111860 loss: 0.0026 lr: 0.02\n",
      "iteration: 111870 loss: 0.0035 lr: 0.02\n",
      "iteration: 111880 loss: 0.0025 lr: 0.02\n",
      "iteration: 111890 loss: 0.0033 lr: 0.02\n",
      "iteration: 111900 loss: 0.0036 lr: 0.02\n",
      "iteration: 111910 loss: 0.0023 lr: 0.02\n",
      "iteration: 111920 loss: 0.0025 lr: 0.02\n",
      "iteration: 111930 loss: 0.0029 lr: 0.02\n",
      "iteration: 111940 loss: 0.0030 lr: 0.02\n",
      "iteration: 111950 loss: 0.0028 lr: 0.02\n",
      "iteration: 111960 loss: 0.0027 lr: 0.02\n",
      "iteration: 111970 loss: 0.0025 lr: 0.02\n",
      "iteration: 111980 loss: 0.0028 lr: 0.02\n",
      "iteration: 111990 loss: 0.0025 lr: 0.02\n",
      "iteration: 112000 loss: 0.0027 lr: 0.02\n",
      "iteration: 112010 loss: 0.0024 lr: 0.02\n",
      "iteration: 112020 loss: 0.0023 lr: 0.02\n",
      "iteration: 112030 loss: 0.0027 lr: 0.02\n",
      "iteration: 112040 loss: 0.0025 lr: 0.02\n",
      "iteration: 112050 loss: 0.0021 lr: 0.02\n",
      "iteration: 112060 loss: 0.0027 lr: 0.02\n",
      "iteration: 112070 loss: 0.0030 lr: 0.02\n",
      "iteration: 112080 loss: 0.0027 lr: 0.02\n",
      "iteration: 112090 loss: 0.0024 lr: 0.02\n",
      "iteration: 112100 loss: 0.0028 lr: 0.02\n",
      "iteration: 112110 loss: 0.0028 lr: 0.02\n",
      "iteration: 112120 loss: 0.0037 lr: 0.02\n",
      "iteration: 112130 loss: 0.0026 lr: 0.02\n",
      "iteration: 112140 loss: 0.0024 lr: 0.02\n",
      "iteration: 112150 loss: 0.0039 lr: 0.02\n",
      "iteration: 112160 loss: 0.0029 lr: 0.02\n",
      "iteration: 112170 loss: 0.0022 lr: 0.02\n",
      "iteration: 112180 loss: 0.0023 lr: 0.02\n",
      "iteration: 112190 loss: 0.0022 lr: 0.02\n",
      "iteration: 112200 loss: 0.0027 lr: 0.02\n",
      "iteration: 112210 loss: 0.0025 lr: 0.02\n",
      "iteration: 112220 loss: 0.0032 lr: 0.02\n",
      "iteration: 112230 loss: 0.0037 lr: 0.02\n",
      "iteration: 112240 loss: 0.0027 lr: 0.02\n",
      "iteration: 112250 loss: 0.0042 lr: 0.02\n",
      "iteration: 112260 loss: 0.0027 lr: 0.02\n",
      "iteration: 112270 loss: 0.0031 lr: 0.02\n",
      "iteration: 112280 loss: 0.0023 lr: 0.02\n",
      "iteration: 112290 loss: 0.0035 lr: 0.02\n",
      "iteration: 112300 loss: 0.0028 lr: 0.02\n",
      "iteration: 112310 loss: 0.0027 lr: 0.02\n",
      "iteration: 112320 loss: 0.0034 lr: 0.02\n",
      "iteration: 112330 loss: 0.0030 lr: 0.02\n",
      "iteration: 112340 loss: 0.0028 lr: 0.02\n",
      "iteration: 112350 loss: 0.0035 lr: 0.02\n",
      "iteration: 112360 loss: 0.0028 lr: 0.02\n",
      "iteration: 112370 loss: 0.0024 lr: 0.02\n",
      "iteration: 112380 loss: 0.0025 lr: 0.02\n",
      "iteration: 112390 loss: 0.0030 lr: 0.02\n",
      "iteration: 112400 loss: 0.0025 lr: 0.02\n",
      "iteration: 112410 loss: 0.0037 lr: 0.02\n",
      "iteration: 112420 loss: 0.0028 lr: 0.02\n",
      "iteration: 112430 loss: 0.0030 lr: 0.02\n",
      "iteration: 112440 loss: 0.0032 lr: 0.02\n",
      "iteration: 112450 loss: 0.0033 lr: 0.02\n",
      "iteration: 112460 loss: 0.0025 lr: 0.02\n",
      "iteration: 112470 loss: 0.0030 lr: 0.02\n",
      "iteration: 112480 loss: 0.0025 lr: 0.02\n",
      "iteration: 112490 loss: 0.0033 lr: 0.02\n",
      "iteration: 112500 loss: 0.0027 lr: 0.02\n",
      "iteration: 112510 loss: 0.0017 lr: 0.02\n",
      "iteration: 112520 loss: 0.0028 lr: 0.02\n",
      "iteration: 112530 loss: 0.0024 lr: 0.02\n",
      "iteration: 112540 loss: 0.0026 lr: 0.02\n",
      "iteration: 112550 loss: 0.0026 lr: 0.02\n",
      "iteration: 112560 loss: 0.0026 lr: 0.02\n",
      "iteration: 112570 loss: 0.0027 lr: 0.02\n",
      "iteration: 112580 loss: 0.0023 lr: 0.02\n",
      "iteration: 112590 loss: 0.0027 lr: 0.02\n",
      "iteration: 112600 loss: 0.0023 lr: 0.02\n",
      "iteration: 112610 loss: 0.0024 lr: 0.02\n",
      "iteration: 112620 loss: 0.0034 lr: 0.02\n",
      "iteration: 112630 loss: 0.0025 lr: 0.02\n",
      "iteration: 112640 loss: 0.0027 lr: 0.02\n",
      "iteration: 112650 loss: 0.0031 lr: 0.02\n",
      "iteration: 112660 loss: 0.0025 lr: 0.02\n",
      "iteration: 112670 loss: 0.0026 lr: 0.02\n",
      "iteration: 112680 loss: 0.0023 lr: 0.02\n",
      "iteration: 112690 loss: 0.0031 lr: 0.02\n",
      "iteration: 112700 loss: 0.0031 lr: 0.02\n",
      "iteration: 112710 loss: 0.0031 lr: 0.02\n",
      "iteration: 112720 loss: 0.0026 lr: 0.02\n",
      "iteration: 112730 loss: 0.0022 lr: 0.02\n",
      "iteration: 112740 loss: 0.0021 lr: 0.02\n",
      "iteration: 112750 loss: 0.0027 lr: 0.02\n",
      "iteration: 112760 loss: 0.0023 lr: 0.02\n",
      "iteration: 112770 loss: 0.0027 lr: 0.02\n",
      "iteration: 112780 loss: 0.0020 lr: 0.02\n",
      "iteration: 112790 loss: 0.0024 lr: 0.02\n",
      "iteration: 112800 loss: 0.0031 lr: 0.02\n",
      "iteration: 112810 loss: 0.0022 lr: 0.02\n",
      "iteration: 112820 loss: 0.0022 lr: 0.02\n",
      "iteration: 112830 loss: 0.0017 lr: 0.02\n",
      "iteration: 112840 loss: 0.0024 lr: 0.02\n",
      "iteration: 112850 loss: 0.0022 lr: 0.02\n",
      "iteration: 112860 loss: 0.0024 lr: 0.02\n",
      "iteration: 112870 loss: 0.0030 lr: 0.02\n",
      "iteration: 112880 loss: 0.0024 lr: 0.02\n",
      "iteration: 112890 loss: 0.0020 lr: 0.02\n",
      "iteration: 112900 loss: 0.0030 lr: 0.02\n",
      "iteration: 112910 loss: 0.0031 lr: 0.02\n",
      "iteration: 112920 loss: 0.0022 lr: 0.02\n",
      "iteration: 112930 loss: 0.0030 lr: 0.02\n",
      "iteration: 112940 loss: 0.0032 lr: 0.02\n",
      "iteration: 112950 loss: 0.0018 lr: 0.02\n",
      "iteration: 112960 loss: 0.0023 lr: 0.02\n",
      "iteration: 112970 loss: 0.0026 lr: 0.02\n",
      "iteration: 112980 loss: 0.0024 lr: 0.02\n",
      "iteration: 112990 loss: 0.0030 lr: 0.02\n",
      "iteration: 113000 loss: 0.0032 lr: 0.02\n",
      "iteration: 113010 loss: 0.0030 lr: 0.02\n",
      "iteration: 113020 loss: 0.0031 lr: 0.02\n",
      "iteration: 113030 loss: 0.0026 lr: 0.02\n",
      "iteration: 113040 loss: 0.0022 lr: 0.02\n",
      "iteration: 113050 loss: 0.0028 lr: 0.02\n",
      "iteration: 113060 loss: 0.0032 lr: 0.02\n",
      "iteration: 113070 loss: 0.0025 lr: 0.02\n",
      "iteration: 113080 loss: 0.0027 lr: 0.02\n",
      "iteration: 113090 loss: 0.0030 lr: 0.02\n",
      "iteration: 113100 loss: 0.0025 lr: 0.02\n",
      "iteration: 113110 loss: 0.0024 lr: 0.02\n",
      "iteration: 113120 loss: 0.0031 lr: 0.02\n",
      "iteration: 113130 loss: 0.0033 lr: 0.02\n",
      "iteration: 113140 loss: 0.0026 lr: 0.02\n",
      "iteration: 113150 loss: 0.0032 lr: 0.02\n",
      "iteration: 113160 loss: 0.0030 lr: 0.02\n",
      "iteration: 113170 loss: 0.0033 lr: 0.02\n",
      "iteration: 113180 loss: 0.0025 lr: 0.02\n",
      "iteration: 113190 loss: 0.0025 lr: 0.02\n",
      "iteration: 113200 loss: 0.0024 lr: 0.02\n",
      "iteration: 113210 loss: 0.0028 lr: 0.02\n",
      "iteration: 113220 loss: 0.0031 lr: 0.02\n",
      "iteration: 113230 loss: 0.0026 lr: 0.02\n",
      "iteration: 113240 loss: 0.0029 lr: 0.02\n",
      "iteration: 113250 loss: 0.0035 lr: 0.02\n",
      "iteration: 113260 loss: 0.0028 lr: 0.02\n",
      "iteration: 113270 loss: 0.0026 lr: 0.02\n",
      "iteration: 113280 loss: 0.0022 lr: 0.02\n",
      "iteration: 113290 loss: 0.0023 lr: 0.02\n",
      "iteration: 113300 loss: 0.0027 lr: 0.02\n",
      "iteration: 113310 loss: 0.0037 lr: 0.02\n",
      "iteration: 113320 loss: 0.0027 lr: 0.02\n",
      "iteration: 113330 loss: 0.0031 lr: 0.02\n",
      "iteration: 113340 loss: 0.0023 lr: 0.02\n",
      "iteration: 113350 loss: 0.0025 lr: 0.02\n",
      "iteration: 113360 loss: 0.0032 lr: 0.02\n",
      "iteration: 113370 loss: 0.0033 lr: 0.02\n",
      "iteration: 113380 loss: 0.0023 lr: 0.02\n",
      "iteration: 113390 loss: 0.0029 lr: 0.02\n",
      "iteration: 113400 loss: 0.0035 lr: 0.02\n",
      "iteration: 113410 loss: 0.0023 lr: 0.02\n",
      "iteration: 113420 loss: 0.0024 lr: 0.02\n",
      "iteration: 113430 loss: 0.0032 lr: 0.02\n",
      "iteration: 113440 loss: 0.0029 lr: 0.02\n",
      "iteration: 113450 loss: 0.0025 lr: 0.02\n",
      "iteration: 113460 loss: 0.0019 lr: 0.02\n",
      "iteration: 113470 loss: 0.0031 lr: 0.02\n",
      "iteration: 113480 loss: 0.0041 lr: 0.02\n",
      "iteration: 113490 loss: 0.0030 lr: 0.02\n",
      "iteration: 113500 loss: 0.0034 lr: 0.02\n",
      "iteration: 113510 loss: 0.0035 lr: 0.02\n",
      "iteration: 113520 loss: 0.0022 lr: 0.02\n",
      "iteration: 113530 loss: 0.0032 lr: 0.02\n",
      "iteration: 113540 loss: 0.0031 lr: 0.02\n",
      "iteration: 113550 loss: 0.0025 lr: 0.02\n",
      "iteration: 113560 loss: 0.0024 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 113570 loss: 0.0023 lr: 0.02\n",
      "iteration: 113580 loss: 0.0032 lr: 0.02\n",
      "iteration: 113590 loss: 0.0025 lr: 0.02\n",
      "iteration: 113600 loss: 0.0024 lr: 0.02\n",
      "iteration: 113610 loss: 0.0040 lr: 0.02\n",
      "iteration: 113620 loss: 0.0025 lr: 0.02\n",
      "iteration: 113630 loss: 0.0023 lr: 0.02\n",
      "iteration: 113640 loss: 0.0028 lr: 0.02\n",
      "iteration: 113650 loss: 0.0023 lr: 0.02\n",
      "iteration: 113660 loss: 0.0027 lr: 0.02\n",
      "iteration: 113670 loss: 0.0031 lr: 0.02\n",
      "iteration: 113680 loss: 0.0024 lr: 0.02\n",
      "iteration: 113690 loss: 0.0027 lr: 0.02\n",
      "iteration: 113700 loss: 0.0029 lr: 0.02\n",
      "iteration: 113710 loss: 0.0027 lr: 0.02\n",
      "iteration: 113720 loss: 0.0023 lr: 0.02\n",
      "iteration: 113730 loss: 0.0032 lr: 0.02\n",
      "iteration: 113740 loss: 0.0021 lr: 0.02\n",
      "iteration: 113750 loss: 0.0031 lr: 0.02\n",
      "iteration: 113760 loss: 0.0036 lr: 0.02\n",
      "iteration: 113770 loss: 0.0024 lr: 0.02\n",
      "iteration: 113780 loss: 0.0027 lr: 0.02\n",
      "iteration: 113790 loss: 0.0027 lr: 0.02\n",
      "iteration: 113800 loss: 0.0027 lr: 0.02\n",
      "iteration: 113810 loss: 0.0034 lr: 0.02\n",
      "iteration: 113820 loss: 0.0032 lr: 0.02\n",
      "iteration: 113830 loss: 0.0026 lr: 0.02\n",
      "iteration: 113840 loss: 0.0027 lr: 0.02\n",
      "iteration: 113850 loss: 0.0032 lr: 0.02\n",
      "iteration: 113860 loss: 0.0024 lr: 0.02\n",
      "iteration: 113870 loss: 0.0026 lr: 0.02\n",
      "iteration: 113880 loss: 0.0038 lr: 0.02\n",
      "iteration: 113890 loss: 0.0022 lr: 0.02\n",
      "iteration: 113900 loss: 0.0030 lr: 0.02\n",
      "iteration: 113910 loss: 0.0026 lr: 0.02\n",
      "iteration: 113920 loss: 0.0031 lr: 0.02\n",
      "iteration: 113930 loss: 0.0030 lr: 0.02\n",
      "iteration: 113940 loss: 0.0028 lr: 0.02\n",
      "iteration: 113950 loss: 0.0024 lr: 0.02\n",
      "iteration: 113960 loss: 0.0022 lr: 0.02\n",
      "iteration: 113970 loss: 0.0021 lr: 0.02\n",
      "iteration: 113980 loss: 0.0018 lr: 0.02\n",
      "iteration: 113990 loss: 0.0033 lr: 0.02\n",
      "iteration: 114000 loss: 0.0033 lr: 0.02\n",
      "iteration: 114010 loss: 0.0024 lr: 0.02\n",
      "iteration: 114020 loss: 0.0025 lr: 0.02\n",
      "iteration: 114030 loss: 0.0024 lr: 0.02\n",
      "iteration: 114040 loss: 0.0034 lr: 0.02\n",
      "iteration: 114050 loss: 0.0028 lr: 0.02\n",
      "iteration: 114060 loss: 0.0028 lr: 0.02\n",
      "iteration: 114070 loss: 0.0020 lr: 0.02\n",
      "iteration: 114080 loss: 0.0028 lr: 0.02\n",
      "iteration: 114090 loss: 0.0020 lr: 0.02\n",
      "iteration: 114100 loss: 0.0024 lr: 0.02\n",
      "iteration: 114110 loss: 0.0026 lr: 0.02\n",
      "iteration: 114120 loss: 0.0028 lr: 0.02\n",
      "iteration: 114130 loss: 0.0031 lr: 0.02\n",
      "iteration: 114140 loss: 0.0035 lr: 0.02\n",
      "iteration: 114150 loss: 0.0022 lr: 0.02\n",
      "iteration: 114160 loss: 0.0030 lr: 0.02\n",
      "iteration: 114170 loss: 0.0026 lr: 0.02\n",
      "iteration: 114180 loss: 0.0026 lr: 0.02\n",
      "iteration: 114190 loss: 0.0019 lr: 0.02\n",
      "iteration: 114200 loss: 0.0024 lr: 0.02\n",
      "iteration: 114210 loss: 0.0021 lr: 0.02\n",
      "iteration: 114220 loss: 0.0029 lr: 0.02\n",
      "iteration: 114230 loss: 0.0021 lr: 0.02\n",
      "iteration: 114240 loss: 0.0023 lr: 0.02\n",
      "iteration: 114250 loss: 0.0022 lr: 0.02\n",
      "iteration: 114260 loss: 0.0019 lr: 0.02\n",
      "iteration: 114270 loss: 0.0030 lr: 0.02\n",
      "iteration: 114280 loss: 0.0028 lr: 0.02\n",
      "iteration: 114290 loss: 0.0026 lr: 0.02\n",
      "iteration: 114300 loss: 0.0030 lr: 0.02\n",
      "iteration: 114310 loss: 0.0036 lr: 0.02\n",
      "iteration: 114320 loss: 0.0029 lr: 0.02\n",
      "iteration: 114330 loss: 0.0026 lr: 0.02\n",
      "iteration: 114340 loss: 0.0036 lr: 0.02\n",
      "iteration: 114350 loss: 0.0025 lr: 0.02\n",
      "iteration: 114360 loss: 0.0028 lr: 0.02\n",
      "iteration: 114370 loss: 0.0023 lr: 0.02\n",
      "iteration: 114380 loss: 0.0028 lr: 0.02\n",
      "iteration: 114390 loss: 0.0038 lr: 0.02\n",
      "iteration: 114400 loss: 0.0032 lr: 0.02\n",
      "iteration: 114410 loss: 0.0027 lr: 0.02\n",
      "iteration: 114420 loss: 0.0028 lr: 0.02\n",
      "iteration: 114430 loss: 0.0030 lr: 0.02\n",
      "iteration: 114440 loss: 0.0024 lr: 0.02\n",
      "iteration: 114450 loss: 0.0023 lr: 0.02\n",
      "iteration: 114460 loss: 0.0026 lr: 0.02\n",
      "iteration: 114470 loss: 0.0023 lr: 0.02\n",
      "iteration: 114480 loss: 0.0024 lr: 0.02\n",
      "iteration: 114490 loss: 0.0033 lr: 0.02\n",
      "iteration: 114500 loss: 0.0031 lr: 0.02\n",
      "iteration: 114510 loss: 0.0031 lr: 0.02\n",
      "iteration: 114520 loss: 0.0034 lr: 0.02\n",
      "iteration: 114530 loss: 0.0027 lr: 0.02\n",
      "iteration: 114540 loss: 0.0030 lr: 0.02\n",
      "iteration: 114550 loss: 0.0028 lr: 0.02\n",
      "iteration: 114560 loss: 0.0030 lr: 0.02\n",
      "iteration: 114570 loss: 0.0032 lr: 0.02\n",
      "iteration: 114580 loss: 0.0038 lr: 0.02\n",
      "iteration: 114590 loss: 0.0024 lr: 0.02\n",
      "iteration: 114600 loss: 0.0032 lr: 0.02\n",
      "iteration: 114610 loss: 0.0027 lr: 0.02\n",
      "iteration: 114620 loss: 0.0037 lr: 0.02\n",
      "iteration: 114630 loss: 0.0030 lr: 0.02\n",
      "iteration: 114640 loss: 0.0024 lr: 0.02\n",
      "iteration: 114650 loss: 0.0030 lr: 0.02\n",
      "iteration: 114660 loss: 0.0024 lr: 0.02\n",
      "iteration: 114670 loss: 0.0022 lr: 0.02\n",
      "iteration: 114680 loss: 0.0029 lr: 0.02\n",
      "iteration: 114690 loss: 0.0024 lr: 0.02\n",
      "iteration: 114700 loss: 0.0025 lr: 0.02\n",
      "iteration: 114710 loss: 0.0028 lr: 0.02\n",
      "iteration: 114720 loss: 0.0020 lr: 0.02\n",
      "iteration: 114730 loss: 0.0026 lr: 0.02\n",
      "iteration: 114740 loss: 0.0023 lr: 0.02\n",
      "iteration: 114750 loss: 0.0033 lr: 0.02\n",
      "iteration: 114760 loss: 0.0024 lr: 0.02\n",
      "iteration: 114770 loss: 0.0031 lr: 0.02\n",
      "iteration: 114780 loss: 0.0026 lr: 0.02\n",
      "iteration: 114790 loss: 0.0024 lr: 0.02\n",
      "iteration: 114800 loss: 0.0025 lr: 0.02\n",
      "iteration: 114810 loss: 0.0025 lr: 0.02\n",
      "iteration: 114820 loss: 0.0023 lr: 0.02\n",
      "iteration: 114830 loss: 0.0033 lr: 0.02\n",
      "iteration: 114840 loss: 0.0032 lr: 0.02\n",
      "iteration: 114850 loss: 0.0028 lr: 0.02\n",
      "iteration: 114860 loss: 0.0023 lr: 0.02\n",
      "iteration: 114870 loss: 0.0025 lr: 0.02\n",
      "iteration: 114880 loss: 0.0037 lr: 0.02\n",
      "iteration: 114890 loss: 0.0027 lr: 0.02\n",
      "iteration: 114900 loss: 0.0025 lr: 0.02\n",
      "iteration: 114910 loss: 0.0021 lr: 0.02\n",
      "iteration: 114920 loss: 0.0027 lr: 0.02\n",
      "iteration: 114930 loss: 0.0026 lr: 0.02\n",
      "iteration: 114940 loss: 0.0026 lr: 0.02\n",
      "iteration: 114950 loss: 0.0026 lr: 0.02\n",
      "iteration: 114960 loss: 0.0031 lr: 0.02\n",
      "iteration: 114970 loss: 0.0020 lr: 0.02\n",
      "iteration: 114980 loss: 0.0023 lr: 0.02\n",
      "iteration: 114990 loss: 0.0026 lr: 0.02\n",
      "iteration: 115000 loss: 0.0021 lr: 0.02\n",
      "iteration: 115010 loss: 0.0040 lr: 0.02\n",
      "iteration: 115020 loss: 0.0025 lr: 0.02\n",
      "iteration: 115030 loss: 0.0028 lr: 0.02\n",
      "iteration: 115040 loss: 0.0021 lr: 0.02\n",
      "iteration: 115050 loss: 0.0018 lr: 0.02\n",
      "iteration: 115060 loss: 0.0023 lr: 0.02\n",
      "iteration: 115070 loss: 0.0031 lr: 0.02\n",
      "iteration: 115080 loss: 0.0033 lr: 0.02\n",
      "iteration: 115090 loss: 0.0021 lr: 0.02\n",
      "iteration: 115100 loss: 0.0022 lr: 0.02\n",
      "iteration: 115110 loss: 0.0024 lr: 0.02\n",
      "iteration: 115120 loss: 0.0025 lr: 0.02\n",
      "iteration: 115130 loss: 0.0027 lr: 0.02\n",
      "iteration: 115140 loss: 0.0030 lr: 0.02\n",
      "iteration: 115150 loss: 0.0032 lr: 0.02\n",
      "iteration: 115160 loss: 0.0026 lr: 0.02\n",
      "iteration: 115170 loss: 0.0060 lr: 0.02\n",
      "iteration: 115180 loss: 0.0047 lr: 0.02\n",
      "iteration: 115190 loss: 0.0028 lr: 0.02\n",
      "iteration: 115200 loss: 0.0026 lr: 0.02\n",
      "iteration: 115210 loss: 0.0029 lr: 0.02\n",
      "iteration: 115220 loss: 0.0024 lr: 0.02\n",
      "iteration: 115230 loss: 0.0033 lr: 0.02\n",
      "iteration: 115240 loss: 0.0028 lr: 0.02\n",
      "iteration: 115250 loss: 0.0027 lr: 0.02\n",
      "iteration: 115260 loss: 0.0036 lr: 0.02\n",
      "iteration: 115270 loss: 0.0031 lr: 0.02\n",
      "iteration: 115280 loss: 0.0024 lr: 0.02\n",
      "iteration: 115290 loss: 0.0037 lr: 0.02\n",
      "iteration: 115300 loss: 0.0025 lr: 0.02\n",
      "iteration: 115310 loss: 0.0025 lr: 0.02\n",
      "iteration: 115320 loss: 0.0024 lr: 0.02\n",
      "iteration: 115330 loss: 0.0026 lr: 0.02\n",
      "iteration: 115340 loss: 0.0032 lr: 0.02\n",
      "iteration: 115350 loss: 0.0027 lr: 0.02\n",
      "iteration: 115360 loss: 0.0030 lr: 0.02\n",
      "iteration: 115370 loss: 0.0033 lr: 0.02\n",
      "iteration: 115380 loss: 0.0023 lr: 0.02\n",
      "iteration: 115390 loss: 0.0029 lr: 0.02\n",
      "iteration: 115400 loss: 0.0023 lr: 0.02\n",
      "iteration: 115410 loss: 0.0028 lr: 0.02\n",
      "iteration: 115420 loss: 0.0023 lr: 0.02\n",
      "iteration: 115430 loss: 0.0025 lr: 0.02\n",
      "iteration: 115440 loss: 0.0020 lr: 0.02\n",
      "iteration: 115450 loss: 0.0020 lr: 0.02\n",
      "iteration: 115460 loss: 0.0019 lr: 0.02\n",
      "iteration: 115470 loss: 0.0027 lr: 0.02\n",
      "iteration: 115480 loss: 0.0046 lr: 0.02\n",
      "iteration: 115490 loss: 0.0028 lr: 0.02\n",
      "iteration: 115500 loss: 0.0026 lr: 0.02\n",
      "iteration: 115510 loss: 0.0027 lr: 0.02\n",
      "iteration: 115520 loss: 0.0024 lr: 0.02\n",
      "iteration: 115530 loss: 0.0026 lr: 0.02\n",
      "iteration: 115540 loss: 0.0024 lr: 0.02\n",
      "iteration: 115550 loss: 0.0041 lr: 0.02\n",
      "iteration: 115560 loss: 0.0026 lr: 0.02\n",
      "iteration: 115570 loss: 0.0022 lr: 0.02\n",
      "iteration: 115580 loss: 0.0026 lr: 0.02\n",
      "iteration: 115590 loss: 0.0033 lr: 0.02\n",
      "iteration: 115600 loss: 0.0028 lr: 0.02\n",
      "iteration: 115610 loss: 0.0030 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 115620 loss: 0.0024 lr: 0.02\n",
      "iteration: 115630 loss: 0.0029 lr: 0.02\n",
      "iteration: 115640 loss: 0.0031 lr: 0.02\n",
      "iteration: 115650 loss: 0.0021 lr: 0.02\n",
      "iteration: 115660 loss: 0.0020 lr: 0.02\n",
      "iteration: 115670 loss: 0.0020 lr: 0.02\n",
      "iteration: 115680 loss: 0.0025 lr: 0.02\n",
      "iteration: 115690 loss: 0.0027 lr: 0.02\n",
      "iteration: 115700 loss: 0.0029 lr: 0.02\n",
      "iteration: 115710 loss: 0.0030 lr: 0.02\n",
      "iteration: 115720 loss: 0.0033 lr: 0.02\n",
      "iteration: 115730 loss: 0.0026 lr: 0.02\n",
      "iteration: 115740 loss: 0.0024 lr: 0.02\n",
      "iteration: 115750 loss: 0.0028 lr: 0.02\n",
      "iteration: 115760 loss: 0.0029 lr: 0.02\n",
      "iteration: 115770 loss: 0.0030 lr: 0.02\n",
      "iteration: 115780 loss: 0.0030 lr: 0.02\n",
      "iteration: 115790 loss: 0.0027 lr: 0.02\n",
      "iteration: 115800 loss: 0.0029 lr: 0.02\n",
      "iteration: 115810 loss: 0.0028 lr: 0.02\n",
      "iteration: 115820 loss: 0.0023 lr: 0.02\n",
      "iteration: 115830 loss: 0.0019 lr: 0.02\n",
      "iteration: 115840 loss: 0.0026 lr: 0.02\n",
      "iteration: 115850 loss: 0.0028 lr: 0.02\n",
      "iteration: 115860 loss: 0.0036 lr: 0.02\n",
      "iteration: 115870 loss: 0.0025 lr: 0.02\n",
      "iteration: 115880 loss: 0.0027 lr: 0.02\n",
      "iteration: 115890 loss: 0.0035 lr: 0.02\n",
      "iteration: 115900 loss: 0.0026 lr: 0.02\n",
      "iteration: 115910 loss: 0.0021 lr: 0.02\n",
      "iteration: 115920 loss: 0.0030 lr: 0.02\n",
      "iteration: 115930 loss: 0.0038 lr: 0.02\n",
      "iteration: 115940 loss: 0.0030 lr: 0.02\n",
      "iteration: 115950 loss: 0.0053 lr: 0.02\n",
      "iteration: 115960 loss: 0.0034 lr: 0.02\n",
      "iteration: 115970 loss: 0.0022 lr: 0.02\n",
      "iteration: 115980 loss: 0.0026 lr: 0.02\n",
      "iteration: 115990 loss: 0.0024 lr: 0.02\n",
      "iteration: 116000 loss: 0.0023 lr: 0.02\n",
      "iteration: 116010 loss: 0.0026 lr: 0.02\n",
      "iteration: 116020 loss: 0.0026 lr: 0.02\n",
      "iteration: 116030 loss: 0.0029 lr: 0.02\n",
      "iteration: 116040 loss: 0.0028 lr: 0.02\n",
      "iteration: 116050 loss: 0.0027 lr: 0.02\n",
      "iteration: 116060 loss: 0.0028 lr: 0.02\n",
      "iteration: 116070 loss: 0.0027 lr: 0.02\n",
      "iteration: 116080 loss: 0.0028 lr: 0.02\n",
      "iteration: 116090 loss: 0.0037 lr: 0.02\n",
      "iteration: 116100 loss: 0.0031 lr: 0.02\n",
      "iteration: 116110 loss: 0.0037 lr: 0.02\n",
      "iteration: 116120 loss: 0.0034 lr: 0.02\n",
      "iteration: 116130 loss: 0.0023 lr: 0.02\n",
      "iteration: 116140 loss: 0.0023 lr: 0.02\n",
      "iteration: 116150 loss: 0.0020 lr: 0.02\n",
      "iteration: 116160 loss: 0.0027 lr: 0.02\n",
      "iteration: 116170 loss: 0.0029 lr: 0.02\n",
      "iteration: 116180 loss: 0.0025 lr: 0.02\n",
      "iteration: 116190 loss: 0.0036 lr: 0.02\n",
      "iteration: 116200 loss: 0.0032 lr: 0.02\n",
      "iteration: 116210 loss: 0.0023 lr: 0.02\n",
      "iteration: 116220 loss: 0.0028 lr: 0.02\n",
      "iteration: 116230 loss: 0.0034 lr: 0.02\n",
      "iteration: 116240 loss: 0.0028 lr: 0.02\n",
      "iteration: 116250 loss: 0.0025 lr: 0.02\n",
      "iteration: 116260 loss: 0.0027 lr: 0.02\n",
      "iteration: 116270 loss: 0.0031 lr: 0.02\n",
      "iteration: 116280 loss: 0.0034 lr: 0.02\n",
      "iteration: 116290 loss: 0.0021 lr: 0.02\n",
      "iteration: 116300 loss: 0.0018 lr: 0.02\n",
      "iteration: 116310 loss: 0.0038 lr: 0.02\n",
      "iteration: 116320 loss: 0.0031 lr: 0.02\n",
      "iteration: 116330 loss: 0.0024 lr: 0.02\n",
      "iteration: 116340 loss: 0.0029 lr: 0.02\n",
      "iteration: 116350 loss: 0.0028 lr: 0.02\n",
      "iteration: 116360 loss: 0.0027 lr: 0.02\n",
      "iteration: 116370 loss: 0.0029 lr: 0.02\n",
      "iteration: 116380 loss: 0.0027 lr: 0.02\n",
      "iteration: 116390 loss: 0.0024 lr: 0.02\n",
      "iteration: 116400 loss: 0.0032 lr: 0.02\n",
      "iteration: 116410 loss: 0.0028 lr: 0.02\n",
      "iteration: 116420 loss: 0.0030 lr: 0.02\n",
      "iteration: 116430 loss: 0.0020 lr: 0.02\n",
      "iteration: 116440 loss: 0.0022 lr: 0.02\n",
      "iteration: 116450 loss: 0.0027 lr: 0.02\n",
      "iteration: 116460 loss: 0.0033 lr: 0.02\n",
      "iteration: 116470 loss: 0.0027 lr: 0.02\n",
      "iteration: 116480 loss: 0.0030 lr: 0.02\n",
      "iteration: 116490 loss: 0.0026 lr: 0.02\n",
      "iteration: 116500 loss: 0.0028 lr: 0.02\n",
      "iteration: 116510 loss: 0.0024 lr: 0.02\n",
      "iteration: 116520 loss: 0.0027 lr: 0.02\n",
      "iteration: 116530 loss: 0.0025 lr: 0.02\n",
      "iteration: 116540 loss: 0.0027 lr: 0.02\n",
      "iteration: 116550 loss: 0.0025 lr: 0.02\n",
      "iteration: 116560 loss: 0.0022 lr: 0.02\n",
      "iteration: 116570 loss: 0.0024 lr: 0.02\n",
      "iteration: 116580 loss: 0.0024 lr: 0.02\n",
      "iteration: 116590 loss: 0.0026 lr: 0.02\n",
      "iteration: 116600 loss: 0.0023 lr: 0.02\n",
      "iteration: 116610 loss: 0.0032 lr: 0.02\n",
      "iteration: 116620 loss: 0.0025 lr: 0.02\n",
      "iteration: 116630 loss: 0.0026 lr: 0.02\n",
      "iteration: 116640 loss: 0.0034 lr: 0.02\n",
      "iteration: 116650 loss: 0.0030 lr: 0.02\n",
      "iteration: 116660 loss: 0.0033 lr: 0.02\n",
      "iteration: 116670 loss: 0.0033 lr: 0.02\n",
      "iteration: 116680 loss: 0.0028 lr: 0.02\n",
      "iteration: 116690 loss: 0.0025 lr: 0.02\n",
      "iteration: 116700 loss: 0.0030 lr: 0.02\n",
      "iteration: 116710 loss: 0.0028 lr: 0.02\n",
      "iteration: 116720 loss: 0.0024 lr: 0.02\n",
      "iteration: 116730 loss: 0.0024 lr: 0.02\n",
      "iteration: 116740 loss: 0.0030 lr: 0.02\n",
      "iteration: 116750 loss: 0.0026 lr: 0.02\n",
      "iteration: 116760 loss: 0.0023 lr: 0.02\n",
      "iteration: 116770 loss: 0.0023 lr: 0.02\n",
      "iteration: 116780 loss: 0.0024 lr: 0.02\n",
      "iteration: 116790 loss: 0.0022 lr: 0.02\n",
      "iteration: 116800 loss: 0.0029 lr: 0.02\n",
      "iteration: 116810 loss: 0.0027 lr: 0.02\n",
      "iteration: 116820 loss: 0.0027 lr: 0.02\n",
      "iteration: 116830 loss: 0.0035 lr: 0.02\n",
      "iteration: 116840 loss: 0.0029 lr: 0.02\n",
      "iteration: 116850 loss: 0.0028 lr: 0.02\n",
      "iteration: 116860 loss: 0.0020 lr: 0.02\n",
      "iteration: 116870 loss: 0.0027 lr: 0.02\n",
      "iteration: 116880 loss: 0.0029 lr: 0.02\n",
      "iteration: 116890 loss: 0.0029 lr: 0.02\n",
      "iteration: 116900 loss: 0.0030 lr: 0.02\n",
      "iteration: 116910 loss: 0.0025 lr: 0.02\n",
      "iteration: 116920 loss: 0.0024 lr: 0.02\n",
      "iteration: 116930 loss: 0.0028 lr: 0.02\n",
      "iteration: 116940 loss: 0.0029 lr: 0.02\n",
      "iteration: 116950 loss: 0.0038 lr: 0.02\n",
      "iteration: 116960 loss: 0.0027 lr: 0.02\n",
      "iteration: 116970 loss: 0.0023 lr: 0.02\n",
      "iteration: 116980 loss: 0.0024 lr: 0.02\n",
      "iteration: 116990 loss: 0.0026 lr: 0.02\n",
      "iteration: 117000 loss: 0.0029 lr: 0.02\n",
      "iteration: 117010 loss: 0.0022 lr: 0.02\n",
      "iteration: 117020 loss: 0.0019 lr: 0.02\n",
      "iteration: 117030 loss: 0.0030 lr: 0.02\n",
      "iteration: 117040 loss: 0.0023 lr: 0.02\n",
      "iteration: 117050 loss: 0.0030 lr: 0.02\n",
      "iteration: 117060 loss: 0.0032 lr: 0.02\n",
      "iteration: 117070 loss: 0.0028 lr: 0.02\n",
      "iteration: 117080 loss: 0.0026 lr: 0.02\n",
      "iteration: 117090 loss: 0.0024 lr: 0.02\n",
      "iteration: 117100 loss: 0.0028 lr: 0.02\n",
      "iteration: 117110 loss: 0.0028 lr: 0.02\n",
      "iteration: 117120 loss: 0.0025 lr: 0.02\n",
      "iteration: 117130 loss: 0.0033 lr: 0.02\n",
      "iteration: 117140 loss: 0.0027 lr: 0.02\n",
      "iteration: 117150 loss: 0.0024 lr: 0.02\n",
      "iteration: 117160 loss: 0.0028 lr: 0.02\n",
      "iteration: 117170 loss: 0.0024 lr: 0.02\n",
      "iteration: 117180 loss: 0.0022 lr: 0.02\n",
      "iteration: 117190 loss: 0.0027 lr: 0.02\n",
      "iteration: 117200 loss: 0.0030 lr: 0.02\n",
      "iteration: 117210 loss: 0.0038 lr: 0.02\n",
      "iteration: 117220 loss: 0.0030 lr: 0.02\n",
      "iteration: 117230 loss: 0.0030 lr: 0.02\n",
      "iteration: 117240 loss: 0.0024 lr: 0.02\n",
      "iteration: 117250 loss: 0.0031 lr: 0.02\n",
      "iteration: 117260 loss: 0.0029 lr: 0.02\n",
      "iteration: 117270 loss: 0.0029 lr: 0.02\n",
      "iteration: 117280 loss: 0.0037 lr: 0.02\n",
      "iteration: 117290 loss: 0.0027 lr: 0.02\n",
      "iteration: 117300 loss: 0.0017 lr: 0.02\n",
      "iteration: 117310 loss: 0.0027 lr: 0.02\n",
      "iteration: 117320 loss: 0.0024 lr: 0.02\n",
      "iteration: 117330 loss: 0.0023 lr: 0.02\n",
      "iteration: 117340 loss: 0.0026 lr: 0.02\n",
      "iteration: 117350 loss: 0.0028 lr: 0.02\n",
      "iteration: 117360 loss: 0.0023 lr: 0.02\n",
      "iteration: 117370 loss: 0.0025 lr: 0.02\n",
      "iteration: 117380 loss: 0.0025 lr: 0.02\n",
      "iteration: 117390 loss: 0.0021 lr: 0.02\n",
      "iteration: 117400 loss: 0.0024 lr: 0.02\n",
      "iteration: 117410 loss: 0.0019 lr: 0.02\n",
      "iteration: 117420 loss: 0.0025 lr: 0.02\n",
      "iteration: 117430 loss: 0.0027 lr: 0.02\n",
      "iteration: 117440 loss: 0.0029 lr: 0.02\n",
      "iteration: 117450 loss: 0.0024 lr: 0.02\n",
      "iteration: 117460 loss: 0.0026 lr: 0.02\n",
      "iteration: 117470 loss: 0.0024 lr: 0.02\n",
      "iteration: 117480 loss: 0.0025 lr: 0.02\n",
      "iteration: 117490 loss: 0.0025 lr: 0.02\n",
      "iteration: 117500 loss: 0.0026 lr: 0.02\n",
      "iteration: 117510 loss: 0.0026 lr: 0.02\n",
      "iteration: 117520 loss: 0.0024 lr: 0.02\n",
      "iteration: 117530 loss: 0.0024 lr: 0.02\n",
      "iteration: 117540 loss: 0.0025 lr: 0.02\n",
      "iteration: 117550 loss: 0.0033 lr: 0.02\n",
      "iteration: 117560 loss: 0.0022 lr: 0.02\n",
      "iteration: 117570 loss: 0.0025 lr: 0.02\n",
      "iteration: 117580 loss: 0.0034 lr: 0.02\n",
      "iteration: 117590 loss: 0.0034 lr: 0.02\n",
      "iteration: 117600 loss: 0.0035 lr: 0.02\n",
      "iteration: 117610 loss: 0.0027 lr: 0.02\n",
      "iteration: 117620 loss: 0.0031 lr: 0.02\n",
      "iteration: 117630 loss: 0.0025 lr: 0.02\n",
      "iteration: 117640 loss: 0.0033 lr: 0.02\n",
      "iteration: 117650 loss: 0.0027 lr: 0.02\n",
      "iteration: 117660 loss: 0.0023 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 117670 loss: 0.0030 lr: 0.02\n",
      "iteration: 117680 loss: 0.0023 lr: 0.02\n",
      "iteration: 117690 loss: 0.0029 lr: 0.02\n",
      "iteration: 117700 loss: 0.0021 lr: 0.02\n",
      "iteration: 117710 loss: 0.0027 lr: 0.02\n",
      "iteration: 117720 loss: 0.0020 lr: 0.02\n",
      "iteration: 117730 loss: 0.0025 lr: 0.02\n",
      "iteration: 117740 loss: 0.0026 lr: 0.02\n",
      "iteration: 117750 loss: 0.0024 lr: 0.02\n",
      "iteration: 117760 loss: 0.0039 lr: 0.02\n",
      "iteration: 117770 loss: 0.0023 lr: 0.02\n",
      "iteration: 117780 loss: 0.0022 lr: 0.02\n",
      "iteration: 117790 loss: 0.0021 lr: 0.02\n",
      "iteration: 117800 loss: 0.0029 lr: 0.02\n",
      "iteration: 117810 loss: 0.0024 lr: 0.02\n",
      "iteration: 117820 loss: 0.0020 lr: 0.02\n",
      "iteration: 117830 loss: 0.0020 lr: 0.02\n",
      "iteration: 117840 loss: 0.0020 lr: 0.02\n",
      "iteration: 117850 loss: 0.0025 lr: 0.02\n",
      "iteration: 117860 loss: 0.0038 lr: 0.02\n",
      "iteration: 117870 loss: 0.0020 lr: 0.02\n",
      "iteration: 117880 loss: 0.0037 lr: 0.02\n",
      "iteration: 117890 loss: 0.0026 lr: 0.02\n",
      "iteration: 117900 loss: 0.0030 lr: 0.02\n",
      "iteration: 117910 loss: 0.0030 lr: 0.02\n",
      "iteration: 117920 loss: 0.0025 lr: 0.02\n",
      "iteration: 117930 loss: 0.0024 lr: 0.02\n",
      "iteration: 117940 loss: 0.0049 lr: 0.02\n",
      "iteration: 117950 loss: 0.0026 lr: 0.02\n",
      "iteration: 117960 loss: 0.0035 lr: 0.02\n",
      "iteration: 117970 loss: 0.0035 lr: 0.02\n",
      "iteration: 117980 loss: 0.0025 lr: 0.02\n",
      "iteration: 117990 loss: 0.0026 lr: 0.02\n",
      "iteration: 118000 loss: 0.0024 lr: 0.02\n",
      "iteration: 118010 loss: 0.0032 lr: 0.02\n",
      "iteration: 118020 loss: 0.0035 lr: 0.02\n",
      "iteration: 118030 loss: 0.0034 lr: 0.02\n",
      "iteration: 118040 loss: 0.0025 lr: 0.02\n",
      "iteration: 118050 loss: 0.0023 lr: 0.02\n",
      "iteration: 118060 loss: 0.0029 lr: 0.02\n",
      "iteration: 118070 loss: 0.0024 lr: 0.02\n",
      "iteration: 118080 loss: 0.0031 lr: 0.02\n",
      "iteration: 118090 loss: 0.0021 lr: 0.02\n",
      "iteration: 118100 loss: 0.0038 lr: 0.02\n",
      "iteration: 118110 loss: 0.0022 lr: 0.02\n",
      "iteration: 118120 loss: 0.0030 lr: 0.02\n",
      "iteration: 118130 loss: 0.0022 lr: 0.02\n",
      "iteration: 118140 loss: 0.0026 lr: 0.02\n",
      "iteration: 118150 loss: 0.0021 lr: 0.02\n",
      "iteration: 118160 loss: 0.0037 lr: 0.02\n",
      "iteration: 118170 loss: 0.0024 lr: 0.02\n",
      "iteration: 118180 loss: 0.0033 lr: 0.02\n",
      "iteration: 118190 loss: 0.0024 lr: 0.02\n",
      "iteration: 118200 loss: 0.0024 lr: 0.02\n",
      "iteration: 118210 loss: 0.0026 lr: 0.02\n",
      "iteration: 118220 loss: 0.0033 lr: 0.02\n",
      "iteration: 118230 loss: 0.0029 lr: 0.02\n",
      "iteration: 118240 loss: 0.0026 lr: 0.02\n",
      "iteration: 118250 loss: 0.0025 lr: 0.02\n",
      "iteration: 118260 loss: 0.0030 lr: 0.02\n",
      "iteration: 118270 loss: 0.0023 lr: 0.02\n",
      "iteration: 118280 loss: 0.0035 lr: 0.02\n",
      "iteration: 118290 loss: 0.0030 lr: 0.02\n",
      "iteration: 118300 loss: 0.0023 lr: 0.02\n",
      "iteration: 118310 loss: 0.0019 lr: 0.02\n",
      "iteration: 118320 loss: 0.0037 lr: 0.02\n",
      "iteration: 118330 loss: 0.0029 lr: 0.02\n",
      "iteration: 118340 loss: 0.0026 lr: 0.02\n",
      "iteration: 118350 loss: 0.0028 lr: 0.02\n",
      "iteration: 118360 loss: 0.0024 lr: 0.02\n",
      "iteration: 118370 loss: 0.0024 lr: 0.02\n",
      "iteration: 118380 loss: 0.0024 lr: 0.02\n",
      "iteration: 118390 loss: 0.0029 lr: 0.02\n",
      "iteration: 118400 loss: 0.0029 lr: 0.02\n",
      "iteration: 118410 loss: 0.0027 lr: 0.02\n",
      "iteration: 118420 loss: 0.0030 lr: 0.02\n",
      "iteration: 118430 loss: 0.0023 lr: 0.02\n",
      "iteration: 118440 loss: 0.0026 lr: 0.02\n",
      "iteration: 118450 loss: 0.0019 lr: 0.02\n",
      "iteration: 118460 loss: 0.0028 lr: 0.02\n",
      "iteration: 118470 loss: 0.0018 lr: 0.02\n",
      "iteration: 118480 loss: 0.0033 lr: 0.02\n",
      "iteration: 118490 loss: 0.0020 lr: 0.02\n",
      "iteration: 118500 loss: 0.0027 lr: 0.02\n",
      "iteration: 118510 loss: 0.0024 lr: 0.02\n",
      "iteration: 118520 loss: 0.0048 lr: 0.02\n",
      "iteration: 118530 loss: 0.0034 lr: 0.02\n",
      "iteration: 118540 loss: 0.0036 lr: 0.02\n",
      "iteration: 118550 loss: 0.0021 lr: 0.02\n",
      "iteration: 118560 loss: 0.0022 lr: 0.02\n",
      "iteration: 118570 loss: 0.0026 lr: 0.02\n",
      "iteration: 118580 loss: 0.0028 lr: 0.02\n",
      "iteration: 118590 loss: 0.0027 lr: 0.02\n",
      "iteration: 118600 loss: 0.0034 lr: 0.02\n",
      "iteration: 118610 loss: 0.0031 lr: 0.02\n",
      "iteration: 118620 loss: 0.0023 lr: 0.02\n",
      "iteration: 118630 loss: 0.0024 lr: 0.02\n",
      "iteration: 118640 loss: 0.0028 lr: 0.02\n",
      "iteration: 118650 loss: 0.0021 lr: 0.02\n",
      "iteration: 118660 loss: 0.0028 lr: 0.02\n",
      "iteration: 118670 loss: 0.0027 lr: 0.02\n",
      "iteration: 118680 loss: 0.0028 lr: 0.02\n",
      "iteration: 118690 loss: 0.0020 lr: 0.02\n",
      "iteration: 118700 loss: 0.0028 lr: 0.02\n",
      "iteration: 118710 loss: 0.0022 lr: 0.02\n",
      "iteration: 118720 loss: 0.0031 lr: 0.02\n",
      "iteration: 118730 loss: 0.0026 lr: 0.02\n",
      "iteration: 118740 loss: 0.0031 lr: 0.02\n",
      "iteration: 118750 loss: 0.0027 lr: 0.02\n",
      "iteration: 118760 loss: 0.0029 lr: 0.02\n",
      "iteration: 118770 loss: 0.0028 lr: 0.02\n",
      "iteration: 118780 loss: 0.0045 lr: 0.02\n",
      "iteration: 118790 loss: 0.0023 lr: 0.02\n",
      "iteration: 118800 loss: 0.0028 lr: 0.02\n",
      "iteration: 118810 loss: 0.0020 lr: 0.02\n",
      "iteration: 118820 loss: 0.0026 lr: 0.02\n",
      "iteration: 118830 loss: 0.0025 lr: 0.02\n",
      "iteration: 118840 loss: 0.0031 lr: 0.02\n",
      "iteration: 118850 loss: 0.0024 lr: 0.02\n",
      "iteration: 118860 loss: 0.0028 lr: 0.02\n",
      "iteration: 118870 loss: 0.0038 lr: 0.02\n",
      "iteration: 118880 loss: 0.0056 lr: 0.02\n",
      "iteration: 118890 loss: 0.0025 lr: 0.02\n",
      "iteration: 118900 loss: 0.0034 lr: 0.02\n",
      "iteration: 118910 loss: 0.0032 lr: 0.02\n",
      "iteration: 118920 loss: 0.0027 lr: 0.02\n",
      "iteration: 118930 loss: 0.0022 lr: 0.02\n",
      "iteration: 118940 loss: 0.0032 lr: 0.02\n",
      "iteration: 118950 loss: 0.0023 lr: 0.02\n",
      "iteration: 118960 loss: 0.0025 lr: 0.02\n",
      "iteration: 118970 loss: 0.0032 lr: 0.02\n",
      "iteration: 118980 loss: 0.0020 lr: 0.02\n",
      "iteration: 118990 loss: 0.0028 lr: 0.02\n",
      "iteration: 119000 loss: 0.0034 lr: 0.02\n",
      "iteration: 119010 loss: 0.0032 lr: 0.02\n",
      "iteration: 119020 loss: 0.0040 lr: 0.02\n",
      "iteration: 119030 loss: 0.0024 lr: 0.02\n",
      "iteration: 119040 loss: 0.0024 lr: 0.02\n",
      "iteration: 119050 loss: 0.0027 lr: 0.02\n",
      "iteration: 119060 loss: 0.0029 lr: 0.02\n",
      "iteration: 119070 loss: 0.0023 lr: 0.02\n",
      "iteration: 119080 loss: 0.0026 lr: 0.02\n",
      "iteration: 119090 loss: 0.0034 lr: 0.02\n",
      "iteration: 119100 loss: 0.0027 lr: 0.02\n",
      "iteration: 119110 loss: 0.0026 lr: 0.02\n",
      "iteration: 119120 loss: 0.0028 lr: 0.02\n",
      "iteration: 119130 loss: 0.0026 lr: 0.02\n",
      "iteration: 119140 loss: 0.0027 lr: 0.02\n",
      "iteration: 119150 loss: 0.0034 lr: 0.02\n",
      "iteration: 119160 loss: 0.0025 lr: 0.02\n",
      "iteration: 119170 loss: 0.0023 lr: 0.02\n",
      "iteration: 119180 loss: 0.0027 lr: 0.02\n",
      "iteration: 119190 loss: 0.0023 lr: 0.02\n",
      "iteration: 119200 loss: 0.0028 lr: 0.02\n",
      "iteration: 119210 loss: 0.0023 lr: 0.02\n",
      "iteration: 119220 loss: 0.0021 lr: 0.02\n",
      "iteration: 119230 loss: 0.0026 lr: 0.02\n",
      "iteration: 119240 loss: 0.0026 lr: 0.02\n",
      "iteration: 119250 loss: 0.0027 lr: 0.02\n",
      "iteration: 119260 loss: 0.0038 lr: 0.02\n",
      "iteration: 119270 loss: 0.0028 lr: 0.02\n",
      "iteration: 119280 loss: 0.0028 lr: 0.02\n",
      "iteration: 119290 loss: 0.0039 lr: 0.02\n",
      "iteration: 119300 loss: 0.0025 lr: 0.02\n",
      "iteration: 119310 loss: 0.0028 lr: 0.02\n",
      "iteration: 119320 loss: 0.0033 lr: 0.02\n",
      "iteration: 119330 loss: 0.0029 lr: 0.02\n",
      "iteration: 119340 loss: 0.0032 lr: 0.02\n",
      "iteration: 119350 loss: 0.0038 lr: 0.02\n",
      "iteration: 119360 loss: 0.0025 lr: 0.02\n",
      "iteration: 119370 loss: 0.0030 lr: 0.02\n",
      "iteration: 119380 loss: 0.0025 lr: 0.02\n",
      "iteration: 119390 loss: 0.0033 lr: 0.02\n",
      "iteration: 119400 loss: 0.0030 lr: 0.02\n",
      "iteration: 119410 loss: 0.0028 lr: 0.02\n",
      "iteration: 119420 loss: 0.0022 lr: 0.02\n",
      "iteration: 119430 loss: 0.0026 lr: 0.02\n",
      "iteration: 119440 loss: 0.0037 lr: 0.02\n",
      "iteration: 119450 loss: 0.0032 lr: 0.02\n",
      "iteration: 119460 loss: 0.0029 lr: 0.02\n",
      "iteration: 119470 loss: 0.0024 lr: 0.02\n",
      "iteration: 119480 loss: 0.0027 lr: 0.02\n",
      "iteration: 119490 loss: 0.0022 lr: 0.02\n",
      "iteration: 119500 loss: 0.0020 lr: 0.02\n",
      "iteration: 119510 loss: 0.0022 lr: 0.02\n",
      "iteration: 119520 loss: 0.0035 lr: 0.02\n",
      "iteration: 119530 loss: 0.0023 lr: 0.02\n",
      "iteration: 119540 loss: 0.0027 lr: 0.02\n",
      "iteration: 119550 loss: 0.0029 lr: 0.02\n",
      "iteration: 119560 loss: 0.0029 lr: 0.02\n",
      "iteration: 119570 loss: 0.0017 lr: 0.02\n",
      "iteration: 119580 loss: 0.0021 lr: 0.02\n",
      "iteration: 119590 loss: 0.0031 lr: 0.02\n",
      "iteration: 119600 loss: 0.0027 lr: 0.02\n",
      "iteration: 119610 loss: 0.0022 lr: 0.02\n",
      "iteration: 119620 loss: 0.0023 lr: 0.02\n",
      "iteration: 119630 loss: 0.0030 lr: 0.02\n",
      "iteration: 119640 loss: 0.0031 lr: 0.02\n",
      "iteration: 119650 loss: 0.0029 lr: 0.02\n",
      "iteration: 119660 loss: 0.0030 lr: 0.02\n",
      "iteration: 119670 loss: 0.0031 lr: 0.02\n",
      "iteration: 119680 loss: 0.0028 lr: 0.02\n",
      "iteration: 119690 loss: 0.0030 lr: 0.02\n",
      "iteration: 119700 loss: 0.0024 lr: 0.02\n",
      "iteration: 119710 loss: 0.0035 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 119720 loss: 0.0031 lr: 0.02\n",
      "iteration: 119730 loss: 0.0024 lr: 0.02\n",
      "iteration: 119740 loss: 0.0023 lr: 0.02\n",
      "iteration: 119750 loss: 0.0029 lr: 0.02\n",
      "iteration: 119760 loss: 0.0028 lr: 0.02\n",
      "iteration: 119770 loss: 0.0028 lr: 0.02\n",
      "iteration: 119780 loss: 0.0025 lr: 0.02\n",
      "iteration: 119790 loss: 0.0034 lr: 0.02\n",
      "iteration: 119800 loss: 0.0020 lr: 0.02\n",
      "iteration: 119810 loss: 0.0030 lr: 0.02\n",
      "iteration: 119820 loss: 0.0035 lr: 0.02\n",
      "iteration: 119830 loss: 0.0023 lr: 0.02\n",
      "iteration: 119840 loss: 0.0028 lr: 0.02\n",
      "iteration: 119850 loss: 0.0029 lr: 0.02\n",
      "iteration: 119860 loss: 0.0036 lr: 0.02\n",
      "iteration: 119870 loss: 0.0025 lr: 0.02\n",
      "iteration: 119880 loss: 0.0029 lr: 0.02\n",
      "iteration: 119890 loss: 0.0026 lr: 0.02\n",
      "iteration: 119900 loss: 0.0018 lr: 0.02\n",
      "iteration: 119910 loss: 0.0027 lr: 0.02\n",
      "iteration: 119920 loss: 0.0032 lr: 0.02\n",
      "iteration: 119930 loss: 0.0035 lr: 0.02\n",
      "iteration: 119940 loss: 0.0020 lr: 0.02\n",
      "iteration: 119950 loss: 0.0021 lr: 0.02\n",
      "iteration: 119960 loss: 0.0021 lr: 0.02\n",
      "iteration: 119970 loss: 0.0024 lr: 0.02\n",
      "iteration: 119980 loss: 0.0028 lr: 0.02\n",
      "iteration: 119990 loss: 0.0028 lr: 0.02\n",
      "iteration: 120000 loss: 0.0027 lr: 0.02\n",
      "iteration: 120010 loss: 0.0048 lr: 0.02\n",
      "iteration: 120020 loss: 0.0035 lr: 0.02\n",
      "iteration: 120030 loss: 0.0025 lr: 0.02\n",
      "iteration: 120040 loss: 0.0024 lr: 0.02\n",
      "iteration: 120050 loss: 0.0023 lr: 0.02\n",
      "iteration: 120060 loss: 0.0020 lr: 0.02\n",
      "iteration: 120070 loss: 0.0026 lr: 0.02\n",
      "iteration: 120080 loss: 0.0024 lr: 0.02\n",
      "iteration: 120090 loss: 0.0020 lr: 0.02\n",
      "iteration: 120100 loss: 0.0028 lr: 0.02\n",
      "iteration: 120110 loss: 0.0026 lr: 0.02\n",
      "iteration: 120120 loss: 0.0027 lr: 0.02\n",
      "iteration: 120130 loss: 0.0022 lr: 0.02\n",
      "iteration: 120140 loss: 0.0035 lr: 0.02\n",
      "iteration: 120150 loss: 0.0026 lr: 0.02\n",
      "iteration: 120160 loss: 0.0042 lr: 0.02\n",
      "iteration: 120170 loss: 0.0031 lr: 0.02\n",
      "iteration: 120180 loss: 0.0030 lr: 0.02\n",
      "iteration: 120190 loss: 0.0027 lr: 0.02\n",
      "iteration: 120200 loss: 0.0036 lr: 0.02\n",
      "iteration: 120210 loss: 0.0027 lr: 0.02\n",
      "iteration: 120220 loss: 0.0025 lr: 0.02\n",
      "iteration: 120230 loss: 0.0029 lr: 0.02\n",
      "iteration: 120240 loss: 0.0024 lr: 0.02\n",
      "iteration: 120250 loss: 0.0032 lr: 0.02\n",
      "iteration: 120260 loss: 0.0031 lr: 0.02\n",
      "iteration: 120270 loss: 0.0033 lr: 0.02\n",
      "iteration: 120280 loss: 0.0017 lr: 0.02\n",
      "iteration: 120290 loss: 0.0027 lr: 0.02\n",
      "iteration: 120300 loss: 0.0023 lr: 0.02\n",
      "iteration: 120310 loss: 0.0026 lr: 0.02\n",
      "iteration: 120320 loss: 0.0027 lr: 0.02\n",
      "iteration: 120330 loss: 0.0019 lr: 0.02\n",
      "iteration: 120340 loss: 0.0024 lr: 0.02\n",
      "iteration: 120350 loss: 0.0021 lr: 0.02\n",
      "iteration: 120360 loss: 0.0027 lr: 0.02\n",
      "iteration: 120370 loss: 0.0035 lr: 0.02\n",
      "iteration: 120380 loss: 0.0032 lr: 0.02\n",
      "iteration: 120390 loss: 0.0025 lr: 0.02\n",
      "iteration: 120400 loss: 0.0036 lr: 0.02\n",
      "iteration: 120410 loss: 0.0034 lr: 0.02\n",
      "iteration: 120420 loss: 0.0032 lr: 0.02\n",
      "iteration: 120430 loss: 0.0028 lr: 0.02\n",
      "iteration: 120440 loss: 0.0020 lr: 0.02\n",
      "iteration: 120450 loss: 0.0027 lr: 0.02\n",
      "iteration: 120460 loss: 0.0025 lr: 0.02\n",
      "iteration: 120470 loss: 0.0028 lr: 0.02\n",
      "iteration: 120480 loss: 0.0026 lr: 0.02\n",
      "iteration: 120490 loss: 0.0027 lr: 0.02\n",
      "iteration: 120500 loss: 0.0034 lr: 0.02\n",
      "iteration: 120510 loss: 0.0031 lr: 0.02\n",
      "iteration: 120520 loss: 0.0028 lr: 0.02\n",
      "iteration: 120530 loss: 0.0027 lr: 0.02\n",
      "iteration: 120540 loss: 0.0021 lr: 0.02\n",
      "iteration: 120550 loss: 0.0025 lr: 0.02\n",
      "iteration: 120560 loss: 0.0024 lr: 0.02\n",
      "iteration: 120570 loss: 0.0033 lr: 0.02\n",
      "iteration: 120580 loss: 0.0021 lr: 0.02\n",
      "iteration: 120590 loss: 0.0025 lr: 0.02\n",
      "iteration: 120600 loss: 0.0024 lr: 0.02\n",
      "iteration: 120610 loss: 0.0024 lr: 0.02\n",
      "iteration: 120620 loss: 0.0028 lr: 0.02\n",
      "iteration: 120630 loss: 0.0027 lr: 0.02\n",
      "iteration: 120640 loss: 0.0025 lr: 0.02\n",
      "iteration: 120650 loss: 0.0021 lr: 0.02\n",
      "iteration: 120660 loss: 0.0022 lr: 0.02\n",
      "iteration: 120670 loss: 0.0023 lr: 0.02\n",
      "iteration: 120680 loss: 0.0026 lr: 0.02\n",
      "iteration: 120690 loss: 0.0024 lr: 0.02\n",
      "iteration: 120700 loss: 0.0034 lr: 0.02\n",
      "iteration: 120710 loss: 0.0026 lr: 0.02\n",
      "iteration: 120720 loss: 0.0018 lr: 0.02\n",
      "iteration: 120730 loss: 0.0027 lr: 0.02\n",
      "iteration: 120740 loss: 0.0032 lr: 0.02\n",
      "iteration: 120750 loss: 0.0029 lr: 0.02\n",
      "iteration: 120760 loss: 0.0028 lr: 0.02\n",
      "iteration: 120770 loss: 0.0039 lr: 0.02\n",
      "iteration: 120780 loss: 0.0024 lr: 0.02\n",
      "iteration: 120790 loss: 0.0025 lr: 0.02\n",
      "iteration: 120800 loss: 0.0022 lr: 0.02\n",
      "iteration: 120810 loss: 0.0030 lr: 0.02\n",
      "iteration: 120820 loss: 0.0029 lr: 0.02\n",
      "iteration: 120830 loss: 0.0018 lr: 0.02\n",
      "iteration: 120840 loss: 0.0025 lr: 0.02\n",
      "iteration: 120850 loss: 0.0022 lr: 0.02\n",
      "iteration: 120860 loss: 0.0026 lr: 0.02\n",
      "iteration: 120870 loss: 0.0027 lr: 0.02\n",
      "iteration: 120880 loss: 0.0028 lr: 0.02\n",
      "iteration: 120890 loss: 0.0025 lr: 0.02\n",
      "iteration: 120900 loss: 0.0025 lr: 0.02\n",
      "iteration: 120910 loss: 0.0028 lr: 0.02\n",
      "iteration: 120920 loss: 0.0026 lr: 0.02\n",
      "iteration: 120930 loss: 0.0020 lr: 0.02\n",
      "iteration: 120940 loss: 0.0023 lr: 0.02\n",
      "iteration: 120950 loss: 0.0029 lr: 0.02\n",
      "iteration: 120960 loss: 0.0022 lr: 0.02\n",
      "iteration: 120970 loss: 0.0029 lr: 0.02\n",
      "iteration: 120980 loss: 0.0021 lr: 0.02\n",
      "iteration: 120990 loss: 0.0030 lr: 0.02\n",
      "iteration: 121000 loss: 0.0022 lr: 0.02\n",
      "iteration: 121010 loss: 0.0021 lr: 0.02\n",
      "iteration: 121020 loss: 0.0022 lr: 0.02\n",
      "iteration: 121030 loss: 0.0021 lr: 0.02\n",
      "iteration: 121040 loss: 0.0023 lr: 0.02\n",
      "iteration: 121050 loss: 0.0035 lr: 0.02\n",
      "iteration: 121060 loss: 0.0026 lr: 0.02\n",
      "iteration: 121070 loss: 0.0024 lr: 0.02\n",
      "iteration: 121080 loss: 0.0022 lr: 0.02\n",
      "iteration: 121090 loss: 0.0022 lr: 0.02\n",
      "iteration: 121100 loss: 0.0021 lr: 0.02\n",
      "iteration: 121110 loss: 0.0028 lr: 0.02\n",
      "iteration: 121120 loss: 0.0022 lr: 0.02\n",
      "iteration: 121130 loss: 0.0025 lr: 0.02\n",
      "iteration: 121140 loss: 0.0024 lr: 0.02\n",
      "iteration: 121150 loss: 0.0033 lr: 0.02\n",
      "iteration: 121160 loss: 0.0029 lr: 0.02\n",
      "iteration: 121170 loss: 0.0026 lr: 0.02\n",
      "iteration: 121180 loss: 0.0024 lr: 0.02\n",
      "iteration: 121190 loss: 0.0025 lr: 0.02\n",
      "iteration: 121200 loss: 0.0040 lr: 0.02\n",
      "iteration: 121210 loss: 0.0026 lr: 0.02\n",
      "iteration: 121220 loss: 0.0030 lr: 0.02\n",
      "iteration: 121230 loss: 0.0022 lr: 0.02\n",
      "iteration: 121240 loss: 0.0027 lr: 0.02\n",
      "iteration: 121250 loss: 0.0025 lr: 0.02\n",
      "iteration: 121260 loss: 0.0021 lr: 0.02\n",
      "iteration: 121270 loss: 0.0030 lr: 0.02\n",
      "iteration: 121280 loss: 0.0025 lr: 0.02\n",
      "iteration: 121290 loss: 0.0029 lr: 0.02\n",
      "iteration: 121300 loss: 0.0022 lr: 0.02\n",
      "iteration: 121310 loss: 0.0033 lr: 0.02\n",
      "iteration: 121320 loss: 0.0030 lr: 0.02\n",
      "iteration: 121330 loss: 0.0030 lr: 0.02\n",
      "iteration: 121340 loss: 0.0037 lr: 0.02\n",
      "iteration: 121350 loss: 0.0036 lr: 0.02\n",
      "iteration: 121360 loss: 0.0027 lr: 0.02\n",
      "iteration: 121370 loss: 0.0026 lr: 0.02\n",
      "iteration: 121380 loss: 0.0020 lr: 0.02\n",
      "iteration: 121390 loss: 0.0024 lr: 0.02\n",
      "iteration: 121400 loss: 0.0024 lr: 0.02\n",
      "iteration: 121410 loss: 0.0030 lr: 0.02\n",
      "iteration: 121420 loss: 0.0030 lr: 0.02\n",
      "iteration: 121430 loss: 0.0023 lr: 0.02\n",
      "iteration: 121440 loss: 0.0045 lr: 0.02\n",
      "iteration: 121450 loss: 0.0027 lr: 0.02\n",
      "iteration: 121460 loss: 0.0026 lr: 0.02\n",
      "iteration: 121470 loss: 0.0024 lr: 0.02\n",
      "iteration: 121480 loss: 0.0024 lr: 0.02\n",
      "iteration: 121490 loss: 0.0028 lr: 0.02\n",
      "iteration: 121500 loss: 0.0029 lr: 0.02\n",
      "iteration: 121510 loss: 0.0023 lr: 0.02\n",
      "iteration: 121520 loss: 0.0030 lr: 0.02\n",
      "iteration: 121530 loss: 0.0034 lr: 0.02\n",
      "iteration: 121540 loss: 0.0024 lr: 0.02\n",
      "iteration: 121550 loss: 0.0024 lr: 0.02\n",
      "iteration: 121560 loss: 0.0026 lr: 0.02\n",
      "iteration: 121570 loss: 0.0025 lr: 0.02\n",
      "iteration: 121580 loss: 0.0028 lr: 0.02\n",
      "iteration: 121590 loss: 0.0026 lr: 0.02\n",
      "iteration: 121600 loss: 0.0019 lr: 0.02\n",
      "iteration: 121610 loss: 0.0018 lr: 0.02\n",
      "iteration: 121620 loss: 0.0026 lr: 0.02\n",
      "iteration: 121630 loss: 0.0023 lr: 0.02\n",
      "iteration: 121640 loss: 0.0024 lr: 0.02\n",
      "iteration: 121650 loss: 0.0029 lr: 0.02\n",
      "iteration: 121660 loss: 0.0030 lr: 0.02\n",
      "iteration: 121670 loss: 0.0032 lr: 0.02\n",
      "iteration: 121680 loss: 0.0028 lr: 0.02\n",
      "iteration: 121690 loss: 0.0027 lr: 0.02\n",
      "iteration: 121700 loss: 0.0027 lr: 0.02\n",
      "iteration: 121710 loss: 0.0025 lr: 0.02\n",
      "iteration: 121720 loss: 0.0026 lr: 0.02\n",
      "iteration: 121730 loss: 0.0028 lr: 0.02\n",
      "iteration: 121740 loss: 0.0032 lr: 0.02\n",
      "iteration: 121750 loss: 0.0033 lr: 0.02\n",
      "iteration: 121760 loss: 0.0030 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 121770 loss: 0.0030 lr: 0.02\n",
      "iteration: 121780 loss: 0.0031 lr: 0.02\n",
      "iteration: 121790 loss: 0.0033 lr: 0.02\n",
      "iteration: 121800 loss: 0.0023 lr: 0.02\n",
      "iteration: 121810 loss: 0.0028 lr: 0.02\n",
      "iteration: 121820 loss: 0.0039 lr: 0.02\n",
      "iteration: 121830 loss: 0.0028 lr: 0.02\n",
      "iteration: 121840 loss: 0.0018 lr: 0.02\n",
      "iteration: 121850 loss: 0.0028 lr: 0.02\n",
      "iteration: 121860 loss: 0.0032 lr: 0.02\n",
      "iteration: 121870 loss: 0.0028 lr: 0.02\n",
      "iteration: 121880 loss: 0.0023 lr: 0.02\n",
      "iteration: 121890 loss: 0.0033 lr: 0.02\n",
      "iteration: 121900 loss: 0.0028 lr: 0.02\n",
      "iteration: 121910 loss: 0.0031 lr: 0.02\n",
      "iteration: 121920 loss: 0.0026 lr: 0.02\n",
      "iteration: 121930 loss: 0.0028 lr: 0.02\n",
      "iteration: 121940 loss: 0.0023 lr: 0.02\n",
      "iteration: 121950 loss: 0.0022 lr: 0.02\n",
      "iteration: 121960 loss: 0.0027 lr: 0.02\n",
      "iteration: 121970 loss: 0.0029 lr: 0.02\n",
      "iteration: 121980 loss: 0.0027 lr: 0.02\n",
      "iteration: 121990 loss: 0.0024 lr: 0.02\n",
      "iteration: 122000 loss: 0.0025 lr: 0.02\n",
      "iteration: 122010 loss: 0.0024 lr: 0.02\n",
      "iteration: 122020 loss: 0.0026 lr: 0.02\n",
      "iteration: 122030 loss: 0.0027 lr: 0.02\n",
      "iteration: 122040 loss: 0.0031 lr: 0.02\n",
      "iteration: 122050 loss: 0.0027 lr: 0.02\n",
      "iteration: 122060 loss: 0.0028 lr: 0.02\n",
      "iteration: 122070 loss: 0.0020 lr: 0.02\n",
      "iteration: 122080 loss: 0.0025 lr: 0.02\n",
      "iteration: 122090 loss: 0.0025 lr: 0.02\n",
      "iteration: 122100 loss: 0.0027 lr: 0.02\n",
      "iteration: 122110 loss: 0.0029 lr: 0.02\n",
      "iteration: 122120 loss: 0.0032 lr: 0.02\n",
      "iteration: 122130 loss: 0.0022 lr: 0.02\n",
      "iteration: 122140 loss: 0.0036 lr: 0.02\n",
      "iteration: 122150 loss: 0.0024 lr: 0.02\n",
      "iteration: 122160 loss: 0.0034 lr: 0.02\n",
      "iteration: 122170 loss: 0.0030 lr: 0.02\n",
      "iteration: 122180 loss: 0.0037 lr: 0.02\n",
      "iteration: 122190 loss: 0.0031 lr: 0.02\n",
      "iteration: 122200 loss: 0.0025 lr: 0.02\n",
      "iteration: 122210 loss: 0.0024 lr: 0.02\n",
      "iteration: 122220 loss: 0.0030 lr: 0.02\n",
      "iteration: 122230 loss: 0.0024 lr: 0.02\n",
      "iteration: 122240 loss: 0.0022 lr: 0.02\n",
      "iteration: 122250 loss: 0.0043 lr: 0.02\n",
      "iteration: 122260 loss: 0.0025 lr: 0.02\n",
      "iteration: 122270 loss: 0.0030 lr: 0.02\n",
      "iteration: 122280 loss: 0.0027 lr: 0.02\n",
      "iteration: 122290 loss: 0.0023 lr: 0.02\n",
      "iteration: 122300 loss: 0.0027 lr: 0.02\n",
      "iteration: 122310 loss: 0.0028 lr: 0.02\n",
      "iteration: 122320 loss: 0.0023 lr: 0.02\n",
      "iteration: 122330 loss: 0.0023 lr: 0.02\n",
      "iteration: 122340 loss: 0.0032 lr: 0.02\n",
      "iteration: 122350 loss: 0.0028 lr: 0.02\n",
      "iteration: 122360 loss: 0.0022 lr: 0.02\n",
      "iteration: 122370 loss: 0.0019 lr: 0.02\n",
      "iteration: 122380 loss: 0.0030 lr: 0.02\n",
      "iteration: 122390 loss: 0.0037 lr: 0.02\n",
      "iteration: 122400 loss: 0.0030 lr: 0.02\n",
      "iteration: 122410 loss: 0.0027 lr: 0.02\n",
      "iteration: 122420 loss: 0.0027 lr: 0.02\n",
      "iteration: 122430 loss: 0.0018 lr: 0.02\n",
      "iteration: 122440 loss: 0.0030 lr: 0.02\n",
      "iteration: 122450 loss: 0.0028 lr: 0.02\n",
      "iteration: 122460 loss: 0.0041 lr: 0.02\n",
      "iteration: 122470 loss: 0.0030 lr: 0.02\n",
      "iteration: 122480 loss: 0.0027 lr: 0.02\n",
      "iteration: 122490 loss: 0.0024 lr: 0.02\n",
      "iteration: 122500 loss: 0.0028 lr: 0.02\n",
      "iteration: 122510 loss: 0.0034 lr: 0.02\n",
      "iteration: 122520 loss: 0.0022 lr: 0.02\n",
      "iteration: 122530 loss: 0.0027 lr: 0.02\n",
      "iteration: 122540 loss: 0.0028 lr: 0.02\n",
      "iteration: 122550 loss: 0.0023 lr: 0.02\n",
      "iteration: 122560 loss: 0.0033 lr: 0.02\n",
      "iteration: 122570 loss: 0.0023 lr: 0.02\n",
      "iteration: 122580 loss: 0.0030 lr: 0.02\n",
      "iteration: 122590 loss: 0.0032 lr: 0.02\n",
      "iteration: 122600 loss: 0.0029 lr: 0.02\n",
      "iteration: 122610 loss: 0.0025 lr: 0.02\n",
      "iteration: 122620 loss: 0.0026 lr: 0.02\n",
      "iteration: 122630 loss: 0.0027 lr: 0.02\n",
      "iteration: 122640 loss: 0.0031 lr: 0.02\n",
      "iteration: 122650 loss: 0.0023 lr: 0.02\n",
      "iteration: 122660 loss: 0.0018 lr: 0.02\n",
      "iteration: 122670 loss: 0.0033 lr: 0.02\n",
      "iteration: 122680 loss: 0.0029 lr: 0.02\n",
      "iteration: 122690 loss: 0.0021 lr: 0.02\n",
      "iteration: 122700 loss: 0.0030 lr: 0.02\n",
      "iteration: 122710 loss: 0.0031 lr: 0.02\n",
      "iteration: 122720 loss: 0.0026 lr: 0.02\n",
      "iteration: 122730 loss: 0.0023 lr: 0.02\n",
      "iteration: 122740 loss: 0.0021 lr: 0.02\n",
      "iteration: 122750 loss: 0.0033 lr: 0.02\n",
      "iteration: 122760 loss: 0.0032 lr: 0.02\n",
      "iteration: 122770 loss: 0.0024 lr: 0.02\n",
      "iteration: 122780 loss: 0.0026 lr: 0.02\n",
      "iteration: 122790 loss: 0.0031 lr: 0.02\n",
      "iteration: 122800 loss: 0.0023 lr: 0.02\n",
      "iteration: 122810 loss: 0.0034 lr: 0.02\n",
      "iteration: 122820 loss: 0.0025 lr: 0.02\n",
      "iteration: 122830 loss: 0.0030 lr: 0.02\n",
      "iteration: 122840 loss: 0.0023 lr: 0.02\n",
      "iteration: 122850 loss: 0.0029 lr: 0.02\n",
      "iteration: 122860 loss: 0.0029 lr: 0.02\n",
      "iteration: 122870 loss: 0.0031 lr: 0.02\n",
      "iteration: 122880 loss: 0.0023 lr: 0.02\n",
      "iteration: 122890 loss: 0.0024 lr: 0.02\n",
      "iteration: 122900 loss: 0.0023 lr: 0.02\n",
      "iteration: 122910 loss: 0.0025 lr: 0.02\n",
      "iteration: 122920 loss: 0.0035 lr: 0.02\n",
      "iteration: 122930 loss: 0.0024 lr: 0.02\n",
      "iteration: 122940 loss: 0.0020 lr: 0.02\n",
      "iteration: 122950 loss: 0.0028 lr: 0.02\n",
      "iteration: 122960 loss: 0.0024 lr: 0.02\n",
      "iteration: 122970 loss: 0.0025 lr: 0.02\n",
      "iteration: 122980 loss: 0.0032 lr: 0.02\n",
      "iteration: 122990 loss: 0.0025 lr: 0.02\n",
      "iteration: 123000 loss: 0.0028 lr: 0.02\n",
      "iteration: 123010 loss: 0.0026 lr: 0.02\n",
      "iteration: 123020 loss: 0.0017 lr: 0.02\n",
      "iteration: 123030 loss: 0.0031 lr: 0.02\n",
      "iteration: 123040 loss: 0.0031 lr: 0.02\n",
      "iteration: 123050 loss: 0.0026 lr: 0.02\n",
      "iteration: 123060 loss: 0.0022 lr: 0.02\n",
      "iteration: 123070 loss: 0.0021 lr: 0.02\n",
      "iteration: 123080 loss: 0.0025 lr: 0.02\n",
      "iteration: 123090 loss: 0.0023 lr: 0.02\n",
      "iteration: 123100 loss: 0.0029 lr: 0.02\n",
      "iteration: 123110 loss: 0.0018 lr: 0.02\n",
      "iteration: 123120 loss: 0.0021 lr: 0.02\n",
      "iteration: 123130 loss: 0.0021 lr: 0.02\n",
      "iteration: 123140 loss: 0.0022 lr: 0.02\n",
      "iteration: 123150 loss: 0.0030 lr: 0.02\n",
      "iteration: 123160 loss: 0.0021 lr: 0.02\n",
      "iteration: 123170 loss: 0.0031 lr: 0.02\n",
      "iteration: 123180 loss: 0.0020 lr: 0.02\n",
      "iteration: 123190 loss: 0.0023 lr: 0.02\n",
      "iteration: 123200 loss: 0.0029 lr: 0.02\n",
      "iteration: 123210 loss: 0.0028 lr: 0.02\n",
      "iteration: 123220 loss: 0.0021 lr: 0.02\n",
      "iteration: 123230 loss: 0.0028 lr: 0.02\n",
      "iteration: 123240 loss: 0.0025 lr: 0.02\n",
      "iteration: 123250 loss: 0.0026 lr: 0.02\n",
      "iteration: 123260 loss: 0.0018 lr: 0.02\n",
      "iteration: 123270 loss: 0.0026 lr: 0.02\n",
      "iteration: 123280 loss: 0.0023 lr: 0.02\n",
      "iteration: 123290 loss: 0.0030 lr: 0.02\n",
      "iteration: 123300 loss: 0.0036 lr: 0.02\n",
      "iteration: 123310 loss: 0.0026 lr: 0.02\n",
      "iteration: 123320 loss: 0.0032 lr: 0.02\n",
      "iteration: 123330 loss: 0.0033 lr: 0.02\n",
      "iteration: 123340 loss: 0.0032 lr: 0.02\n",
      "iteration: 123350 loss: 0.0027 lr: 0.02\n",
      "iteration: 123360 loss: 0.0030 lr: 0.02\n",
      "iteration: 123370 loss: 0.0019 lr: 0.02\n",
      "iteration: 123380 loss: 0.0032 lr: 0.02\n",
      "iteration: 123390 loss: 0.0028 lr: 0.02\n",
      "iteration: 123400 loss: 0.0029 lr: 0.02\n",
      "iteration: 123410 loss: 0.0025 lr: 0.02\n",
      "iteration: 123420 loss: 0.0033 lr: 0.02\n",
      "iteration: 123430 loss: 0.0029 lr: 0.02\n",
      "iteration: 123440 loss: 0.0028 lr: 0.02\n",
      "iteration: 123450 loss: 0.0022 lr: 0.02\n",
      "iteration: 123460 loss: 0.0030 lr: 0.02\n",
      "iteration: 123470 loss: 0.0036 lr: 0.02\n",
      "iteration: 123480 loss: 0.0031 lr: 0.02\n",
      "iteration: 123490 loss: 0.0026 lr: 0.02\n",
      "iteration: 123500 loss: 0.0029 lr: 0.02\n",
      "iteration: 123510 loss: 0.0033 lr: 0.02\n",
      "iteration: 123520 loss: 0.0022 lr: 0.02\n",
      "iteration: 123530 loss: 0.0026 lr: 0.02\n",
      "iteration: 123540 loss: 0.0029 lr: 0.02\n",
      "iteration: 123550 loss: 0.0021 lr: 0.02\n",
      "iteration: 123560 loss: 0.0028 lr: 0.02\n",
      "iteration: 123570 loss: 0.0031 lr: 0.02\n",
      "iteration: 123580 loss: 0.0026 lr: 0.02\n",
      "iteration: 123590 loss: 0.0026 lr: 0.02\n",
      "iteration: 123600 loss: 0.0019 lr: 0.02\n",
      "iteration: 123610 loss: 0.0024 lr: 0.02\n",
      "iteration: 123620 loss: 0.0022 lr: 0.02\n",
      "iteration: 123630 loss: 0.0021 lr: 0.02\n",
      "iteration: 123640 loss: 0.0021 lr: 0.02\n",
      "iteration: 123650 loss: 0.0024 lr: 0.02\n",
      "iteration: 123660 loss: 0.0020 lr: 0.02\n",
      "iteration: 123670 loss: 0.0024 lr: 0.02\n",
      "iteration: 123680 loss: 0.0029 lr: 0.02\n",
      "iteration: 123690 loss: 0.0024 lr: 0.02\n",
      "iteration: 123700 loss: 0.0024 lr: 0.02\n",
      "iteration: 123710 loss: 0.0026 lr: 0.02\n",
      "iteration: 123720 loss: 0.0026 lr: 0.02\n",
      "iteration: 123730 loss: 0.0022 lr: 0.02\n",
      "iteration: 123740 loss: 0.0020 lr: 0.02\n",
      "iteration: 123750 loss: 0.0042 lr: 0.02\n",
      "iteration: 123760 loss: 0.0024 lr: 0.02\n",
      "iteration: 123770 loss: 0.0033 lr: 0.02\n",
      "iteration: 123780 loss: 0.0025 lr: 0.02\n",
      "iteration: 123790 loss: 0.0020 lr: 0.02\n",
      "iteration: 123800 loss: 0.0033 lr: 0.02\n",
      "iteration: 123810 loss: 0.0029 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 123820 loss: 0.0032 lr: 0.02\n",
      "iteration: 123830 loss: 0.0030 lr: 0.02\n",
      "iteration: 123840 loss: 0.0027 lr: 0.02\n",
      "iteration: 123850 loss: 0.0025 lr: 0.02\n",
      "iteration: 123860 loss: 0.0042 lr: 0.02\n",
      "iteration: 123870 loss: 0.0023 lr: 0.02\n",
      "iteration: 123880 loss: 0.0026 lr: 0.02\n",
      "iteration: 123890 loss: 0.0025 lr: 0.02\n",
      "iteration: 123900 loss: 0.0023 lr: 0.02\n",
      "iteration: 123910 loss: 0.0033 lr: 0.02\n",
      "iteration: 123920 loss: 0.0027 lr: 0.02\n",
      "iteration: 123930 loss: 0.0025 lr: 0.02\n",
      "iteration: 123940 loss: 0.0024 lr: 0.02\n",
      "iteration: 123950 loss: 0.0034 lr: 0.02\n",
      "iteration: 123960 loss: 0.0031 lr: 0.02\n",
      "iteration: 123970 loss: 0.0023 lr: 0.02\n",
      "iteration: 123980 loss: 0.0024 lr: 0.02\n",
      "iteration: 123990 loss: 0.0023 lr: 0.02\n",
      "iteration: 124000 loss: 0.0029 lr: 0.02\n",
      "iteration: 124010 loss: 0.0025 lr: 0.02\n",
      "iteration: 124020 loss: 0.0025 lr: 0.02\n",
      "iteration: 124030 loss: 0.0022 lr: 0.02\n",
      "iteration: 124040 loss: 0.0019 lr: 0.02\n",
      "iteration: 124050 loss: 0.0025 lr: 0.02\n",
      "iteration: 124060 loss: 0.0030 lr: 0.02\n",
      "iteration: 124070 loss: 0.0021 lr: 0.02\n",
      "iteration: 124080 loss: 0.0022 lr: 0.02\n",
      "iteration: 124090 loss: 0.0025 lr: 0.02\n",
      "iteration: 124100 loss: 0.0029 lr: 0.02\n",
      "iteration: 124110 loss: 0.0029 lr: 0.02\n",
      "iteration: 124120 loss: 0.0025 lr: 0.02\n",
      "iteration: 124130 loss: 0.0022 lr: 0.02\n",
      "iteration: 124140 loss: 0.0040 lr: 0.02\n",
      "iteration: 124150 loss: 0.0029 lr: 0.02\n",
      "iteration: 124160 loss: 0.0028 lr: 0.02\n",
      "iteration: 124170 loss: 0.0021 lr: 0.02\n",
      "iteration: 124180 loss: 0.0026 lr: 0.02\n",
      "iteration: 124190 loss: 0.0025 lr: 0.02\n",
      "iteration: 124200 loss: 0.0026 lr: 0.02\n",
      "iteration: 124210 loss: 0.0019 lr: 0.02\n",
      "iteration: 124220 loss: 0.0024 lr: 0.02\n",
      "iteration: 124230 loss: 0.0029 lr: 0.02\n",
      "iteration: 124240 loss: 0.0029 lr: 0.02\n",
      "iteration: 124250 loss: 0.0021 lr: 0.02\n",
      "iteration: 124260 loss: 0.0022 lr: 0.02\n",
      "iteration: 124270 loss: 0.0020 lr: 0.02\n",
      "iteration: 124280 loss: 0.0027 lr: 0.02\n",
      "iteration: 124290 loss: 0.0023 lr: 0.02\n",
      "iteration: 124300 loss: 0.0022 lr: 0.02\n",
      "iteration: 124310 loss: 0.0019 lr: 0.02\n",
      "iteration: 124320 loss: 0.0024 lr: 0.02\n",
      "iteration: 124330 loss: 0.0028 lr: 0.02\n",
      "iteration: 124340 loss: 0.0034 lr: 0.02\n",
      "iteration: 124350 loss: 0.0024 lr: 0.02\n",
      "iteration: 124360 loss: 0.0029 lr: 0.02\n",
      "iteration: 124370 loss: 0.0029 lr: 0.02\n",
      "iteration: 124380 loss: 0.0024 lr: 0.02\n",
      "iteration: 124390 loss: 0.0021 lr: 0.02\n",
      "iteration: 124400 loss: 0.0031 lr: 0.02\n",
      "iteration: 124410 loss: 0.0027 lr: 0.02\n",
      "iteration: 124420 loss: 0.0023 lr: 0.02\n",
      "iteration: 124430 loss: 0.0027 lr: 0.02\n",
      "iteration: 124440 loss: 0.0027 lr: 0.02\n",
      "iteration: 124450 loss: 0.0030 lr: 0.02\n",
      "iteration: 124460 loss: 0.0025 lr: 0.02\n",
      "iteration: 124470 loss: 0.0027 lr: 0.02\n",
      "iteration: 124480 loss: 0.0026 lr: 0.02\n",
      "iteration: 124490 loss: 0.0019 lr: 0.02\n",
      "iteration: 124500 loss: 0.0026 lr: 0.02\n",
      "iteration: 124510 loss: 0.0029 lr: 0.02\n",
      "iteration: 124520 loss: 0.0026 lr: 0.02\n",
      "iteration: 124530 loss: 0.0026 lr: 0.02\n",
      "iteration: 124540 loss: 0.0020 lr: 0.02\n",
      "iteration: 124550 loss: 0.0032 lr: 0.02\n",
      "iteration: 124560 loss: 0.0030 lr: 0.02\n",
      "iteration: 124570 loss: 0.0024 lr: 0.02\n",
      "iteration: 124580 loss: 0.0028 lr: 0.02\n",
      "iteration: 124590 loss: 0.0033 lr: 0.02\n",
      "iteration: 124600 loss: 0.0036 lr: 0.02\n",
      "iteration: 124610 loss: 0.0042 lr: 0.02\n",
      "iteration: 124620 loss: 0.0027 lr: 0.02\n",
      "iteration: 124630 loss: 0.0032 lr: 0.02\n",
      "iteration: 124640 loss: 0.0025 lr: 0.02\n",
      "iteration: 124650 loss: 0.0024 lr: 0.02\n",
      "iteration: 124660 loss: 0.0030 lr: 0.02\n",
      "iteration: 124670 loss: 0.0029 lr: 0.02\n",
      "iteration: 124680 loss: 0.0018 lr: 0.02\n",
      "iteration: 124690 loss: 0.0034 lr: 0.02\n",
      "iteration: 124700 loss: 0.0034 lr: 0.02\n",
      "iteration: 124710 loss: 0.0026 lr: 0.02\n",
      "iteration: 124720 loss: 0.0035 lr: 0.02\n",
      "iteration: 124730 loss: 0.0031 lr: 0.02\n",
      "iteration: 124740 loss: 0.0019 lr: 0.02\n",
      "iteration: 124750 loss: 0.0025 lr: 0.02\n",
      "iteration: 124760 loss: 0.0026 lr: 0.02\n",
      "iteration: 124770 loss: 0.0027 lr: 0.02\n",
      "iteration: 124780 loss: 0.0027 lr: 0.02\n",
      "iteration: 124790 loss: 0.0029 lr: 0.02\n",
      "iteration: 124800 loss: 0.0025 lr: 0.02\n",
      "iteration: 124810 loss: 0.0034 lr: 0.02\n",
      "iteration: 124820 loss: 0.0029 lr: 0.02\n",
      "iteration: 124830 loss: 0.0023 lr: 0.02\n",
      "iteration: 124840 loss: 0.0021 lr: 0.02\n",
      "iteration: 124850 loss: 0.0023 lr: 0.02\n",
      "iteration: 124860 loss: 0.0037 lr: 0.02\n",
      "iteration: 124870 loss: 0.0030 lr: 0.02\n",
      "iteration: 124880 loss: 0.0027 lr: 0.02\n",
      "iteration: 124890 loss: 0.0031 lr: 0.02\n",
      "iteration: 124900 loss: 0.0026 lr: 0.02\n",
      "iteration: 124910 loss: 0.0025 lr: 0.02\n",
      "iteration: 124920 loss: 0.0026 lr: 0.02\n",
      "iteration: 124930 loss: 0.0024 lr: 0.02\n",
      "iteration: 124940 loss: 0.0020 lr: 0.02\n",
      "iteration: 124950 loss: 0.0026 lr: 0.02\n",
      "iteration: 124960 loss: 0.0018 lr: 0.02\n",
      "iteration: 124970 loss: 0.0034 lr: 0.02\n",
      "iteration: 124980 loss: 0.0025 lr: 0.02\n",
      "iteration: 124990 loss: 0.0027 lr: 0.02\n",
      "iteration: 125000 loss: 0.0030 lr: 0.02\n",
      "iteration: 125010 loss: 0.0030 lr: 0.02\n",
      "iteration: 125020 loss: 0.0022 lr: 0.02\n",
      "iteration: 125030 loss: 0.0023 lr: 0.02\n",
      "iteration: 125040 loss: 0.0022 lr: 0.02\n",
      "iteration: 125050 loss: 0.0027 lr: 0.02\n",
      "iteration: 125060 loss: 0.0024 lr: 0.02\n",
      "iteration: 125070 loss: 0.0028 lr: 0.02\n",
      "iteration: 125080 loss: 0.0024 lr: 0.02\n",
      "iteration: 125090 loss: 0.0029 lr: 0.02\n",
      "iteration: 125100 loss: 0.0023 lr: 0.02\n",
      "iteration: 125110 loss: 0.0034 lr: 0.02\n",
      "iteration: 125120 loss: 0.0033 lr: 0.02\n",
      "iteration: 125130 loss: 0.0025 lr: 0.02\n",
      "iteration: 125140 loss: 0.0026 lr: 0.02\n",
      "iteration: 125150 loss: 0.0029 lr: 0.02\n",
      "iteration: 125160 loss: 0.0043 lr: 0.02\n",
      "iteration: 125170 loss: 0.0028 lr: 0.02\n",
      "iteration: 125180 loss: 0.0031 lr: 0.02\n",
      "iteration: 125190 loss: 0.0026 lr: 0.02\n",
      "iteration: 125200 loss: 0.0034 lr: 0.02\n",
      "iteration: 125210 loss: 0.0029 lr: 0.02\n",
      "iteration: 125220 loss: 0.0031 lr: 0.02\n",
      "iteration: 125230 loss: 0.0027 lr: 0.02\n",
      "iteration: 125240 loss: 0.0025 lr: 0.02\n",
      "iteration: 125250 loss: 0.0036 lr: 0.02\n",
      "iteration: 125260 loss: 0.0027 lr: 0.02\n",
      "iteration: 125270 loss: 0.0030 lr: 0.02\n",
      "iteration: 125280 loss: 0.0024 lr: 0.02\n",
      "iteration: 125290 loss: 0.0028 lr: 0.02\n",
      "iteration: 125300 loss: 0.0031 lr: 0.02\n",
      "iteration: 125310 loss: 0.0031 lr: 0.02\n",
      "iteration: 125320 loss: 0.0021 lr: 0.02\n",
      "iteration: 125330 loss: 0.0024 lr: 0.02\n",
      "iteration: 125340 loss: 0.0021 lr: 0.02\n",
      "iteration: 125350 loss: 0.0025 lr: 0.02\n",
      "iteration: 125360 loss: 0.0032 lr: 0.02\n",
      "iteration: 125370 loss: 0.0018 lr: 0.02\n",
      "iteration: 125380 loss: 0.0030 lr: 0.02\n",
      "iteration: 125390 loss: 0.0038 lr: 0.02\n",
      "iteration: 125400 loss: 0.0022 lr: 0.02\n",
      "iteration: 125410 loss: 0.0025 lr: 0.02\n",
      "iteration: 125420 loss: 0.0028 lr: 0.02\n",
      "iteration: 125430 loss: 0.0029 lr: 0.02\n",
      "iteration: 125440 loss: 0.0044 lr: 0.02\n",
      "iteration: 125450 loss: 0.0021 lr: 0.02\n",
      "iteration: 125460 loss: 0.0027 lr: 0.02\n",
      "iteration: 125470 loss: 0.0033 lr: 0.02\n",
      "iteration: 125480 loss: 0.0034 lr: 0.02\n",
      "iteration: 125490 loss: 0.0017 lr: 0.02\n",
      "iteration: 125500 loss: 0.0023 lr: 0.02\n",
      "iteration: 125510 loss: 0.0040 lr: 0.02\n",
      "iteration: 125520 loss: 0.0025 lr: 0.02\n",
      "iteration: 125530 loss: 0.0030 lr: 0.02\n",
      "iteration: 125540 loss: 0.0030 lr: 0.02\n",
      "iteration: 125550 loss: 0.0031 lr: 0.02\n",
      "iteration: 125560 loss: 0.0027 lr: 0.02\n",
      "iteration: 125570 loss: 0.0018 lr: 0.02\n",
      "iteration: 125580 loss: 0.0030 lr: 0.02\n",
      "iteration: 125590 loss: 0.0041 lr: 0.02\n",
      "iteration: 125600 loss: 0.0029 lr: 0.02\n",
      "iteration: 125610 loss: 0.0033 lr: 0.02\n",
      "iteration: 125620 loss: 0.0022 lr: 0.02\n",
      "iteration: 125630 loss: 0.0022 lr: 0.02\n",
      "iteration: 125640 loss: 0.0026 lr: 0.02\n",
      "iteration: 125650 loss: 0.0027 lr: 0.02\n",
      "iteration: 125660 loss: 0.0019 lr: 0.02\n",
      "iteration: 125670 loss: 0.0027 lr: 0.02\n",
      "iteration: 125680 loss: 0.0025 lr: 0.02\n",
      "iteration: 125690 loss: 0.0026 lr: 0.02\n",
      "iteration: 125700 loss: 0.0025 lr: 0.02\n",
      "iteration: 125710 loss: 0.0023 lr: 0.02\n",
      "iteration: 125720 loss: 0.0028 lr: 0.02\n",
      "iteration: 125730 loss: 0.0028 lr: 0.02\n",
      "iteration: 125740 loss: 0.0030 lr: 0.02\n",
      "iteration: 125750 loss: 0.0032 lr: 0.02\n",
      "iteration: 125760 loss: 0.0025 lr: 0.02\n",
      "iteration: 125770 loss: 0.0030 lr: 0.02\n",
      "iteration: 125780 loss: 0.0026 lr: 0.02\n",
      "iteration: 125790 loss: 0.0026 lr: 0.02\n",
      "iteration: 125800 loss: 0.0019 lr: 0.02\n",
      "iteration: 125810 loss: 0.0022 lr: 0.02\n",
      "iteration: 125820 loss: 0.0023 lr: 0.02\n",
      "iteration: 125830 loss: 0.0021 lr: 0.02\n",
      "iteration: 125840 loss: 0.0023 lr: 0.02\n",
      "iteration: 125850 loss: 0.0024 lr: 0.02\n",
      "iteration: 125860 loss: 0.0020 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 125870 loss: 0.0024 lr: 0.02\n",
      "iteration: 125880 loss: 0.0026 lr: 0.02\n",
      "iteration: 125890 loss: 0.0029 lr: 0.02\n",
      "iteration: 125900 loss: 0.0022 lr: 0.02\n",
      "iteration: 125910 loss: 0.0023 lr: 0.02\n",
      "iteration: 125920 loss: 0.0026 lr: 0.02\n",
      "iteration: 125930 loss: 0.0024 lr: 0.02\n",
      "iteration: 125940 loss: 0.0031 lr: 0.02\n",
      "iteration: 125950 loss: 0.0020 lr: 0.02\n",
      "iteration: 125960 loss: 0.0036 lr: 0.02\n",
      "iteration: 125970 loss: 0.0034 lr: 0.02\n",
      "iteration: 125980 loss: 0.0025 lr: 0.02\n",
      "iteration: 125990 loss: 0.0023 lr: 0.02\n",
      "iteration: 126000 loss: 0.0029 lr: 0.02\n",
      "iteration: 126010 loss: 0.0023 lr: 0.02\n",
      "iteration: 126020 loss: 0.0021 lr: 0.02\n",
      "iteration: 126030 loss: 0.0031 lr: 0.02\n",
      "iteration: 126040 loss: 0.0036 lr: 0.02\n",
      "iteration: 126050 loss: 0.0038 lr: 0.02\n",
      "iteration: 126060 loss: 0.0022 lr: 0.02\n",
      "iteration: 126070 loss: 0.0023 lr: 0.02\n",
      "iteration: 126080 loss: 0.0032 lr: 0.02\n",
      "iteration: 126090 loss: 0.0031 lr: 0.02\n",
      "iteration: 126100 loss: 0.0022 lr: 0.02\n",
      "iteration: 126110 loss: 0.0023 lr: 0.02\n",
      "iteration: 126120 loss: 0.0021 lr: 0.02\n",
      "iteration: 126130 loss: 0.0021 lr: 0.02\n",
      "iteration: 126140 loss: 0.0020 lr: 0.02\n",
      "iteration: 126150 loss: 0.0035 lr: 0.02\n",
      "iteration: 126160 loss: 0.0026 lr: 0.02\n",
      "iteration: 126170 loss: 0.0029 lr: 0.02\n",
      "iteration: 126180 loss: 0.0028 lr: 0.02\n",
      "iteration: 126190 loss: 0.0028 lr: 0.02\n",
      "iteration: 126200 loss: 0.0022 lr: 0.02\n",
      "iteration: 126210 loss: 0.0018 lr: 0.02\n",
      "iteration: 126220 loss: 0.0031 lr: 0.02\n",
      "iteration: 126230 loss: 0.0030 lr: 0.02\n",
      "iteration: 126240 loss: 0.0030 lr: 0.02\n",
      "iteration: 126250 loss: 0.0028 lr: 0.02\n",
      "iteration: 126260 loss: 0.0025 lr: 0.02\n",
      "iteration: 126270 loss: 0.0027 lr: 0.02\n",
      "iteration: 126280 loss: 0.0025 lr: 0.02\n",
      "iteration: 126290 loss: 0.0021 lr: 0.02\n",
      "iteration: 126300 loss: 0.0023 lr: 0.02\n",
      "iteration: 126310 loss: 0.0020 lr: 0.02\n",
      "iteration: 126320 loss: 0.0021 lr: 0.02\n",
      "iteration: 126330 loss: 0.0045 lr: 0.02\n",
      "iteration: 126340 loss: 0.0019 lr: 0.02\n",
      "iteration: 126350 loss: 0.0035 lr: 0.02\n",
      "iteration: 126360 loss: 0.0028 lr: 0.02\n",
      "iteration: 126370 loss: 0.0021 lr: 0.02\n",
      "iteration: 126380 loss: 0.0033 lr: 0.02\n",
      "iteration: 126390 loss: 0.0024 lr: 0.02\n",
      "iteration: 126400 loss: 0.0035 lr: 0.02\n",
      "iteration: 126410 loss: 0.0024 lr: 0.02\n",
      "iteration: 126420 loss: 0.0020 lr: 0.02\n",
      "iteration: 126430 loss: 0.0024 lr: 0.02\n",
      "iteration: 126440 loss: 0.0025 lr: 0.02\n",
      "iteration: 126450 loss: 0.0035 lr: 0.02\n",
      "iteration: 126460 loss: 0.0024 lr: 0.02\n",
      "iteration: 126470 loss: 0.0027 lr: 0.02\n",
      "iteration: 126480 loss: 0.0033 lr: 0.02\n",
      "iteration: 126490 loss: 0.0023 lr: 0.02\n",
      "iteration: 126500 loss: 0.0035 lr: 0.02\n",
      "iteration: 126510 loss: 0.0030 lr: 0.02\n",
      "iteration: 126520 loss: 0.0031 lr: 0.02\n",
      "iteration: 126530 loss: 0.0022 lr: 0.02\n",
      "iteration: 126540 loss: 0.0027 lr: 0.02\n",
      "iteration: 126550 loss: 0.0025 lr: 0.02\n",
      "iteration: 126560 loss: 0.0033 lr: 0.02\n",
      "iteration: 126570 loss: 0.0025 lr: 0.02\n",
      "iteration: 126580 loss: 0.0024 lr: 0.02\n",
      "iteration: 126590 loss: 0.0028 lr: 0.02\n",
      "iteration: 126600 loss: 0.0019 lr: 0.02\n",
      "iteration: 126610 loss: 0.0027 lr: 0.02\n",
      "iteration: 126620 loss: 0.0028 lr: 0.02\n",
      "iteration: 126630 loss: 0.0030 lr: 0.02\n",
      "iteration: 126640 loss: 0.0024 lr: 0.02\n",
      "iteration: 126650 loss: 0.0027 lr: 0.02\n",
      "iteration: 126660 loss: 0.0023 lr: 0.02\n",
      "iteration: 126670 loss: 0.0021 lr: 0.02\n",
      "iteration: 126680 loss: 0.0024 lr: 0.02\n",
      "iteration: 126690 loss: 0.0020 lr: 0.02\n",
      "iteration: 126700 loss: 0.0029 lr: 0.02\n",
      "iteration: 126710 loss: 0.0031 lr: 0.02\n",
      "iteration: 126720 loss: 0.0028 lr: 0.02\n",
      "iteration: 126730 loss: 0.0031 lr: 0.02\n",
      "iteration: 126740 loss: 0.0024 lr: 0.02\n",
      "iteration: 126750 loss: 0.0021 lr: 0.02\n",
      "iteration: 126760 loss: 0.0030 lr: 0.02\n",
      "iteration: 126770 loss: 0.0025 lr: 0.02\n",
      "iteration: 126780 loss: 0.0023 lr: 0.02\n",
      "iteration: 126790 loss: 0.0023 lr: 0.02\n",
      "iteration: 126800 loss: 0.0024 lr: 0.02\n",
      "iteration: 126810 loss: 0.0021 lr: 0.02\n",
      "iteration: 126820 loss: 0.0027 lr: 0.02\n",
      "iteration: 126830 loss: 0.0024 lr: 0.02\n",
      "iteration: 126840 loss: 0.0025 lr: 0.02\n",
      "iteration: 126850 loss: 0.0023 lr: 0.02\n",
      "iteration: 126860 loss: 0.0028 lr: 0.02\n",
      "iteration: 126870 loss: 0.0031 lr: 0.02\n",
      "iteration: 126880 loss: 0.0022 lr: 0.02\n",
      "iteration: 126890 loss: 0.0031 lr: 0.02\n",
      "iteration: 126900 loss: 0.0032 lr: 0.02\n",
      "iteration: 126910 loss: 0.0031 lr: 0.02\n",
      "iteration: 126920 loss: 0.0034 lr: 0.02\n",
      "iteration: 126930 loss: 0.0023 lr: 0.02\n",
      "iteration: 126940 loss: 0.0030 lr: 0.02\n",
      "iteration: 126950 loss: 0.0030 lr: 0.02\n",
      "iteration: 126960 loss: 0.0024 lr: 0.02\n",
      "iteration: 126970 loss: 0.0027 lr: 0.02\n",
      "iteration: 126980 loss: 0.0018 lr: 0.02\n",
      "iteration: 126990 loss: 0.0027 lr: 0.02\n",
      "iteration: 127000 loss: 0.0027 lr: 0.02\n",
      "iteration: 127010 loss: 0.0029 lr: 0.02\n",
      "iteration: 127020 loss: 0.0021 lr: 0.02\n",
      "iteration: 127030 loss: 0.0022 lr: 0.02\n",
      "iteration: 127040 loss: 0.0024 lr: 0.02\n",
      "iteration: 127050 loss: 0.0033 lr: 0.02\n",
      "iteration: 127060 loss: 0.0030 lr: 0.02\n",
      "iteration: 127070 loss: 0.0025 lr: 0.02\n",
      "iteration: 127080 loss: 0.0029 lr: 0.02\n",
      "iteration: 127090 loss: 0.0025 lr: 0.02\n",
      "iteration: 127100 loss: 0.0019 lr: 0.02\n",
      "iteration: 127110 loss: 0.0023 lr: 0.02\n",
      "iteration: 127120 loss: 0.0027 lr: 0.02\n",
      "iteration: 127130 loss: 0.0022 lr: 0.02\n",
      "iteration: 127140 loss: 0.0030 lr: 0.02\n",
      "iteration: 127150 loss: 0.0023 lr: 0.02\n",
      "iteration: 127160 loss: 0.0036 lr: 0.02\n",
      "iteration: 127170 loss: 0.0029 lr: 0.02\n",
      "iteration: 127180 loss: 0.0026 lr: 0.02\n",
      "iteration: 127190 loss: 0.0024 lr: 0.02\n",
      "iteration: 127200 loss: 0.0021 lr: 0.02\n",
      "iteration: 127210 loss: 0.0036 lr: 0.02\n",
      "iteration: 127220 loss: 0.0031 lr: 0.02\n",
      "iteration: 127230 loss: 0.0021 lr: 0.02\n",
      "iteration: 127240 loss: 0.0018 lr: 0.02\n",
      "iteration: 127250 loss: 0.0023 lr: 0.02\n",
      "iteration: 127260 loss: 0.0026 lr: 0.02\n",
      "iteration: 127270 loss: 0.0025 lr: 0.02\n",
      "iteration: 127280 loss: 0.0027 lr: 0.02\n",
      "iteration: 127290 loss: 0.0026 lr: 0.02\n",
      "iteration: 127300 loss: 0.0037 lr: 0.02\n",
      "iteration: 127310 loss: 0.0028 lr: 0.02\n",
      "iteration: 127320 loss: 0.0029 lr: 0.02\n",
      "iteration: 127330 loss: 0.0027 lr: 0.02\n",
      "iteration: 127340 loss: 0.0031 lr: 0.02\n",
      "iteration: 127350 loss: 0.0031 lr: 0.02\n",
      "iteration: 127360 loss: 0.0025 lr: 0.02\n",
      "iteration: 127370 loss: 0.0047 lr: 0.02\n",
      "iteration: 127380 loss: 0.0018 lr: 0.02\n",
      "iteration: 127390 loss: 0.0023 lr: 0.02\n",
      "iteration: 127400 loss: 0.0019 lr: 0.02\n",
      "iteration: 127410 loss: 0.0027 lr: 0.02\n",
      "iteration: 127420 loss: 0.0023 lr: 0.02\n",
      "iteration: 127430 loss: 0.0025 lr: 0.02\n",
      "iteration: 127440 loss: 0.0046 lr: 0.02\n",
      "iteration: 127450 loss: 0.0029 lr: 0.02\n",
      "iteration: 127460 loss: 0.0020 lr: 0.02\n",
      "iteration: 127470 loss: 0.0022 lr: 0.02\n",
      "iteration: 127480 loss: 0.0025 lr: 0.02\n",
      "iteration: 127490 loss: 0.0022 lr: 0.02\n",
      "iteration: 127500 loss: 0.0026 lr: 0.02\n",
      "iteration: 127510 loss: 0.0031 lr: 0.02\n",
      "iteration: 127520 loss: 0.0031 lr: 0.02\n",
      "iteration: 127530 loss: 0.0019 lr: 0.02\n",
      "iteration: 127540 loss: 0.0025 lr: 0.02\n",
      "iteration: 127550 loss: 0.0022 lr: 0.02\n",
      "iteration: 127560 loss: 0.0027 lr: 0.02\n",
      "iteration: 127570 loss: 0.0032 lr: 0.02\n",
      "iteration: 127580 loss: 0.0025 lr: 0.02\n",
      "iteration: 127590 loss: 0.0020 lr: 0.02\n",
      "iteration: 127600 loss: 0.0026 lr: 0.02\n",
      "iteration: 127610 loss: 0.0037 lr: 0.02\n",
      "iteration: 127620 loss: 0.0024 lr: 0.02\n",
      "iteration: 127630 loss: 0.0024 lr: 0.02\n",
      "iteration: 127640 loss: 0.0025 lr: 0.02\n",
      "iteration: 127650 loss: 0.0027 lr: 0.02\n",
      "iteration: 127660 loss: 0.0024 lr: 0.02\n",
      "iteration: 127670 loss: 0.0023 lr: 0.02\n",
      "iteration: 127680 loss: 0.0033 lr: 0.02\n",
      "iteration: 127690 loss: 0.0024 lr: 0.02\n",
      "iteration: 127700 loss: 0.0026 lr: 0.02\n",
      "iteration: 127710 loss: 0.0032 lr: 0.02\n",
      "iteration: 127720 loss: 0.0022 lr: 0.02\n",
      "iteration: 127730 loss: 0.0027 lr: 0.02\n",
      "iteration: 127740 loss: 0.0026 lr: 0.02\n",
      "iteration: 127750 loss: 0.0024 lr: 0.02\n",
      "iteration: 127760 loss: 0.0020 lr: 0.02\n",
      "iteration: 127770 loss: 0.0039 lr: 0.02\n",
      "iteration: 127780 loss: 0.0025 lr: 0.02\n",
      "iteration: 127790 loss: 0.0034 lr: 0.02\n",
      "iteration: 127800 loss: 0.0016 lr: 0.02\n",
      "iteration: 127810 loss: 0.0018 lr: 0.02\n",
      "iteration: 127820 loss: 0.0023 lr: 0.02\n",
      "iteration: 127830 loss: 0.0025 lr: 0.02\n",
      "iteration: 127840 loss: 0.0026 lr: 0.02\n",
      "iteration: 127850 loss: 0.0028 lr: 0.02\n",
      "iteration: 127860 loss: 0.0031 lr: 0.02\n",
      "iteration: 127870 loss: 0.0026 lr: 0.02\n",
      "iteration: 127880 loss: 0.0028 lr: 0.02\n",
      "iteration: 127890 loss: 0.0023 lr: 0.02\n",
      "iteration: 127900 loss: 0.0022 lr: 0.02\n",
      "iteration: 127910 loss: 0.0025 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 127920 loss: 0.0028 lr: 0.02\n",
      "iteration: 127930 loss: 0.0030 lr: 0.02\n",
      "iteration: 127940 loss: 0.0026 lr: 0.02\n",
      "iteration: 127950 loss: 0.0025 lr: 0.02\n",
      "iteration: 127960 loss: 0.0029 lr: 0.02\n",
      "iteration: 127970 loss: 0.0031 lr: 0.02\n",
      "iteration: 127980 loss: 0.0028 lr: 0.02\n",
      "iteration: 127990 loss: 0.0034 lr: 0.02\n",
      "iteration: 128000 loss: 0.0027 lr: 0.02\n",
      "iteration: 128010 loss: 0.0023 lr: 0.02\n",
      "iteration: 128020 loss: 0.0026 lr: 0.02\n",
      "iteration: 128030 loss: 0.0027 lr: 0.02\n",
      "iteration: 128040 loss: 0.0021 lr: 0.02\n",
      "iteration: 128050 loss: 0.0026 lr: 0.02\n",
      "iteration: 128060 loss: 0.0027 lr: 0.02\n",
      "iteration: 128070 loss: 0.0026 lr: 0.02\n",
      "iteration: 128080 loss: 0.0022 lr: 0.02\n",
      "iteration: 128090 loss: 0.0032 lr: 0.02\n",
      "iteration: 128100 loss: 0.0027 lr: 0.02\n",
      "iteration: 128110 loss: 0.0026 lr: 0.02\n",
      "iteration: 128120 loss: 0.0024 lr: 0.02\n",
      "iteration: 128130 loss: 0.0026 lr: 0.02\n",
      "iteration: 128140 loss: 0.0023 lr: 0.02\n",
      "iteration: 128150 loss: 0.0022 lr: 0.02\n",
      "iteration: 128160 loss: 0.0018 lr: 0.02\n",
      "iteration: 128170 loss: 0.0031 lr: 0.02\n",
      "iteration: 128180 loss: 0.0022 lr: 0.02\n",
      "iteration: 128190 loss: 0.0027 lr: 0.02\n",
      "iteration: 128200 loss: 0.0036 lr: 0.02\n",
      "iteration: 128210 loss: 0.0031 lr: 0.02\n",
      "iteration: 128220 loss: 0.0027 lr: 0.02\n",
      "iteration: 128230 loss: 0.0026 lr: 0.02\n",
      "iteration: 128240 loss: 0.0023 lr: 0.02\n",
      "iteration: 128250 loss: 0.0019 lr: 0.02\n",
      "iteration: 128260 loss: 0.0035 lr: 0.02\n",
      "iteration: 128270 loss: 0.0025 lr: 0.02\n",
      "iteration: 128280 loss: 0.0033 lr: 0.02\n",
      "iteration: 128290 loss: 0.0025 lr: 0.02\n",
      "iteration: 128300 loss: 0.0026 lr: 0.02\n",
      "iteration: 128310 loss: 0.0024 lr: 0.02\n",
      "iteration: 128320 loss: 0.0024 lr: 0.02\n",
      "iteration: 128330 loss: 0.0021 lr: 0.02\n",
      "iteration: 128340 loss: 0.0023 lr: 0.02\n",
      "iteration: 128350 loss: 0.0027 lr: 0.02\n",
      "iteration: 128360 loss: 0.0023 lr: 0.02\n",
      "iteration: 128370 loss: 0.0022 lr: 0.02\n",
      "iteration: 128380 loss: 0.0022 lr: 0.02\n",
      "iteration: 128390 loss: 0.0022 lr: 0.02\n",
      "iteration: 128400 loss: 0.0031 lr: 0.02\n",
      "iteration: 128410 loss: 0.0030 lr: 0.02\n",
      "iteration: 128420 loss: 0.0026 lr: 0.02\n",
      "iteration: 128430 loss: 0.0035 lr: 0.02\n",
      "iteration: 128440 loss: 0.0031 lr: 0.02\n",
      "iteration: 128450 loss: 0.0055 lr: 0.02\n",
      "iteration: 128460 loss: 0.0022 lr: 0.02\n",
      "iteration: 128470 loss: 0.0029 lr: 0.02\n",
      "iteration: 128480 loss: 0.0024 lr: 0.02\n",
      "iteration: 128490 loss: 0.0027 lr: 0.02\n",
      "iteration: 128500 loss: 0.0056 lr: 0.02\n",
      "iteration: 128510 loss: 0.0023 lr: 0.02\n",
      "iteration: 128520 loss: 0.0028 lr: 0.02\n",
      "iteration: 128530 loss: 0.0024 lr: 0.02\n",
      "iteration: 128540 loss: 0.0036 lr: 0.02\n",
      "iteration: 128550 loss: 0.0022 lr: 0.02\n",
      "iteration: 128560 loss: 0.0034 lr: 0.02\n",
      "iteration: 128570 loss: 0.0030 lr: 0.02\n",
      "iteration: 128580 loss: 0.0030 lr: 0.02\n",
      "iteration: 128590 loss: 0.0026 lr: 0.02\n",
      "iteration: 128600 loss: 0.0030 lr: 0.02\n",
      "iteration: 128610 loss: 0.0021 lr: 0.02\n",
      "iteration: 128620 loss: 0.0021 lr: 0.02\n",
      "iteration: 128630 loss: 0.0026 lr: 0.02\n",
      "iteration: 128640 loss: 0.0027 lr: 0.02\n",
      "iteration: 128650 loss: 0.0034 lr: 0.02\n",
      "iteration: 128660 loss: 0.0031 lr: 0.02\n",
      "iteration: 128670 loss: 0.0021 lr: 0.02\n",
      "iteration: 128680 loss: 0.0026 lr: 0.02\n",
      "iteration: 128690 loss: 0.0021 lr: 0.02\n",
      "iteration: 128700 loss: 0.0023 lr: 0.02\n",
      "iteration: 128710 loss: 0.0025 lr: 0.02\n",
      "iteration: 128720 loss: 0.0032 lr: 0.02\n",
      "iteration: 128730 loss: 0.0026 lr: 0.02\n",
      "iteration: 128740 loss: 0.0025 lr: 0.02\n",
      "iteration: 128750 loss: 0.0026 lr: 0.02\n",
      "iteration: 128760 loss: 0.0024 lr: 0.02\n",
      "iteration: 128770 loss: 0.0027 lr: 0.02\n",
      "iteration: 128780 loss: 0.0026 lr: 0.02\n",
      "iteration: 128790 loss: 0.0032 lr: 0.02\n",
      "iteration: 128800 loss: 0.0028 lr: 0.02\n",
      "iteration: 128810 loss: 0.0029 lr: 0.02\n",
      "iteration: 128820 loss: 0.0025 lr: 0.02\n",
      "iteration: 128830 loss: 0.0030 lr: 0.02\n",
      "iteration: 128840 loss: 0.0024 lr: 0.02\n",
      "iteration: 128850 loss: 0.0030 lr: 0.02\n",
      "iteration: 128860 loss: 0.0024 lr: 0.02\n",
      "iteration: 128870 loss: 0.0022 lr: 0.02\n",
      "iteration: 128880 loss: 0.0026 lr: 0.02\n",
      "iteration: 128890 loss: 0.0029 lr: 0.02\n",
      "iteration: 128900 loss: 0.0026 lr: 0.02\n",
      "iteration: 128910 loss: 0.0024 lr: 0.02\n",
      "iteration: 128920 loss: 0.0029 lr: 0.02\n",
      "iteration: 128930 loss: 0.0041 lr: 0.02\n",
      "iteration: 128940 loss: 0.0021 lr: 0.02\n",
      "iteration: 128950 loss: 0.0030 lr: 0.02\n",
      "iteration: 128960 loss: 0.0021 lr: 0.02\n",
      "iteration: 128970 loss: 0.0026 lr: 0.02\n",
      "iteration: 128980 loss: 0.0030 lr: 0.02\n",
      "iteration: 128990 loss: 0.0026 lr: 0.02\n",
      "iteration: 129000 loss: 0.0027 lr: 0.02\n",
      "iteration: 129010 loss: 0.0030 lr: 0.02\n",
      "iteration: 129020 loss: 0.0022 lr: 0.02\n",
      "iteration: 129030 loss: 0.0026 lr: 0.02\n",
      "iteration: 129040 loss: 0.0033 lr: 0.02\n",
      "iteration: 129050 loss: 0.0021 lr: 0.02\n",
      "iteration: 129060 loss: 0.0025 lr: 0.02\n",
      "iteration: 129070 loss: 0.0024 lr: 0.02\n",
      "iteration: 129080 loss: 0.0017 lr: 0.02\n",
      "iteration: 129090 loss: 0.0021 lr: 0.02\n",
      "iteration: 129100 loss: 0.0023 lr: 0.02\n",
      "iteration: 129110 loss: 0.0025 lr: 0.02\n",
      "iteration: 129120 loss: 0.0022 lr: 0.02\n",
      "iteration: 129130 loss: 0.0019 lr: 0.02\n",
      "iteration: 129140 loss: 0.0021 lr: 0.02\n",
      "iteration: 129150 loss: 0.0035 lr: 0.02\n",
      "iteration: 129160 loss: 0.0026 lr: 0.02\n",
      "iteration: 129170 loss: 0.0025 lr: 0.02\n",
      "iteration: 129180 loss: 0.0022 lr: 0.02\n",
      "iteration: 129190 loss: 0.0039 lr: 0.02\n",
      "iteration: 129200 loss: 0.0020 lr: 0.02\n",
      "iteration: 129210 loss: 0.0023 lr: 0.02\n",
      "iteration: 129220 loss: 0.0020 lr: 0.02\n",
      "iteration: 129230 loss: 0.0028 lr: 0.02\n",
      "iteration: 129240 loss: 0.0029 lr: 0.02\n",
      "iteration: 129250 loss: 0.0025 lr: 0.02\n",
      "iteration: 129260 loss: 0.0024 lr: 0.02\n",
      "iteration: 129270 loss: 0.0027 lr: 0.02\n",
      "iteration: 129280 loss: 0.0025 lr: 0.02\n",
      "iteration: 129290 loss: 0.0031 lr: 0.02\n",
      "iteration: 129300 loss: 0.0031 lr: 0.02\n",
      "iteration: 129310 loss: 0.0018 lr: 0.02\n",
      "iteration: 129320 loss: 0.0027 lr: 0.02\n",
      "iteration: 129330 loss: 0.0027 lr: 0.02\n",
      "iteration: 129340 loss: 0.0026 lr: 0.02\n",
      "iteration: 129350 loss: 0.0039 lr: 0.02\n",
      "iteration: 129360 loss: 0.0029 lr: 0.02\n",
      "iteration: 129370 loss: 0.0029 lr: 0.02\n",
      "iteration: 129380 loss: 0.0026 lr: 0.02\n",
      "iteration: 129390 loss: 0.0023 lr: 0.02\n",
      "iteration: 129400 loss: 0.0020 lr: 0.02\n",
      "iteration: 129410 loss: 0.0023 lr: 0.02\n",
      "iteration: 129420 loss: 0.0032 lr: 0.02\n",
      "iteration: 129430 loss: 0.0029 lr: 0.02\n",
      "iteration: 129440 loss: 0.0029 lr: 0.02\n",
      "iteration: 129450 loss: 0.0022 lr: 0.02\n",
      "iteration: 129460 loss: 0.0028 lr: 0.02\n",
      "iteration: 129470 loss: 0.0032 lr: 0.02\n",
      "iteration: 129480 loss: 0.0027 lr: 0.02\n",
      "iteration: 129490 loss: 0.0027 lr: 0.02\n",
      "iteration: 129500 loss: 0.0024 lr: 0.02\n",
      "iteration: 129510 loss: 0.0030 lr: 0.02\n",
      "iteration: 129520 loss: 0.0032 lr: 0.02\n",
      "iteration: 129530 loss: 0.0027 lr: 0.02\n",
      "iteration: 129540 loss: 0.0027 lr: 0.02\n",
      "iteration: 129550 loss: 0.0022 lr: 0.02\n",
      "iteration: 129560 loss: 0.0025 lr: 0.02\n",
      "iteration: 129570 loss: 0.0024 lr: 0.02\n",
      "iteration: 129580 loss: 0.0031 lr: 0.02\n",
      "iteration: 129590 loss: 0.0029 lr: 0.02\n",
      "iteration: 129600 loss: 0.0030 lr: 0.02\n",
      "iteration: 129610 loss: 0.0019 lr: 0.02\n",
      "iteration: 129620 loss: 0.0021 lr: 0.02\n",
      "iteration: 129630 loss: 0.0032 lr: 0.02\n",
      "iteration: 129640 loss: 0.0025 lr: 0.02\n",
      "iteration: 129650 loss: 0.0026 lr: 0.02\n",
      "iteration: 129660 loss: 0.0024 lr: 0.02\n",
      "iteration: 129670 loss: 0.0024 lr: 0.02\n",
      "iteration: 129680 loss: 0.0021 lr: 0.02\n",
      "iteration: 129690 loss: 0.0022 lr: 0.02\n",
      "iteration: 129700 loss: 0.0032 lr: 0.02\n",
      "iteration: 129710 loss: 0.0029 lr: 0.02\n",
      "iteration: 129720 loss: 0.0024 lr: 0.02\n",
      "iteration: 129730 loss: 0.0024 lr: 0.02\n",
      "iteration: 129740 loss: 0.0024 lr: 0.02\n",
      "iteration: 129750 loss: 0.0024 lr: 0.02\n",
      "iteration: 129760 loss: 0.0020 lr: 0.02\n",
      "iteration: 129770 loss: 0.0031 lr: 0.02\n",
      "iteration: 129780 loss: 0.0023 lr: 0.02\n",
      "iteration: 129790 loss: 0.0021 lr: 0.02\n",
      "iteration: 129800 loss: 0.0024 lr: 0.02\n",
      "iteration: 129810 loss: 0.0020 lr: 0.02\n",
      "iteration: 129820 loss: 0.0026 lr: 0.02\n",
      "iteration: 129830 loss: 0.0021 lr: 0.02\n",
      "iteration: 129840 loss: 0.0029 lr: 0.02\n",
      "iteration: 129850 loss: 0.0024 lr: 0.02\n",
      "iteration: 129860 loss: 0.0022 lr: 0.02\n",
      "iteration: 129870 loss: 0.0022 lr: 0.02\n",
      "iteration: 129880 loss: 0.0024 lr: 0.02\n",
      "iteration: 129890 loss: 0.0025 lr: 0.02\n",
      "iteration: 129900 loss: 0.0024 lr: 0.02\n",
      "iteration: 129910 loss: 0.0027 lr: 0.02\n",
      "iteration: 129920 loss: 0.0021 lr: 0.02\n",
      "iteration: 129930 loss: 0.0019 lr: 0.02\n",
      "iteration: 129940 loss: 0.0022 lr: 0.02\n",
      "iteration: 129950 loss: 0.0029 lr: 0.02\n",
      "iteration: 129960 loss: 0.0024 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 129970 loss: 0.0025 lr: 0.02\n",
      "iteration: 129980 loss: 0.0019 lr: 0.02\n",
      "iteration: 129990 loss: 0.0018 lr: 0.02\n",
      "iteration: 130000 loss: 0.0028 lr: 0.02\n",
      "iteration: 130010 loss: 0.0025 lr: 0.02\n",
      "iteration: 130020 loss: 0.0022 lr: 0.02\n",
      "iteration: 130030 loss: 0.0030 lr: 0.02\n",
      "iteration: 130040 loss: 0.0024 lr: 0.02\n",
      "iteration: 130050 loss: 0.0028 lr: 0.02\n",
      "iteration: 130060 loss: 0.0029 lr: 0.02\n",
      "iteration: 130070 loss: 0.0033 lr: 0.02\n",
      "iteration: 130080 loss: 0.0033 lr: 0.02\n",
      "iteration: 130090 loss: 0.0021 lr: 0.02\n",
      "iteration: 130100 loss: 0.0028 lr: 0.02\n",
      "iteration: 130110 loss: 0.0026 lr: 0.02\n",
      "iteration: 130120 loss: 0.0024 lr: 0.02\n",
      "iteration: 130130 loss: 0.0029 lr: 0.02\n",
      "iteration: 130140 loss: 0.0024 lr: 0.02\n",
      "iteration: 130150 loss: 0.0032 lr: 0.02\n",
      "iteration: 130160 loss: 0.0023 lr: 0.02\n",
      "iteration: 130170 loss: 0.0025 lr: 0.02\n",
      "iteration: 130180 loss: 0.0028 lr: 0.02\n",
      "iteration: 130190 loss: 0.0023 lr: 0.02\n",
      "iteration: 130200 loss: 0.0025 lr: 0.02\n",
      "iteration: 130210 loss: 0.0020 lr: 0.02\n",
      "iteration: 130220 loss: 0.0029 lr: 0.02\n",
      "iteration: 130230 loss: 0.0032 lr: 0.02\n",
      "iteration: 130240 loss: 0.0030 lr: 0.02\n",
      "iteration: 130250 loss: 0.0021 lr: 0.02\n",
      "iteration: 130260 loss: 0.0029 lr: 0.02\n",
      "iteration: 130270 loss: 0.0029 lr: 0.02\n",
      "iteration: 130280 loss: 0.0024 lr: 0.02\n",
      "iteration: 130290 loss: 0.0023 lr: 0.02\n",
      "iteration: 130300 loss: 0.0028 lr: 0.02\n",
      "iteration: 130310 loss: 0.0022 lr: 0.02\n",
      "iteration: 130320 loss: 0.0021 lr: 0.02\n",
      "iteration: 130330 loss: 0.0032 lr: 0.02\n",
      "iteration: 130340 loss: 0.0026 lr: 0.02\n",
      "iteration: 130350 loss: 0.0027 lr: 0.02\n",
      "iteration: 130360 loss: 0.0029 lr: 0.02\n",
      "iteration: 130370 loss: 0.0028 lr: 0.02\n",
      "iteration: 130380 loss: 0.0028 lr: 0.02\n",
      "iteration: 130390 loss: 0.0016 lr: 0.02\n",
      "iteration: 130400 loss: 0.0022 lr: 0.02\n",
      "iteration: 130410 loss: 0.0022 lr: 0.02\n",
      "iteration: 130420 loss: 0.0030 lr: 0.02\n",
      "iteration: 130430 loss: 0.0029 lr: 0.02\n",
      "iteration: 130440 loss: 0.0024 lr: 0.02\n",
      "iteration: 130450 loss: 0.0028 lr: 0.02\n",
      "iteration: 130460 loss: 0.0021 lr: 0.02\n",
      "iteration: 130470 loss: 0.0026 lr: 0.02\n",
      "iteration: 130480 loss: 0.0021 lr: 0.02\n",
      "iteration: 130490 loss: 0.0027 lr: 0.02\n",
      "iteration: 130500 loss: 0.0021 lr: 0.02\n",
      "iteration: 130510 loss: 0.0030 lr: 0.02\n",
      "iteration: 130520 loss: 0.0033 lr: 0.02\n",
      "iteration: 130530 loss: 0.0029 lr: 0.02\n",
      "iteration: 130540 loss: 0.0026 lr: 0.02\n",
      "iteration: 130550 loss: 0.0032 lr: 0.02\n",
      "iteration: 130560 loss: 0.0025 lr: 0.02\n",
      "iteration: 130570 loss: 0.0018 lr: 0.02\n",
      "iteration: 130580 loss: 0.0032 lr: 0.02\n",
      "iteration: 130590 loss: 0.0026 lr: 0.02\n",
      "iteration: 130600 loss: 0.0023 lr: 0.02\n",
      "iteration: 130610 loss: 0.0029 lr: 0.02\n",
      "iteration: 130620 loss: 0.0030 lr: 0.02\n",
      "iteration: 130630 loss: 0.0026 lr: 0.02\n",
      "iteration: 130640 loss: 0.0023 lr: 0.02\n",
      "iteration: 130650 loss: 0.0028 lr: 0.02\n",
      "iteration: 130660 loss: 0.0028 lr: 0.02\n",
      "iteration: 130670 loss: 0.0025 lr: 0.02\n",
      "iteration: 130680 loss: 0.0026 lr: 0.02\n",
      "iteration: 130690 loss: 0.0032 lr: 0.02\n",
      "iteration: 130700 loss: 0.0024 lr: 0.02\n",
      "iteration: 130710 loss: 0.0026 lr: 0.02\n",
      "iteration: 130720 loss: 0.0024 lr: 0.02\n",
      "iteration: 130730 loss: 0.0036 lr: 0.02\n",
      "iteration: 130740 loss: 0.0026 lr: 0.02\n",
      "iteration: 130750 loss: 0.0024 lr: 0.02\n",
      "iteration: 130760 loss: 0.0029 lr: 0.02\n",
      "iteration: 130770 loss: 0.0020 lr: 0.02\n",
      "iteration: 130780 loss: 0.0027 lr: 0.02\n",
      "iteration: 130790 loss: 0.0032 lr: 0.02\n",
      "iteration: 130800 loss: 0.0022 lr: 0.02\n",
      "iteration: 130810 loss: 0.0030 lr: 0.02\n",
      "iteration: 130820 loss: 0.0031 lr: 0.02\n",
      "iteration: 130830 loss: 0.0031 lr: 0.02\n",
      "iteration: 130840 loss: 0.0021 lr: 0.02\n",
      "iteration: 130850 loss: 0.0026 lr: 0.02\n",
      "iteration: 130860 loss: 0.0029 lr: 0.02\n",
      "iteration: 130870 loss: 0.0025 lr: 0.02\n",
      "iteration: 130880 loss: 0.0033 lr: 0.02\n",
      "iteration: 130890 loss: 0.0033 lr: 0.02\n",
      "iteration: 130900 loss: 0.0030 lr: 0.02\n",
      "iteration: 130910 loss: 0.0032 lr: 0.02\n",
      "iteration: 130920 loss: 0.0023 lr: 0.02\n",
      "iteration: 130930 loss: 0.0034 lr: 0.02\n",
      "iteration: 130940 loss: 0.0027 lr: 0.02\n",
      "iteration: 130950 loss: 0.0023 lr: 0.02\n",
      "iteration: 130960 loss: 0.0026 lr: 0.02\n",
      "iteration: 130970 loss: 0.0026 lr: 0.02\n",
      "iteration: 130980 loss: 0.0038 lr: 0.02\n",
      "iteration: 130990 loss: 0.0022 lr: 0.02\n",
      "iteration: 131000 loss: 0.0027 lr: 0.02\n",
      "iteration: 131010 loss: 0.0028 lr: 0.02\n",
      "iteration: 131020 loss: 0.0019 lr: 0.02\n",
      "iteration: 131030 loss: 0.0030 lr: 0.02\n",
      "iteration: 131040 loss: 0.0036 lr: 0.02\n",
      "iteration: 131050 loss: 0.0039 lr: 0.02\n",
      "iteration: 131060 loss: 0.0029 lr: 0.02\n",
      "iteration: 131070 loss: 0.0028 lr: 0.02\n",
      "iteration: 131080 loss: 0.0028 lr: 0.02\n",
      "iteration: 131090 loss: 0.0029 lr: 0.02\n",
      "iteration: 131100 loss: 0.0023 lr: 0.02\n",
      "iteration: 131110 loss: 0.0027 lr: 0.02\n",
      "iteration: 131120 loss: 0.0024 lr: 0.02\n",
      "iteration: 131130 loss: 0.0026 lr: 0.02\n",
      "iteration: 131140 loss: 0.0026 lr: 0.02\n",
      "iteration: 131150 loss: 0.0018 lr: 0.02\n",
      "iteration: 131160 loss: 0.0024 lr: 0.02\n",
      "iteration: 131170 loss: 0.0024 lr: 0.02\n",
      "iteration: 131180 loss: 0.0028 lr: 0.02\n",
      "iteration: 131190 loss: 0.0024 lr: 0.02\n",
      "iteration: 131200 loss: 0.0029 lr: 0.02\n",
      "iteration: 131210 loss: 0.0022 lr: 0.02\n",
      "iteration: 131220 loss: 0.0022 lr: 0.02\n",
      "iteration: 131230 loss: 0.0021 lr: 0.02\n",
      "iteration: 131240 loss: 0.0019 lr: 0.02\n",
      "iteration: 131250 loss: 0.0033 lr: 0.02\n",
      "iteration: 131260 loss: 0.0027 lr: 0.02\n",
      "iteration: 131270 loss: 0.0020 lr: 0.02\n",
      "iteration: 131280 loss: 0.0016 lr: 0.02\n",
      "iteration: 131290 loss: 0.0028 lr: 0.02\n",
      "iteration: 131300 loss: 0.0049 lr: 0.02\n",
      "iteration: 131310 loss: 0.0032 lr: 0.02\n",
      "iteration: 131320 loss: 0.0044 lr: 0.02\n",
      "iteration: 131330 loss: 0.0016 lr: 0.02\n",
      "iteration: 131340 loss: 0.0027 lr: 0.02\n",
      "iteration: 131350 loss: 0.0031 lr: 0.02\n",
      "iteration: 131360 loss: 0.0024 lr: 0.02\n",
      "iteration: 131370 loss: 0.0028 lr: 0.02\n",
      "iteration: 131380 loss: 0.0034 lr: 0.02\n",
      "iteration: 131390 loss: 0.0029 lr: 0.02\n",
      "iteration: 131400 loss: 0.0020 lr: 0.02\n",
      "iteration: 131410 loss: 0.0026 lr: 0.02\n",
      "iteration: 131420 loss: 0.0032 lr: 0.02\n",
      "iteration: 131430 loss: 0.0025 lr: 0.02\n",
      "iteration: 131440 loss: 0.0020 lr: 0.02\n",
      "iteration: 131450 loss: 0.0021 lr: 0.02\n",
      "iteration: 131460 loss: 0.0022 lr: 0.02\n",
      "iteration: 131470 loss: 0.0024 lr: 0.02\n",
      "iteration: 131480 loss: 0.0022 lr: 0.02\n",
      "iteration: 131490 loss: 0.0019 lr: 0.02\n",
      "iteration: 131500 loss: 0.0027 lr: 0.02\n",
      "iteration: 131510 loss: 0.0037 lr: 0.02\n",
      "iteration: 131520 loss: 0.0035 lr: 0.02\n",
      "iteration: 131530 loss: 0.0025 lr: 0.02\n",
      "iteration: 131540 loss: 0.0025 lr: 0.02\n",
      "iteration: 131550 loss: 0.0031 lr: 0.02\n",
      "iteration: 131560 loss: 0.0025 lr: 0.02\n",
      "iteration: 131570 loss: 0.0030 lr: 0.02\n",
      "iteration: 131580 loss: 0.0024 lr: 0.02\n",
      "iteration: 131590 loss: 0.0024 lr: 0.02\n",
      "iteration: 131600 loss: 0.0019 lr: 0.02\n",
      "iteration: 131610 loss: 0.0025 lr: 0.02\n",
      "iteration: 131620 loss: 0.0022 lr: 0.02\n",
      "iteration: 131630 loss: 0.0022 lr: 0.02\n",
      "iteration: 131640 loss: 0.0023 lr: 0.02\n",
      "iteration: 131650 loss: 0.0031 lr: 0.02\n",
      "iteration: 131660 loss: 0.0024 lr: 0.02\n",
      "iteration: 131670 loss: 0.0024 lr: 0.02\n",
      "iteration: 131680 loss: 0.0028 lr: 0.02\n",
      "iteration: 131690 loss: 0.0023 lr: 0.02\n",
      "iteration: 131700 loss: 0.0023 lr: 0.02\n",
      "iteration: 131710 loss: 0.0021 lr: 0.02\n",
      "iteration: 131720 loss: 0.0022 lr: 0.02\n",
      "iteration: 131730 loss: 0.0018 lr: 0.02\n",
      "iteration: 131740 loss: 0.0021 lr: 0.02\n",
      "iteration: 131750 loss: 0.0027 lr: 0.02\n",
      "iteration: 131760 loss: 0.0028 lr: 0.02\n",
      "iteration: 131770 loss: 0.0021 lr: 0.02\n",
      "iteration: 131780 loss: 0.0028 lr: 0.02\n",
      "iteration: 131790 loss: 0.0025 lr: 0.02\n",
      "iteration: 131800 loss: 0.0019 lr: 0.02\n",
      "iteration: 131810 loss: 0.0019 lr: 0.02\n",
      "iteration: 131820 loss: 0.0024 lr: 0.02\n",
      "iteration: 131830 loss: 0.0028 lr: 0.02\n",
      "iteration: 131840 loss: 0.0021 lr: 0.02\n",
      "iteration: 131850 loss: 0.0028 lr: 0.02\n",
      "iteration: 131860 loss: 0.0024 lr: 0.02\n",
      "iteration: 131870 loss: 0.0023 lr: 0.02\n",
      "iteration: 131880 loss: 0.0026 lr: 0.02\n",
      "iteration: 131890 loss: 0.0026 lr: 0.02\n",
      "iteration: 131900 loss: 0.0018 lr: 0.02\n",
      "iteration: 131910 loss: 0.0022 lr: 0.02\n",
      "iteration: 131920 loss: 0.0022 lr: 0.02\n",
      "iteration: 131930 loss: 0.0028 lr: 0.02\n",
      "iteration: 131940 loss: 0.0026 lr: 0.02\n",
      "iteration: 131950 loss: 0.0029 lr: 0.02\n",
      "iteration: 131960 loss: 0.0027 lr: 0.02\n",
      "iteration: 131970 loss: 0.0024 lr: 0.02\n",
      "iteration: 131980 loss: 0.0020 lr: 0.02\n",
      "iteration: 131990 loss: 0.0029 lr: 0.02\n",
      "iteration: 132000 loss: 0.0030 lr: 0.02\n",
      "iteration: 132010 loss: 0.0023 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 132020 loss: 0.0029 lr: 0.02\n",
      "iteration: 132030 loss: 0.0028 lr: 0.02\n",
      "iteration: 132040 loss: 0.0037 lr: 0.02\n",
      "iteration: 132050 loss: 0.0019 lr: 0.02\n",
      "iteration: 132060 loss: 0.0034 lr: 0.02\n",
      "iteration: 132070 loss: 0.0027 lr: 0.02\n",
      "iteration: 132080 loss: 0.0021 lr: 0.02\n",
      "iteration: 132090 loss: 0.0026 lr: 0.02\n",
      "iteration: 132100 loss: 0.0020 lr: 0.02\n",
      "iteration: 132110 loss: 0.0026 lr: 0.02\n",
      "iteration: 132120 loss: 0.0033 lr: 0.02\n",
      "iteration: 132130 loss: 0.0027 lr: 0.02\n",
      "iteration: 132140 loss: 0.0022 lr: 0.02\n",
      "iteration: 132150 loss: 0.0023 lr: 0.02\n",
      "iteration: 132160 loss: 0.0029 lr: 0.02\n",
      "iteration: 132170 loss: 0.0030 lr: 0.02\n",
      "iteration: 132180 loss: 0.0033 lr: 0.02\n",
      "iteration: 132190 loss: 0.0027 lr: 0.02\n",
      "iteration: 132200 loss: 0.0017 lr: 0.02\n",
      "iteration: 132210 loss: 0.0032 lr: 0.02\n",
      "iteration: 132220 loss: 0.0021 lr: 0.02\n",
      "iteration: 132230 loss: 0.0031 lr: 0.02\n",
      "iteration: 132240 loss: 0.0034 lr: 0.02\n",
      "iteration: 132250 loss: 0.0020 lr: 0.02\n",
      "iteration: 132260 loss: 0.0027 lr: 0.02\n",
      "iteration: 132270 loss: 0.0023 lr: 0.02\n",
      "iteration: 132280 loss: 0.0028 lr: 0.02\n",
      "iteration: 132290 loss: 0.0022 lr: 0.02\n",
      "iteration: 132300 loss: 0.0024 lr: 0.02\n",
      "iteration: 132310 loss: 0.0040 lr: 0.02\n",
      "iteration: 132320 loss: 0.0018 lr: 0.02\n",
      "iteration: 132330 loss: 0.0038 lr: 0.02\n",
      "iteration: 132340 loss: 0.0023 lr: 0.02\n",
      "iteration: 132350 loss: 0.0023 lr: 0.02\n",
      "iteration: 132360 loss: 0.0023 lr: 0.02\n",
      "iteration: 132370 loss: 0.0018 lr: 0.02\n",
      "iteration: 132380 loss: 0.0023 lr: 0.02\n",
      "iteration: 132390 loss: 0.0027 lr: 0.02\n",
      "iteration: 132400 loss: 0.0022 lr: 0.02\n",
      "iteration: 132410 loss: 0.0023 lr: 0.02\n",
      "iteration: 132420 loss: 0.0023 lr: 0.02\n",
      "iteration: 132430 loss: 0.0026 lr: 0.02\n",
      "iteration: 132440 loss: 0.0025 lr: 0.02\n",
      "iteration: 132450 loss: 0.0026 lr: 0.02\n",
      "iteration: 132460 loss: 0.0027 lr: 0.02\n",
      "iteration: 132470 loss: 0.0031 lr: 0.02\n",
      "iteration: 132480 loss: 0.0029 lr: 0.02\n",
      "iteration: 132490 loss: 0.0024 lr: 0.02\n",
      "iteration: 132500 loss: 0.0037 lr: 0.02\n",
      "iteration: 132510 loss: 0.0028 lr: 0.02\n",
      "iteration: 132520 loss: 0.0023 lr: 0.02\n",
      "iteration: 132530 loss: 0.0023 lr: 0.02\n",
      "iteration: 132540 loss: 0.0026 lr: 0.02\n",
      "iteration: 132550 loss: 0.0021 lr: 0.02\n",
      "iteration: 132560 loss: 0.0024 lr: 0.02\n",
      "iteration: 132570 loss: 0.0030 lr: 0.02\n",
      "iteration: 132580 loss: 0.0022 lr: 0.02\n",
      "iteration: 132590 loss: 0.0022 lr: 0.02\n",
      "iteration: 132600 loss: 0.0041 lr: 0.02\n",
      "iteration: 132610 loss: 0.0030 lr: 0.02\n",
      "iteration: 132620 loss: 0.0021 lr: 0.02\n",
      "iteration: 132630 loss: 0.0023 lr: 0.02\n",
      "iteration: 132640 loss: 0.0029 lr: 0.02\n",
      "iteration: 132650 loss: 0.0023 lr: 0.02\n",
      "iteration: 132660 loss: 0.0019 lr: 0.02\n",
      "iteration: 132670 loss: 0.0020 lr: 0.02\n",
      "iteration: 132680 loss: 0.0022 lr: 0.02\n",
      "iteration: 132690 loss: 0.0025 lr: 0.02\n",
      "iteration: 132700 loss: 0.0024 lr: 0.02\n",
      "iteration: 132710 loss: 0.0026 lr: 0.02\n",
      "iteration: 132720 loss: 0.0027 lr: 0.02\n",
      "iteration: 132730 loss: 0.0023 lr: 0.02\n",
      "iteration: 132740 loss: 0.0029 lr: 0.02\n",
      "iteration: 132750 loss: 0.0032 lr: 0.02\n",
      "iteration: 132760 loss: 0.0024 lr: 0.02\n",
      "iteration: 132770 loss: 0.0026 lr: 0.02\n",
      "iteration: 132780 loss: 0.0022 lr: 0.02\n",
      "iteration: 132790 loss: 0.0021 lr: 0.02\n",
      "iteration: 132800 loss: 0.0025 lr: 0.02\n",
      "iteration: 132810 loss: 0.0027 lr: 0.02\n",
      "iteration: 132820 loss: 0.0027 lr: 0.02\n",
      "iteration: 132830 loss: 0.0024 lr: 0.02\n",
      "iteration: 132840 loss: 0.0027 lr: 0.02\n",
      "iteration: 132850 loss: 0.0019 lr: 0.02\n",
      "iteration: 132860 loss: 0.0022 lr: 0.02\n",
      "iteration: 132870 loss: 0.0024 lr: 0.02\n",
      "iteration: 132880 loss: 0.0034 lr: 0.02\n",
      "iteration: 132890 loss: 0.0026 lr: 0.02\n",
      "iteration: 132900 loss: 0.0022 lr: 0.02\n",
      "iteration: 132910 loss: 0.0035 lr: 0.02\n",
      "iteration: 132920 loss: 0.0022 lr: 0.02\n",
      "iteration: 132930 loss: 0.0024 lr: 0.02\n",
      "iteration: 132940 loss: 0.0023 lr: 0.02\n",
      "iteration: 132950 loss: 0.0019 lr: 0.02\n",
      "iteration: 132960 loss: 0.0025 lr: 0.02\n",
      "iteration: 132970 loss: 0.0022 lr: 0.02\n",
      "iteration: 132980 loss: 0.0028 lr: 0.02\n",
      "iteration: 132990 loss: 0.0022 lr: 0.02\n",
      "iteration: 133000 loss: 0.0030 lr: 0.02\n",
      "iteration: 133010 loss: 0.0022 lr: 0.02\n",
      "iteration: 133020 loss: 0.0028 lr: 0.02\n",
      "iteration: 133030 loss: 0.0025 lr: 0.02\n",
      "iteration: 133040 loss: 0.0027 lr: 0.02\n",
      "iteration: 133050 loss: 0.0022 lr: 0.02\n",
      "iteration: 133060 loss: 0.0025 lr: 0.02\n",
      "iteration: 133070 loss: 0.0021 lr: 0.02\n",
      "iteration: 133080 loss: 0.0025 lr: 0.02\n",
      "iteration: 133090 loss: 0.0019 lr: 0.02\n",
      "iteration: 133100 loss: 0.0023 lr: 0.02\n",
      "iteration: 133110 loss: 0.0018 lr: 0.02\n",
      "iteration: 133120 loss: 0.0020 lr: 0.02\n",
      "iteration: 133130 loss: 0.0025 lr: 0.02\n",
      "iteration: 133140 loss: 0.0027 lr: 0.02\n",
      "iteration: 133150 loss: 0.0027 lr: 0.02\n",
      "iteration: 133160 loss: 0.0028 lr: 0.02\n",
      "iteration: 133170 loss: 0.0026 lr: 0.02\n",
      "iteration: 133180 loss: 0.0022 lr: 0.02\n",
      "iteration: 133190 loss: 0.0019 lr: 0.02\n",
      "iteration: 133200 loss: 0.0026 lr: 0.02\n",
      "iteration: 133210 loss: 0.0029 lr: 0.02\n",
      "iteration: 133220 loss: 0.0024 lr: 0.02\n",
      "iteration: 133230 loss: 0.0025 lr: 0.02\n",
      "iteration: 133240 loss: 0.0027 lr: 0.02\n",
      "iteration: 133250 loss: 0.0029 lr: 0.02\n",
      "iteration: 133260 loss: 0.0030 lr: 0.02\n",
      "iteration: 133270 loss: 0.0033 lr: 0.02\n",
      "iteration: 133280 loss: 0.0027 lr: 0.02\n",
      "iteration: 133290 loss: 0.0028 lr: 0.02\n",
      "iteration: 133300 loss: 0.0025 lr: 0.02\n",
      "iteration: 133310 loss: 0.0022 lr: 0.02\n",
      "iteration: 133320 loss: 0.0029 lr: 0.02\n",
      "iteration: 133330 loss: 0.0025 lr: 0.02\n",
      "iteration: 133340 loss: 0.0022 lr: 0.02\n",
      "iteration: 133350 loss: 0.0023 lr: 0.02\n",
      "iteration: 133360 loss: 0.0019 lr: 0.02\n",
      "iteration: 133370 loss: 0.0028 lr: 0.02\n",
      "iteration: 133380 loss: 0.0027 lr: 0.02\n",
      "iteration: 133390 loss: 0.0023 lr: 0.02\n",
      "iteration: 133400 loss: 0.0027 lr: 0.02\n",
      "iteration: 133410 loss: 0.0024 lr: 0.02\n",
      "iteration: 133420 loss: 0.0032 lr: 0.02\n",
      "iteration: 133430 loss: 0.0033 lr: 0.02\n",
      "iteration: 133440 loss: 0.0025 lr: 0.02\n",
      "iteration: 133450 loss: 0.0023 lr: 0.02\n",
      "iteration: 133460 loss: 0.0024 lr: 0.02\n",
      "iteration: 133470 loss: 0.0032 lr: 0.02\n",
      "iteration: 133480 loss: 0.0022 lr: 0.02\n",
      "iteration: 133490 loss: 0.0030 lr: 0.02\n",
      "iteration: 133500 loss: 0.0021 lr: 0.02\n",
      "iteration: 133510 loss: 0.0020 lr: 0.02\n",
      "iteration: 133520 loss: 0.0026 lr: 0.02\n",
      "iteration: 133530 loss: 0.0024 lr: 0.02\n",
      "iteration: 133540 loss: 0.0031 lr: 0.02\n",
      "iteration: 133550 loss: 0.0025 lr: 0.02\n",
      "iteration: 133560 loss: 0.0024 lr: 0.02\n",
      "iteration: 133570 loss: 0.0033 lr: 0.02\n",
      "iteration: 133580 loss: 0.0023 lr: 0.02\n",
      "iteration: 133590 loss: 0.0027 lr: 0.02\n",
      "iteration: 133600 loss: 0.0022 lr: 0.02\n",
      "iteration: 133610 loss: 0.0032 lr: 0.02\n",
      "iteration: 133620 loss: 0.0023 lr: 0.02\n",
      "iteration: 133630 loss: 0.0030 lr: 0.02\n",
      "iteration: 133640 loss: 0.0032 lr: 0.02\n",
      "iteration: 133650 loss: 0.0033 lr: 0.02\n",
      "iteration: 133660 loss: 0.0027 lr: 0.02\n",
      "iteration: 133670 loss: 0.0024 lr: 0.02\n",
      "iteration: 133680 loss: 0.0029 lr: 0.02\n",
      "iteration: 133690 loss: 0.0035 lr: 0.02\n",
      "iteration: 133700 loss: 0.0026 lr: 0.02\n",
      "iteration: 133710 loss: 0.0022 lr: 0.02\n",
      "iteration: 133720 loss: 0.0029 lr: 0.02\n",
      "iteration: 133730 loss: 0.0032 lr: 0.02\n",
      "iteration: 133740 loss: 0.0020 lr: 0.02\n",
      "iteration: 133750 loss: 0.0028 lr: 0.02\n",
      "iteration: 133760 loss: 0.0023 lr: 0.02\n",
      "iteration: 133770 loss: 0.0023 lr: 0.02\n",
      "iteration: 133780 loss: 0.0024 lr: 0.02\n",
      "iteration: 133790 loss: 0.0026 lr: 0.02\n",
      "iteration: 133800 loss: 0.0031 lr: 0.02\n",
      "iteration: 133810 loss: 0.0031 lr: 0.02\n",
      "iteration: 133820 loss: 0.0027 lr: 0.02\n",
      "iteration: 133830 loss: 0.0024 lr: 0.02\n",
      "iteration: 133840 loss: 0.0034 lr: 0.02\n",
      "iteration: 133850 loss: 0.0018 lr: 0.02\n",
      "iteration: 133860 loss: 0.0032 lr: 0.02\n",
      "iteration: 133870 loss: 0.0032 lr: 0.02\n",
      "iteration: 133880 loss: 0.0035 lr: 0.02\n",
      "iteration: 133890 loss: 0.0021 lr: 0.02\n",
      "iteration: 133900 loss: 0.0021 lr: 0.02\n",
      "iteration: 133910 loss: 0.0026 lr: 0.02\n",
      "iteration: 133920 loss: 0.0024 lr: 0.02\n",
      "iteration: 133930 loss: 0.0024 lr: 0.02\n",
      "iteration: 133940 loss: 0.0025 lr: 0.02\n",
      "iteration: 133950 loss: 0.0028 lr: 0.02\n",
      "iteration: 133960 loss: 0.0022 lr: 0.02\n",
      "iteration: 133970 loss: 0.0026 lr: 0.02\n",
      "iteration: 133980 loss: 0.0019 lr: 0.02\n",
      "iteration: 133990 loss: 0.0026 lr: 0.02\n",
      "iteration: 134000 loss: 0.0026 lr: 0.02\n",
      "iteration: 134010 loss: 0.0024 lr: 0.02\n",
      "iteration: 134020 loss: 0.0031 lr: 0.02\n",
      "iteration: 134030 loss: 0.0024 lr: 0.02\n",
      "iteration: 134040 loss: 0.0036 lr: 0.02\n",
      "iteration: 134050 loss: 0.0034 lr: 0.02\n",
      "iteration: 134060 loss: 0.0019 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 134070 loss: 0.0023 lr: 0.02\n",
      "iteration: 134080 loss: 0.0024 lr: 0.02\n",
      "iteration: 134090 loss: 0.0024 lr: 0.02\n",
      "iteration: 134100 loss: 0.0027 lr: 0.02\n",
      "iteration: 134110 loss: 0.0028 lr: 0.02\n",
      "iteration: 134120 loss: 0.0023 lr: 0.02\n",
      "iteration: 134130 loss: 0.0037 lr: 0.02\n",
      "iteration: 134140 loss: 0.0016 lr: 0.02\n",
      "iteration: 134150 loss: 0.0026 lr: 0.02\n",
      "iteration: 134160 loss: 0.0027 lr: 0.02\n",
      "iteration: 134170 loss: 0.0028 lr: 0.02\n",
      "iteration: 134180 loss: 0.0027 lr: 0.02\n",
      "iteration: 134190 loss: 0.0024 lr: 0.02\n",
      "iteration: 134200 loss: 0.0024 lr: 0.02\n",
      "iteration: 134210 loss: 0.0028 lr: 0.02\n",
      "iteration: 134220 loss: 0.0028 lr: 0.02\n",
      "iteration: 134230 loss: 0.0018 lr: 0.02\n",
      "iteration: 134240 loss: 0.0027 lr: 0.02\n",
      "iteration: 134250 loss: 0.0020 lr: 0.02\n",
      "iteration: 134260 loss: 0.0024 lr: 0.02\n",
      "iteration: 134270 loss: 0.0023 lr: 0.02\n",
      "iteration: 134280 loss: 0.0032 lr: 0.02\n",
      "iteration: 134290 loss: 0.0029 lr: 0.02\n",
      "iteration: 134300 loss: 0.0035 lr: 0.02\n",
      "iteration: 134310 loss: 0.0033 lr: 0.02\n",
      "iteration: 134320 loss: 0.0028 lr: 0.02\n",
      "iteration: 134330 loss: 0.0029 lr: 0.02\n",
      "iteration: 134340 loss: 0.0020 lr: 0.02\n",
      "iteration: 134350 loss: 0.0019 lr: 0.02\n",
      "iteration: 134360 loss: 0.0023 lr: 0.02\n",
      "iteration: 134370 loss: 0.0028 lr: 0.02\n",
      "iteration: 134380 loss: 0.0021 lr: 0.02\n",
      "iteration: 134390 loss: 0.0027 lr: 0.02\n",
      "iteration: 134400 loss: 0.0031 lr: 0.02\n",
      "iteration: 134410 loss: 0.0035 lr: 0.02\n",
      "iteration: 134420 loss: 0.0025 lr: 0.02\n",
      "iteration: 134430 loss: 0.0021 lr: 0.02\n",
      "iteration: 134440 loss: 0.0038 lr: 0.02\n",
      "iteration: 134450 loss: 0.0028 lr: 0.02\n",
      "iteration: 134460 loss: 0.0023 lr: 0.02\n",
      "iteration: 134470 loss: 0.0029 lr: 0.02\n",
      "iteration: 134480 loss: 0.0028 lr: 0.02\n",
      "iteration: 134490 loss: 0.0028 lr: 0.02\n",
      "iteration: 134500 loss: 0.0021 lr: 0.02\n",
      "iteration: 134510 loss: 0.0023 lr: 0.02\n",
      "iteration: 134520 loss: 0.0031 lr: 0.02\n",
      "iteration: 134530 loss: 0.0026 lr: 0.02\n",
      "iteration: 134540 loss: 0.0024 lr: 0.02\n",
      "iteration: 134550 loss: 0.0026 lr: 0.02\n",
      "iteration: 134560 loss: 0.0019 lr: 0.02\n",
      "iteration: 134570 loss: 0.0022 lr: 0.02\n",
      "iteration: 134580 loss: 0.0033 lr: 0.02\n",
      "iteration: 134590 loss: 0.0040 lr: 0.02\n",
      "iteration: 134600 loss: 0.0024 lr: 0.02\n",
      "iteration: 134610 loss: 0.0024 lr: 0.02\n",
      "iteration: 134620 loss: 0.0024 lr: 0.02\n",
      "iteration: 134630 loss: 0.0029 lr: 0.02\n",
      "iteration: 134640 loss: 0.0022 lr: 0.02\n",
      "iteration: 134650 loss: 0.0024 lr: 0.02\n",
      "iteration: 134660 loss: 0.0023 lr: 0.02\n",
      "iteration: 134670 loss: 0.0020 lr: 0.02\n",
      "iteration: 134680 loss: 0.0021 lr: 0.02\n",
      "iteration: 134690 loss: 0.0020 lr: 0.02\n",
      "iteration: 134700 loss: 0.0033 lr: 0.02\n",
      "iteration: 134710 loss: 0.0025 lr: 0.02\n",
      "iteration: 134720 loss: 0.0020 lr: 0.02\n",
      "iteration: 134730 loss: 0.0026 lr: 0.02\n",
      "iteration: 134740 loss: 0.0023 lr: 0.02\n",
      "iteration: 134750 loss: 0.0024 lr: 0.02\n",
      "iteration: 134760 loss: 0.0024 lr: 0.02\n",
      "iteration: 134770 loss: 0.0019 lr: 0.02\n",
      "iteration: 134780 loss: 0.0027 lr: 0.02\n",
      "iteration: 134790 loss: 0.0024 lr: 0.02\n",
      "iteration: 134800 loss: 0.0031 lr: 0.02\n",
      "iteration: 134810 loss: 0.0034 lr: 0.02\n",
      "iteration: 134820 loss: 0.0027 lr: 0.02\n",
      "iteration: 134830 loss: 0.0024 lr: 0.02\n",
      "iteration: 134840 loss: 0.0032 lr: 0.02\n",
      "iteration: 134850 loss: 0.0027 lr: 0.02\n",
      "iteration: 134860 loss: 0.0027 lr: 0.02\n",
      "iteration: 134870 loss: 0.0029 lr: 0.02\n",
      "iteration: 134880 loss: 0.0018 lr: 0.02\n",
      "iteration: 134890 loss: 0.0022 lr: 0.02\n",
      "iteration: 134900 loss: 0.0027 lr: 0.02\n",
      "iteration: 134910 loss: 0.0025 lr: 0.02\n",
      "iteration: 134920 loss: 0.0026 lr: 0.02\n",
      "iteration: 134930 loss: 0.0023 lr: 0.02\n",
      "iteration: 134940 loss: 0.0017 lr: 0.02\n",
      "iteration: 134950 loss: 0.0022 lr: 0.02\n",
      "iteration: 134960 loss: 0.0021 lr: 0.02\n",
      "iteration: 134970 loss: 0.0027 lr: 0.02\n",
      "iteration: 134980 loss: 0.0025 lr: 0.02\n",
      "iteration: 134990 loss: 0.0028 lr: 0.02\n",
      "iteration: 135000 loss: 0.0031 lr: 0.02\n",
      "iteration: 135010 loss: 0.0025 lr: 0.02\n",
      "iteration: 135020 loss: 0.0022 lr: 0.02\n",
      "iteration: 135030 loss: 0.0020 lr: 0.02\n",
      "iteration: 135040 loss: 0.0024 lr: 0.02\n",
      "iteration: 135050 loss: 0.0021 lr: 0.02\n",
      "iteration: 135060 loss: 0.0025 lr: 0.02\n",
      "iteration: 135070 loss: 0.0027 lr: 0.02\n",
      "iteration: 135080 loss: 0.0022 lr: 0.02\n",
      "iteration: 135090 loss: 0.0026 lr: 0.02\n",
      "iteration: 135100 loss: 0.0028 lr: 0.02\n",
      "iteration: 135110 loss: 0.0032 lr: 0.02\n",
      "iteration: 135120 loss: 0.0032 lr: 0.02\n",
      "iteration: 135130 loss: 0.0032 lr: 0.02\n",
      "iteration: 135140 loss: 0.0024 lr: 0.02\n",
      "iteration: 135150 loss: 0.0026 lr: 0.02\n",
      "iteration: 135160 loss: 0.0020 lr: 0.02\n",
      "iteration: 135170 loss: 0.0027 lr: 0.02\n",
      "iteration: 135180 loss: 0.0030 lr: 0.02\n",
      "iteration: 135190 loss: 0.0021 lr: 0.02\n",
      "iteration: 135200 loss: 0.0020 lr: 0.02\n",
      "iteration: 135210 loss: 0.0024 lr: 0.02\n",
      "iteration: 135220 loss: 0.0024 lr: 0.02\n",
      "iteration: 135230 loss: 0.0027 lr: 0.02\n",
      "iteration: 135240 loss: 0.0025 lr: 0.02\n",
      "iteration: 135250 loss: 0.0020 lr: 0.02\n",
      "iteration: 135260 loss: 0.0022 lr: 0.02\n",
      "iteration: 135270 loss: 0.0025 lr: 0.02\n",
      "iteration: 135280 loss: 0.0027 lr: 0.02\n",
      "iteration: 135290 loss: 0.0025 lr: 0.02\n",
      "iteration: 135300 loss: 0.0025 lr: 0.02\n",
      "iteration: 135310 loss: 0.0018 lr: 0.02\n",
      "iteration: 135320 loss: 0.0022 lr: 0.02\n",
      "iteration: 135330 loss: 0.0028 lr: 0.02\n",
      "iteration: 135340 loss: 0.0025 lr: 0.02\n",
      "iteration: 135350 loss: 0.0038 lr: 0.02\n",
      "iteration: 135360 loss: 0.0031 lr: 0.02\n",
      "iteration: 135370 loss: 0.0026 lr: 0.02\n",
      "iteration: 135380 loss: 0.0025 lr: 0.02\n",
      "iteration: 135390 loss: 0.0023 lr: 0.02\n",
      "iteration: 135400 loss: 0.0020 lr: 0.02\n",
      "iteration: 135410 loss: 0.0026 lr: 0.02\n",
      "iteration: 135420 loss: 0.0027 lr: 0.02\n",
      "iteration: 135430 loss: 0.0027 lr: 0.02\n",
      "iteration: 135440 loss: 0.0034 lr: 0.02\n",
      "iteration: 135450 loss: 0.0031 lr: 0.02\n",
      "iteration: 135460 loss: 0.0030 lr: 0.02\n",
      "iteration: 135470 loss: 0.0029 lr: 0.02\n",
      "iteration: 135480 loss: 0.0025 lr: 0.02\n",
      "iteration: 135490 loss: 0.0018 lr: 0.02\n",
      "iteration: 135500 loss: 0.0021 lr: 0.02\n",
      "iteration: 135510 loss: 0.0041 lr: 0.02\n",
      "iteration: 135520 loss: 0.0028 lr: 0.02\n",
      "iteration: 135530 loss: 0.0023 lr: 0.02\n",
      "iteration: 135540 loss: 0.0025 lr: 0.02\n",
      "iteration: 135550 loss: 0.0028 lr: 0.02\n",
      "iteration: 135560 loss: 0.0025 lr: 0.02\n",
      "iteration: 135570 loss: 0.0034 lr: 0.02\n",
      "iteration: 135580 loss: 0.0029 lr: 0.02\n",
      "iteration: 135590 loss: 0.0016 lr: 0.02\n",
      "iteration: 135600 loss: 0.0027 lr: 0.02\n",
      "iteration: 135610 loss: 0.0027 lr: 0.02\n",
      "iteration: 135620 loss: 0.0028 lr: 0.02\n",
      "iteration: 135630 loss: 0.0025 lr: 0.02\n",
      "iteration: 135640 loss: 0.0028 lr: 0.02\n",
      "iteration: 135650 loss: 0.0024 lr: 0.02\n",
      "iteration: 135660 loss: 0.0029 lr: 0.02\n",
      "iteration: 135670 loss: 0.0022 lr: 0.02\n",
      "iteration: 135680 loss: 0.0025 lr: 0.02\n",
      "iteration: 135690 loss: 0.0028 lr: 0.02\n",
      "iteration: 135700 loss: 0.0025 lr: 0.02\n",
      "iteration: 135710 loss: 0.0034 lr: 0.02\n",
      "iteration: 135720 loss: 0.0026 lr: 0.02\n",
      "iteration: 135730 loss: 0.0024 lr: 0.02\n",
      "iteration: 135740 loss: 0.0027 lr: 0.02\n",
      "iteration: 135750 loss: 0.0020 lr: 0.02\n",
      "iteration: 135760 loss: 0.0025 lr: 0.02\n",
      "iteration: 135770 loss: 0.0027 lr: 0.02\n",
      "iteration: 135780 loss: 0.0022 lr: 0.02\n",
      "iteration: 135790 loss: 0.0025 lr: 0.02\n",
      "iteration: 135800 loss: 0.0021 lr: 0.02\n",
      "iteration: 135810 loss: 0.0031 lr: 0.02\n",
      "iteration: 135820 loss: 0.0025 lr: 0.02\n",
      "iteration: 135830 loss: 0.0018 lr: 0.02\n",
      "iteration: 135840 loss: 0.0030 lr: 0.02\n",
      "iteration: 135850 loss: 0.0025 lr: 0.02\n",
      "iteration: 135860 loss: 0.0025 lr: 0.02\n",
      "iteration: 135870 loss: 0.0023 lr: 0.02\n",
      "iteration: 135880 loss: 0.0023 lr: 0.02\n",
      "iteration: 135890 loss: 0.0021 lr: 0.02\n",
      "iteration: 135900 loss: 0.0019 lr: 0.02\n",
      "iteration: 135910 loss: 0.0024 lr: 0.02\n",
      "iteration: 135920 loss: 0.0029 lr: 0.02\n",
      "iteration: 135930 loss: 0.0017 lr: 0.02\n",
      "iteration: 135940 loss: 0.0024 lr: 0.02\n",
      "iteration: 135950 loss: 0.0020 lr: 0.02\n",
      "iteration: 135960 loss: 0.0022 lr: 0.02\n",
      "iteration: 135970 loss: 0.0020 lr: 0.02\n",
      "iteration: 135980 loss: 0.0025 lr: 0.02\n",
      "iteration: 135990 loss: 0.0026 lr: 0.02\n",
      "iteration: 136000 loss: 0.0026 lr: 0.02\n",
      "iteration: 136010 loss: 0.0020 lr: 0.02\n",
      "iteration: 136020 loss: 0.0022 lr: 0.02\n",
      "iteration: 136030 loss: 0.0019 lr: 0.02\n",
      "iteration: 136040 loss: 0.0024 lr: 0.02\n",
      "iteration: 136050 loss: 0.0023 lr: 0.02\n",
      "iteration: 136060 loss: 0.0025 lr: 0.02\n",
      "iteration: 136070 loss: 0.0022 lr: 0.02\n",
      "iteration: 136080 loss: 0.0029 lr: 0.02\n",
      "iteration: 136090 loss: 0.0022 lr: 0.02\n",
      "iteration: 136100 loss: 0.0021 lr: 0.02\n",
      "iteration: 136110 loss: 0.0021 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 136120 loss: 0.0028 lr: 0.02\n",
      "iteration: 136130 loss: 0.0024 lr: 0.02\n",
      "iteration: 136140 loss: 0.0022 lr: 0.02\n",
      "iteration: 136150 loss: 0.0032 lr: 0.02\n",
      "iteration: 136160 loss: 0.0025 lr: 0.02\n",
      "iteration: 136170 loss: 0.0026 lr: 0.02\n",
      "iteration: 136180 loss: 0.0034 lr: 0.02\n",
      "iteration: 136190 loss: 0.0029 lr: 0.02\n",
      "iteration: 136200 loss: 0.0027 lr: 0.02\n",
      "iteration: 136210 loss: 0.0029 lr: 0.02\n",
      "iteration: 136220 loss: 0.0023 lr: 0.02\n",
      "iteration: 136230 loss: 0.0026 lr: 0.02\n",
      "iteration: 136240 loss: 0.0025 lr: 0.02\n",
      "iteration: 136250 loss: 0.0032 lr: 0.02\n",
      "iteration: 136260 loss: 0.0024 lr: 0.02\n",
      "iteration: 136270 loss: 0.0024 lr: 0.02\n",
      "iteration: 136280 loss: 0.0024 lr: 0.02\n",
      "iteration: 136290 loss: 0.0035 lr: 0.02\n",
      "iteration: 136300 loss: 0.0026 lr: 0.02\n",
      "iteration: 136310 loss: 0.0037 lr: 0.02\n",
      "iteration: 136320 loss: 0.0028 lr: 0.02\n",
      "iteration: 136330 loss: 0.0026 lr: 0.02\n",
      "iteration: 136340 loss: 0.0020 lr: 0.02\n",
      "iteration: 136350 loss: 0.0026 lr: 0.02\n",
      "iteration: 136360 loss: 0.0025 lr: 0.02\n",
      "iteration: 136370 loss: 0.0025 lr: 0.02\n",
      "iteration: 136380 loss: 0.0023 lr: 0.02\n",
      "iteration: 136390 loss: 0.0030 lr: 0.02\n",
      "iteration: 136400 loss: 0.0026 lr: 0.02\n",
      "iteration: 136410 loss: 0.0025 lr: 0.02\n",
      "iteration: 136420 loss: 0.0021 lr: 0.02\n",
      "iteration: 136430 loss: 0.0022 lr: 0.02\n",
      "iteration: 136440 loss: 0.0023 lr: 0.02\n",
      "iteration: 136450 loss: 0.0025 lr: 0.02\n",
      "iteration: 136460 loss: 0.0026 lr: 0.02\n",
      "iteration: 136470 loss: 0.0026 lr: 0.02\n",
      "iteration: 136480 loss: 0.0021 lr: 0.02\n",
      "iteration: 136490 loss: 0.0023 lr: 0.02\n",
      "iteration: 136500 loss: 0.0022 lr: 0.02\n",
      "iteration: 136510 loss: 0.0025 lr: 0.02\n",
      "iteration: 136520 loss: 0.0026 lr: 0.02\n",
      "iteration: 136530 loss: 0.0024 lr: 0.02\n",
      "iteration: 136540 loss: 0.0023 lr: 0.02\n",
      "iteration: 136550 loss: 0.0019 lr: 0.02\n",
      "iteration: 136560 loss: 0.0026 lr: 0.02\n",
      "iteration: 136570 loss: 0.0024 lr: 0.02\n",
      "iteration: 136580 loss: 0.0022 lr: 0.02\n",
      "iteration: 136590 loss: 0.0021 lr: 0.02\n",
      "iteration: 136600 loss: 0.0032 lr: 0.02\n",
      "iteration: 136610 loss: 0.0035 lr: 0.02\n",
      "iteration: 136620 loss: 0.0023 lr: 0.02\n",
      "iteration: 136630 loss: 0.0016 lr: 0.02\n",
      "iteration: 136640 loss: 0.0022 lr: 0.02\n",
      "iteration: 136650 loss: 0.0023 lr: 0.02\n",
      "iteration: 136660 loss: 0.0032 lr: 0.02\n",
      "iteration: 136670 loss: 0.0023 lr: 0.02\n",
      "iteration: 136680 loss: 0.0025 lr: 0.02\n",
      "iteration: 136690 loss: 0.0023 lr: 0.02\n",
      "iteration: 136700 loss: 0.0029 lr: 0.02\n",
      "iteration: 136710 loss: 0.0027 lr: 0.02\n",
      "iteration: 136720 loss: 0.0024 lr: 0.02\n",
      "iteration: 136730 loss: 0.0019 lr: 0.02\n",
      "iteration: 136740 loss: 0.0022 lr: 0.02\n",
      "iteration: 136750 loss: 0.0030 lr: 0.02\n",
      "iteration: 136760 loss: 0.0037 lr: 0.02\n",
      "iteration: 136770 loss: 0.0019 lr: 0.02\n",
      "iteration: 136780 loss: 0.0036 lr: 0.02\n",
      "iteration: 136790 loss: 0.0024 lr: 0.02\n",
      "iteration: 136800 loss: 0.0028 lr: 0.02\n",
      "iteration: 136810 loss: 0.0026 lr: 0.02\n",
      "iteration: 136820 loss: 0.0022 lr: 0.02\n",
      "iteration: 136830 loss: 0.0025 lr: 0.02\n",
      "iteration: 136840 loss: 0.0021 lr: 0.02\n",
      "iteration: 136850 loss: 0.0027 lr: 0.02\n",
      "iteration: 136860 loss: 0.0029 lr: 0.02\n",
      "iteration: 136870 loss: 0.0028 lr: 0.02\n",
      "iteration: 136880 loss: 0.0022 lr: 0.02\n",
      "iteration: 136890 loss: 0.0026 lr: 0.02\n",
      "iteration: 136900 loss: 0.0025 lr: 0.02\n",
      "iteration: 136910 loss: 0.0026 lr: 0.02\n",
      "iteration: 136920 loss: 0.0022 lr: 0.02\n",
      "iteration: 136930 loss: 0.0022 lr: 0.02\n",
      "iteration: 136940 loss: 0.0029 lr: 0.02\n",
      "iteration: 136950 loss: 0.0018 lr: 0.02\n",
      "iteration: 136960 loss: 0.0023 lr: 0.02\n",
      "iteration: 136970 loss: 0.0021 lr: 0.02\n",
      "iteration: 136980 loss: 0.0028 lr: 0.02\n",
      "iteration: 136990 loss: 0.0022 lr: 0.02\n",
      "iteration: 137000 loss: 0.0033 lr: 0.02\n",
      "iteration: 137010 loss: 0.0030 lr: 0.02\n",
      "iteration: 137020 loss: 0.0032 lr: 0.02\n",
      "iteration: 137030 loss: 0.0026 lr: 0.02\n",
      "iteration: 137040 loss: 0.0026 lr: 0.02\n",
      "iteration: 137050 loss: 0.0029 lr: 0.02\n",
      "iteration: 137060 loss: 0.0024 lr: 0.02\n",
      "iteration: 137070 loss: 0.0025 lr: 0.02\n",
      "iteration: 137080 loss: 0.0024 lr: 0.02\n",
      "iteration: 137090 loss: 0.0023 lr: 0.02\n",
      "iteration: 137100 loss: 0.0027 lr: 0.02\n",
      "iteration: 137110 loss: 0.0020 lr: 0.02\n",
      "iteration: 137120 loss: 0.0036 lr: 0.02\n",
      "iteration: 137130 loss: 0.0020 lr: 0.02\n",
      "iteration: 137140 loss: 0.0032 lr: 0.02\n",
      "iteration: 137150 loss: 0.0037 lr: 0.02\n",
      "iteration: 137160 loss: 0.0026 lr: 0.02\n",
      "iteration: 137170 loss: 0.0024 lr: 0.02\n",
      "iteration: 137180 loss: 0.0024 lr: 0.02\n",
      "iteration: 137190 loss: 0.0018 lr: 0.02\n",
      "iteration: 137200 loss: 0.0022 lr: 0.02\n",
      "iteration: 137210 loss: 0.0018 lr: 0.02\n",
      "iteration: 137220 loss: 0.0022 lr: 0.02\n",
      "iteration: 137230 loss: 0.0025 lr: 0.02\n",
      "iteration: 137240 loss: 0.0030 lr: 0.02\n",
      "iteration: 137250 loss: 0.0027 lr: 0.02\n",
      "iteration: 137260 loss: 0.0021 lr: 0.02\n",
      "iteration: 137270 loss: 0.0019 lr: 0.02\n",
      "iteration: 137280 loss: 0.0027 lr: 0.02\n",
      "iteration: 137290 loss: 0.0022 lr: 0.02\n",
      "iteration: 137300 loss: 0.0031 lr: 0.02\n",
      "iteration: 137310 loss: 0.0020 lr: 0.02\n",
      "iteration: 137320 loss: 0.0029 lr: 0.02\n",
      "iteration: 137330 loss: 0.0021 lr: 0.02\n",
      "iteration: 137340 loss: 0.0025 lr: 0.02\n",
      "iteration: 137350 loss: 0.0021 lr: 0.02\n",
      "iteration: 137360 loss: 0.0023 lr: 0.02\n",
      "iteration: 137370 loss: 0.0028 lr: 0.02\n",
      "iteration: 137380 loss: 0.0032 lr: 0.02\n",
      "iteration: 137390 loss: 0.0026 lr: 0.02\n",
      "iteration: 137400 loss: 0.0027 lr: 0.02\n",
      "iteration: 137410 loss: 0.0026 lr: 0.02\n",
      "iteration: 137420 loss: 0.0026 lr: 0.02\n",
      "iteration: 137430 loss: 0.0030 lr: 0.02\n",
      "iteration: 137440 loss: 0.0023 lr: 0.02\n",
      "iteration: 137450 loss: 0.0027 lr: 0.02\n",
      "iteration: 137460 loss: 0.0023 lr: 0.02\n",
      "iteration: 137470 loss: 0.0028 lr: 0.02\n",
      "iteration: 137480 loss: 0.0027 lr: 0.02\n",
      "iteration: 137490 loss: 0.0020 lr: 0.02\n",
      "iteration: 137500 loss: 0.0023 lr: 0.02\n",
      "iteration: 137510 loss: 0.0029 lr: 0.02\n",
      "iteration: 137520 loss: 0.0019 lr: 0.02\n",
      "iteration: 137530 loss: 0.0030 lr: 0.02\n",
      "iteration: 137540 loss: 0.0026 lr: 0.02\n",
      "iteration: 137550 loss: 0.0029 lr: 0.02\n",
      "iteration: 137560 loss: 0.0031 lr: 0.02\n",
      "iteration: 137570 loss: 0.0025 lr: 0.02\n",
      "iteration: 137580 loss: 0.0027 lr: 0.02\n",
      "iteration: 137590 loss: 0.0036 lr: 0.02\n",
      "iteration: 137600 loss: 0.0021 lr: 0.02\n",
      "iteration: 137610 loss: 0.0020 lr: 0.02\n",
      "iteration: 137620 loss: 0.0028 lr: 0.02\n",
      "iteration: 137630 loss: 0.0023 lr: 0.02\n",
      "iteration: 137640 loss: 0.0022 lr: 0.02\n",
      "iteration: 137650 loss: 0.0032 lr: 0.02\n",
      "iteration: 137660 loss: 0.0036 lr: 0.02\n",
      "iteration: 137670 loss: 0.0031 lr: 0.02\n",
      "iteration: 137680 loss: 0.0024 lr: 0.02\n",
      "iteration: 137690 loss: 0.0024 lr: 0.02\n",
      "iteration: 137700 loss: 0.0023 lr: 0.02\n",
      "iteration: 137710 loss: 0.0027 lr: 0.02\n",
      "iteration: 137720 loss: 0.0026 lr: 0.02\n",
      "iteration: 137730 loss: 0.0016 lr: 0.02\n",
      "iteration: 137740 loss: 0.0026 lr: 0.02\n",
      "iteration: 137750 loss: 0.0021 lr: 0.02\n",
      "iteration: 137760 loss: 0.0025 lr: 0.02\n",
      "iteration: 137770 loss: 0.0023 lr: 0.02\n",
      "iteration: 137780 loss: 0.0026 lr: 0.02\n",
      "iteration: 137790 loss: 0.0026 lr: 0.02\n",
      "iteration: 137800 loss: 0.0022 lr: 0.02\n",
      "iteration: 137810 loss: 0.0029 lr: 0.02\n",
      "iteration: 137820 loss: 0.0020 lr: 0.02\n",
      "iteration: 137830 loss: 0.0024 lr: 0.02\n",
      "iteration: 137840 loss: 0.0031 lr: 0.02\n",
      "iteration: 137850 loss: 0.0026 lr: 0.02\n",
      "iteration: 137860 loss: 0.0021 lr: 0.02\n",
      "iteration: 137870 loss: 0.0028 lr: 0.02\n",
      "iteration: 137880 loss: 0.0025 lr: 0.02\n",
      "iteration: 137890 loss: 0.0025 lr: 0.02\n",
      "iteration: 137900 loss: 0.0024 lr: 0.02\n",
      "iteration: 137910 loss: 0.0025 lr: 0.02\n",
      "iteration: 137920 loss: 0.0022 lr: 0.02\n",
      "iteration: 137930 loss: 0.0035 lr: 0.02\n",
      "iteration: 137940 loss: 0.0026 lr: 0.02\n",
      "iteration: 137950 loss: 0.0035 lr: 0.02\n",
      "iteration: 137960 loss: 0.0027 lr: 0.02\n",
      "iteration: 137970 loss: 0.0036 lr: 0.02\n",
      "iteration: 137980 loss: 0.0021 lr: 0.02\n",
      "iteration: 137990 loss: 0.0024 lr: 0.02\n",
      "iteration: 138000 loss: 0.0024 lr: 0.02\n",
      "iteration: 138010 loss: 0.0030 lr: 0.02\n",
      "iteration: 138020 loss: 0.0028 lr: 0.02\n",
      "iteration: 138030 loss: 0.0030 lr: 0.02\n",
      "iteration: 138040 loss: 0.0027 lr: 0.02\n",
      "iteration: 138050 loss: 0.0027 lr: 0.02\n",
      "iteration: 138060 loss: 0.0021 lr: 0.02\n",
      "iteration: 138070 loss: 0.0028 lr: 0.02\n",
      "iteration: 138080 loss: 0.0026 lr: 0.02\n",
      "iteration: 138090 loss: 0.0028 lr: 0.02\n",
      "iteration: 138100 loss: 0.0023 lr: 0.02\n",
      "iteration: 138110 loss: 0.0026 lr: 0.02\n",
      "iteration: 138120 loss: 0.0023 lr: 0.02\n",
      "iteration: 138130 loss: 0.0019 lr: 0.02\n",
      "iteration: 138140 loss: 0.0028 lr: 0.02\n",
      "iteration: 138150 loss: 0.0022 lr: 0.02\n",
      "iteration: 138160 loss: 0.0027 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 138170 loss: 0.0027 lr: 0.02\n",
      "iteration: 138180 loss: 0.0030 lr: 0.02\n",
      "iteration: 138190 loss: 0.0023 lr: 0.02\n",
      "iteration: 138200 loss: 0.0027 lr: 0.02\n",
      "iteration: 138210 loss: 0.0031 lr: 0.02\n",
      "iteration: 138220 loss: 0.0025 lr: 0.02\n",
      "iteration: 138230 loss: 0.0023 lr: 0.02\n",
      "iteration: 138240 loss: 0.0032 lr: 0.02\n",
      "iteration: 138250 loss: 0.0025 lr: 0.02\n",
      "iteration: 138260 loss: 0.0027 lr: 0.02\n",
      "iteration: 138270 loss: 0.0024 lr: 0.02\n",
      "iteration: 138280 loss: 0.0023 lr: 0.02\n",
      "iteration: 138290 loss: 0.0023 lr: 0.02\n",
      "iteration: 138300 loss: 0.0022 lr: 0.02\n",
      "iteration: 138310 loss: 0.0023 lr: 0.02\n",
      "iteration: 138320 loss: 0.0028 lr: 0.02\n",
      "iteration: 138330 loss: 0.0025 lr: 0.02\n",
      "iteration: 138340 loss: 0.0024 lr: 0.02\n",
      "iteration: 138350 loss: 0.0030 lr: 0.02\n",
      "iteration: 138360 loss: 0.0024 lr: 0.02\n",
      "iteration: 138370 loss: 0.0021 lr: 0.02\n",
      "iteration: 138380 loss: 0.0020 lr: 0.02\n",
      "iteration: 138390 loss: 0.0018 lr: 0.02\n",
      "iteration: 138400 loss: 0.0022 lr: 0.02\n",
      "iteration: 138410 loss: 0.0031 lr: 0.02\n",
      "iteration: 138420 loss: 0.0030 lr: 0.02\n",
      "iteration: 138430 loss: 0.0021 lr: 0.02\n",
      "iteration: 138440 loss: 0.0032 lr: 0.02\n",
      "iteration: 138450 loss: 0.0028 lr: 0.02\n",
      "iteration: 138460 loss: 0.0022 lr: 0.02\n",
      "iteration: 138470 loss: 0.0023 lr: 0.02\n",
      "iteration: 138480 loss: 0.0030 lr: 0.02\n",
      "iteration: 138490 loss: 0.0021 lr: 0.02\n",
      "iteration: 138500 loss: 0.0021 lr: 0.02\n",
      "iteration: 138510 loss: 0.0022 lr: 0.02\n",
      "iteration: 138520 loss: 0.0031 lr: 0.02\n",
      "iteration: 138530 loss: 0.0021 lr: 0.02\n",
      "iteration: 138540 loss: 0.0023 lr: 0.02\n",
      "iteration: 138550 loss: 0.0025 lr: 0.02\n",
      "iteration: 138560 loss: 0.0023 lr: 0.02\n",
      "iteration: 138570 loss: 0.0024 lr: 0.02\n",
      "iteration: 138580 loss: 0.0025 lr: 0.02\n",
      "iteration: 138590 loss: 0.0021 lr: 0.02\n",
      "iteration: 138600 loss: 0.0024 lr: 0.02\n",
      "iteration: 138610 loss: 0.0022 lr: 0.02\n",
      "iteration: 138620 loss: 0.0024 lr: 0.02\n",
      "iteration: 138630 loss: 0.0019 lr: 0.02\n",
      "iteration: 138640 loss: 0.0029 lr: 0.02\n",
      "iteration: 138650 loss: 0.0025 lr: 0.02\n",
      "iteration: 138660 loss: 0.0023 lr: 0.02\n",
      "iteration: 138670 loss: 0.0024 lr: 0.02\n",
      "iteration: 138680 loss: 0.0038 lr: 0.02\n",
      "iteration: 138690 loss: 0.0024 lr: 0.02\n",
      "iteration: 138700 loss: 0.0031 lr: 0.02\n",
      "iteration: 138710 loss: 0.0023 lr: 0.02\n",
      "iteration: 138720 loss: 0.0024 lr: 0.02\n",
      "iteration: 138730 loss: 0.0023 lr: 0.02\n",
      "iteration: 138740 loss: 0.0025 lr: 0.02\n",
      "iteration: 138750 loss: 0.0032 lr: 0.02\n",
      "iteration: 138760 loss: 0.0023 lr: 0.02\n",
      "iteration: 138770 loss: 0.0034 lr: 0.02\n",
      "iteration: 138780 loss: 0.0021 lr: 0.02\n",
      "iteration: 138790 loss: 0.0023 lr: 0.02\n",
      "iteration: 138800 loss: 0.0017 lr: 0.02\n",
      "iteration: 138810 loss: 0.0027 lr: 0.02\n",
      "iteration: 138820 loss: 0.0020 lr: 0.02\n",
      "iteration: 138830 loss: 0.0026 lr: 0.02\n",
      "iteration: 138840 loss: 0.0022 lr: 0.02\n",
      "iteration: 138850 loss: 0.0029 lr: 0.02\n",
      "iteration: 138860 loss: 0.0023 lr: 0.02\n",
      "iteration: 138870 loss: 0.0021 lr: 0.02\n",
      "iteration: 138880 loss: 0.0023 lr: 0.02\n",
      "iteration: 138890 loss: 0.0024 lr: 0.02\n",
      "iteration: 138900 loss: 0.0026 lr: 0.02\n",
      "iteration: 138910 loss: 0.0022 lr: 0.02\n",
      "iteration: 138920 loss: 0.0026 lr: 0.02\n",
      "iteration: 138930 loss: 0.0022 lr: 0.02\n",
      "iteration: 138940 loss: 0.0020 lr: 0.02\n",
      "iteration: 138950 loss: 0.0028 lr: 0.02\n",
      "iteration: 138960 loss: 0.0028 lr: 0.02\n",
      "iteration: 138970 loss: 0.0028 lr: 0.02\n",
      "iteration: 138980 loss: 0.0027 lr: 0.02\n",
      "iteration: 138990 loss: 0.0035 lr: 0.02\n",
      "iteration: 139000 loss: 0.0028 lr: 0.02\n",
      "iteration: 139010 loss: 0.0025 lr: 0.02\n",
      "iteration: 139020 loss: 0.0032 lr: 0.02\n",
      "iteration: 139030 loss: 0.0020 lr: 0.02\n",
      "iteration: 139040 loss: 0.0022 lr: 0.02\n",
      "iteration: 139050 loss: 0.0026 lr: 0.02\n",
      "iteration: 139060 loss: 0.0030 lr: 0.02\n",
      "iteration: 139070 loss: 0.0022 lr: 0.02\n",
      "iteration: 139080 loss: 0.0024 lr: 0.02\n",
      "iteration: 139090 loss: 0.0025 lr: 0.02\n",
      "iteration: 139100 loss: 0.0027 lr: 0.02\n",
      "iteration: 139110 loss: 0.0031 lr: 0.02\n",
      "iteration: 139120 loss: 0.0027 lr: 0.02\n",
      "iteration: 139130 loss: 0.0038 lr: 0.02\n",
      "iteration: 139140 loss: 0.0027 lr: 0.02\n",
      "iteration: 139150 loss: 0.0023 lr: 0.02\n",
      "iteration: 139160 loss: 0.0028 lr: 0.02\n",
      "iteration: 139170 loss: 0.0026 lr: 0.02\n",
      "iteration: 139180 loss: 0.0023 lr: 0.02\n",
      "iteration: 139190 loss: 0.0031 lr: 0.02\n",
      "iteration: 139200 loss: 0.0026 lr: 0.02\n",
      "iteration: 139210 loss: 0.0027 lr: 0.02\n",
      "iteration: 139220 loss: 0.0025 lr: 0.02\n",
      "iteration: 139230 loss: 0.0023 lr: 0.02\n",
      "iteration: 139240 loss: 0.0021 lr: 0.02\n",
      "iteration: 139250 loss: 0.0022 lr: 0.02\n",
      "iteration: 139260 loss: 0.0026 lr: 0.02\n",
      "iteration: 139270 loss: 0.0021 lr: 0.02\n",
      "iteration: 139280 loss: 0.0030 lr: 0.02\n",
      "iteration: 139290 loss: 0.0032 lr: 0.02\n",
      "iteration: 139300 loss: 0.0031 lr: 0.02\n",
      "iteration: 139310 loss: 0.0018 lr: 0.02\n",
      "iteration: 139320 loss: 0.0022 lr: 0.02\n",
      "iteration: 139330 loss: 0.0025 lr: 0.02\n",
      "iteration: 139340 loss: 0.0026 lr: 0.02\n",
      "iteration: 139350 loss: 0.0035 lr: 0.02\n",
      "iteration: 139360 loss: 0.0026 lr: 0.02\n",
      "iteration: 139370 loss: 0.0024 lr: 0.02\n",
      "iteration: 139380 loss: 0.0026 lr: 0.02\n",
      "iteration: 139390 loss: 0.0028 lr: 0.02\n",
      "iteration: 139400 loss: 0.0023 lr: 0.02\n",
      "iteration: 139410 loss: 0.0024 lr: 0.02\n",
      "iteration: 139420 loss: 0.0025 lr: 0.02\n",
      "iteration: 139430 loss: 0.0030 lr: 0.02\n",
      "iteration: 139440 loss: 0.0026 lr: 0.02\n",
      "iteration: 139450 loss: 0.0024 lr: 0.02\n",
      "iteration: 139460 loss: 0.0023 lr: 0.02\n",
      "iteration: 139470 loss: 0.0030 lr: 0.02\n",
      "iteration: 139480 loss: 0.0024 lr: 0.02\n",
      "iteration: 139490 loss: 0.0021 lr: 0.02\n",
      "iteration: 139500 loss: 0.0024 lr: 0.02\n",
      "iteration: 139510 loss: 0.0028 lr: 0.02\n",
      "iteration: 139520 loss: 0.0027 lr: 0.02\n",
      "iteration: 139530 loss: 0.0018 lr: 0.02\n",
      "iteration: 139540 loss: 0.0035 lr: 0.02\n",
      "iteration: 139550 loss: 0.0024 lr: 0.02\n",
      "iteration: 139560 loss: 0.0027 lr: 0.02\n",
      "iteration: 139570 loss: 0.0018 lr: 0.02\n",
      "iteration: 139580 loss: 0.0026 lr: 0.02\n",
      "iteration: 139590 loss: 0.0022 lr: 0.02\n",
      "iteration: 139600 loss: 0.0039 lr: 0.02\n",
      "iteration: 139610 loss: 0.0024 lr: 0.02\n",
      "iteration: 139620 loss: 0.0024 lr: 0.02\n",
      "iteration: 139630 loss: 0.0028 lr: 0.02\n",
      "iteration: 139640 loss: 0.0035 lr: 0.02\n",
      "iteration: 139650 loss: 0.0030 lr: 0.02\n",
      "iteration: 139660 loss: 0.0023 lr: 0.02\n",
      "iteration: 139670 loss: 0.0021 lr: 0.02\n",
      "iteration: 139680 loss: 0.0019 lr: 0.02\n",
      "iteration: 139690 loss: 0.0024 lr: 0.02\n",
      "iteration: 139700 loss: 0.0023 lr: 0.02\n",
      "iteration: 139710 loss: 0.0022 lr: 0.02\n",
      "iteration: 139720 loss: 0.0025 lr: 0.02\n",
      "iteration: 139730 loss: 0.0023 lr: 0.02\n",
      "iteration: 139740 loss: 0.0024 lr: 0.02\n",
      "iteration: 139750 loss: 0.0023 lr: 0.02\n",
      "iteration: 139760 loss: 0.0022 lr: 0.02\n",
      "iteration: 139770 loss: 0.0021 lr: 0.02\n",
      "iteration: 139780 loss: 0.0022 lr: 0.02\n",
      "iteration: 139790 loss: 0.0023 lr: 0.02\n",
      "iteration: 139800 loss: 0.0028 lr: 0.02\n",
      "iteration: 139810 loss: 0.0026 lr: 0.02\n",
      "iteration: 139820 loss: 0.0028 lr: 0.02\n",
      "iteration: 139830 loss: 0.0027 lr: 0.02\n",
      "iteration: 139840 loss: 0.0029 lr: 0.02\n",
      "iteration: 139850 loss: 0.0020 lr: 0.02\n",
      "iteration: 139860 loss: 0.0022 lr: 0.02\n",
      "iteration: 139870 loss: 0.0025 lr: 0.02\n",
      "iteration: 139880 loss: 0.0032 lr: 0.02\n",
      "iteration: 139890 loss: 0.0019 lr: 0.02\n",
      "iteration: 139900 loss: 0.0024 lr: 0.02\n",
      "iteration: 139910 loss: 0.0025 lr: 0.02\n",
      "iteration: 139920 loss: 0.0027 lr: 0.02\n",
      "iteration: 139930 loss: 0.0024 lr: 0.02\n",
      "iteration: 139940 loss: 0.0022 lr: 0.02\n",
      "iteration: 139950 loss: 0.0020 lr: 0.02\n",
      "iteration: 139960 loss: 0.0028 lr: 0.02\n",
      "iteration: 139970 loss: 0.0031 lr: 0.02\n",
      "iteration: 139980 loss: 0.0019 lr: 0.02\n",
      "iteration: 139990 loss: 0.0021 lr: 0.02\n",
      "iteration: 140000 loss: 0.0031 lr: 0.02\n",
      "iteration: 140010 loss: 0.0026 lr: 0.02\n",
      "iteration: 140020 loss: 0.0032 lr: 0.02\n",
      "iteration: 140030 loss: 0.0029 lr: 0.02\n",
      "iteration: 140040 loss: 0.0022 lr: 0.02\n",
      "iteration: 140050 loss: 0.0028 lr: 0.02\n",
      "iteration: 140060 loss: 0.0021 lr: 0.02\n",
      "iteration: 140070 loss: 0.0022 lr: 0.02\n",
      "iteration: 140080 loss: 0.0027 lr: 0.02\n",
      "iteration: 140090 loss: 0.0025 lr: 0.02\n",
      "iteration: 140100 loss: 0.0024 lr: 0.02\n",
      "iteration: 140110 loss: 0.0022 lr: 0.02\n",
      "iteration: 140120 loss: 0.0026 lr: 0.02\n",
      "iteration: 140130 loss: 0.0025 lr: 0.02\n",
      "iteration: 140140 loss: 0.0022 lr: 0.02\n",
      "iteration: 140150 loss: 0.0028 lr: 0.02\n",
      "iteration: 140160 loss: 0.0021 lr: 0.02\n",
      "iteration: 140170 loss: 0.0021 lr: 0.02\n",
      "iteration: 140180 loss: 0.0018 lr: 0.02\n",
      "iteration: 140190 loss: 0.0025 lr: 0.02\n",
      "iteration: 140200 loss: 0.0023 lr: 0.02\n",
      "iteration: 140210 loss: 0.0018 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 140220 loss: 0.0028 lr: 0.02\n",
      "iteration: 140230 loss: 0.0029 lr: 0.02\n",
      "iteration: 140240 loss: 0.0024 lr: 0.02\n",
      "iteration: 140250 loss: 0.0027 lr: 0.02\n",
      "iteration: 140260 loss: 0.0023 lr: 0.02\n",
      "iteration: 140270 loss: 0.0024 lr: 0.02\n",
      "iteration: 140280 loss: 0.0023 lr: 0.02\n",
      "iteration: 140290 loss: 0.0025 lr: 0.02\n",
      "iteration: 140300 loss: 0.0028 lr: 0.02\n",
      "iteration: 140310 loss: 0.0024 lr: 0.02\n",
      "iteration: 140320 loss: 0.0026 lr: 0.02\n",
      "iteration: 140330 loss: 0.0026 lr: 0.02\n",
      "iteration: 140340 loss: 0.0032 lr: 0.02\n",
      "iteration: 140350 loss: 0.0032 lr: 0.02\n",
      "iteration: 140360 loss: 0.0033 lr: 0.02\n",
      "iteration: 140370 loss: 0.0019 lr: 0.02\n",
      "iteration: 140380 loss: 0.0022 lr: 0.02\n",
      "iteration: 140390 loss: 0.0024 lr: 0.02\n",
      "iteration: 140400 loss: 0.0027 lr: 0.02\n",
      "iteration: 140410 loss: 0.0023 lr: 0.02\n",
      "iteration: 140420 loss: 0.0024 lr: 0.02\n",
      "iteration: 140430 loss: 0.0020 lr: 0.02\n",
      "iteration: 140440 loss: 0.0028 lr: 0.02\n",
      "iteration: 140450 loss: 0.0024 lr: 0.02\n",
      "iteration: 140460 loss: 0.0022 lr: 0.02\n",
      "iteration: 140470 loss: 0.0025 lr: 0.02\n",
      "iteration: 140480 loss: 0.0024 lr: 0.02\n",
      "iteration: 140490 loss: 0.0026 lr: 0.02\n",
      "iteration: 140500 loss: 0.0030 lr: 0.02\n",
      "iteration: 140510 loss: 0.0032 lr: 0.02\n",
      "iteration: 140520 loss: 0.0024 lr: 0.02\n",
      "iteration: 140530 loss: 0.0025 lr: 0.02\n",
      "iteration: 140540 loss: 0.0028 lr: 0.02\n",
      "iteration: 140550 loss: 0.0032 lr: 0.02\n",
      "iteration: 140560 loss: 0.0029 lr: 0.02\n",
      "iteration: 140570 loss: 0.0040 lr: 0.02\n",
      "iteration: 140580 loss: 0.0027 lr: 0.02\n",
      "iteration: 140590 loss: 0.0023 lr: 0.02\n",
      "iteration: 140600 loss: 0.0033 lr: 0.02\n",
      "iteration: 140610 loss: 0.0044 lr: 0.02\n",
      "iteration: 140620 loss: 0.0030 lr: 0.02\n",
      "iteration: 140630 loss: 0.0028 lr: 0.02\n",
      "iteration: 140640 loss: 0.0027 lr: 0.02\n",
      "iteration: 140650 loss: 0.0038 lr: 0.02\n",
      "iteration: 140660 loss: 0.0022 lr: 0.02\n",
      "iteration: 140670 loss: 0.0025 lr: 0.02\n",
      "iteration: 140680 loss: 0.0021 lr: 0.02\n",
      "iteration: 140690 loss: 0.0024 lr: 0.02\n",
      "iteration: 140700 loss: 0.0025 lr: 0.02\n",
      "iteration: 140710 loss: 0.0026 lr: 0.02\n",
      "iteration: 140720 loss: 0.0024 lr: 0.02\n",
      "iteration: 140730 loss: 0.0030 lr: 0.02\n",
      "iteration: 140740 loss: 0.0027 lr: 0.02\n",
      "iteration: 140750 loss: 0.0034 lr: 0.02\n",
      "iteration: 140760 loss: 0.0023 lr: 0.02\n",
      "iteration: 140770 loss: 0.0021 lr: 0.02\n",
      "iteration: 140780 loss: 0.0025 lr: 0.02\n",
      "iteration: 140790 loss: 0.0025 lr: 0.02\n",
      "iteration: 140800 loss: 0.0026 lr: 0.02\n",
      "iteration: 140810 loss: 0.0032 lr: 0.02\n",
      "iteration: 140820 loss: 0.0024 lr: 0.02\n",
      "iteration: 140830 loss: 0.0031 lr: 0.02\n",
      "iteration: 140840 loss: 0.0019 lr: 0.02\n",
      "iteration: 140850 loss: 0.0019 lr: 0.02\n",
      "iteration: 140860 loss: 0.0021 lr: 0.02\n",
      "iteration: 140870 loss: 0.0029 lr: 0.02\n",
      "iteration: 140880 loss: 0.0028 lr: 0.02\n",
      "iteration: 140890 loss: 0.0031 lr: 0.02\n",
      "iteration: 140900 loss: 0.0025 lr: 0.02\n",
      "iteration: 140910 loss: 0.0023 lr: 0.02\n",
      "iteration: 140920 loss: 0.0022 lr: 0.02\n",
      "iteration: 140930 loss: 0.0024 lr: 0.02\n",
      "iteration: 140940 loss: 0.0018 lr: 0.02\n",
      "iteration: 140950 loss: 0.0027 lr: 0.02\n",
      "iteration: 140960 loss: 0.0026 lr: 0.02\n",
      "iteration: 140970 loss: 0.0030 lr: 0.02\n",
      "iteration: 140980 loss: 0.0023 lr: 0.02\n",
      "iteration: 140990 loss: 0.0024 lr: 0.02\n",
      "iteration: 141000 loss: 0.0023 lr: 0.02\n",
      "iteration: 141010 loss: 0.0021 lr: 0.02\n",
      "iteration: 141020 loss: 0.0025 lr: 0.02\n",
      "iteration: 141030 loss: 0.0023 lr: 0.02\n",
      "iteration: 141040 loss: 0.0027 lr: 0.02\n",
      "iteration: 141050 loss: 0.0019 lr: 0.02\n",
      "iteration: 141060 loss: 0.0031 lr: 0.02\n",
      "iteration: 141070 loss: 0.0031 lr: 0.02\n",
      "iteration: 141080 loss: 0.0021 lr: 0.02\n",
      "iteration: 141090 loss: 0.0029 lr: 0.02\n",
      "iteration: 141100 loss: 0.0021 lr: 0.02\n",
      "iteration: 141110 loss: 0.0018 lr: 0.02\n",
      "iteration: 141120 loss: 0.0030 lr: 0.02\n",
      "iteration: 141130 loss: 0.0029 lr: 0.02\n",
      "iteration: 141140 loss: 0.0020 lr: 0.02\n",
      "iteration: 141150 loss: 0.0036 lr: 0.02\n",
      "iteration: 141160 loss: 0.0030 lr: 0.02\n",
      "iteration: 141170 loss: 0.0023 lr: 0.02\n",
      "iteration: 141180 loss: 0.0031 lr: 0.02\n",
      "iteration: 141190 loss: 0.0024 lr: 0.02\n",
      "iteration: 141200 loss: 0.0024 lr: 0.02\n",
      "iteration: 141210 loss: 0.0028 lr: 0.02\n",
      "iteration: 141220 loss: 0.0020 lr: 0.02\n",
      "iteration: 141230 loss: 0.0022 lr: 0.02\n",
      "iteration: 141240 loss: 0.0018 lr: 0.02\n",
      "iteration: 141250 loss: 0.0026 lr: 0.02\n",
      "iteration: 141260 loss: 0.0021 lr: 0.02\n",
      "iteration: 141270 loss: 0.0036 lr: 0.02\n",
      "iteration: 141280 loss: 0.0023 lr: 0.02\n",
      "iteration: 141290 loss: 0.0031 lr: 0.02\n",
      "iteration: 141300 loss: 0.0022 lr: 0.02\n",
      "iteration: 141310 loss: 0.0025 lr: 0.02\n",
      "iteration: 141320 loss: 0.0027 lr: 0.02\n",
      "iteration: 141330 loss: 0.0020 lr: 0.02\n",
      "iteration: 141340 loss: 0.0027 lr: 0.02\n",
      "iteration: 141350 loss: 0.0041 lr: 0.02\n",
      "iteration: 141360 loss: 0.0026 lr: 0.02\n",
      "iteration: 141370 loss: 0.0026 lr: 0.02\n",
      "iteration: 141380 loss: 0.0024 lr: 0.02\n",
      "iteration: 141390 loss: 0.0020 lr: 0.02\n",
      "iteration: 141400 loss: 0.0024 lr: 0.02\n",
      "iteration: 141410 loss: 0.0020 lr: 0.02\n",
      "iteration: 141420 loss: 0.0034 lr: 0.02\n",
      "iteration: 141430 loss: 0.0034 lr: 0.02\n",
      "iteration: 141440 loss: 0.0017 lr: 0.02\n",
      "iteration: 141450 loss: 0.0022 lr: 0.02\n",
      "iteration: 141460 loss: 0.0023 lr: 0.02\n",
      "iteration: 141470 loss: 0.0020 lr: 0.02\n",
      "iteration: 141480 loss: 0.0021 lr: 0.02\n",
      "iteration: 141490 loss: 0.0021 lr: 0.02\n",
      "iteration: 141500 loss: 0.0022 lr: 0.02\n",
      "iteration: 141510 loss: 0.0020 lr: 0.02\n",
      "iteration: 141520 loss: 0.0028 lr: 0.02\n",
      "iteration: 141530 loss: 0.0031 lr: 0.02\n",
      "iteration: 141540 loss: 0.0025 lr: 0.02\n",
      "iteration: 141550 loss: 0.0029 lr: 0.02\n",
      "iteration: 141560 loss: 0.0034 lr: 0.02\n",
      "iteration: 141570 loss: 0.0024 lr: 0.02\n",
      "iteration: 141580 loss: 0.0023 lr: 0.02\n",
      "iteration: 141590 loss: 0.0022 lr: 0.02\n",
      "iteration: 141600 loss: 0.0026 lr: 0.02\n",
      "iteration: 141610 loss: 0.0020 lr: 0.02\n",
      "iteration: 141620 loss: 0.0030 lr: 0.02\n",
      "iteration: 141630 loss: 0.0024 lr: 0.02\n",
      "iteration: 141640 loss: 0.0024 lr: 0.02\n",
      "iteration: 141650 loss: 0.0027 lr: 0.02\n",
      "iteration: 141660 loss: 0.0021 lr: 0.02\n",
      "iteration: 141670 loss: 0.0025 lr: 0.02\n",
      "iteration: 141680 loss: 0.0031 lr: 0.02\n",
      "iteration: 141690 loss: 0.0029 lr: 0.02\n",
      "iteration: 141700 loss: 0.0031 lr: 0.02\n",
      "iteration: 141710 loss: 0.0021 lr: 0.02\n",
      "iteration: 141720 loss: 0.0033 lr: 0.02\n",
      "iteration: 141730 loss: 0.0023 lr: 0.02\n",
      "iteration: 141740 loss: 0.0021 lr: 0.02\n",
      "iteration: 141750 loss: 0.0026 lr: 0.02\n",
      "iteration: 141760 loss: 0.0031 lr: 0.02\n",
      "iteration: 141770 loss: 0.0023 lr: 0.02\n",
      "iteration: 141780 loss: 0.0027 lr: 0.02\n",
      "iteration: 141790 loss: 0.0023 lr: 0.02\n",
      "iteration: 141800 loss: 0.0026 lr: 0.02\n",
      "iteration: 141810 loss: 0.0027 lr: 0.02\n",
      "iteration: 141820 loss: 0.0033 lr: 0.02\n",
      "iteration: 141830 loss: 0.0027 lr: 0.02\n",
      "iteration: 141840 loss: 0.0025 lr: 0.02\n",
      "iteration: 141850 loss: 0.0021 lr: 0.02\n",
      "iteration: 141860 loss: 0.0023 lr: 0.02\n",
      "iteration: 141870 loss: 0.0028 lr: 0.02\n",
      "iteration: 141880 loss: 0.0025 lr: 0.02\n",
      "iteration: 141890 loss: 0.0033 lr: 0.02\n",
      "iteration: 141900 loss: 0.0025 lr: 0.02\n",
      "iteration: 141910 loss: 0.0026 lr: 0.02\n",
      "iteration: 141920 loss: 0.0026 lr: 0.02\n",
      "iteration: 141930 loss: 0.0023 lr: 0.02\n",
      "iteration: 141940 loss: 0.0032 lr: 0.02\n",
      "iteration: 141950 loss: 0.0022 lr: 0.02\n",
      "iteration: 141960 loss: 0.0024 lr: 0.02\n",
      "iteration: 141970 loss: 0.0023 lr: 0.02\n",
      "iteration: 141980 loss: 0.0021 lr: 0.02\n",
      "iteration: 141990 loss: 0.0022 lr: 0.02\n",
      "iteration: 142000 loss: 0.0025 lr: 0.02\n",
      "iteration: 142010 loss: 0.0022 lr: 0.02\n",
      "iteration: 142020 loss: 0.0028 lr: 0.02\n",
      "iteration: 142030 loss: 0.0020 lr: 0.02\n",
      "iteration: 142040 loss: 0.0023 lr: 0.02\n",
      "iteration: 142050 loss: 0.0026 lr: 0.02\n",
      "iteration: 142060 loss: 0.0027 lr: 0.02\n",
      "iteration: 142070 loss: 0.0023 lr: 0.02\n",
      "iteration: 142080 loss: 0.0028 lr: 0.02\n",
      "iteration: 142090 loss: 0.0024 lr: 0.02\n",
      "iteration: 142100 loss: 0.0024 lr: 0.02\n",
      "iteration: 142110 loss: 0.0031 lr: 0.02\n",
      "iteration: 142120 loss: 0.0023 lr: 0.02\n",
      "iteration: 142130 loss: 0.0036 lr: 0.02\n",
      "iteration: 142140 loss: 0.0024 lr: 0.02\n",
      "iteration: 142150 loss: 0.0023 lr: 0.02\n",
      "iteration: 142160 loss: 0.0026 lr: 0.02\n",
      "iteration: 142170 loss: 0.0024 lr: 0.02\n",
      "iteration: 142180 loss: 0.0027 lr: 0.02\n",
      "iteration: 142190 loss: 0.0028 lr: 0.02\n",
      "iteration: 142200 loss: 0.0025 lr: 0.02\n",
      "iteration: 142210 loss: 0.0020 lr: 0.02\n",
      "iteration: 142220 loss: 0.0034 lr: 0.02\n",
      "iteration: 142230 loss: 0.0029 lr: 0.02\n",
      "iteration: 142240 loss: 0.0022 lr: 0.02\n",
      "iteration: 142250 loss: 0.0022 lr: 0.02\n",
      "iteration: 142260 loss: 0.0031 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 142270 loss: 0.0037 lr: 0.02\n",
      "iteration: 142280 loss: 0.0028 lr: 0.02\n",
      "iteration: 142290 loss: 0.0020 lr: 0.02\n",
      "iteration: 142300 loss: 0.0024 lr: 0.02\n",
      "iteration: 142310 loss: 0.0023 lr: 0.02\n",
      "iteration: 142320 loss: 0.0024 lr: 0.02\n",
      "iteration: 142330 loss: 0.0023 lr: 0.02\n",
      "iteration: 142340 loss: 0.0023 lr: 0.02\n",
      "iteration: 142350 loss: 0.0036 lr: 0.02\n",
      "iteration: 142360 loss: 0.0020 lr: 0.02\n",
      "iteration: 142370 loss: 0.0020 lr: 0.02\n",
      "iteration: 142380 loss: 0.0023 lr: 0.02\n",
      "iteration: 142390 loss: 0.0020 lr: 0.02\n",
      "iteration: 142400 loss: 0.0016 lr: 0.02\n",
      "iteration: 142410 loss: 0.0019 lr: 0.02\n",
      "iteration: 142420 loss: 0.0023 lr: 0.02\n",
      "iteration: 142430 loss: 0.0025 lr: 0.02\n",
      "iteration: 142440 loss: 0.0022 lr: 0.02\n",
      "iteration: 142450 loss: 0.0021 lr: 0.02\n",
      "iteration: 142460 loss: 0.0027 lr: 0.02\n",
      "iteration: 142470 loss: 0.0021 lr: 0.02\n",
      "iteration: 142480 loss: 0.0034 lr: 0.02\n",
      "iteration: 142490 loss: 0.0025 lr: 0.02\n",
      "iteration: 142500 loss: 0.0026 lr: 0.02\n",
      "iteration: 142510 loss: 0.0021 lr: 0.02\n",
      "iteration: 142520 loss: 0.0024 lr: 0.02\n",
      "iteration: 142530 loss: 0.0022 lr: 0.02\n",
      "iteration: 142540 loss: 0.0016 lr: 0.02\n",
      "iteration: 142550 loss: 0.0026 lr: 0.02\n",
      "iteration: 142560 loss: 0.0023 lr: 0.02\n",
      "iteration: 142570 loss: 0.0022 lr: 0.02\n",
      "iteration: 142580 loss: 0.0030 lr: 0.02\n",
      "iteration: 142590 loss: 0.0024 lr: 0.02\n",
      "iteration: 142600 loss: 0.0025 lr: 0.02\n",
      "iteration: 142610 loss: 0.0026 lr: 0.02\n",
      "iteration: 142620 loss: 0.0025 lr: 0.02\n",
      "iteration: 142630 loss: 0.0030 lr: 0.02\n",
      "iteration: 142640 loss: 0.0029 lr: 0.02\n",
      "iteration: 142650 loss: 0.0024 lr: 0.02\n",
      "iteration: 142660 loss: 0.0026 lr: 0.02\n",
      "iteration: 142670 loss: 0.0023 lr: 0.02\n",
      "iteration: 142680 loss: 0.0027 lr: 0.02\n",
      "iteration: 142690 loss: 0.0020 lr: 0.02\n",
      "iteration: 142700 loss: 0.0022 lr: 0.02\n",
      "iteration: 142710 loss: 0.0028 lr: 0.02\n",
      "iteration: 142720 loss: 0.0028 lr: 0.02\n",
      "iteration: 142730 loss: 0.0027 lr: 0.02\n",
      "iteration: 142740 loss: 0.0023 lr: 0.02\n",
      "iteration: 142750 loss: 0.0048 lr: 0.02\n",
      "iteration: 142760 loss: 0.0026 lr: 0.02\n",
      "iteration: 142770 loss: 0.0030 lr: 0.02\n",
      "iteration: 142780 loss: 0.0023 lr: 0.02\n",
      "iteration: 142790 loss: 0.0025 lr: 0.02\n",
      "iteration: 142800 loss: 0.0021 lr: 0.02\n",
      "iteration: 142810 loss: 0.0032 lr: 0.02\n",
      "iteration: 142820 loss: 0.0019 lr: 0.02\n",
      "iteration: 142830 loss: 0.0026 lr: 0.02\n",
      "iteration: 142840 loss: 0.0023 lr: 0.02\n",
      "iteration: 142850 loss: 0.0028 lr: 0.02\n",
      "iteration: 142860 loss: 0.0025 lr: 0.02\n",
      "iteration: 142870 loss: 0.0030 lr: 0.02\n",
      "iteration: 142880 loss: 0.0024 lr: 0.02\n",
      "iteration: 142890 loss: 0.0019 lr: 0.02\n",
      "iteration: 142900 loss: 0.0025 lr: 0.02\n",
      "iteration: 142910 loss: 0.0032 lr: 0.02\n",
      "iteration: 142920 loss: 0.0024 lr: 0.02\n",
      "iteration: 142930 loss: 0.0027 lr: 0.02\n",
      "iteration: 142940 loss: 0.0024 lr: 0.02\n",
      "iteration: 142950 loss: 0.0025 lr: 0.02\n",
      "iteration: 142960 loss: 0.0022 lr: 0.02\n",
      "iteration: 142970 loss: 0.0024 lr: 0.02\n",
      "iteration: 142980 loss: 0.0030 lr: 0.02\n",
      "iteration: 142990 loss: 0.0026 lr: 0.02\n",
      "iteration: 143000 loss: 0.0028 lr: 0.02\n",
      "iteration: 143010 loss: 0.0020 lr: 0.02\n",
      "iteration: 143020 loss: 0.0036 lr: 0.02\n",
      "iteration: 143030 loss: 0.0023 lr: 0.02\n",
      "iteration: 143040 loss: 0.0025 lr: 0.02\n",
      "iteration: 143050 loss: 0.0025 lr: 0.02\n",
      "iteration: 143060 loss: 0.0024 lr: 0.02\n",
      "iteration: 143070 loss: 0.0029 lr: 0.02\n",
      "iteration: 143080 loss: 0.0046 lr: 0.02\n",
      "iteration: 143090 loss: 0.0027 lr: 0.02\n",
      "iteration: 143100 loss: 0.0035 lr: 0.02\n",
      "iteration: 143110 loss: 0.0030 lr: 0.02\n",
      "iteration: 143120 loss: 0.0029 lr: 0.02\n",
      "iteration: 143130 loss: 0.0032 lr: 0.02\n",
      "iteration: 143140 loss: 0.0026 lr: 0.02\n",
      "iteration: 143150 loss: 0.0026 lr: 0.02\n",
      "iteration: 143160 loss: 0.0027 lr: 0.02\n",
      "iteration: 143170 loss: 0.0020 lr: 0.02\n",
      "iteration: 143180 loss: 0.0020 lr: 0.02\n",
      "iteration: 143190 loss: 0.0020 lr: 0.02\n",
      "iteration: 143200 loss: 0.0029 lr: 0.02\n",
      "iteration: 143210 loss: 0.0030 lr: 0.02\n",
      "iteration: 143220 loss: 0.0028 lr: 0.02\n",
      "iteration: 143230 loss: 0.0022 lr: 0.02\n",
      "iteration: 143240 loss: 0.0023 lr: 0.02\n",
      "iteration: 143250 loss: 0.0023 lr: 0.02\n",
      "iteration: 143260 loss: 0.0021 lr: 0.02\n",
      "iteration: 143270 loss: 0.0026 lr: 0.02\n",
      "iteration: 143280 loss: 0.0022 lr: 0.02\n",
      "iteration: 143290 loss: 0.0022 lr: 0.02\n",
      "iteration: 143300 loss: 0.0028 lr: 0.02\n",
      "iteration: 143310 loss: 0.0023 lr: 0.02\n",
      "iteration: 143320 loss: 0.0023 lr: 0.02\n",
      "iteration: 143330 loss: 0.0041 lr: 0.02\n",
      "iteration: 143340 loss: 0.0031 lr: 0.02\n",
      "iteration: 143350 loss: 0.0024 lr: 0.02\n",
      "iteration: 143360 loss: 0.0023 lr: 0.02\n",
      "iteration: 143370 loss: 0.0026 lr: 0.02\n",
      "iteration: 143380 loss: 0.0030 lr: 0.02\n",
      "iteration: 143390 loss: 0.0033 lr: 0.02\n",
      "iteration: 143400 loss: 0.0021 lr: 0.02\n",
      "iteration: 143410 loss: 0.0028 lr: 0.02\n",
      "iteration: 143420 loss: 0.0024 lr: 0.02\n",
      "iteration: 143430 loss: 0.0031 lr: 0.02\n",
      "iteration: 143440 loss: 0.0030 lr: 0.02\n",
      "iteration: 143450 loss: 0.0025 lr: 0.02\n",
      "iteration: 143460 loss: 0.0022 lr: 0.02\n",
      "iteration: 143470 loss: 0.0025 lr: 0.02\n",
      "iteration: 143480 loss: 0.0022 lr: 0.02\n",
      "iteration: 143490 loss: 0.0025 lr: 0.02\n",
      "iteration: 143500 loss: 0.0022 lr: 0.02\n",
      "iteration: 143510 loss: 0.0023 lr: 0.02\n",
      "iteration: 143520 loss: 0.0029 lr: 0.02\n",
      "iteration: 143530 loss: 0.0025 lr: 0.02\n",
      "iteration: 143540 loss: 0.0032 lr: 0.02\n",
      "iteration: 143550 loss: 0.0018 lr: 0.02\n",
      "iteration: 143560 loss: 0.0021 lr: 0.02\n",
      "iteration: 143570 loss: 0.0022 lr: 0.02\n",
      "iteration: 143580 loss: 0.0024 lr: 0.02\n",
      "iteration: 143590 loss: 0.0023 lr: 0.02\n",
      "iteration: 143600 loss: 0.0031 lr: 0.02\n",
      "iteration: 143610 loss: 0.0020 lr: 0.02\n",
      "iteration: 143620 loss: 0.0023 lr: 0.02\n",
      "iteration: 143630 loss: 0.0017 lr: 0.02\n",
      "iteration: 143640 loss: 0.0028 lr: 0.02\n",
      "iteration: 143650 loss: 0.0026 lr: 0.02\n",
      "iteration: 143660 loss: 0.0026 lr: 0.02\n",
      "iteration: 143670 loss: 0.0027 lr: 0.02\n",
      "iteration: 143680 loss: 0.0024 lr: 0.02\n",
      "iteration: 143690 loss: 0.0032 lr: 0.02\n",
      "iteration: 143700 loss: 0.0034 lr: 0.02\n",
      "iteration: 143710 loss: 0.0034 lr: 0.02\n",
      "iteration: 143720 loss: 0.0026 lr: 0.02\n",
      "iteration: 143730 loss: 0.0031 lr: 0.02\n",
      "iteration: 143740 loss: 0.0026 lr: 0.02\n",
      "iteration: 143750 loss: 0.0024 lr: 0.02\n",
      "iteration: 143760 loss: 0.0030 lr: 0.02\n",
      "iteration: 143770 loss: 0.0044 lr: 0.02\n",
      "iteration: 143780 loss: 0.0017 lr: 0.02\n",
      "iteration: 143790 loss: 0.0030 lr: 0.02\n",
      "iteration: 143800 loss: 0.0030 lr: 0.02\n",
      "iteration: 143810 loss: 0.0027 lr: 0.02\n",
      "iteration: 143820 loss: 0.0026 lr: 0.02\n",
      "iteration: 143830 loss: 0.0031 lr: 0.02\n",
      "iteration: 143840 loss: 0.0027 lr: 0.02\n",
      "iteration: 143850 loss: 0.0023 lr: 0.02\n",
      "iteration: 143860 loss: 0.0024 lr: 0.02\n",
      "iteration: 143870 loss: 0.0024 lr: 0.02\n",
      "iteration: 143880 loss: 0.0025 lr: 0.02\n",
      "iteration: 143890 loss: 0.0027 lr: 0.02\n",
      "iteration: 143900 loss: 0.0027 lr: 0.02\n",
      "iteration: 143910 loss: 0.0033 lr: 0.02\n",
      "iteration: 143920 loss: 0.0033 lr: 0.02\n",
      "iteration: 143930 loss: 0.0032 lr: 0.02\n",
      "iteration: 143940 loss: 0.0025 lr: 0.02\n",
      "iteration: 143950 loss: 0.0024 lr: 0.02\n",
      "iteration: 143960 loss: 0.0023 lr: 0.02\n",
      "iteration: 143970 loss: 0.0024 lr: 0.02\n",
      "iteration: 143980 loss: 0.0021 lr: 0.02\n",
      "iteration: 143990 loss: 0.0025 lr: 0.02\n",
      "iteration: 144000 loss: 0.0024 lr: 0.02\n",
      "iteration: 144010 loss: 0.0032 lr: 0.02\n",
      "iteration: 144020 loss: 0.0029 lr: 0.02\n",
      "iteration: 144030 loss: 0.0027 lr: 0.02\n",
      "iteration: 144040 loss: 0.0027 lr: 0.02\n",
      "iteration: 144050 loss: 0.0024 lr: 0.02\n",
      "iteration: 144060 loss: 0.0021 lr: 0.02\n",
      "iteration: 144070 loss: 0.0024 lr: 0.02\n",
      "iteration: 144080 loss: 0.0024 lr: 0.02\n",
      "iteration: 144090 loss: 0.0028 lr: 0.02\n",
      "iteration: 144100 loss: 0.0021 lr: 0.02\n",
      "iteration: 144110 loss: 0.0020 lr: 0.02\n",
      "iteration: 144120 loss: 0.0034 lr: 0.02\n",
      "iteration: 144130 loss: 0.0017 lr: 0.02\n",
      "iteration: 144140 loss: 0.0028 lr: 0.02\n",
      "iteration: 144150 loss: 0.0024 lr: 0.02\n",
      "iteration: 144160 loss: 0.0016 lr: 0.02\n",
      "iteration: 144170 loss: 0.0031 lr: 0.02\n",
      "iteration: 144180 loss: 0.0030 lr: 0.02\n",
      "iteration: 144190 loss: 0.0039 lr: 0.02\n",
      "iteration: 144200 loss: 0.0027 lr: 0.02\n",
      "iteration: 144210 loss: 0.0022 lr: 0.02\n",
      "iteration: 144220 loss: 0.0027 lr: 0.02\n",
      "iteration: 144230 loss: 0.0024 lr: 0.02\n",
      "iteration: 144240 loss: 0.0023 lr: 0.02\n",
      "iteration: 144250 loss: 0.0031 lr: 0.02\n",
      "iteration: 144260 loss: 0.0023 lr: 0.02\n",
      "iteration: 144270 loss: 0.0023 lr: 0.02\n",
      "iteration: 144280 loss: 0.0022 lr: 0.02\n",
      "iteration: 144290 loss: 0.0037 lr: 0.02\n",
      "iteration: 144300 loss: 0.0023 lr: 0.02\n",
      "iteration: 144310 loss: 0.0021 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 144320 loss: 0.0022 lr: 0.02\n",
      "iteration: 144330 loss: 0.0023 lr: 0.02\n",
      "iteration: 144340 loss: 0.0019 lr: 0.02\n",
      "iteration: 144350 loss: 0.0018 lr: 0.02\n",
      "iteration: 144360 loss: 0.0026 lr: 0.02\n",
      "iteration: 144370 loss: 0.0034 lr: 0.02\n",
      "iteration: 144380 loss: 0.0022 lr: 0.02\n",
      "iteration: 144390 loss: 0.0024 lr: 0.02\n",
      "iteration: 144400 loss: 0.0021 lr: 0.02\n",
      "iteration: 144410 loss: 0.0023 lr: 0.02\n",
      "iteration: 144420 loss: 0.0026 lr: 0.02\n",
      "iteration: 144430 loss: 0.0027 lr: 0.02\n",
      "iteration: 144440 loss: 0.0026 lr: 0.02\n",
      "iteration: 144450 loss: 0.0032 lr: 0.02\n",
      "iteration: 144460 loss: 0.0036 lr: 0.02\n",
      "iteration: 144470 loss: 0.0026 lr: 0.02\n",
      "iteration: 144480 loss: 0.0025 lr: 0.02\n",
      "iteration: 144490 loss: 0.0028 lr: 0.02\n",
      "iteration: 144500 loss: 0.0027 lr: 0.02\n",
      "iteration: 144510 loss: 0.0021 lr: 0.02\n",
      "iteration: 144520 loss: 0.0032 lr: 0.02\n",
      "iteration: 144530 loss: 0.0025 lr: 0.02\n",
      "iteration: 144540 loss: 0.0028 lr: 0.02\n",
      "iteration: 144550 loss: 0.0033 lr: 0.02\n",
      "iteration: 144560 loss: 0.0025 lr: 0.02\n",
      "iteration: 144570 loss: 0.0023 lr: 0.02\n",
      "iteration: 144580 loss: 0.0027 lr: 0.02\n",
      "iteration: 144590 loss: 0.0021 lr: 0.02\n",
      "iteration: 144600 loss: 0.0020 lr: 0.02\n",
      "iteration: 144610 loss: 0.0028 lr: 0.02\n",
      "iteration: 144620 loss: 0.0025 lr: 0.02\n",
      "iteration: 144630 loss: 0.0029 lr: 0.02\n",
      "iteration: 144640 loss: 0.0031 lr: 0.02\n",
      "iteration: 144650 loss: 0.0019 lr: 0.02\n",
      "iteration: 144660 loss: 0.0026 lr: 0.02\n",
      "iteration: 144670 loss: 0.0024 lr: 0.02\n",
      "iteration: 144680 loss: 0.0025 lr: 0.02\n",
      "iteration: 144690 loss: 0.0014 lr: 0.02\n",
      "iteration: 144700 loss: 0.0019 lr: 0.02\n",
      "iteration: 144710 loss: 0.0028 lr: 0.02\n",
      "iteration: 144720 loss: 0.0029 lr: 0.02\n",
      "iteration: 144730 loss: 0.0022 lr: 0.02\n",
      "iteration: 144740 loss: 0.0025 lr: 0.02\n",
      "iteration: 144750 loss: 0.0022 lr: 0.02\n",
      "iteration: 144760 loss: 0.0021 lr: 0.02\n",
      "iteration: 144770 loss: 0.0028 lr: 0.02\n",
      "iteration: 144780 loss: 0.0024 lr: 0.02\n",
      "iteration: 144790 loss: 0.0024 lr: 0.02\n",
      "iteration: 144800 loss: 0.0030 lr: 0.02\n",
      "iteration: 144810 loss: 0.0024 lr: 0.02\n",
      "iteration: 144820 loss: 0.0029 lr: 0.02\n",
      "iteration: 144830 loss: 0.0020 lr: 0.02\n",
      "iteration: 144840 loss: 0.0019 lr: 0.02\n",
      "iteration: 144850 loss: 0.0025 lr: 0.02\n",
      "iteration: 144860 loss: 0.0021 lr: 0.02\n",
      "iteration: 144870 loss: 0.0018 lr: 0.02\n",
      "iteration: 144880 loss: 0.0028 lr: 0.02\n",
      "iteration: 144890 loss: 0.0019 lr: 0.02\n",
      "iteration: 144900 loss: 0.0020 lr: 0.02\n",
      "iteration: 144910 loss: 0.0019 lr: 0.02\n",
      "iteration: 144920 loss: 0.0022 lr: 0.02\n",
      "iteration: 144930 loss: 0.0024 lr: 0.02\n",
      "iteration: 144940 loss: 0.0025 lr: 0.02\n",
      "iteration: 144950 loss: 0.0024 lr: 0.02\n",
      "iteration: 144960 loss: 0.0024 lr: 0.02\n",
      "iteration: 144970 loss: 0.0028 lr: 0.02\n",
      "iteration: 144980 loss: 0.0023 lr: 0.02\n",
      "iteration: 144990 loss: 0.0024 lr: 0.02\n",
      "iteration: 145000 loss: 0.0019 lr: 0.02\n",
      "iteration: 145010 loss: 0.0023 lr: 0.02\n",
      "iteration: 145020 loss: 0.0025 lr: 0.02\n",
      "iteration: 145030 loss: 0.0031 lr: 0.02\n",
      "iteration: 145040 loss: 0.0030 lr: 0.02\n",
      "iteration: 145050 loss: 0.0021 lr: 0.02\n",
      "iteration: 145060 loss: 0.0027 lr: 0.02\n",
      "iteration: 145070 loss: 0.0031 lr: 0.02\n",
      "iteration: 145080 loss: 0.0026 lr: 0.02\n",
      "iteration: 145090 loss: 0.0020 lr: 0.02\n",
      "iteration: 145100 loss: 0.0024 lr: 0.02\n",
      "iteration: 145110 loss: 0.0027 lr: 0.02\n",
      "iteration: 145120 loss: 0.0026 lr: 0.02\n",
      "iteration: 145130 loss: 0.0026 lr: 0.02\n",
      "iteration: 145140 loss: 0.0025 lr: 0.02\n",
      "iteration: 145150 loss: 0.0022 lr: 0.02\n",
      "iteration: 145160 loss: 0.0035 lr: 0.02\n",
      "iteration: 145170 loss: 0.0027 lr: 0.02\n",
      "iteration: 145180 loss: 0.0026 lr: 0.02\n",
      "iteration: 145190 loss: 0.0023 lr: 0.02\n",
      "iteration: 145200 loss: 0.0021 lr: 0.02\n",
      "iteration: 145210 loss: 0.0027 lr: 0.02\n",
      "iteration: 145220 loss: 0.0026 lr: 0.02\n",
      "iteration: 145230 loss: 0.0022 lr: 0.02\n",
      "iteration: 145240 loss: 0.0020 lr: 0.02\n",
      "iteration: 145250 loss: 0.0016 lr: 0.02\n",
      "iteration: 145260 loss: 0.0020 lr: 0.02\n",
      "iteration: 145270 loss: 0.0022 lr: 0.02\n",
      "iteration: 145280 loss: 0.0024 lr: 0.02\n",
      "iteration: 145290 loss: 0.0031 lr: 0.02\n",
      "iteration: 145300 loss: 0.0027 lr: 0.02\n",
      "iteration: 145310 loss: 0.0023 lr: 0.02\n",
      "iteration: 145320 loss: 0.0035 lr: 0.02\n",
      "iteration: 145330 loss: 0.0028 lr: 0.02\n",
      "iteration: 145340 loss: 0.0030 lr: 0.02\n",
      "iteration: 145350 loss: 0.0028 lr: 0.02\n",
      "iteration: 145360 loss: 0.0036 lr: 0.02\n",
      "iteration: 145370 loss: 0.0021 lr: 0.02\n",
      "iteration: 145380 loss: 0.0022 lr: 0.02\n",
      "iteration: 145390 loss: 0.0019 lr: 0.02\n",
      "iteration: 145400 loss: 0.0031 lr: 0.02\n",
      "iteration: 145410 loss: 0.0028 lr: 0.02\n",
      "iteration: 145420 loss: 0.0020 lr: 0.02\n",
      "iteration: 145430 loss: 0.0025 lr: 0.02\n",
      "iteration: 145440 loss: 0.0029 lr: 0.02\n",
      "iteration: 145450 loss: 0.0036 lr: 0.02\n",
      "iteration: 145460 loss: 0.0037 lr: 0.02\n",
      "iteration: 145470 loss: 0.0029 lr: 0.02\n",
      "iteration: 145480 loss: 0.0027 lr: 0.02\n",
      "iteration: 145490 loss: 0.0028 lr: 0.02\n",
      "iteration: 145500 loss: 0.0030 lr: 0.02\n",
      "iteration: 145510 loss: 0.0021 lr: 0.02\n",
      "iteration: 145520 loss: 0.0018 lr: 0.02\n",
      "iteration: 145530 loss: 0.0021 lr: 0.02\n",
      "iteration: 145540 loss: 0.0025 lr: 0.02\n",
      "iteration: 145550 loss: 0.0024 lr: 0.02\n",
      "iteration: 145560 loss: 0.0028 lr: 0.02\n",
      "iteration: 145570 loss: 0.0018 lr: 0.02\n",
      "iteration: 145580 loss: 0.0024 lr: 0.02\n",
      "iteration: 145590 loss: 0.0019 lr: 0.02\n",
      "iteration: 145600 loss: 0.0021 lr: 0.02\n",
      "iteration: 145610 loss: 0.0024 lr: 0.02\n",
      "iteration: 145620 loss: 0.0037 lr: 0.02\n",
      "iteration: 145630 loss: 0.0022 lr: 0.02\n",
      "iteration: 145640 loss: 0.0022 lr: 0.02\n",
      "iteration: 145650 loss: 0.0026 lr: 0.02\n",
      "iteration: 145660 loss: 0.0029 lr: 0.02\n",
      "iteration: 145670 loss: 0.0018 lr: 0.02\n",
      "iteration: 145680 loss: 0.0022 lr: 0.02\n",
      "iteration: 145690 loss: 0.0030 lr: 0.02\n",
      "iteration: 145700 loss: 0.0021 lr: 0.02\n",
      "iteration: 145710 loss: 0.0026 lr: 0.02\n",
      "iteration: 145720 loss: 0.0027 lr: 0.02\n",
      "iteration: 145730 loss: 0.0027 lr: 0.02\n",
      "iteration: 145740 loss: 0.0021 lr: 0.02\n",
      "iteration: 145750 loss: 0.0032 lr: 0.02\n",
      "iteration: 145760 loss: 0.0030 lr: 0.02\n",
      "iteration: 145770 loss: 0.0021 lr: 0.02\n",
      "iteration: 145780 loss: 0.0035 lr: 0.02\n",
      "iteration: 145790 loss: 0.0022 lr: 0.02\n",
      "iteration: 145800 loss: 0.0027 lr: 0.02\n",
      "iteration: 145810 loss: 0.0027 lr: 0.02\n",
      "iteration: 145820 loss: 0.0027 lr: 0.02\n",
      "iteration: 145830 loss: 0.0022 lr: 0.02\n",
      "iteration: 145840 loss: 0.0020 lr: 0.02\n",
      "iteration: 145850 loss: 0.0030 lr: 0.02\n",
      "iteration: 145860 loss: 0.0025 lr: 0.02\n",
      "iteration: 145870 loss: 0.0032 lr: 0.02\n",
      "iteration: 145880 loss: 0.0022 lr: 0.02\n",
      "iteration: 145890 loss: 0.0024 lr: 0.02\n",
      "iteration: 145900 loss: 0.0034 lr: 0.02\n",
      "iteration: 145910 loss: 0.0030 lr: 0.02\n",
      "iteration: 145920 loss: 0.0027 lr: 0.02\n",
      "iteration: 145930 loss: 0.0021 lr: 0.02\n",
      "iteration: 145940 loss: 0.0023 lr: 0.02\n",
      "iteration: 145950 loss: 0.0027 lr: 0.02\n",
      "iteration: 145960 loss: 0.0029 lr: 0.02\n",
      "iteration: 145970 loss: 0.0024 lr: 0.02\n",
      "iteration: 145980 loss: 0.0026 lr: 0.02\n",
      "iteration: 145990 loss: 0.0034 lr: 0.02\n",
      "iteration: 146000 loss: 0.0028 lr: 0.02\n",
      "iteration: 146010 loss: 0.0027 lr: 0.02\n",
      "iteration: 146020 loss: 0.0023 lr: 0.02\n",
      "iteration: 146030 loss: 0.0025 lr: 0.02\n",
      "iteration: 146040 loss: 0.0024 lr: 0.02\n",
      "iteration: 146050 loss: 0.0027 lr: 0.02\n",
      "iteration: 146060 loss: 0.0019 lr: 0.02\n",
      "iteration: 146070 loss: 0.0023 lr: 0.02\n",
      "iteration: 146080 loss: 0.0027 lr: 0.02\n",
      "iteration: 146090 loss: 0.0022 lr: 0.02\n",
      "iteration: 146100 loss: 0.0033 lr: 0.02\n",
      "iteration: 146110 loss: 0.0023 lr: 0.02\n",
      "iteration: 146120 loss: 0.0027 lr: 0.02\n",
      "iteration: 146130 loss: 0.0026 lr: 0.02\n",
      "iteration: 146140 loss: 0.0032 lr: 0.02\n",
      "iteration: 146150 loss: 0.0031 lr: 0.02\n",
      "iteration: 146160 loss: 0.0026 lr: 0.02\n",
      "iteration: 146170 loss: 0.0034 lr: 0.02\n",
      "iteration: 146180 loss: 0.0026 lr: 0.02\n",
      "iteration: 146190 loss: 0.0024 lr: 0.02\n",
      "iteration: 146200 loss: 0.0030 lr: 0.02\n",
      "iteration: 146210 loss: 0.0028 lr: 0.02\n",
      "iteration: 146220 loss: 0.0029 lr: 0.02\n",
      "iteration: 146230 loss: 0.0028 lr: 0.02\n",
      "iteration: 146240 loss: 0.0026 lr: 0.02\n",
      "iteration: 146250 loss: 0.0025 lr: 0.02\n",
      "iteration: 146260 loss: 0.0023 lr: 0.02\n",
      "iteration: 146270 loss: 0.0024 lr: 0.02\n",
      "iteration: 146280 loss: 0.0024 lr: 0.02\n",
      "iteration: 146290 loss: 0.0026 lr: 0.02\n",
      "iteration: 146300 loss: 0.0020 lr: 0.02\n",
      "iteration: 146310 loss: 0.0018 lr: 0.02\n",
      "iteration: 146320 loss: 0.0027 lr: 0.02\n",
      "iteration: 146330 loss: 0.0025 lr: 0.02\n",
      "iteration: 146340 loss: 0.0017 lr: 0.02\n",
      "iteration: 146350 loss: 0.0019 lr: 0.02\n",
      "iteration: 146360 loss: 0.0028 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 146370 loss: 0.0027 lr: 0.02\n",
      "iteration: 146380 loss: 0.0025 lr: 0.02\n",
      "iteration: 146390 loss: 0.0032 lr: 0.02\n",
      "iteration: 146400 loss: 0.0021 lr: 0.02\n",
      "iteration: 146410 loss: 0.0022 lr: 0.02\n",
      "iteration: 146420 loss: 0.0032 lr: 0.02\n",
      "iteration: 146430 loss: 0.0027 lr: 0.02\n",
      "iteration: 146440 loss: 0.0017 lr: 0.02\n",
      "iteration: 146450 loss: 0.0035 lr: 0.02\n",
      "iteration: 146460 loss: 0.0025 lr: 0.02\n",
      "iteration: 146470 loss: 0.0031 lr: 0.02\n",
      "iteration: 146480 loss: 0.0024 lr: 0.02\n",
      "iteration: 146490 loss: 0.0019 lr: 0.02\n",
      "iteration: 146500 loss: 0.0024 lr: 0.02\n",
      "iteration: 146510 loss: 0.0021 lr: 0.02\n",
      "iteration: 146520 loss: 0.0025 lr: 0.02\n",
      "iteration: 146530 loss: 0.0033 lr: 0.02\n",
      "iteration: 146540 loss: 0.0027 lr: 0.02\n",
      "iteration: 146550 loss: 0.0031 lr: 0.02\n",
      "iteration: 146560 loss: 0.0020 lr: 0.02\n",
      "iteration: 146570 loss: 0.0023 lr: 0.02\n",
      "iteration: 146580 loss: 0.0022 lr: 0.02\n",
      "iteration: 146590 loss: 0.0031 lr: 0.02\n",
      "iteration: 146600 loss: 0.0025 lr: 0.02\n",
      "iteration: 146610 loss: 0.0019 lr: 0.02\n",
      "iteration: 146620 loss: 0.0025 lr: 0.02\n",
      "iteration: 146630 loss: 0.0027 lr: 0.02\n",
      "iteration: 146640 loss: 0.0032 lr: 0.02\n",
      "iteration: 146650 loss: 0.0025 lr: 0.02\n",
      "iteration: 146660 loss: 0.0018 lr: 0.02\n",
      "iteration: 146670 loss: 0.0017 lr: 0.02\n",
      "iteration: 146680 loss: 0.0023 lr: 0.02\n",
      "iteration: 146690 loss: 0.0025 lr: 0.02\n",
      "iteration: 146700 loss: 0.0019 lr: 0.02\n",
      "iteration: 146710 loss: 0.0027 lr: 0.02\n",
      "iteration: 146720 loss: 0.0022 lr: 0.02\n",
      "iteration: 146730 loss: 0.0024 lr: 0.02\n",
      "iteration: 146740 loss: 0.0022 lr: 0.02\n",
      "iteration: 146750 loss: 0.0024 lr: 0.02\n",
      "iteration: 146760 loss: 0.0018 lr: 0.02\n",
      "iteration: 146770 loss: 0.0026 lr: 0.02\n",
      "iteration: 146780 loss: 0.0022 lr: 0.02\n",
      "iteration: 146790 loss: 0.0018 lr: 0.02\n",
      "iteration: 146800 loss: 0.0024 lr: 0.02\n",
      "iteration: 146810 loss: 0.0028 lr: 0.02\n",
      "iteration: 146820 loss: 0.0025 lr: 0.02\n",
      "iteration: 146830 loss: 0.0025 lr: 0.02\n",
      "iteration: 146840 loss: 0.0023 lr: 0.02\n",
      "iteration: 146850 loss: 0.0027 lr: 0.02\n",
      "iteration: 146860 loss: 0.0025 lr: 0.02\n",
      "iteration: 146870 loss: 0.0031 lr: 0.02\n",
      "iteration: 146880 loss: 0.0037 lr: 0.02\n",
      "iteration: 146890 loss: 0.0025 lr: 0.02\n",
      "iteration: 146900 loss: 0.0023 lr: 0.02\n",
      "iteration: 146910 loss: 0.0021 lr: 0.02\n",
      "iteration: 146920 loss: 0.0024 lr: 0.02\n",
      "iteration: 146930 loss: 0.0024 lr: 0.02\n",
      "iteration: 146940 loss: 0.0028 lr: 0.02\n",
      "iteration: 146950 loss: 0.0033 lr: 0.02\n",
      "iteration: 146960 loss: 0.0038 lr: 0.02\n",
      "iteration: 146970 loss: 0.0034 lr: 0.02\n",
      "iteration: 146980 loss: 0.0025 lr: 0.02\n",
      "iteration: 146990 loss: 0.0029 lr: 0.02\n",
      "iteration: 147000 loss: 0.0023 lr: 0.02\n",
      "iteration: 147010 loss: 0.0025 lr: 0.02\n",
      "iteration: 147020 loss: 0.0022 lr: 0.02\n",
      "iteration: 147030 loss: 0.0020 lr: 0.02\n",
      "iteration: 147040 loss: 0.0021 lr: 0.02\n",
      "iteration: 147050 loss: 0.0022 lr: 0.02\n",
      "iteration: 147060 loss: 0.0036 lr: 0.02\n",
      "iteration: 147070 loss: 0.0023 lr: 0.02\n",
      "iteration: 147080 loss: 0.0025 lr: 0.02\n",
      "iteration: 147090 loss: 0.0024 lr: 0.02\n",
      "iteration: 147100 loss: 0.0023 lr: 0.02\n",
      "iteration: 147110 loss: 0.0026 lr: 0.02\n",
      "iteration: 147120 loss: 0.0023 lr: 0.02\n",
      "iteration: 147130 loss: 0.0031 lr: 0.02\n",
      "iteration: 147140 loss: 0.0032 lr: 0.02\n",
      "iteration: 147150 loss: 0.0024 lr: 0.02\n",
      "iteration: 147160 loss: 0.0024 lr: 0.02\n",
      "iteration: 147170 loss: 0.0026 lr: 0.02\n",
      "iteration: 147180 loss: 0.0039 lr: 0.02\n",
      "iteration: 147190 loss: 0.0021 lr: 0.02\n",
      "iteration: 147200 loss: 0.0024 lr: 0.02\n",
      "iteration: 147210 loss: 0.0027 lr: 0.02\n",
      "iteration: 147220 loss: 0.0025 lr: 0.02\n",
      "iteration: 147230 loss: 0.0027 lr: 0.02\n",
      "iteration: 147240 loss: 0.0023 lr: 0.02\n",
      "iteration: 147250 loss: 0.0027 lr: 0.02\n",
      "iteration: 147260 loss: 0.0025 lr: 0.02\n",
      "iteration: 147270 loss: 0.0020 lr: 0.02\n",
      "iteration: 147280 loss: 0.0020 lr: 0.02\n",
      "iteration: 147290 loss: 0.0026 lr: 0.02\n",
      "iteration: 147300 loss: 0.0022 lr: 0.02\n",
      "iteration: 147310 loss: 0.0027 lr: 0.02\n",
      "iteration: 147320 loss: 0.0020 lr: 0.02\n",
      "iteration: 147330 loss: 0.0028 lr: 0.02\n",
      "iteration: 147340 loss: 0.0022 lr: 0.02\n",
      "iteration: 147350 loss: 0.0026 lr: 0.02\n",
      "iteration: 147360 loss: 0.0028 lr: 0.02\n",
      "iteration: 147370 loss: 0.0021 lr: 0.02\n",
      "iteration: 147380 loss: 0.0025 lr: 0.02\n",
      "iteration: 147390 loss: 0.0029 lr: 0.02\n",
      "iteration: 147400 loss: 0.0024 lr: 0.02\n",
      "iteration: 147410 loss: 0.0019 lr: 0.02\n",
      "iteration: 147420 loss: 0.0036 lr: 0.02\n",
      "iteration: 147430 loss: 0.0033 lr: 0.02\n",
      "iteration: 147440 loss: 0.0021 lr: 0.02\n",
      "iteration: 147450 loss: 0.0028 lr: 0.02\n",
      "iteration: 147460 loss: 0.0022 lr: 0.02\n",
      "iteration: 147470 loss: 0.0031 lr: 0.02\n",
      "iteration: 147480 loss: 0.0020 lr: 0.02\n",
      "iteration: 147490 loss: 0.0020 lr: 0.02\n",
      "iteration: 147500 loss: 0.0024 lr: 0.02\n",
      "iteration: 147510 loss: 0.0018 lr: 0.02\n",
      "iteration: 147520 loss: 0.0026 lr: 0.02\n",
      "iteration: 147530 loss: 0.0018 lr: 0.02\n",
      "iteration: 147540 loss: 0.0024 lr: 0.02\n",
      "iteration: 147550 loss: 0.0020 lr: 0.02\n",
      "iteration: 147560 loss: 0.0020 lr: 0.02\n",
      "iteration: 147570 loss: 0.0034 lr: 0.02\n",
      "iteration: 147580 loss: 0.0024 lr: 0.02\n",
      "iteration: 147590 loss: 0.0025 lr: 0.02\n",
      "iteration: 147600 loss: 0.0027 lr: 0.02\n",
      "iteration: 147610 loss: 0.0030 lr: 0.02\n",
      "iteration: 147620 loss: 0.0022 lr: 0.02\n",
      "iteration: 147630 loss: 0.0021 lr: 0.02\n",
      "iteration: 147640 loss: 0.0030 lr: 0.02\n",
      "iteration: 147650 loss: 0.0026 lr: 0.02\n",
      "iteration: 147660 loss: 0.0029 lr: 0.02\n",
      "iteration: 147670 loss: 0.0022 lr: 0.02\n",
      "iteration: 147680 loss: 0.0025 lr: 0.02\n",
      "iteration: 147690 loss: 0.0023 lr: 0.02\n",
      "iteration: 147700 loss: 0.0030 lr: 0.02\n",
      "iteration: 147710 loss: 0.0032 lr: 0.02\n",
      "iteration: 147720 loss: 0.0026 lr: 0.02\n",
      "iteration: 147730 loss: 0.0020 lr: 0.02\n",
      "iteration: 147740 loss: 0.0025 lr: 0.02\n",
      "iteration: 147750 loss: 0.0029 lr: 0.02\n",
      "iteration: 147760 loss: 0.0022 lr: 0.02\n",
      "iteration: 147770 loss: 0.0025 lr: 0.02\n",
      "iteration: 147780 loss: 0.0031 lr: 0.02\n",
      "iteration: 147790 loss: 0.0019 lr: 0.02\n",
      "iteration: 147800 loss: 0.0022 lr: 0.02\n",
      "iteration: 147810 loss: 0.0029 lr: 0.02\n",
      "iteration: 147820 loss: 0.0028 lr: 0.02\n",
      "iteration: 147830 loss: 0.0027 lr: 0.02\n",
      "iteration: 147840 loss: 0.0026 lr: 0.02\n",
      "iteration: 147850 loss: 0.0021 lr: 0.02\n",
      "iteration: 147860 loss: 0.0025 lr: 0.02\n",
      "iteration: 147870 loss: 0.0026 lr: 0.02\n",
      "iteration: 147880 loss: 0.0024 lr: 0.02\n",
      "iteration: 147890 loss: 0.0033 lr: 0.02\n",
      "iteration: 147900 loss: 0.0028 lr: 0.02\n",
      "iteration: 147910 loss: 0.0028 lr: 0.02\n",
      "iteration: 147920 loss: 0.0025 lr: 0.02\n",
      "iteration: 147930 loss: 0.0026 lr: 0.02\n",
      "iteration: 147940 loss: 0.0021 lr: 0.02\n",
      "iteration: 147950 loss: 0.0031 lr: 0.02\n",
      "iteration: 147960 loss: 0.0027 lr: 0.02\n",
      "iteration: 147970 loss: 0.0021 lr: 0.02\n",
      "iteration: 147980 loss: 0.0019 lr: 0.02\n",
      "iteration: 147990 loss: 0.0021 lr: 0.02\n",
      "iteration: 148000 loss: 0.0022 lr: 0.02\n",
      "iteration: 148010 loss: 0.0025 lr: 0.02\n",
      "iteration: 148020 loss: 0.0036 lr: 0.02\n",
      "iteration: 148030 loss: 0.0023 lr: 0.02\n",
      "iteration: 148040 loss: 0.0022 lr: 0.02\n",
      "iteration: 148050 loss: 0.0026 lr: 0.02\n",
      "iteration: 148060 loss: 0.0028 lr: 0.02\n",
      "iteration: 148070 loss: 0.0022 lr: 0.02\n",
      "iteration: 148080 loss: 0.0035 lr: 0.02\n",
      "iteration: 148090 loss: 0.0020 lr: 0.02\n",
      "iteration: 148100 loss: 0.0025 lr: 0.02\n",
      "iteration: 148110 loss: 0.0019 lr: 0.02\n",
      "iteration: 148120 loss: 0.0027 lr: 0.02\n",
      "iteration: 148130 loss: 0.0026 lr: 0.02\n",
      "iteration: 148140 loss: 0.0027 lr: 0.02\n",
      "iteration: 148150 loss: 0.0028 lr: 0.02\n",
      "iteration: 148160 loss: 0.0021 lr: 0.02\n",
      "iteration: 148170 loss: 0.0028 lr: 0.02\n",
      "iteration: 148180 loss: 0.0027 lr: 0.02\n",
      "iteration: 148190 loss: 0.0026 lr: 0.02\n",
      "iteration: 148200 loss: 0.0023 lr: 0.02\n",
      "iteration: 148210 loss: 0.0025 lr: 0.02\n",
      "iteration: 148220 loss: 0.0024 lr: 0.02\n",
      "iteration: 148230 loss: 0.0033 lr: 0.02\n",
      "iteration: 148240 loss: 0.0026 lr: 0.02\n",
      "iteration: 148250 loss: 0.0029 lr: 0.02\n",
      "iteration: 148260 loss: 0.0025 lr: 0.02\n",
      "iteration: 148270 loss: 0.0019 lr: 0.02\n",
      "iteration: 148280 loss: 0.0025 lr: 0.02\n",
      "iteration: 148290 loss: 0.0022 lr: 0.02\n",
      "iteration: 148300 loss: 0.0023 lr: 0.02\n",
      "iteration: 148310 loss: 0.0026 lr: 0.02\n",
      "iteration: 148320 loss: 0.0029 lr: 0.02\n",
      "iteration: 148330 loss: 0.0026 lr: 0.02\n",
      "iteration: 148340 loss: 0.0025 lr: 0.02\n",
      "iteration: 148350 loss: 0.0020 lr: 0.02\n",
      "iteration: 148360 loss: 0.0021 lr: 0.02\n",
      "iteration: 148370 loss: 0.0030 lr: 0.02\n",
      "iteration: 148380 loss: 0.0020 lr: 0.02\n",
      "iteration: 148390 loss: 0.0021 lr: 0.02\n",
      "iteration: 148400 loss: 0.0021 lr: 0.02\n",
      "iteration: 148410 loss: 0.0023 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 148420 loss: 0.0035 lr: 0.02\n",
      "iteration: 148430 loss: 0.0025 lr: 0.02\n",
      "iteration: 148440 loss: 0.0021 lr: 0.02\n",
      "iteration: 148450 loss: 0.0027 lr: 0.02\n",
      "iteration: 148460 loss: 0.0019 lr: 0.02\n",
      "iteration: 148470 loss: 0.0022 lr: 0.02\n",
      "iteration: 148480 loss: 0.0025 lr: 0.02\n",
      "iteration: 148490 loss: 0.0026 lr: 0.02\n",
      "iteration: 148500 loss: 0.0021 lr: 0.02\n",
      "iteration: 148510 loss: 0.0032 lr: 0.02\n",
      "iteration: 148520 loss: 0.0024 lr: 0.02\n",
      "iteration: 148530 loss: 0.0023 lr: 0.02\n",
      "iteration: 148540 loss: 0.0025 lr: 0.02\n",
      "iteration: 148550 loss: 0.0031 lr: 0.02\n",
      "iteration: 148560 loss: 0.0021 lr: 0.02\n",
      "iteration: 148570 loss: 0.0024 lr: 0.02\n",
      "iteration: 148580 loss: 0.0017 lr: 0.02\n",
      "iteration: 148590 loss: 0.0022 lr: 0.02\n",
      "iteration: 148600 loss: 0.0025 lr: 0.02\n",
      "iteration: 148610 loss: 0.0031 lr: 0.02\n",
      "iteration: 148620 loss: 0.0019 lr: 0.02\n",
      "iteration: 148630 loss: 0.0021 lr: 0.02\n",
      "iteration: 148640 loss: 0.0022 lr: 0.02\n",
      "iteration: 148650 loss: 0.0016 lr: 0.02\n",
      "iteration: 148660 loss: 0.0018 lr: 0.02\n",
      "iteration: 148670 loss: 0.0037 lr: 0.02\n",
      "iteration: 148680 loss: 0.0023 lr: 0.02\n",
      "iteration: 148690 loss: 0.0023 lr: 0.02\n",
      "iteration: 148700 loss: 0.0027 lr: 0.02\n",
      "iteration: 148710 loss: 0.0023 lr: 0.02\n",
      "iteration: 148720 loss: 0.0029 lr: 0.02\n",
      "iteration: 148730 loss: 0.0031 lr: 0.02\n",
      "iteration: 148740 loss: 0.0029 lr: 0.02\n",
      "iteration: 148750 loss: 0.0022 lr: 0.02\n",
      "iteration: 148760 loss: 0.0024 lr: 0.02\n",
      "iteration: 148770 loss: 0.0025 lr: 0.02\n",
      "iteration: 148780 loss: 0.0019 lr: 0.02\n",
      "iteration: 148790 loss: 0.0028 lr: 0.02\n",
      "iteration: 148800 loss: 0.0026 lr: 0.02\n",
      "iteration: 148810 loss: 0.0026 lr: 0.02\n",
      "iteration: 148820 loss: 0.0022 lr: 0.02\n",
      "iteration: 148830 loss: 0.0024 lr: 0.02\n",
      "iteration: 148840 loss: 0.0022 lr: 0.02\n",
      "iteration: 148850 loss: 0.0025 lr: 0.02\n",
      "iteration: 148860 loss: 0.0026 lr: 0.02\n",
      "iteration: 148870 loss: 0.0027 lr: 0.02\n",
      "iteration: 148880 loss: 0.0028 lr: 0.02\n",
      "iteration: 148890 loss: 0.0020 lr: 0.02\n",
      "iteration: 148900 loss: 0.0022 lr: 0.02\n",
      "iteration: 148910 loss: 0.0023 lr: 0.02\n",
      "iteration: 148920 loss: 0.0028 lr: 0.02\n",
      "iteration: 148930 loss: 0.0020 lr: 0.02\n",
      "iteration: 148940 loss: 0.0026 lr: 0.02\n",
      "iteration: 148950 loss: 0.0019 lr: 0.02\n",
      "iteration: 148960 loss: 0.0022 lr: 0.02\n",
      "iteration: 148970 loss: 0.0035 lr: 0.02\n",
      "iteration: 148980 loss: 0.0027 lr: 0.02\n",
      "iteration: 148990 loss: 0.0025 lr: 0.02\n",
      "iteration: 149000 loss: 0.0026 lr: 0.02\n",
      "iteration: 149010 loss: 0.0030 lr: 0.02\n",
      "iteration: 149020 loss: 0.0026 lr: 0.02\n",
      "iteration: 149030 loss: 0.0021 lr: 0.02\n",
      "iteration: 149040 loss: 0.0023 lr: 0.02\n",
      "iteration: 149050 loss: 0.0027 lr: 0.02\n",
      "iteration: 149060 loss: 0.0024 lr: 0.02\n",
      "iteration: 149070 loss: 0.0022 lr: 0.02\n",
      "iteration: 149080 loss: 0.0024 lr: 0.02\n",
      "iteration: 149090 loss: 0.0025 lr: 0.02\n",
      "iteration: 149100 loss: 0.0027 lr: 0.02\n",
      "iteration: 149110 loss: 0.0023 lr: 0.02\n",
      "iteration: 149120 loss: 0.0022 lr: 0.02\n",
      "iteration: 149130 loss: 0.0028 lr: 0.02\n",
      "iteration: 149140 loss: 0.0025 lr: 0.02\n",
      "iteration: 149150 loss: 0.0032 lr: 0.02\n",
      "iteration: 149160 loss: 0.0017 lr: 0.02\n",
      "iteration: 149170 loss: 0.0026 lr: 0.02\n",
      "iteration: 149180 loss: 0.0021 lr: 0.02\n",
      "iteration: 149190 loss: 0.0027 lr: 0.02\n",
      "iteration: 149200 loss: 0.0031 lr: 0.02\n",
      "iteration: 149210 loss: 0.0016 lr: 0.02\n",
      "iteration: 149220 loss: 0.0025 lr: 0.02\n",
      "iteration: 149230 loss: 0.0036 lr: 0.02\n",
      "iteration: 149240 loss: 0.0024 lr: 0.02\n",
      "iteration: 149250 loss: 0.0021 lr: 0.02\n",
      "iteration: 149260 loss: 0.0022 lr: 0.02\n",
      "iteration: 149270 loss: 0.0020 lr: 0.02\n",
      "iteration: 149280 loss: 0.0024 lr: 0.02\n",
      "iteration: 149290 loss: 0.0021 lr: 0.02\n",
      "iteration: 149300 loss: 0.0020 lr: 0.02\n",
      "iteration: 149310 loss: 0.0019 lr: 0.02\n",
      "iteration: 149320 loss: 0.0030 lr: 0.02\n",
      "iteration: 149330 loss: 0.0021 lr: 0.02\n",
      "iteration: 149340 loss: 0.0017 lr: 0.02\n",
      "iteration: 149350 loss: 0.0018 lr: 0.02\n",
      "iteration: 149360 loss: 0.0041 lr: 0.02\n",
      "iteration: 149370 loss: 0.0025 lr: 0.02\n",
      "iteration: 149380 loss: 0.0024 lr: 0.02\n",
      "iteration: 149390 loss: 0.0021 lr: 0.02\n",
      "iteration: 149400 loss: 0.0023 lr: 0.02\n",
      "iteration: 149410 loss: 0.0031 lr: 0.02\n",
      "iteration: 149420 loss: 0.0025 lr: 0.02\n",
      "iteration: 149430 loss: 0.0022 lr: 0.02\n",
      "iteration: 149440 loss: 0.0015 lr: 0.02\n",
      "iteration: 149450 loss: 0.0024 lr: 0.02\n",
      "iteration: 149460 loss: 0.0027 lr: 0.02\n",
      "iteration: 149470 loss: 0.0020 lr: 0.02\n",
      "iteration: 149480 loss: 0.0019 lr: 0.02\n",
      "iteration: 149490 loss: 0.0025 lr: 0.02\n",
      "iteration: 149500 loss: 0.0023 lr: 0.02\n",
      "iteration: 149510 loss: 0.0028 lr: 0.02\n",
      "iteration: 149520 loss: 0.0027 lr: 0.02\n",
      "iteration: 149530 loss: 0.0023 lr: 0.02\n",
      "iteration: 149540 loss: 0.0023 lr: 0.02\n",
      "iteration: 149550 loss: 0.0020 lr: 0.02\n",
      "iteration: 149560 loss: 0.0035 lr: 0.02\n",
      "iteration: 149570 loss: 0.0021 lr: 0.02\n",
      "iteration: 149580 loss: 0.0030 lr: 0.02\n",
      "iteration: 149590 loss: 0.0027 lr: 0.02\n",
      "iteration: 149600 loss: 0.0027 lr: 0.02\n",
      "iteration: 149610 loss: 0.0022 lr: 0.02\n",
      "iteration: 149620 loss: 0.0026 lr: 0.02\n",
      "iteration: 149630 loss: 0.0023 lr: 0.02\n",
      "iteration: 149640 loss: 0.0034 lr: 0.02\n",
      "iteration: 149650 loss: 0.0033 lr: 0.02\n",
      "iteration: 149660 loss: 0.0019 lr: 0.02\n",
      "iteration: 149670 loss: 0.0022 lr: 0.02\n",
      "iteration: 149680 loss: 0.0029 lr: 0.02\n",
      "iteration: 149690 loss: 0.0025 lr: 0.02\n",
      "iteration: 149700 loss: 0.0030 lr: 0.02\n",
      "iteration: 149710 loss: 0.0024 lr: 0.02\n",
      "iteration: 149720 loss: 0.0026 lr: 0.02\n",
      "iteration: 149730 loss: 0.0021 lr: 0.02\n",
      "iteration: 149740 loss: 0.0021 lr: 0.02\n",
      "iteration: 149750 loss: 0.0022 lr: 0.02\n",
      "iteration: 149760 loss: 0.0022 lr: 0.02\n",
      "iteration: 149770 loss: 0.0022 lr: 0.02\n",
      "iteration: 149780 loss: 0.0029 lr: 0.02\n",
      "iteration: 149790 loss: 0.0021 lr: 0.02\n"
     ]
    }
   ],
   "source": [
    "#reset in case you started a session before...\n",
    "#tf.reset_default_graph()\n",
    "\n",
    "deeplabcut.train_network(path_config_file, shuffle=1, saveiters=1000, displayiters=10, gputouse=1)\n",
    "\n",
    "#this will run until you stop it (CTRL+C), or hit \"STOP\" icon, or when it hits the end (default, 1.3M iterations). \n",
    "#Whichever you chose, you will see what looks like an error message, but it's not an error - don't worry....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A couple tips for possible troubleshooting (1): \n",
    "\n",
    "if you get **permission errors** when you run this step (above), first check if the weights downloaded. As some docker containers might not have privileges for this (it can be user specific). They should be under 'init_weights' (see path in the pose_cfg.yaml file). You can enter the DOCKER in the terminal:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see more here: https://github.com/MMathisLab/Docker4DeepLabCut2.0#using-the-docker-for-training-and-video-analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Troubleshooting (2): \n",
    "if it appears the training does not start (i.e. \"Starting training...\" does not print immediately),\n",
    "then you have another session running on your GPU. Go check \"nvidia-smi\" and look at the process names. You can only have 1 per GPU!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xZygsb2DoEJc"
   },
   "source": [
    "## Start evaluating\n",
    "This funtion evaluates a trained model for a specific shuffle/shuffles at a particular state or all the states on the data set (images)\n",
    "and stores the results as .csv file in a subdirectory under **evaluation-results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nv4zlbrnoEJg",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "deeplabcut.evaluate_network(path_config_file)\n",
    "\n",
    "# Here you want to see a low pixel error! Of course, it can only be as good as the labeler, so be sure your labels are good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BaLBl3TQtrfB"
   },
   "source": [
    "## There is an optional refinement step\n",
    "- if your pixel errors are not low enough, use DLC docs on how to refine yur network!\n",
    "- You will need to adjust the labels outside of DOCKER! (you can use the createDLCproject notebook) \n",
    "-  see DLC protocol instructions on how to refine your data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OVFLSKKfoEJk"
   },
   "source": [
    "## Start Analyzing videos\n",
    "This function analyzes the new video. The user can choose the best model from the evaluation results and specify the correct snapshot index for the variable **snapshotindex** in the **config.yaml** file. Otherwise, by default the most recent snapshot is used to analyse the video.\n",
    "\n",
    "The results are stored in hd5 file in the same directory where the video resides. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y_LZiS_0oEJl",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0530 01:40:38.111350 127876 deprecation_wrapper.py:119] From C:\\Users\\pbs-mysorelab\\.conda\\envs\\nbkGPU\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\predict_videos.py:127: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using snapshot-1030000 for model F:\\MysoreData\\nbk\\TEST_50fps_mitg05_042120_cam2-spencer loggia-2020-04-24\\dlc-models\\iteration-0\\TEST_50fps_mitg05_042120_cam2Apr24-trainset95shuffle1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0530 01:40:38.254388 127876 deprecation_wrapper.py:119] From C:\\Users\\pbs-mysorelab\\.conda\\envs\\nbkGPU\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\nnet\\predict.py:187: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ResNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0530 01:40:44.157921 127876 deprecation_wrapper.py:119] From C:\\Users\\pbs-mysorelab\\.conda\\envs\\nbkGPU\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\nnet\\pose_net.py:62: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W0530 01:40:45.488962 127876 deprecation_wrapper.py:119] From C:\\Users\\pbs-mysorelab\\.conda\\envs\\nbkGPU\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\nnet\\predict.py:191: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "W0530 01:40:46.036719 127876 deprecation_wrapper.py:119] From C:\\Users\\pbs-mysorelab\\.conda\\envs\\nbkGPU\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\nnet\\predict.py:192: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W0530 01:40:46.155978 127876 deprecation_wrapper.py:119] From C:\\Users\\pbs-mysorelab\\.conda\\envs\\nbkGPU\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\nnet\\predict.py:194: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "W0530 01:40:47.451187 127876 deprecation.py:323] From C:\\Users\\pbs-mysorelab\\.conda\\envs\\nbkGPU\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to analyze %  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_1.avi\n",
      "Loading  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_1.avi\n",
      "Duration of video [s]:  1.67 , recorded with  30.0 fps!\n",
      "Overall # of frames:  50  found with (before cropping) frame dimensions:  768 768\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [00:08,  9.14it/s]                                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [00:08,  7.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips...\n",
      "Starting to analyze %  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_10.avi\n",
      "Loading  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_10.avi\n",
      "Duration of video [s]:  1.4 , recorded with  30.0 fps!\n",
      "Overall # of frames:  42  found with (before cropping) frame dimensions:  768 768\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 30.87it/s]                                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 29.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips...\n",
      "Starting to analyze %  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_100.avi\n",
      "Loading  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_100.avi\n",
      "Duration of video [s]:  1.57 , recorded with  30.0 fps!\n",
      "Overall # of frames:  47  found with (before cropping) frame dimensions:  768 768\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 30.15it/s]                                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 27.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips...\n",
      "Starting to analyze %  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_101.avi\n",
      "Loading  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_101.avi\n",
      "Duration of video [s]:  1.47 , recorded with  30.0 fps!\n",
      "Overall # of frames:  44  found with (before cropping) frame dimensions:  768 768\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 30.50it/s]                                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 28.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips...\n",
      "Starting to analyze %  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_102.avi\n",
      "Loading  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_102.avi\n",
      "Duration of video [s]:  1.43 , recorded with  30.0 fps!\n",
      "Overall # of frames:  43  found with (before cropping) frame dimensions:  768 768\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 30.81it/s]                                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 29.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips...\n",
      "Starting to analyze %  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_103.avi\n",
      "Loading  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_103.avi\n",
      "Duration of video [s]:  2.5 , recorded with  30.0 fps!\n",
      "Overall # of frames:  75  found with (before cropping) frame dimensions:  768 768\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:02, 32.29it/s]                                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:02, 27.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips...\n",
      "Starting to analyze %  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_104.avi\n",
      "Loading  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_104.avi\n",
      "Duration of video [s]:  1.43 , recorded with  30.0 fps!\n",
      "Overall # of frames:  43  found with (before cropping) frame dimensions:  768 768\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 30.94it/s]                                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 29.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips...\n",
      "Starting to analyze %  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_105.avi\n",
      "Loading  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_105.avi\n",
      "Duration of video [s]:  1.43 , recorded with  30.0 fps!\n",
      "Overall # of frames:  43  found with (before cropping) frame dimensions:  768 768\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 30.52it/s]                                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 28.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips...\n",
      "Starting to analyze %  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_106.avi\n",
      "Loading  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_106.avi\n",
      "Duration of video [s]:  1.43 , recorded with  30.0 fps!\n",
      "Overall # of frames:  43  found with (before cropping) frame dimensions:  768 768\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 30.83it/s]                                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 28.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips...\n",
      "Starting to analyze %  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_107.avi\n",
      "Loading  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_107.avi\n",
      "Duration of video [s]:  1.47 , recorded with  30.0 fps!\n",
      "Overall # of frames:  44  found with (before cropping) frame dimensions:  768 768\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 30.40it/s]                                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 28.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips...\n",
      "Starting to analyze %  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_108.avi\n",
      "Loading  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_108.avi\n",
      "Duration of video [s]:  1.53 , recorded with  30.0 fps!\n",
      "Overall # of frames:  46  found with (before cropping) frame dimensions:  768 768\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 31.04it/s]                                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 28.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips...\n",
      "Starting to analyze %  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_109.avi\n",
      "Loading  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_109.avi\n",
      "Duration of video [s]:  7.73 , recorded with  30.0 fps!\n",
      "Overall # of frames:  232  found with (before cropping) frame dimensions:  768 768\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "240it [00:08, 29.78it/s]                                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "240it [00:08, 28.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips...\n",
      "Starting to analyze %  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_11.avi\n",
      "Loading  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_11.avi\n",
      "Duration of video [s]:  1.53 , recorded with  30.0 fps!\n",
      "Overall # of frames:  46  found with (before cropping) frame dimensions:  768 768\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 30.63it/s]                                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 28.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips...\n",
      "Starting to analyze %  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_110.avi\n",
      "Loading  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_110.avi\n",
      "Duration of video [s]:  5.43 , recorded with  30.0 fps!\n",
      "Overall # of frames:  163  found with (before cropping) frame dimensions:  768 768\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "170it [00:05, 25.39it/s]                                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "170it [00:06, 28.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips...\n",
      "Starting to analyze %  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_111.avi\n",
      "Loading  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_111.avi\n",
      "Duration of video [s]:  1.47 , recorded with  30.0 fps!\n",
      "Overall # of frames:  44  found with (before cropping) frame dimensions:  768 768\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 30.30it/s]                                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 28.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips...\n",
      "Starting to analyze %  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_112.avi\n",
      "Loading  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_112.avi\n",
      "Duration of video [s]:  1.53 , recorded with  30.0 fps!\n",
      "Overall # of frames:  46  found with (before cropping) frame dimensions:  768 768\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 30.54it/s]                                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 28.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips...\n",
      "Starting to analyze %  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_113.avi\n",
      "Loading  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_113.avi\n",
      "Duration of video [s]:  1.5 , recorded with  30.0 fps!\n",
      "Overall # of frames:  45  found with (before cropping) frame dimensions:  768 768\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 30.48it/s]                                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 28.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips...\n",
      "Starting to analyze %  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_114.avi\n",
      "Loading  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_114.avi\n",
      "Duration of video [s]:  1.53 , recorded with  30.0 fps!\n",
      "Overall # of frames:  46  found with (before cropping) frame dimensions:  768 768\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 30.77it/s]                                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 28.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips...\n",
      "Starting to analyze %  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_115.avi\n",
      "Loading  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_115.avi\n",
      "Duration of video [s]:  1.43 , recorded with  30.0 fps!\n",
      "Overall # of frames:  43  found with (before cropping) frame dimensions:  768 768\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 30.56it/s]                                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 28.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips...\n",
      "Starting to analyze %  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_116.avi\n",
      "Loading  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_116.avi\n",
      "Duration of video [s]:  1.53 , recorded with  30.0 fps!\n",
      "Overall # of frames:  46  found with (before cropping) frame dimensions:  768 768\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 30.45it/s]                                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 28.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips...\n",
      "Starting to analyze %  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_117.avi\n",
      "Loading  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_117.avi\n",
      "Duration of video [s]:  1.43 , recorded with  30.0 fps!\n",
      "Overall # of frames:  43  found with (before cropping) frame dimensions:  768 768\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 31.08it/s]                                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 29.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips...\n",
      "Starting to analyze %  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_118.avi\n",
      "Loading  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_118.avi\n",
      "Duration of video [s]:  1.43 , recorded with  30.0 fps!\n",
      "Overall # of frames:  43  found with (before cropping) frame dimensions:  768 768\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 30.34it/s]                                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 28.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips...\n",
      "Starting to analyze %  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_119.avi\n",
      "Loading  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_119.avi\n",
      "Duration of video [s]:  1.33 , recorded with  30.0 fps!\n",
      "Overall # of frames:  40  found with (before cropping) frame dimensions:  768 768\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 30.82it/s]                                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 34.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips...\n",
      "Starting to analyze %  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_12.avi\n",
      "Loading  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_12.avi\n",
      "Duration of video [s]:  1.47 , recorded with  30.0 fps!\n",
      "Overall # of frames:  44  found with (before cropping) frame dimensions:  768 768\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 30.37it/s]                                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 28.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips...\n",
      "Starting to analyze %  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_120.avi\n",
      "Loading  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_120.avi\n",
      "Duration of video [s]:  1.5 , recorded with  30.0 fps!\n",
      "Overall # of frames:  45  found with (before cropping) frame dimensions:  768 768\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 31.16it/s]                                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 28.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips...\n",
      "Starting to analyze %  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_121.avi\n",
      "Loading  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_121.avi\n",
      "Duration of video [s]:  1.4 , recorded with  30.0 fps!\n",
      "Overall # of frames:  42  found with (before cropping) frame dimensions:  768 768\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 30.64it/s]                                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 28.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips...\n",
      "Starting to analyze %  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_122.avi\n",
      "Loading  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_122.avi\n",
      "Duration of video [s]:  1.9 , recorded with  30.0 fps!\n",
      "Overall # of frames:  57  found with (before cropping) frame dimensions:  768 768\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [00:01, 31.40it/s]                                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [00:02, 25.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips...\n",
      "Starting to analyze %  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_123.avi\n",
      "Loading  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_123.avi\n",
      "Duration of video [s]:  1.4 , recorded with  30.0 fps!\n",
      "Overall # of frames:  42  found with (before cropping) frame dimensions:  768 768\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 30.73it/s]                                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 28.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips...\n",
      "Starting to analyze %  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_124.avi\n",
      "Loading  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_124.avi\n",
      "Duration of video [s]:  3.37 , recorded with  30.0 fps!\n",
      "Overall # of frames:  101  found with (before cropping) frame dimensions:  768 768\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [00:03, 29.89it/s]                                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [00:03, 29.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips...\n",
      "Starting to analyze %  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_125.avi\n",
      "Loading  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_125.avi\n",
      "Duration of video [s]:  1.47 , recorded with  30.0 fps!\n",
      "Overall # of frames:  44  found with (before cropping) frame dimensions:  768 768\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 30.37it/s]                                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 28.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips...\n",
      "Starting to analyze %  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_126.avi\n",
      "Loading  F:\\MysoreData\\nbk\\mouse_videos\\mitg05\\05252020\\cam2\\clips\\mitg05-533--05252020095948.avi_clip_126.avi\n",
      "Duration of video [s]:  2.8 , recorded with  30.0 fps!\n",
      "Overall # of frames:  84  found with (before cropping) frame dimensions:  768 768\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|                                                    | 30/84 [00:00<00:01, 49.50it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "VIDEO_SOURCE = 'F:\\\\MysoreData\\\\nbk\\\\mouse_videos\\\\mitg05\\\\05252020\\\\cam2\\\\clips'\n",
    "video_all = os.listdir(VIDEO_SOURCE)\n",
    "video = []\n",
    "for i in range(0,len(video_all)):\n",
    "    video.append(VIDEO_SOURCE + '\\\\' + video_all[i])\n",
    "    \n",
    "deeplabcut.analyze_videos(path_config_file,video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pCrUvQIvoEKD"
   },
   "source": [
    "## Create labeled video\n",
    "This funtion is for visualiztion purpose and can be used to create a video in .mp4 format with labels predicted by the network. This video is saved in the same directory where the original video resides. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6aDF7Q7KoEKE",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "deeplabcut.create_labeled_video(path_config_file,video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8GTiuJESoEKH"
   },
   "source": [
    "## Plot the trajectories of the analyzed videos\n",
    "This function plots the trajectories of all the body parts across the entire video. Each body part is identified by a unique color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gX21zZbXoEKJ",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook \n",
    "#for making interactive plots.\n",
    "#deeplabcut.plot_trajectories(path_config_file,videofile_path, plotting=True)\n",
    "\n",
    "deeplabcut.plot_trajectories(path_config_file,video,showfigures=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Colab_TrainNetwork_VideoAnalysis.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
