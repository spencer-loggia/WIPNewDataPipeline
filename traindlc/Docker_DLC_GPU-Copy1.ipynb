{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Using Labeled Data to Train a Network, Use The Network to Label New Videos, and Create Trajectories </h1> \n",
    "<p> Use another notebook create a project, extract frames, and label training/testing data </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RK255E7YoEIt"
   },
   "source": [
    "# DLC with docker\n",
    "This notebook illustrates how to use the Docker container to:\n",
    "- train a network\n",
    "- evaluate a network\n",
    "- analyze a novel video\n",
    "\n",
    "many of the functions have additional parameters / complexity, see the DLC docs for more inf on each.\n",
    "\n",
    "This assumes you already have a project folder with labeled data! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "txoddlM8hLKm"
   },
   "source": [
    "## info about the Docker Environment:\n",
    "Docker is essentially a better alternative to virtual machines. It is able to containerize applications in a way that keeps them seperate the OS and other software. \n",
    "\n",
    "For tensorflow / deeplab cut, docker is critical for two reasons. \n",
    "- It 'claims' the GPU, making it closed for use by other processes.\n",
    "- It queries the GPU properly, in some cases tf (and the python kernal itself) does not know how to properly communincate with the GPU on windows systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4C5WRoS9g5Od",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2017 NVIDIA Corporation\n",
      "Built on Fri_Sep__1_21:08:32_Central_Daylight_Time_2017\n",
      "Cuda compilation tools, release 9.0, V9.0.176\n"
     ]
    }
   ],
   "source": [
    "# make sure you graphic driver is accessable\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HxVNyimFp-PJ",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the tensorflow version\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> The following is very important as it allow the current GPU process to grow dynamically </h1>\n",
    "Without this option tf will likely run out of VRAM when trying to update the weight tensor. In theory, these options could cause the GPU to run out of memory entirely, but there is no other way to allow training to complete successfully. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#allow video memory growth as network expands to avoid convolutional network errors\n",
    "TF_FORCE_GPU_ALLOW_GROWTH = True\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pm_PC1Q8lRrH",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 13537146552544263055,\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 6586089472\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 17967924156679516769\n",
       " physical_device_desc: \"device: 0, name: GeForce RTX 2080, pci bus id: 0000:41:00.0, compute capability: 7.5\",\n",
       " name: \"/device:GPU:1\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 6586089472\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 13782940445016201425\n",
       " physical_device_desc: \"device: 1, name: GeForce RTX 2080, pci bus id: 0000:81:00.0, compute capability: 7.5\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's make sure we see a GPU:\n",
    "#tf.test.gpu_device_name()\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start here for training DeepLabCut and analyzing new videos!\n",
    "<p><br>If the first imports fail, there is again - sadly - an issue with you enviroment. Make sure all packages beside DLC are installed via conda. </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sXufoX6INe6w",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#GUIs don't work on in Docker (or the cloud), so label your data locally on your computer! \n",
    "#This notebook is for you to train and run video analysis!\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3K9Ndy1beyfG",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.10.1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we are ready to train!\n",
    "#should see version 2.0.8\n",
    "import deeplabcut\n",
    "deeplabcut.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> change to your path: </h1>\n",
    "<p> this should be the same path as the one in the createDLCproject notebook. The path is the path to the config.yaml file, not the project directory itself </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z7ZlDr3wV4D1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_config_file = r'F:\\MysoreData\\nbk\\mouseVideoAnalysis\\Box1\\cam1\\box1_cam1-spencerloggia-2021-03-18\\config.yaml'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xNi9s1dboEJN"
   },
   "source": [
    "## Create a training dataset\n",
    "This function generates the training data information for DeepCut (which requires a mat file) based on the pandas dataframes that hold label information. The user can set the fraction of the training set size (from all labeled image in the hd5 file) in the config.yaml file. While creating the dataset, the user can create multiple shuffles. \n",
    "\n",
    "After running this script the training dataset is created and saved in the project directory under the subdirectory **'training-datasets'**\n",
    "\n",
    "This function also creates new subdirectories under **dlc-models** and appends the project config.yaml file with the correct path to the training and testing pose configuration file. These files hold the parameters for training the network. Such an example file is provided with the toolbox and named as **pose_cfg.yaml**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\MysoreData\\nbk\\mouseVideoAnalysis\\Box1\\cam1\\box1_cam1-spencerloggia-2021-03-18\\training-datasets\\iteration-0\\UnaugmentedDataSet_box1_cam1Mar18  already exists!\n",
      "failed on F:\\MysoreData\\nbk\\mouseVideoAnalysis\\Box1\\cam1\\box1_cam1-spencerloggia-2021-03-18\\labeled-data\\camera-1_clip-7\\CollectedData_spencerloggia.h5\n",
      "failed on F:\\MysoreData\\nbk\\mouseVideoAnalysis\\Box1\\cam1\\box1_cam1-spencerloggia-2021-03-18\\labeled-data\\camera-1_clip-57\\CollectedData_spencerloggia.h5\n",
      "failed on F:\\MysoreData\\nbk\\mouseVideoAnalysis\\Box1\\cam1\\box1_cam1-spencerloggia-2021-03-18\\labeled-data\\camera-1_clip-135\\CollectedData_spencerloggia.h5\n",
      "F:\\MysoreData\\nbk\\mouseVideoAnalysis\\Box1\\cam1\\box1_cam1-spencerloggia-2021-03-18\\dlc-models\\iteration-0\\box1_cam1Mar18-trainset95shuffle1  already exists!\n",
      "F:\\MysoreData\\nbk\\mouseVideoAnalysis\\Box1\\cam1\\box1_cam1-spencerloggia-2021-03-18\\dlc-models\\iteration-0\\box1_cam1Mar18-trainset95shuffle1/train  already exists!\n",
      "F:\\MysoreData\\nbk\\mouseVideoAnalysis\\Box1\\cam1\\box1_cam1-spencerloggia-2021-03-18\\dlc-models\\iteration-0\\box1_cam1Mar18-trainset95shuffle1/test  already exists!\n",
      "The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.95,\n",
       "  1,\n",
       "  (array([ 65, 123, 172,  85, 147,  50,  41,  52, 167,  81, 129, 191, 130,\n",
       "          132, 127, 159,  54,  44,  59, 150,  10, 201, 165,  99, 105, 182,\n",
       "           63, 103,  93, 175, 134,   3, 109,  15,  37,  27, 139, 108,  35,\n",
       "          157, 146, 169,  76,  22,  61, 102,  16, 200, 152, 133,  45,  29,\n",
       "          192,  32, 112,  69, 120,  70, 164, 153, 135,  75,   6,  71,  90,\n",
       "          100,   7, 181, 111,  97, 197,  64,   4, 114,  38, 184,  21, 180,\n",
       "           87, 106,  19,  80, 124,   9,  62, 162, 185, 115, 117, 158,  25,\n",
       "            1, 195,  51, 125,  83,  13,  36,  96,  82,  55,  20, 190, 193,\n",
       "          122, 204,  67,  34, 189,  11,  28,  66, 203,   5,  98,  48, 149,\n",
       "          186, 187, 174, 171,  91,  78, 179,  89, 183, 126,  12, 170, 202,\n",
       "          173,  72, 154, 131,  68, 163,  24, 161,  73, 188, 138, 110,  86,\n",
       "          121,  31,  40,  79, 143,  33,  17, 156, 136, 141, 116,   8,  47,\n",
       "          151,  26, 144,  56, 113,  84, 178,  94,  58, 196, 166, 140,  23,\n",
       "           74,   2, 101,  92, 104, 199,  53, 160, 107, 155,  39, 176,  14,\n",
       "          148,  60,  88, 119, 142,  43, 145,  49,  30,  95,  18,  42]),\n",
       "   array([ 46,   0, 194, 198,  77, 168, 177,  57, 118, 137, 128])))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deeplabcut.create_training_dataset(path_config_file, net_type='resnet_50', augmenter_type='imgaug')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now go edit the pose_cfg.yaml to make display_iters: low (i.e. 10), and save_iters: 500\n",
    "\n",
    "Now it is the time to start training the network!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c4FczXGDoEJU"
   },
   "source": [
    "## Start training\n",
    "This function trains the network for a specific shuffle of the training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_pOvDq_2oEJW",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "{'all_joints': [[0], [1], [2]],\n",
      " 'all_joints_names': ['bodypart1', 'bodypart2', 'bodypart3'],\n",
      " 'alpha_r': 0.02,\n",
      " 'batch_size': 1,\n",
      " 'crop_pad': 0,\n",
      " 'cropratio': 0.4,\n",
      " 'dataset': 'training-datasets\\\\iteration-0\\\\UnaugmentedDataSet_box1_cam1Mar18\\\\box1_cam1_spencerloggia95shuffle1.mat',\n",
      " 'dataset_type': 'imgaug',\n",
      " 'decay_steps': 30000,\n",
      " 'deterministic': False,\n",
      " 'display_iters': 1000,\n",
      " 'fg_fraction': 0.25,\n",
      " 'global_scale': 0.8,\n",
      " 'init_weights': 'f:\\\\mysoredata\\\\nbk\\\\mousevideoanalysis\\\\dlc_env_conda\\\\lib\\\\site-packages\\\\deeplabcut\\\\pose_estimation_tensorflow\\\\models\\\\pretrained\\\\resnet_v1_50.ckpt',\n",
      " 'intermediate_supervision': False,\n",
      " 'intermediate_supervision_layer': 12,\n",
      " 'location_refinement': True,\n",
      " 'locref_huber_loss': True,\n",
      " 'locref_loss_weight': 0.05,\n",
      " 'locref_stdev': 7.2801,\n",
      " 'log_dir': 'log',\n",
      " 'lr_init': 0.0005,\n",
      " 'max_input_size': 1500,\n",
      " 'mean_pixel': [123.68, 116.779, 103.939],\n",
      " 'metadataset': 'training-datasets\\\\iteration-0\\\\UnaugmentedDataSet_box1_cam1Mar18\\\\Documentation_data-box1_cam1_95shuffle1.pickle',\n",
      " 'min_input_size': 64,\n",
      " 'mirror': False,\n",
      " 'multi_step': [[0.005, 10000],\n",
      "                [0.02, 430000],\n",
      "                [0.002, 730000],\n",
      "                [0.001, 1030000]],\n",
      " 'net_type': 'resnet_50',\n",
      " 'num_joints': 3,\n",
      " 'optimizer': 'sgd',\n",
      " 'pairwise_huber_loss': False,\n",
      " 'pairwise_predict': False,\n",
      " 'partaffinityfield_predict': False,\n",
      " 'pos_dist_thresh': 17,\n",
      " 'project_path': 'F:\\\\MysoreData\\\\nbk\\\\mouseVideoAnalysis\\\\Box1\\\\cam1\\\\box1_cam1-spencerloggia-2021-03-18',\n",
      " 'regularize': False,\n",
      " 'rotation': 25,\n",
      " 'rotratio': 0.4,\n",
      " 'save_iters': 50000,\n",
      " 'scale_jitter_lo': 0.5,\n",
      " 'scale_jitter_up': 1.25,\n",
      " 'scoremap_dir': 'test',\n",
      " 'shuffle': True,\n",
      " 'snapshot_prefix': 'F:\\\\MysoreData\\\\nbk\\\\mouseVideoAnalysis\\\\Box1\\\\cam1\\\\box1_cam1-spencerloggia-2021-03-18\\\\dlc-models\\\\iteration-0\\\\box1_cam1Mar18-trainset95shuffle1\\\\train\\\\snapshot',\n",
      " 'stride': 8.0,\n",
      " 'weigh_negatives': False,\n",
      " 'weigh_only_present_joints': False,\n",
      " 'weigh_part_predictions': False,\n",
      " 'weight_decay': 0.0001}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting single-animal trainer\n",
      "Starting with imgaug pose-dataset loader (=default).\n",
      "Batch Size is 1\n",
      "Initializing ResNet\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000211E79C2470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000211E79C2470>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002124B6CEB00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002124B6CEB00>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x00000211E79C2DD8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x00000211E79C2DD8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000211E7A2D4A8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000211E7A2D4A8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002124B6CE748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002124B6CE748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000211E7A2DF98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000211E7A2DF98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002124B7B3400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002124B7B3400>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000021249109710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000021249109710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002124B7D5CC0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002124B7D5CC0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002124B7D5CC0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002124B7D5CC0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000211E7A3BAC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000211E7A3BAC8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000211E7A2D4A8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000211E7A2D4A8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002124B7D5080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002124B7D5080>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000211E7B9A588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000211E7B9A588>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002124BB5B748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002124BB5B748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000211E7B83470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000211E7B83470>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002124BB11208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002124BB11208>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x00000211E7BEEB00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x00000211E7BEEB00>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002124BA74518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002124BA74518>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000211E7CFE8D0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000211E7CFE8D0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000211E7A3BC88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000211E7A3BC88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000211E79CB4A8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000211E79CB4A8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000211E7DA4898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000211E7DA4898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000211E7AC37B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000211E7AC37B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000211E7BBE400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000211E7BBE400>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000211E7AC3240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000211E7AC3240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000211E7CFE898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000211E7CFE898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000211E7AC3240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000211E7AC3240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000211E7BBE400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000211E7BBE400>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002124C59AF28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002124C59AF28>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000211E7E0D400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000211E7E0D400>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000211E7BBE9B0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000211E7BBE9B0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000211E79CBA58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000211E79CBA58>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000211E7BBE0F0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000211E7BBE0F0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002124C5D2CF8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002124C5D2CF8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000211E7D49198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000211E7D49198>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000211E7D49198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000211E7D49198>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000211E7D493C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000211E7D493C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000211E7B50860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000211E7B50860>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000211E7E48128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000211E7E48128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002124F98FDD8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002124F98FDD8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212530C15F8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212530C15F8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002124F98F7B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002124F98F7B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000211E7E48400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000211E7E48400>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x00000212519CCB70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x00000212519CCB70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212519CC278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212519CC278>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002124C6040F0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002124C6040F0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212530F90F0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212530F90F0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002124C64F940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002124C64F940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002124F96BC88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002124F96BC88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002124C5F1710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002124C5F1710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212530D1AC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212530D1AC8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002124C5F13C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002124C5F13C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212532E1E48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212532E1E48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002124C5F1CF8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002124C5F1CF8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002125313E5F8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002125313E5F8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002124FA2A160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002124FA2A160>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212532E1CC0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212532E1CC0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212530D1240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212530D1240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002125335D6D8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002125335D6D8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212530D1278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212530D1278>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000021253441080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000021253441080>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212531D2B38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212531D2B38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212533A5D30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212533A5D30>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212532168D0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212532168D0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002125335D6D8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002125335D6D8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021253216C18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021253216C18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002125352A4E0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002125352A4E0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021253216710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021253216710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212533D2BA8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212533D2BA8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002125331B780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002125331B780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002125355A9B0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002125355A9B0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212533A5DD8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212533A5DD8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000021253617BE0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000021253617BE0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021253412EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021253412EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000021253412EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000021253412EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002125345DDA0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002125345DDA0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002125352AC50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002125352AC50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212537EC160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212537EC160>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002125389DE48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002125389DE48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002125395EEF0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002125395EEF0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002125382FB70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002125382FB70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212535988D0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212535988D0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000021253412B38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000021253412B38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021253598D68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021253598D68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000021253952550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000021253952550>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021253922D30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021253922D30>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212539529B0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212539529B0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021253598518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021253598518>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212539529B0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212539529B0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212537ECE48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212537ECE48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000021253952550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000021253952550>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021253BD27F0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021253BD27F0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000021253AABE80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000021253AABE80>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000211E79EB9B0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000211E79EB9B0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212539C2C88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212539C2C88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021253D186A0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021253D186A0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000021253D26518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000021253D26518>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021253DD6D68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021253DD6D68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000021253D26BA8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000021253D26BA8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021253B28278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021253B28278>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000021253D186A0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000021253D186A0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021253F18240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021253F18240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000021253D182B0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000021253D182B0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212537669B0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212537669B0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212539FFE48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212539FFE48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021253766AC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021253766AC8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000021253D186A0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000021253D186A0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021253C0FA90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021253C0FA90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x0000021253C977F0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x0000021253C977F0>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x000002124C4E2DD8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x000002124C4E2DD8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "Loading ImageNet-pretrained resnet_50\n",
      "Display_iters overwritten as 10\n",
      "Save_iters overwritten as 1000\n",
      "Training parameter:\n",
      "{'stride': 8.0, 'weigh_part_predictions': False, 'weigh_negatives': False, 'fg_fraction': 0.25, 'mean_pixel': [123.68, 116.779, 103.939], 'shuffle': True, 'snapshot_prefix': 'F:\\\\MysoreData\\\\nbk\\\\mouseVideoAnalysis\\\\Box1\\\\cam1\\\\box1_cam1-spencerloggia-2021-03-18\\\\dlc-models\\\\iteration-0\\\\box1_cam1Mar18-trainset95shuffle1\\\\train\\\\snapshot', 'log_dir': 'log', 'global_scale': 0.8, 'location_refinement': True, 'locref_stdev': 7.2801, 'locref_loss_weight': 0.05, 'locref_huber_loss': True, 'optimizer': 'sgd', 'intermediate_supervision': False, 'intermediate_supervision_layer': 12, 'regularize': False, 'weight_decay': 0.0001, 'crop_pad': 0, 'scoremap_dir': 'test', 'batch_size': 1, 'dataset_type': 'imgaug', 'deterministic': False, 'mirror': False, 'pairwise_huber_loss': False, 'weigh_only_present_joints': False, 'partaffinityfield_predict': False, 'pairwise_predict': False, 'all_joints': [[0], [1], [2]], 'all_joints_names': ['bodypart1', 'bodypart2', 'bodypart3'], 'alpha_r': 0.02, 'cropratio': 0.4, 'dataset': 'training-datasets\\\\iteration-0\\\\UnaugmentedDataSet_box1_cam1Mar18\\\\box1_cam1_spencerloggia95shuffle1.mat', 'decay_steps': 30000, 'display_iters': 1000, 'init_weights': 'f:\\\\mysoredata\\\\nbk\\\\mousevideoanalysis\\\\dlc_env_conda\\\\lib\\\\site-packages\\\\deeplabcut\\\\pose_estimation_tensorflow\\\\models\\\\pretrained\\\\resnet_v1_50.ckpt', 'lr_init': 0.0005, 'max_input_size': 1500, 'metadataset': 'training-datasets\\\\iteration-0\\\\UnaugmentedDataSet_box1_cam1Mar18\\\\Documentation_data-box1_cam1_95shuffle1.pickle', 'min_input_size': 64, 'multi_step': [[0.005, 10000], [0.02, 430000], [0.002, 730000], [0.001, 1030000]], 'net_type': 'resnet_50', 'num_joints': 3, 'pos_dist_thresh': 17, 'project_path': 'F:\\\\MysoreData\\\\nbk\\\\mouseVideoAnalysis\\\\Box1\\\\cam1\\\\box1_cam1-spencerloggia-2021-03-18', 'rotation': 25, 'rotratio': 0.4, 'save_iters': 50000, 'scale_jitter_lo': 0.5, 'scale_jitter_up': 1.25, 'covering': True, 'elastic_transform': True, 'motion_blur': True, 'motion_blur_params': {'k': 7, 'angle': [-90, 90]}}\n",
      "Starting training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 10 loss: 0.2430 lr: 0.005\n",
      "iteration: 20 loss: 0.0564 lr: 0.005\n",
      "iteration: 30 loss: 0.0407 lr: 0.005\n",
      "iteration: 40 loss: 0.0330 lr: 0.005\n",
      "iteration: 50 loss: 0.0390 lr: 0.005\n",
      "iteration: 60 loss: 0.0361 lr: 0.005\n",
      "iteration: 70 loss: 0.0277 lr: 0.005\n",
      "iteration: 80 loss: 0.0390 lr: 0.005\n",
      "iteration: 90 loss: 0.0259 lr: 0.005\n",
      "iteration: 100 loss: 0.0232 lr: 0.005\n",
      "iteration: 110 loss: 0.0315 lr: 0.005\n",
      "iteration: 120 loss: 0.0224 lr: 0.005\n",
      "iteration: 130 loss: 0.0240 lr: 0.005\n",
      "iteration: 140 loss: 0.0208 lr: 0.005\n",
      "iteration: 150 loss: 0.0240 lr: 0.005\n",
      "iteration: 160 loss: 0.0204 lr: 0.005\n",
      "iteration: 170 loss: 0.0270 lr: 0.005\n",
      "iteration: 180 loss: 0.0206 lr: 0.005\n",
      "iteration: 190 loss: 0.0192 lr: 0.005\n",
      "iteration: 200 loss: 0.0203 lr: 0.005\n",
      "iteration: 210 loss: 0.0251 lr: 0.005\n",
      "iteration: 220 loss: 0.0227 lr: 0.005\n",
      "iteration: 230 loss: 0.0202 lr: 0.005\n",
      "iteration: 240 loss: 0.0187 lr: 0.005\n",
      "iteration: 250 loss: 0.0206 lr: 0.005\n",
      "iteration: 260 loss: 0.0208 lr: 0.005\n",
      "iteration: 270 loss: 0.0200 lr: 0.005\n",
      "iteration: 280 loss: 0.0249 lr: 0.005\n",
      "iteration: 290 loss: 0.0247 lr: 0.005\n",
      "iteration: 300 loss: 0.0262 lr: 0.005\n",
      "iteration: 310 loss: 0.0243 lr: 0.005\n",
      "iteration: 320 loss: 0.0190 lr: 0.005\n",
      "iteration: 330 loss: 0.0242 lr: 0.005\n",
      "iteration: 340 loss: 0.0206 lr: 0.005\n",
      "iteration: 350 loss: 0.0250 lr: 0.005\n",
      "iteration: 360 loss: 0.0178 lr: 0.005\n",
      "iteration: 370 loss: 0.0156 lr: 0.005\n",
      "iteration: 380 loss: 0.0281 lr: 0.005\n",
      "iteration: 390 loss: 0.0223 lr: 0.005\n",
      "iteration: 400 loss: 0.0230 lr: 0.005\n",
      "iteration: 410 loss: 0.0274 lr: 0.005\n",
      "iteration: 420 loss: 0.0221 lr: 0.005\n",
      "iteration: 430 loss: 0.0222 lr: 0.005\n",
      "iteration: 440 loss: 0.0190 lr: 0.005\n",
      "iteration: 450 loss: 0.0198 lr: 0.005\n",
      "iteration: 460 loss: 0.0241 lr: 0.005\n",
      "iteration: 470 loss: 0.0199 lr: 0.005\n",
      "iteration: 480 loss: 0.0227 lr: 0.005\n",
      "iteration: 490 loss: 0.0192 lr: 0.005\n",
      "iteration: 500 loss: 0.0220 lr: 0.005\n",
      "iteration: 510 loss: 0.0183 lr: 0.005\n",
      "iteration: 520 loss: 0.0173 lr: 0.005\n",
      "iteration: 530 loss: 0.0244 lr: 0.005\n",
      "iteration: 540 loss: 0.0179 lr: 0.005\n",
      "iteration: 550 loss: 0.0214 lr: 0.005\n",
      "iteration: 560 loss: 0.0228 lr: 0.005\n",
      "iteration: 570 loss: 0.0192 lr: 0.005\n",
      "iteration: 580 loss: 0.0206 lr: 0.005\n",
      "iteration: 590 loss: 0.0208 lr: 0.005\n",
      "iteration: 600 loss: 0.0176 lr: 0.005\n",
      "iteration: 610 loss: 0.0165 lr: 0.005\n",
      "iteration: 620 loss: 0.0220 lr: 0.005\n",
      "iteration: 630 loss: 0.0204 lr: 0.005\n",
      "iteration: 640 loss: 0.0217 lr: 0.005\n",
      "iteration: 650 loss: 0.0167 lr: 0.005\n",
      "iteration: 660 loss: 0.0227 lr: 0.005\n",
      "iteration: 670 loss: 0.0185 lr: 0.005\n",
      "iteration: 680 loss: 0.0174 lr: 0.005\n",
      "iteration: 690 loss: 0.0232 lr: 0.005\n",
      "iteration: 700 loss: 0.0182 lr: 0.005\n",
      "iteration: 710 loss: 0.0159 lr: 0.005\n",
      "iteration: 720 loss: 0.0162 lr: 0.005\n",
      "iteration: 730 loss: 0.0207 lr: 0.005\n",
      "iteration: 740 loss: 0.0280 lr: 0.005\n",
      "iteration: 750 loss: 0.0174 lr: 0.005\n",
      "iteration: 760 loss: 0.0234 lr: 0.005\n",
      "iteration: 770 loss: 0.0307 lr: 0.005\n",
      "iteration: 780 loss: 0.0191 lr: 0.005\n",
      "iteration: 790 loss: 0.0165 lr: 0.005\n",
      "iteration: 800 loss: 0.0168 lr: 0.005\n",
      "iteration: 810 loss: 0.0147 lr: 0.005\n",
      "iteration: 820 loss: 0.0215 lr: 0.005\n",
      "iteration: 830 loss: 0.0159 lr: 0.005\n",
      "iteration: 840 loss: 0.0162 lr: 0.005\n",
      "iteration: 850 loss: 0.0206 lr: 0.005\n",
      "iteration: 860 loss: 0.0157 lr: 0.005\n",
      "iteration: 870 loss: 0.0210 lr: 0.005\n",
      "iteration: 880 loss: 0.0191 lr: 0.005\n",
      "iteration: 890 loss: 0.0156 lr: 0.005\n",
      "iteration: 900 loss: 0.0207 lr: 0.005\n",
      "iteration: 910 loss: 0.0199 lr: 0.005\n",
      "iteration: 920 loss: 0.0183 lr: 0.005\n",
      "iteration: 930 loss: 0.0185 lr: 0.005\n",
      "iteration: 940 loss: 0.0200 lr: 0.005\n",
      "iteration: 950 loss: 0.0198 lr: 0.005\n",
      "iteration: 960 loss: 0.0202 lr: 0.005\n",
      "iteration: 970 loss: 0.0158 lr: 0.005\n",
      "iteration: 980 loss: 0.0156 lr: 0.005\n",
      "iteration: 990 loss: 0.0194 lr: 0.005\n",
      "iteration: 1000 loss: 0.0177 lr: 0.005\n",
      "iteration: 1010 loss: 0.0168 lr: 0.005\n",
      "iteration: 1020 loss: 0.0188 lr: 0.005\n",
      "iteration: 1030 loss: 0.0196 lr: 0.005\n",
      "iteration: 1040 loss: 0.0183 lr: 0.005\n",
      "iteration: 1050 loss: 0.0198 lr: 0.005\n",
      "iteration: 1060 loss: 0.0176 lr: 0.005\n",
      "iteration: 1070 loss: 0.0135 lr: 0.005\n",
      "iteration: 1080 loss: 0.0183 lr: 0.005\n",
      "iteration: 1090 loss: 0.0127 lr: 0.005\n",
      "iteration: 1100 loss: 0.0191 lr: 0.005\n",
      "iteration: 1110 loss: 0.0156 lr: 0.005\n",
      "iteration: 1120 loss: 0.0152 lr: 0.005\n",
      "iteration: 1130 loss: 0.0140 lr: 0.005\n",
      "iteration: 1140 loss: 0.0146 lr: 0.005\n",
      "iteration: 1150 loss: 0.0210 lr: 0.005\n",
      "iteration: 1160 loss: 0.0156 lr: 0.005\n",
      "iteration: 1170 loss: 0.0153 lr: 0.005\n",
      "iteration: 1180 loss: 0.0167 lr: 0.005\n",
      "iteration: 1190 loss: 0.0191 lr: 0.005\n",
      "iteration: 1200 loss: 0.0180 lr: 0.005\n",
      "iteration: 1210 loss: 0.0203 lr: 0.005\n",
      "iteration: 1220 loss: 0.0189 lr: 0.005\n",
      "iteration: 1230 loss: 0.0142 lr: 0.005\n",
      "iteration: 1240 loss: 0.0189 lr: 0.005\n",
      "iteration: 1250 loss: 0.0103 lr: 0.005\n",
      "iteration: 1260 loss: 0.0144 lr: 0.005\n",
      "iteration: 1270 loss: 0.0188 lr: 0.005\n",
      "iteration: 1280 loss: 0.0187 lr: 0.005\n",
      "iteration: 1290 loss: 0.0175 lr: 0.005\n",
      "iteration: 1300 loss: 0.0138 lr: 0.005\n",
      "iteration: 1310 loss: 0.0154 lr: 0.005\n",
      "iteration: 1320 loss: 0.0206 lr: 0.005\n",
      "iteration: 1330 loss: 0.0170 lr: 0.005\n",
      "iteration: 1340 loss: 0.0183 lr: 0.005\n",
      "iteration: 1350 loss: 0.0167 lr: 0.005\n",
      "iteration: 1360 loss: 0.0137 lr: 0.005\n",
      "iteration: 1370 loss: 0.0132 lr: 0.005\n",
      "iteration: 1380 loss: 0.0145 lr: 0.005\n",
      "iteration: 1390 loss: 0.0160 lr: 0.005\n",
      "iteration: 1400 loss: 0.0165 lr: 0.005\n",
      "iteration: 1410 loss: 0.0129 lr: 0.005\n",
      "iteration: 1420 loss: 0.0179 lr: 0.005\n",
      "iteration: 1430 loss: 0.0204 lr: 0.005\n",
      "iteration: 1440 loss: 0.0165 lr: 0.005\n",
      "iteration: 1450 loss: 0.0166 lr: 0.005\n",
      "iteration: 1460 loss: 0.0159 lr: 0.005\n",
      "iteration: 1470 loss: 0.0142 lr: 0.005\n",
      "iteration: 1480 loss: 0.0103 lr: 0.005\n",
      "iteration: 1490 loss: 0.0238 lr: 0.005\n",
      "iteration: 1500 loss: 0.0160 lr: 0.005\n",
      "iteration: 1510 loss: 0.0203 lr: 0.005\n",
      "iteration: 1520 loss: 0.0169 lr: 0.005\n",
      "iteration: 1530 loss: 0.0165 lr: 0.005\n",
      "iteration: 1540 loss: 0.0165 lr: 0.005\n",
      "iteration: 1550 loss: 0.0158 lr: 0.005\n",
      "iteration: 1560 loss: 0.0180 lr: 0.005\n",
      "iteration: 1570 loss: 0.0167 lr: 0.005\n",
      "iteration: 1580 loss: 0.0143 lr: 0.005\n",
      "iteration: 1590 loss: 0.0146 lr: 0.005\n",
      "iteration: 1600 loss: 0.0220 lr: 0.005\n",
      "iteration: 1610 loss: 0.0164 lr: 0.005\n",
      "iteration: 1620 loss: 0.0153 lr: 0.005\n",
      "iteration: 1630 loss: 0.0181 lr: 0.005\n",
      "iteration: 1640 loss: 0.0152 lr: 0.005\n",
      "iteration: 1650 loss: 0.0140 lr: 0.005\n",
      "iteration: 1660 loss: 0.0147 lr: 0.005\n",
      "iteration: 1670 loss: 0.0146 lr: 0.005\n",
      "iteration: 1680 loss: 0.0199 lr: 0.005\n",
      "iteration: 1690 loss: 0.0199 lr: 0.005\n",
      "iteration: 1700 loss: 0.0208 lr: 0.005\n",
      "iteration: 1710 loss: 0.0115 lr: 0.005\n",
      "iteration: 1720 loss: 0.0151 lr: 0.005\n",
      "iteration: 1730 loss: 0.0183 lr: 0.005\n",
      "iteration: 1740 loss: 0.0122 lr: 0.005\n",
      "iteration: 1750 loss: 0.0128 lr: 0.005\n",
      "iteration: 1760 loss: 0.0198 lr: 0.005\n",
      "iteration: 1770 loss: 0.0148 lr: 0.005\n",
      "iteration: 1780 loss: 0.0128 lr: 0.005\n",
      "iteration: 1790 loss: 0.0116 lr: 0.005\n",
      "iteration: 1800 loss: 0.0162 lr: 0.005\n",
      "iteration: 1810 loss: 0.0235 lr: 0.005\n",
      "iteration: 1820 loss: 0.0195 lr: 0.005\n",
      "iteration: 1830 loss: 0.0139 lr: 0.005\n",
      "iteration: 1840 loss: 0.0141 lr: 0.005\n",
      "iteration: 1850 loss: 0.0176 lr: 0.005\n",
      "iteration: 1860 loss: 0.0127 lr: 0.005\n",
      "iteration: 1870 loss: 0.0200 lr: 0.005\n",
      "iteration: 1880 loss: 0.0158 lr: 0.005\n",
      "iteration: 1890 loss: 0.0132 lr: 0.005\n",
      "iteration: 1900 loss: 0.0152 lr: 0.005\n",
      "iteration: 1910 loss: 0.0131 lr: 0.005\n",
      "iteration: 1920 loss: 0.0151 lr: 0.005\n",
      "iteration: 1930 loss: 0.0187 lr: 0.005\n",
      "iteration: 1940 loss: 0.0150 lr: 0.005\n",
      "iteration: 1950 loss: 0.0118 lr: 0.005\n",
      "iteration: 1960 loss: 0.0148 lr: 0.005\n",
      "iteration: 1970 loss: 0.0155 lr: 0.005\n",
      "iteration: 1980 loss: 0.0116 lr: 0.005\n",
      "iteration: 1990 loss: 0.0101 lr: 0.005\n",
      "iteration: 2000 loss: 0.0136 lr: 0.005\n",
      "iteration: 2010 loss: 0.0148 lr: 0.005\n",
      "iteration: 2020 loss: 0.0164 lr: 0.005\n",
      "iteration: 2030 loss: 0.0176 lr: 0.005\n",
      "iteration: 2040 loss: 0.0177 lr: 0.005\n",
      "iteration: 2050 loss: 0.0153 lr: 0.005\n",
      "iteration: 2060 loss: 0.0122 lr: 0.005\n",
      "iteration: 2070 loss: 0.0172 lr: 0.005\n",
      "iteration: 2080 loss: 0.0153 lr: 0.005\n",
      "iteration: 2090 loss: 0.0160 lr: 0.005\n",
      "iteration: 2100 loss: 0.0148 lr: 0.005\n",
      "iteration: 2110 loss: 0.0154 lr: 0.005\n",
      "iteration: 2120 loss: 0.0146 lr: 0.005\n",
      "iteration: 2130 loss: 0.0169 lr: 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 2140 loss: 0.0139 lr: 0.005\n",
      "iteration: 2150 loss: 0.0150 lr: 0.005\n",
      "iteration: 2160 loss: 0.0138 lr: 0.005\n",
      "iteration: 2170 loss: 0.0137 lr: 0.005\n",
      "iteration: 2180 loss: 0.0155 lr: 0.005\n",
      "iteration: 2190 loss: 0.0125 lr: 0.005\n",
      "iteration: 2200 loss: 0.0160 lr: 0.005\n",
      "iteration: 2210 loss: 0.0170 lr: 0.005\n",
      "iteration: 2220 loss: 0.0140 lr: 0.005\n",
      "iteration: 2230 loss: 0.0156 lr: 0.005\n",
      "iteration: 2240 loss: 0.0120 lr: 0.005\n",
      "iteration: 2250 loss: 0.0142 lr: 0.005\n",
      "iteration: 2260 loss: 0.0187 lr: 0.005\n",
      "iteration: 2270 loss: 0.0105 lr: 0.005\n",
      "iteration: 2280 loss: 0.0131 lr: 0.005\n",
      "iteration: 2290 loss: 0.0142 lr: 0.005\n",
      "iteration: 2300 loss: 0.0131 lr: 0.005\n",
      "iteration: 2310 loss: 0.0111 lr: 0.005\n",
      "iteration: 2320 loss: 0.0128 lr: 0.005\n",
      "iteration: 2330 loss: 0.0147 lr: 0.005\n",
      "iteration: 2340 loss: 0.0136 lr: 0.005\n",
      "iteration: 2350 loss: 0.0124 lr: 0.005\n",
      "iteration: 2360 loss: 0.0129 lr: 0.005\n",
      "iteration: 2370 loss: 0.0178 lr: 0.005\n",
      "iteration: 2380 loss: 0.0152 lr: 0.005\n",
      "iteration: 2390 loss: 0.0219 lr: 0.005\n",
      "iteration: 2400 loss: 0.0125 lr: 0.005\n",
      "iteration: 2410 loss: 0.0129 lr: 0.005\n",
      "iteration: 2420 loss: 0.0215 lr: 0.005\n",
      "iteration: 2430 loss: 0.0159 lr: 0.005\n",
      "iteration: 2440 loss: 0.0126 lr: 0.005\n",
      "iteration: 2450 loss: 0.0192 lr: 0.005\n",
      "iteration: 2460 loss: 0.0160 lr: 0.005\n",
      "iteration: 2470 loss: 0.0100 lr: 0.005\n",
      "iteration: 2480 loss: 0.0125 lr: 0.005\n",
      "iteration: 2490 loss: 0.0186 lr: 0.005\n",
      "iteration: 2500 loss: 0.0151 lr: 0.005\n",
      "iteration: 2510 loss: 0.0126 lr: 0.005\n",
      "iteration: 2520 loss: 0.0175 lr: 0.005\n",
      "iteration: 2530 loss: 0.0131 lr: 0.005\n",
      "iteration: 2540 loss: 0.0163 lr: 0.005\n",
      "iteration: 2550 loss: 0.0132 lr: 0.005\n",
      "iteration: 2560 loss: 0.0128 lr: 0.005\n",
      "iteration: 2570 loss: 0.0187 lr: 0.005\n",
      "iteration: 2580 loss: 0.0133 lr: 0.005\n",
      "iteration: 2590 loss: 0.0123 lr: 0.005\n",
      "iteration: 2600 loss: 0.0163 lr: 0.005\n",
      "iteration: 2610 loss: 0.0110 lr: 0.005\n",
      "iteration: 2620 loss: 0.0096 lr: 0.005\n",
      "iteration: 2630 loss: 0.0115 lr: 0.005\n",
      "iteration: 2640 loss: 0.0161 lr: 0.005\n",
      "iteration: 2650 loss: 0.0116 lr: 0.005\n",
      "iteration: 2660 loss: 0.0174 lr: 0.005\n",
      "iteration: 2670 loss: 0.0140 lr: 0.005\n",
      "iteration: 2680 loss: 0.0129 lr: 0.005\n",
      "iteration: 2690 loss: 0.0118 lr: 0.005\n",
      "iteration: 2700 loss: 0.0126 lr: 0.005\n",
      "iteration: 2710 loss: 0.0138 lr: 0.005\n",
      "iteration: 2720 loss: 0.0151 lr: 0.005\n",
      "iteration: 2730 loss: 0.0150 lr: 0.005\n",
      "iteration: 2740 loss: 0.0138 lr: 0.005\n",
      "iteration: 2750 loss: 0.0138 lr: 0.005\n",
      "iteration: 2760 loss: 0.0132 lr: 0.005\n",
      "iteration: 2770 loss: 0.0116 lr: 0.005\n",
      "iteration: 2780 loss: 0.0140 lr: 0.005\n",
      "iteration: 2790 loss: 0.0119 lr: 0.005\n",
      "iteration: 2800 loss: 0.0117 lr: 0.005\n",
      "iteration: 2810 loss: 0.0136 lr: 0.005\n",
      "iteration: 2820 loss: 0.0156 lr: 0.005\n",
      "iteration: 2830 loss: 0.0139 lr: 0.005\n",
      "iteration: 2840 loss: 0.0178 lr: 0.005\n",
      "iteration: 2850 loss: 0.0111 lr: 0.005\n",
      "iteration: 2860 loss: 0.0136 lr: 0.005\n",
      "iteration: 2870 loss: 0.0126 lr: 0.005\n",
      "iteration: 2880 loss: 0.0156 lr: 0.005\n",
      "iteration: 2890 loss: 0.0150 lr: 0.005\n",
      "iteration: 2900 loss: 0.0140 lr: 0.005\n",
      "iteration: 2910 loss: 0.0141 lr: 0.005\n",
      "iteration: 2920 loss: 0.0180 lr: 0.005\n",
      "iteration: 2930 loss: 0.0122 lr: 0.005\n",
      "iteration: 2940 loss: 0.0141 lr: 0.005\n",
      "iteration: 2950 loss: 0.0144 lr: 0.005\n",
      "iteration: 2960 loss: 0.0148 lr: 0.005\n",
      "iteration: 2970 loss: 0.0146 lr: 0.005\n",
      "iteration: 2980 loss: 0.0189 lr: 0.005\n",
      "iteration: 2990 loss: 0.0100 lr: 0.005\n",
      "iteration: 3000 loss: 0.0161 lr: 0.005\n",
      "iteration: 3010 loss: 0.0101 lr: 0.005\n",
      "iteration: 3020 loss: 0.0128 lr: 0.005\n",
      "iteration: 3030 loss: 0.0142 lr: 0.005\n",
      "iteration: 3040 loss: 0.0140 lr: 0.005\n",
      "iteration: 3050 loss: 0.0142 lr: 0.005\n",
      "iteration: 3060 loss: 0.0085 lr: 0.005\n",
      "iteration: 3070 loss: 0.0153 lr: 0.005\n",
      "iteration: 3080 loss: 0.0146 lr: 0.005\n",
      "iteration: 3090 loss: 0.0141 lr: 0.005\n",
      "iteration: 3100 loss: 0.0124 lr: 0.005\n",
      "iteration: 3110 loss: 0.0190 lr: 0.005\n",
      "iteration: 3120 loss: 0.0167 lr: 0.005\n",
      "iteration: 3130 loss: 0.0168 lr: 0.005\n",
      "iteration: 3140 loss: 0.0118 lr: 0.005\n",
      "iteration: 3150 loss: 0.0184 lr: 0.005\n",
      "iteration: 3160 loss: 0.0160 lr: 0.005\n",
      "iteration: 3170 loss: 0.0109 lr: 0.005\n",
      "iteration: 3180 loss: 0.0112 lr: 0.005\n",
      "iteration: 3190 loss: 0.0200 lr: 0.005\n",
      "iteration: 3200 loss: 0.0116 lr: 0.005\n",
      "iteration: 3210 loss: 0.0174 lr: 0.005\n",
      "iteration: 3220 loss: 0.0148 lr: 0.005\n",
      "iteration: 3230 loss: 0.0120 lr: 0.005\n",
      "iteration: 3240 loss: 0.0121 lr: 0.005\n",
      "iteration: 3250 loss: 0.0152 lr: 0.005\n",
      "iteration: 3260 loss: 0.0148 lr: 0.005\n",
      "iteration: 3270 loss: 0.0121 lr: 0.005\n",
      "iteration: 3280 loss: 0.0114 lr: 0.005\n",
      "iteration: 3290 loss: 0.0113 lr: 0.005\n",
      "iteration: 3300 loss: 0.0146 lr: 0.005\n",
      "iteration: 3310 loss: 0.0133 lr: 0.005\n",
      "iteration: 3320 loss: 0.0156 lr: 0.005\n",
      "iteration: 3330 loss: 0.0130 lr: 0.005\n",
      "iteration: 3340 loss: 0.0118 lr: 0.005\n",
      "iteration: 3350 loss: 0.0122 lr: 0.005\n",
      "iteration: 3360 loss: 0.0154 lr: 0.005\n",
      "iteration: 3370 loss: 0.0181 lr: 0.005\n",
      "iteration: 3380 loss: 0.0157 lr: 0.005\n",
      "iteration: 3390 loss: 0.0148 lr: 0.005\n",
      "iteration: 3400 loss: 0.0140 lr: 0.005\n",
      "iteration: 3410 loss: 0.0098 lr: 0.005\n",
      "iteration: 3420 loss: 0.0149 lr: 0.005\n",
      "iteration: 3430 loss: 0.0155 lr: 0.005\n",
      "iteration: 3440 loss: 0.0176 lr: 0.005\n",
      "iteration: 3450 loss: 0.0155 lr: 0.005\n",
      "iteration: 3460 loss: 0.0111 lr: 0.005\n",
      "iteration: 3470 loss: 0.0113 lr: 0.005\n",
      "iteration: 3480 loss: 0.0126 lr: 0.005\n",
      "iteration: 3490 loss: 0.0173 lr: 0.005\n",
      "iteration: 3500 loss: 0.0132 lr: 0.005\n",
      "iteration: 3510 loss: 0.0118 lr: 0.005\n",
      "iteration: 3520 loss: 0.0135 lr: 0.005\n",
      "iteration: 3530 loss: 0.0141 lr: 0.005\n",
      "iteration: 3540 loss: 0.0144 lr: 0.005\n",
      "iteration: 3550 loss: 0.0171 lr: 0.005\n",
      "iteration: 3560 loss: 0.0135 lr: 0.005\n",
      "iteration: 3570 loss: 0.0177 lr: 0.005\n",
      "iteration: 3580 loss: 0.0143 lr: 0.005\n",
      "iteration: 3590 loss: 0.0101 lr: 0.005\n",
      "iteration: 3600 loss: 0.0108 lr: 0.005\n",
      "iteration: 3610 loss: 0.0101 lr: 0.005\n",
      "iteration: 3620 loss: 0.0126 lr: 0.005\n",
      "iteration: 3630 loss: 0.0165 lr: 0.005\n",
      "iteration: 3640 loss: 0.0122 lr: 0.005\n",
      "iteration: 3650 loss: 0.0143 lr: 0.005\n",
      "iteration: 3660 loss: 0.0131 lr: 0.005\n",
      "iteration: 3670 loss: 0.0134 lr: 0.005\n",
      "iteration: 3680 loss: 0.0104 lr: 0.005\n",
      "iteration: 3690 loss: 0.0151 lr: 0.005\n",
      "iteration: 3700 loss: 0.0152 lr: 0.005\n",
      "iteration: 3710 loss: 0.0116 lr: 0.005\n",
      "iteration: 3720 loss: 0.0133 lr: 0.005\n",
      "iteration: 3730 loss: 0.0143 lr: 0.005\n",
      "iteration: 3740 loss: 0.0106 lr: 0.005\n",
      "iteration: 3750 loss: 0.0128 lr: 0.005\n",
      "iteration: 3760 loss: 0.0173 lr: 0.005\n",
      "iteration: 3770 loss: 0.0145 lr: 0.005\n",
      "iteration: 3780 loss: 0.0123 lr: 0.005\n",
      "iteration: 3790 loss: 0.0147 lr: 0.005\n",
      "iteration: 3800 loss: 0.0131 lr: 0.005\n",
      "iteration: 3810 loss: 0.0191 lr: 0.005\n",
      "iteration: 3820 loss: 0.0165 lr: 0.005\n",
      "iteration: 3830 loss: 0.0143 lr: 0.005\n",
      "iteration: 3840 loss: 0.0115 lr: 0.005\n",
      "iteration: 3850 loss: 0.0172 lr: 0.005\n",
      "iteration: 3860 loss: 0.0141 lr: 0.005\n",
      "iteration: 3870 loss: 0.0205 lr: 0.005\n",
      "iteration: 3880 loss: 0.0157 lr: 0.005\n",
      "iteration: 3890 loss: 0.0141 lr: 0.005\n",
      "iteration: 3900 loss: 0.0148 lr: 0.005\n",
      "iteration: 3910 loss: 0.0130 lr: 0.005\n",
      "iteration: 3920 loss: 0.0139 lr: 0.005\n",
      "iteration: 3930 loss: 0.0116 lr: 0.005\n",
      "iteration: 3940 loss: 0.0127 lr: 0.005\n",
      "iteration: 3950 loss: 0.0072 lr: 0.005\n",
      "iteration: 3960 loss: 0.0127 lr: 0.005\n",
      "iteration: 3970 loss: 0.0131 lr: 0.005\n",
      "iteration: 3980 loss: 0.0113 lr: 0.005\n",
      "iteration: 3990 loss: 0.0115 lr: 0.005\n",
      "iteration: 4000 loss: 0.0153 lr: 0.005\n",
      "iteration: 4010 loss: 0.0161 lr: 0.005\n",
      "iteration: 4020 loss: 0.0137 lr: 0.005\n",
      "iteration: 4030 loss: 0.0162 lr: 0.005\n",
      "iteration: 4040 loss: 0.0116 lr: 0.005\n",
      "iteration: 4050 loss: 0.0115 lr: 0.005\n",
      "iteration: 4060 loss: 0.0105 lr: 0.005\n",
      "iteration: 4070 loss: 0.0117 lr: 0.005\n",
      "iteration: 4080 loss: 0.0115 lr: 0.005\n",
      "iteration: 4090 loss: 0.0114 lr: 0.005\n",
      "iteration: 4100 loss: 0.0112 lr: 0.005\n",
      "iteration: 4110 loss: 0.0175 lr: 0.005\n",
      "iteration: 4120 loss: 0.0165 lr: 0.005\n",
      "iteration: 4130 loss: 0.0129 lr: 0.005\n",
      "iteration: 4140 loss: 0.0123 lr: 0.005\n",
      "iteration: 4150 loss: 0.0085 lr: 0.005\n",
      "iteration: 4160 loss: 0.0109 lr: 0.005\n",
      "iteration: 4170 loss: 0.0095 lr: 0.005\n",
      "iteration: 4180 loss: 0.0099 lr: 0.005\n",
      "iteration: 4190 loss: 0.0110 lr: 0.005\n",
      "iteration: 4200 loss: 0.0142 lr: 0.005\n",
      "iteration: 4210 loss: 0.0126 lr: 0.005\n",
      "iteration: 4220 loss: 0.0158 lr: 0.005\n",
      "iteration: 4230 loss: 0.0123 lr: 0.005\n",
      "iteration: 4240 loss: 0.0130 lr: 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 4250 loss: 0.0111 lr: 0.005\n",
      "iteration: 4260 loss: 0.0132 lr: 0.005\n",
      "iteration: 4270 loss: 0.0121 lr: 0.005\n",
      "iteration: 4280 loss: 0.0142 lr: 0.005\n",
      "iteration: 4290 loss: 0.0125 lr: 0.005\n",
      "iteration: 4300 loss: 0.0084 lr: 0.005\n",
      "iteration: 4310 loss: 0.0116 lr: 0.005\n",
      "iteration: 4320 loss: 0.0115 lr: 0.005\n",
      "iteration: 4330 loss: 0.0142 lr: 0.005\n",
      "iteration: 4340 loss: 0.0164 lr: 0.005\n",
      "iteration: 4350 loss: 0.0170 lr: 0.005\n",
      "iteration: 4360 loss: 0.0111 lr: 0.005\n",
      "iteration: 4370 loss: 0.0148 lr: 0.005\n",
      "iteration: 4380 loss: 0.0110 lr: 0.005\n",
      "iteration: 4390 loss: 0.0130 lr: 0.005\n",
      "iteration: 4400 loss: 0.0148 lr: 0.005\n",
      "iteration: 4410 loss: 0.0158 lr: 0.005\n",
      "iteration: 4420 loss: 0.0150 lr: 0.005\n",
      "iteration: 4430 loss: 0.0113 lr: 0.005\n",
      "iteration: 4440 loss: 0.0151 lr: 0.005\n",
      "iteration: 4450 loss: 0.0106 lr: 0.005\n",
      "iteration: 4460 loss: 0.0146 lr: 0.005\n",
      "iteration: 4470 loss: 0.0089 lr: 0.005\n",
      "iteration: 4480 loss: 0.0124 lr: 0.005\n",
      "iteration: 4490 loss: 0.0092 lr: 0.005\n",
      "iteration: 4500 loss: 0.0130 lr: 0.005\n",
      "iteration: 4510 loss: 0.0133 lr: 0.005\n",
      "iteration: 4520 loss: 0.0122 lr: 0.005\n",
      "iteration: 4530 loss: 0.0111 lr: 0.005\n",
      "iteration: 4540 loss: 0.0154 lr: 0.005\n",
      "iteration: 4550 loss: 0.0129 lr: 0.005\n",
      "iteration: 4560 loss: 0.0137 lr: 0.005\n",
      "iteration: 4570 loss: 0.0122 lr: 0.005\n",
      "iteration: 4580 loss: 0.0123 lr: 0.005\n",
      "iteration: 4590 loss: 0.0111 lr: 0.005\n",
      "iteration: 4600 loss: 0.0126 lr: 0.005\n",
      "iteration: 4610 loss: 0.0130 lr: 0.005\n",
      "iteration: 4620 loss: 0.0112 lr: 0.005\n",
      "iteration: 4630 loss: 0.0162 lr: 0.005\n",
      "iteration: 4640 loss: 0.0142 lr: 0.005\n",
      "iteration: 4650 loss: 0.0146 lr: 0.005\n",
      "iteration: 4660 loss: 0.0094 lr: 0.005\n",
      "iteration: 4670 loss: 0.0104 lr: 0.005\n",
      "iteration: 4680 loss: 0.0095 lr: 0.005\n",
      "iteration: 4690 loss: 0.0126 lr: 0.005\n",
      "iteration: 4700 loss: 0.0101 lr: 0.005\n",
      "iteration: 4710 loss: 0.0101 lr: 0.005\n",
      "iteration: 4720 loss: 0.0115 lr: 0.005\n",
      "iteration: 4730 loss: 0.0102 lr: 0.005\n",
      "iteration: 4740 loss: 0.0100 lr: 0.005\n",
      "iteration: 4750 loss: 0.0112 lr: 0.005\n",
      "iteration: 4760 loss: 0.0130 lr: 0.005\n",
      "iteration: 4770 loss: 0.0137 lr: 0.005\n",
      "iteration: 4780 loss: 0.0138 lr: 0.005\n",
      "iteration: 4790 loss: 0.0111 lr: 0.005\n",
      "iteration: 4800 loss: 0.0159 lr: 0.005\n",
      "iteration: 4810 loss: 0.0103 lr: 0.005\n",
      "iteration: 4820 loss: 0.0124 lr: 0.005\n",
      "iteration: 4830 loss: 0.0149 lr: 0.005\n",
      "iteration: 4840 loss: 0.0139 lr: 0.005\n",
      "iteration: 4850 loss: 0.0139 lr: 0.005\n",
      "iteration: 4860 loss: 0.0124 lr: 0.005\n",
      "iteration: 4870 loss: 0.0087 lr: 0.005\n",
      "iteration: 4880 loss: 0.0151 lr: 0.005\n",
      "iteration: 4890 loss: 0.0122 lr: 0.005\n",
      "iteration: 4900 loss: 0.0146 lr: 0.005\n",
      "iteration: 4910 loss: 0.0132 lr: 0.005\n",
      "iteration: 4920 loss: 0.0152 lr: 0.005\n",
      "iteration: 4930 loss: 0.0118 lr: 0.005\n",
      "iteration: 4940 loss: 0.0114 lr: 0.005\n",
      "iteration: 4950 loss: 0.0166 lr: 0.005\n",
      "iteration: 4960 loss: 0.0112 lr: 0.005\n",
      "iteration: 4970 loss: 0.0139 lr: 0.005\n",
      "iteration: 4980 loss: 0.0127 lr: 0.005\n",
      "iteration: 4990 loss: 0.0109 lr: 0.005\n",
      "iteration: 5000 loss: 0.0110 lr: 0.005\n",
      "iteration: 5010 loss: 0.0125 lr: 0.005\n",
      "iteration: 5020 loss: 0.0130 lr: 0.005\n",
      "iteration: 5030 loss: 0.0108 lr: 0.005\n",
      "iteration: 5040 loss: 0.0133 lr: 0.005\n",
      "iteration: 5050 loss: 0.0150 lr: 0.005\n",
      "iteration: 5060 loss: 0.0133 lr: 0.005\n",
      "iteration: 5070 loss: 0.0132 lr: 0.005\n",
      "iteration: 5080 loss: 0.0117 lr: 0.005\n",
      "iteration: 5090 loss: 0.0097 lr: 0.005\n",
      "iteration: 5100 loss: 0.0161 lr: 0.005\n",
      "iteration: 5110 loss: 0.0164 lr: 0.005\n",
      "iteration: 5120 loss: 0.0102 lr: 0.005\n",
      "iteration: 5130 loss: 0.0122 lr: 0.005\n",
      "iteration: 5140 loss: 0.0115 lr: 0.005\n",
      "iteration: 5150 loss: 0.0110 lr: 0.005\n",
      "iteration: 5160 loss: 0.0130 lr: 0.005\n",
      "iteration: 5170 loss: 0.0128 lr: 0.005\n",
      "iteration: 5180 loss: 0.0114 lr: 0.005\n",
      "iteration: 5190 loss: 0.0147 lr: 0.005\n",
      "iteration: 5200 loss: 0.0133 lr: 0.005\n",
      "iteration: 5210 loss: 0.0142 lr: 0.005\n",
      "iteration: 5220 loss: 0.0128 lr: 0.005\n",
      "iteration: 5230 loss: 0.0107 lr: 0.005\n",
      "iteration: 5240 loss: 0.0134 lr: 0.005\n",
      "iteration: 5250 loss: 0.0104 lr: 0.005\n",
      "iteration: 5260 loss: 0.0119 lr: 0.005\n",
      "iteration: 5270 loss: 0.0110 lr: 0.005\n",
      "iteration: 5280 loss: 0.0108 lr: 0.005\n",
      "iteration: 5290 loss: 0.0108 lr: 0.005\n",
      "iteration: 5300 loss: 0.0104 lr: 0.005\n",
      "iteration: 5310 loss: 0.0088 lr: 0.005\n",
      "iteration: 5320 loss: 0.0115 lr: 0.005\n",
      "iteration: 5330 loss: 0.0142 lr: 0.005\n",
      "iteration: 5340 loss: 0.0119 lr: 0.005\n",
      "iteration: 5350 loss: 0.0141 lr: 0.005\n",
      "iteration: 5360 loss: 0.0119 lr: 0.005\n",
      "iteration: 5370 loss: 0.0114 lr: 0.005\n",
      "iteration: 5380 loss: 0.0108 lr: 0.005\n",
      "iteration: 5390 loss: 0.0094 lr: 0.005\n",
      "iteration: 5400 loss: 0.0094 lr: 0.005\n",
      "iteration: 5410 loss: 0.0170 lr: 0.005\n",
      "iteration: 5420 loss: 0.0152 lr: 0.005\n",
      "iteration: 5430 loss: 0.0150 lr: 0.005\n",
      "iteration: 5440 loss: 0.0110 lr: 0.005\n",
      "iteration: 5450 loss: 0.0152 lr: 0.005\n",
      "iteration: 5460 loss: 0.0142 lr: 0.005\n",
      "iteration: 5470 loss: 0.0154 lr: 0.005\n",
      "iteration: 5480 loss: 0.0103 lr: 0.005\n",
      "iteration: 5490 loss: 0.0125 lr: 0.005\n",
      "iteration: 5500 loss: 0.0111 lr: 0.005\n",
      "iteration: 5510 loss: 0.0176 lr: 0.005\n",
      "iteration: 5520 loss: 0.0090 lr: 0.005\n",
      "iteration: 5530 loss: 0.0145 lr: 0.005\n",
      "iteration: 5540 loss: 0.0154 lr: 0.005\n",
      "iteration: 5550 loss: 0.0102 lr: 0.005\n",
      "iteration: 5560 loss: 0.0078 lr: 0.005\n",
      "iteration: 5570 loss: 0.0108 lr: 0.005\n",
      "iteration: 5580 loss: 0.0114 lr: 0.005\n",
      "iteration: 5590 loss: 0.0098 lr: 0.005\n",
      "iteration: 5600 loss: 0.0169 lr: 0.005\n",
      "iteration: 5610 loss: 0.0094 lr: 0.005\n",
      "iteration: 5620 loss: 0.0177 lr: 0.005\n",
      "iteration: 5630 loss: 0.0112 lr: 0.005\n",
      "iteration: 5640 loss: 0.0165 lr: 0.005\n",
      "iteration: 5650 loss: 0.0114 lr: 0.005\n",
      "iteration: 5660 loss: 0.0135 lr: 0.005\n",
      "iteration: 5670 loss: 0.0115 lr: 0.005\n",
      "iteration: 5680 loss: 0.0109 lr: 0.005\n",
      "iteration: 5690 loss: 0.0109 lr: 0.005\n",
      "iteration: 5700 loss: 0.0098 lr: 0.005\n",
      "iteration: 5710 loss: 0.0139 lr: 0.005\n",
      "iteration: 5720 loss: 0.0098 lr: 0.005\n",
      "iteration: 5730 loss: 0.0111 lr: 0.005\n",
      "iteration: 5740 loss: 0.0110 lr: 0.005\n",
      "iteration: 5750 loss: 0.0126 lr: 0.005\n",
      "iteration: 5760 loss: 0.0132 lr: 0.005\n",
      "iteration: 5770 loss: 0.0156 lr: 0.005\n",
      "iteration: 5780 loss: 0.0113 lr: 0.005\n",
      "iteration: 5790 loss: 0.0154 lr: 0.005\n",
      "iteration: 5800 loss: 0.0090 lr: 0.005\n",
      "iteration: 5810 loss: 0.0129 lr: 0.005\n",
      "iteration: 5820 loss: 0.0112 lr: 0.005\n",
      "iteration: 5830 loss: 0.0151 lr: 0.005\n",
      "iteration: 5840 loss: 0.0102 lr: 0.005\n",
      "iteration: 5850 loss: 0.0122 lr: 0.005\n",
      "iteration: 5860 loss: 0.0103 lr: 0.005\n",
      "iteration: 5870 loss: 0.0148 lr: 0.005\n",
      "iteration: 5880 loss: 0.0113 lr: 0.005\n",
      "iteration: 5890 loss: 0.0119 lr: 0.005\n",
      "iteration: 5900 loss: 0.0164 lr: 0.005\n",
      "iteration: 5910 loss: 0.0128 lr: 0.005\n",
      "iteration: 5920 loss: 0.0101 lr: 0.005\n",
      "iteration: 5930 loss: 0.0121 lr: 0.005\n",
      "iteration: 5940 loss: 0.0120 lr: 0.005\n",
      "iteration: 5950 loss: 0.0107 lr: 0.005\n",
      "iteration: 5960 loss: 0.0131 lr: 0.005\n",
      "iteration: 5970 loss: 0.0121 lr: 0.005\n",
      "iteration: 5980 loss: 0.0102 lr: 0.005\n",
      "iteration: 5990 loss: 0.0144 lr: 0.005\n",
      "iteration: 6000 loss: 0.0110 lr: 0.005\n",
      "iteration: 6010 loss: 0.0094 lr: 0.005\n",
      "iteration: 6020 loss: 0.0102 lr: 0.005\n",
      "iteration: 6030 loss: 0.0093 lr: 0.005\n",
      "iteration: 6040 loss: 0.0135 lr: 0.005\n",
      "iteration: 6050 loss: 0.0120 lr: 0.005\n",
      "iteration: 6060 loss: 0.0166 lr: 0.005\n",
      "iteration: 6070 loss: 0.0112 lr: 0.005\n",
      "iteration: 6080 loss: 0.0150 lr: 0.005\n",
      "iteration: 6090 loss: 0.0116 lr: 0.005\n",
      "iteration: 6100 loss: 0.0135 lr: 0.005\n",
      "iteration: 6110 loss: 0.0086 lr: 0.005\n",
      "iteration: 6120 loss: 0.0096 lr: 0.005\n",
      "iteration: 6130 loss: 0.0126 lr: 0.005\n",
      "iteration: 6140 loss: 0.0110 lr: 0.005\n",
      "iteration: 6150 loss: 0.0116 lr: 0.005\n",
      "iteration: 6160 loss: 0.0126 lr: 0.005\n",
      "iteration: 6170 loss: 0.0100 lr: 0.005\n",
      "iteration: 6180 loss: 0.0144 lr: 0.005\n",
      "iteration: 6190 loss: 0.0109 lr: 0.005\n",
      "iteration: 6200 loss: 0.0140 lr: 0.005\n",
      "iteration: 6210 loss: 0.0109 lr: 0.005\n",
      "iteration: 6220 loss: 0.0144 lr: 0.005\n",
      "iteration: 6230 loss: 0.0110 lr: 0.005\n",
      "iteration: 6240 loss: 0.0100 lr: 0.005\n",
      "iteration: 6250 loss: 0.0110 lr: 0.005\n",
      "iteration: 6260 loss: 0.0105 lr: 0.005\n",
      "iteration: 6270 loss: 0.0125 lr: 0.005\n",
      "iteration: 6280 loss: 0.0090 lr: 0.005\n",
      "iteration: 6290 loss: 0.0100 lr: 0.005\n",
      "iteration: 6300 loss: 0.0133 lr: 0.005\n",
      "iteration: 6310 loss: 0.0100 lr: 0.005\n",
      "iteration: 6320 loss: 0.0099 lr: 0.005\n",
      "iteration: 6330 loss: 0.0084 lr: 0.005\n",
      "iteration: 6340 loss: 0.0120 lr: 0.005\n",
      "iteration: 6350 loss: 0.0117 lr: 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 6360 loss: 0.0106 lr: 0.005\n",
      "iteration: 6370 loss: 0.0173 lr: 0.005\n",
      "iteration: 6380 loss: 0.0145 lr: 0.005\n",
      "iteration: 6390 loss: 0.0147 lr: 0.005\n",
      "iteration: 6400 loss: 0.0099 lr: 0.005\n",
      "iteration: 6410 loss: 0.0111 lr: 0.005\n",
      "iteration: 6420 loss: 0.0096 lr: 0.005\n",
      "iteration: 6430 loss: 0.0126 lr: 0.005\n",
      "iteration: 6440 loss: 0.0089 lr: 0.005\n",
      "iteration: 6450 loss: 0.0122 lr: 0.005\n",
      "iteration: 6460 loss: 0.0095 lr: 0.005\n",
      "iteration: 6470 loss: 0.0104 lr: 0.005\n",
      "iteration: 6480 loss: 0.0146 lr: 0.005\n",
      "iteration: 6490 loss: 0.0116 lr: 0.005\n",
      "iteration: 6500 loss: 0.0111 lr: 0.005\n",
      "iteration: 6510 loss: 0.0099 lr: 0.005\n",
      "iteration: 6520 loss: 0.0124 lr: 0.005\n",
      "iteration: 6530 loss: 0.0092 lr: 0.005\n",
      "iteration: 6540 loss: 0.0119 lr: 0.005\n",
      "iteration: 6550 loss: 0.0117 lr: 0.005\n",
      "iteration: 6560 loss: 0.0157 lr: 0.005\n",
      "iteration: 6570 loss: 0.0099 lr: 0.005\n",
      "iteration: 6580 loss: 0.0135 lr: 0.005\n",
      "iteration: 6590 loss: 0.0123 lr: 0.005\n",
      "iteration: 6600 loss: 0.0077 lr: 0.005\n",
      "iteration: 6610 loss: 0.0102 lr: 0.005\n",
      "iteration: 6620 loss: 0.0142 lr: 0.005\n",
      "iteration: 6630 loss: 0.0117 lr: 0.005\n",
      "iteration: 6640 loss: 0.0087 lr: 0.005\n",
      "iteration: 6650 loss: 0.0164 lr: 0.005\n",
      "iteration: 6660 loss: 0.0098 lr: 0.005\n",
      "iteration: 6670 loss: 0.0124 lr: 0.005\n",
      "iteration: 6680 loss: 0.0093 lr: 0.005\n",
      "iteration: 6690 loss: 0.0124 lr: 0.005\n",
      "iteration: 6700 loss: 0.0091 lr: 0.005\n",
      "iteration: 6710 loss: 0.0103 lr: 0.005\n",
      "iteration: 6720 loss: 0.0110 lr: 0.005\n",
      "iteration: 6730 loss: 0.0143 lr: 0.005\n",
      "iteration: 6740 loss: 0.0158 lr: 0.005\n",
      "iteration: 6750 loss: 0.0084 lr: 0.005\n",
      "iteration: 6760 loss: 0.0090 lr: 0.005\n",
      "iteration: 6770 loss: 0.0105 lr: 0.005\n",
      "iteration: 6780 loss: 0.0138 lr: 0.005\n",
      "iteration: 6790 loss: 0.0101 lr: 0.005\n",
      "iteration: 6800 loss: 0.0135 lr: 0.005\n",
      "iteration: 6810 loss: 0.0118 lr: 0.005\n",
      "iteration: 6820 loss: 0.0130 lr: 0.005\n",
      "iteration: 6830 loss: 0.0119 lr: 0.005\n",
      "iteration: 6840 loss: 0.0092 lr: 0.005\n",
      "iteration: 6850 loss: 0.0115 lr: 0.005\n",
      "iteration: 6860 loss: 0.0110 lr: 0.005\n",
      "iteration: 6870 loss: 0.0120 lr: 0.005\n",
      "iteration: 6880 loss: 0.0138 lr: 0.005\n",
      "iteration: 6890 loss: 0.0114 lr: 0.005\n",
      "iteration: 6900 loss: 0.0131 lr: 0.005\n",
      "iteration: 6910 loss: 0.0127 lr: 0.005\n",
      "iteration: 6920 loss: 0.0118 lr: 0.005\n",
      "iteration: 6930 loss: 0.0119 lr: 0.005\n",
      "iteration: 6940 loss: 0.0130 lr: 0.005\n",
      "iteration: 6950 loss: 0.0127 lr: 0.005\n",
      "iteration: 6960 loss: 0.0127 lr: 0.005\n",
      "iteration: 6970 loss: 0.0130 lr: 0.005\n",
      "iteration: 6980 loss: 0.0127 lr: 0.005\n",
      "iteration: 6990 loss: 0.0153 lr: 0.005\n",
      "iteration: 7000 loss: 0.0115 lr: 0.005\n",
      "iteration: 7010 loss: 0.0089 lr: 0.005\n",
      "iteration: 7020 loss: 0.0092 lr: 0.005\n",
      "iteration: 7030 loss: 0.0090 lr: 0.005\n",
      "iteration: 7040 loss: 0.0085 lr: 0.005\n",
      "iteration: 7050 loss: 0.0116 lr: 0.005\n",
      "iteration: 7060 loss: 0.0139 lr: 0.005\n",
      "iteration: 7070 loss: 0.0094 lr: 0.005\n",
      "iteration: 7080 loss: 0.0102 lr: 0.005\n",
      "iteration: 7090 loss: 0.0135 lr: 0.005\n",
      "iteration: 7100 loss: 0.0136 lr: 0.005\n",
      "iteration: 7110 loss: 0.0111 lr: 0.005\n",
      "iteration: 7120 loss: 0.0107 lr: 0.005\n",
      "iteration: 7130 loss: 0.0125 lr: 0.005\n",
      "iteration: 7140 loss: 0.0106 lr: 0.005\n",
      "iteration: 7150 loss: 0.0106 lr: 0.005\n",
      "iteration: 7160 loss: 0.0081 lr: 0.005\n",
      "iteration: 7170 loss: 0.0094 lr: 0.005\n",
      "iteration: 7180 loss: 0.0111 lr: 0.005\n",
      "iteration: 7190 loss: 0.0122 lr: 0.005\n",
      "iteration: 7200 loss: 0.0105 lr: 0.005\n",
      "iteration: 7210 loss: 0.0102 lr: 0.005\n",
      "iteration: 7220 loss: 0.0113 lr: 0.005\n",
      "iteration: 7230 loss: 0.0106 lr: 0.005\n",
      "iteration: 7240 loss: 0.0152 lr: 0.005\n",
      "iteration: 7250 loss: 0.0089 lr: 0.005\n",
      "iteration: 7260 loss: 0.0117 lr: 0.005\n",
      "iteration: 7270 loss: 0.0099 lr: 0.005\n",
      "iteration: 7280 loss: 0.0114 lr: 0.005\n",
      "iteration: 7290 loss: 0.0101 lr: 0.005\n",
      "iteration: 7300 loss: 0.0119 lr: 0.005\n",
      "iteration: 7310 loss: 0.0088 lr: 0.005\n",
      "iteration: 7320 loss: 0.0135 lr: 0.005\n",
      "iteration: 7330 loss: 0.0136 lr: 0.005\n",
      "iteration: 7340 loss: 0.0102 lr: 0.005\n",
      "iteration: 7350 loss: 0.0117 lr: 0.005\n",
      "iteration: 7360 loss: 0.0115 lr: 0.005\n",
      "iteration: 7370 loss: 0.0114 lr: 0.005\n",
      "iteration: 7380 loss: 0.0107 lr: 0.005\n",
      "iteration: 7390 loss: 0.0093 lr: 0.005\n",
      "iteration: 7400 loss: 0.0104 lr: 0.005\n",
      "iteration: 7410 loss: 0.0101 lr: 0.005\n",
      "iteration: 7420 loss: 0.0117 lr: 0.005\n",
      "iteration: 7430 loss: 0.0145 lr: 0.005\n",
      "iteration: 7440 loss: 0.0111 lr: 0.005\n",
      "iteration: 7450 loss: 0.0140 lr: 0.005\n",
      "iteration: 7460 loss: 0.0121 lr: 0.005\n",
      "iteration: 7470 loss: 0.0113 lr: 0.005\n",
      "iteration: 7480 loss: 0.0120 lr: 0.005\n",
      "iteration: 7490 loss: 0.0091 lr: 0.005\n",
      "iteration: 7500 loss: 0.0101 lr: 0.005\n",
      "iteration: 7510 loss: 0.0115 lr: 0.005\n",
      "iteration: 7520 loss: 0.0133 lr: 0.005\n",
      "iteration: 7530 loss: 0.0099 lr: 0.005\n",
      "iteration: 7540 loss: 0.0118 lr: 0.005\n",
      "iteration: 7550 loss: 0.0100 lr: 0.005\n",
      "iteration: 7560 loss: 0.0127 lr: 0.005\n",
      "iteration: 7570 loss: 0.0075 lr: 0.005\n",
      "iteration: 7580 loss: 0.0111 lr: 0.005\n",
      "iteration: 7590 loss: 0.0124 lr: 0.005\n",
      "iteration: 7600 loss: 0.0123 lr: 0.005\n",
      "iteration: 7610 loss: 0.0100 lr: 0.005\n",
      "iteration: 7620 loss: 0.0116 lr: 0.005\n",
      "iteration: 7630 loss: 0.0081 lr: 0.005\n",
      "iteration: 7640 loss: 0.0101 lr: 0.005\n",
      "iteration: 7650 loss: 0.0107 lr: 0.005\n",
      "iteration: 7660 loss: 0.0127 lr: 0.005\n",
      "iteration: 7670 loss: 0.0103 lr: 0.005\n",
      "iteration: 7680 loss: 0.0099 lr: 0.005\n",
      "iteration: 7690 loss: 0.0096 lr: 0.005\n",
      "iteration: 7700 loss: 0.0111 lr: 0.005\n",
      "iteration: 7710 loss: 0.0104 lr: 0.005\n",
      "iteration: 7720 loss: 0.0099 lr: 0.005\n",
      "iteration: 7730 loss: 0.0079 lr: 0.005\n",
      "iteration: 7740 loss: 0.0092 lr: 0.005\n",
      "iteration: 7750 loss: 0.0105 lr: 0.005\n",
      "iteration: 7760 loss: 0.0093 lr: 0.005\n",
      "iteration: 7770 loss: 0.0098 lr: 0.005\n",
      "iteration: 7780 loss: 0.0084 lr: 0.005\n",
      "iteration: 7790 loss: 0.0121 lr: 0.005\n",
      "iteration: 7800 loss: 0.0085 lr: 0.005\n",
      "iteration: 7810 loss: 0.0098 lr: 0.005\n",
      "iteration: 7820 loss: 0.0150 lr: 0.005\n",
      "iteration: 7830 loss: 0.0144 lr: 0.005\n",
      "iteration: 7840 loss: 0.0124 lr: 0.005\n",
      "iteration: 7850 loss: 0.0145 lr: 0.005\n",
      "iteration: 7860 loss: 0.0108 lr: 0.005\n",
      "iteration: 7870 loss: 0.0138 lr: 0.005\n",
      "iteration: 7880 loss: 0.0098 lr: 0.005\n",
      "iteration: 7890 loss: 0.0087 lr: 0.005\n",
      "iteration: 7900 loss: 0.0102 lr: 0.005\n",
      "iteration: 7910 loss: 0.0109 lr: 0.005\n",
      "iteration: 7920 loss: 0.0101 lr: 0.005\n",
      "iteration: 7930 loss: 0.0084 lr: 0.005\n",
      "iteration: 7940 loss: 0.0111 lr: 0.005\n",
      "iteration: 7950 loss: 0.0116 lr: 0.005\n",
      "iteration: 7960 loss: 0.0128 lr: 0.005\n",
      "iteration: 7970 loss: 0.0096 lr: 0.005\n",
      "iteration: 7980 loss: 0.0130 lr: 0.005\n",
      "iteration: 7990 loss: 0.0096 lr: 0.005\n",
      "iteration: 8000 loss: 0.0106 lr: 0.005\n",
      "iteration: 8010 loss: 0.0088 lr: 0.005\n",
      "iteration: 8020 loss: 0.0124 lr: 0.005\n",
      "iteration: 8030 loss: 0.0125 lr: 0.005\n",
      "iteration: 8040 loss: 0.0147 lr: 0.005\n",
      "iteration: 8050 loss: 0.0109 lr: 0.005\n",
      "iteration: 8060 loss: 0.0126 lr: 0.005\n",
      "iteration: 8070 loss: 0.0122 lr: 0.005\n",
      "iteration: 8080 loss: 0.0116 lr: 0.005\n",
      "iteration: 8090 loss: 0.0075 lr: 0.005\n",
      "iteration: 8100 loss: 0.0113 lr: 0.005\n",
      "iteration: 8110 loss: 0.0102 lr: 0.005\n",
      "iteration: 8120 loss: 0.0163 lr: 0.005\n",
      "iteration: 8130 loss: 0.0104 lr: 0.005\n",
      "iteration: 8140 loss: 0.0108 lr: 0.005\n",
      "iteration: 8150 loss: 0.0120 lr: 0.005\n",
      "iteration: 8160 loss: 0.0136 lr: 0.005\n",
      "iteration: 8170 loss: 0.0099 lr: 0.005\n",
      "iteration: 8180 loss: 0.0068 lr: 0.005\n",
      "iteration: 8190 loss: 0.0112 lr: 0.005\n",
      "iteration: 8200 loss: 0.0123 lr: 0.005\n",
      "iteration: 8210 loss: 0.0102 lr: 0.005\n",
      "iteration: 8220 loss: 0.0096 lr: 0.005\n",
      "iteration: 8230 loss: 0.0098 lr: 0.005\n",
      "iteration: 8240 loss: 0.0134 lr: 0.005\n",
      "iteration: 8250 loss: 0.0112 lr: 0.005\n",
      "iteration: 8260 loss: 0.0128 lr: 0.005\n",
      "iteration: 8270 loss: 0.0121 lr: 0.005\n",
      "iteration: 8280 loss: 0.0096 lr: 0.005\n",
      "iteration: 8290 loss: 0.0127 lr: 0.005\n",
      "iteration: 8300 loss: 0.0114 lr: 0.005\n",
      "iteration: 8310 loss: 0.0095 lr: 0.005\n",
      "iteration: 8320 loss: 0.0118 lr: 0.005\n",
      "iteration: 8330 loss: 0.0093 lr: 0.005\n",
      "iteration: 8340 loss: 0.0131 lr: 0.005\n",
      "iteration: 8350 loss: 0.0104 lr: 0.005\n",
      "iteration: 8360 loss: 0.0091 lr: 0.005\n",
      "iteration: 8370 loss: 0.0081 lr: 0.005\n",
      "iteration: 8380 loss: 0.0098 lr: 0.005\n",
      "iteration: 8390 loss: 0.0101 lr: 0.005\n",
      "iteration: 8400 loss: 0.0097 lr: 0.005\n",
      "iteration: 8410 loss: 0.0107 lr: 0.005\n",
      "iteration: 8420 loss: 0.0130 lr: 0.005\n",
      "iteration: 8430 loss: 0.0118 lr: 0.005\n",
      "iteration: 8440 loss: 0.0111 lr: 0.005\n",
      "iteration: 8450 loss: 0.0116 lr: 0.005\n",
      "iteration: 8460 loss: 0.0111 lr: 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 8470 loss: 0.0105 lr: 0.005\n",
      "iteration: 8480 loss: 0.0106 lr: 0.005\n",
      "iteration: 8490 loss: 0.0133 lr: 0.005\n",
      "iteration: 8500 loss: 0.0071 lr: 0.005\n",
      "iteration: 8510 loss: 0.0122 lr: 0.005\n",
      "iteration: 8520 loss: 0.0116 lr: 0.005\n",
      "iteration: 8530 loss: 0.0112 lr: 0.005\n",
      "iteration: 8540 loss: 0.0122 lr: 0.005\n",
      "iteration: 8550 loss: 0.0100 lr: 0.005\n",
      "iteration: 8560 loss: 0.0080 lr: 0.005\n",
      "iteration: 8570 loss: 0.0080 lr: 0.005\n",
      "iteration: 8580 loss: 0.0096 lr: 0.005\n",
      "iteration: 8590 loss: 0.0113 lr: 0.005\n",
      "iteration: 8600 loss: 0.0087 lr: 0.005\n",
      "iteration: 8610 loss: 0.0132 lr: 0.005\n",
      "iteration: 8620 loss: 0.0101 lr: 0.005\n",
      "iteration: 8630 loss: 0.0106 lr: 0.005\n",
      "iteration: 8640 loss: 0.0098 lr: 0.005\n",
      "iteration: 8650 loss: 0.0116 lr: 0.005\n",
      "iteration: 8660 loss: 0.0090 lr: 0.005\n",
      "iteration: 8670 loss: 0.0122 lr: 0.005\n",
      "iteration: 8680 loss: 0.0084 lr: 0.005\n",
      "iteration: 8690 loss: 0.0077 lr: 0.005\n",
      "iteration: 8700 loss: 0.0105 lr: 0.005\n",
      "iteration: 8710 loss: 0.0119 lr: 0.005\n",
      "iteration: 8720 loss: 0.0105 lr: 0.005\n",
      "iteration: 8730 loss: 0.0121 lr: 0.005\n",
      "iteration: 8740 loss: 0.0129 lr: 0.005\n",
      "iteration: 8750 loss: 0.0113 lr: 0.005\n",
      "iteration: 8760 loss: 0.0080 lr: 0.005\n",
      "iteration: 8770 loss: 0.0098 lr: 0.005\n",
      "iteration: 8780 loss: 0.0093 lr: 0.005\n",
      "iteration: 8790 loss: 0.0115 lr: 0.005\n",
      "iteration: 8800 loss: 0.0107 lr: 0.005\n",
      "iteration: 8810 loss: 0.0096 lr: 0.005\n",
      "iteration: 8820 loss: 0.0086 lr: 0.005\n",
      "iteration: 8830 loss: 0.0099 lr: 0.005\n",
      "iteration: 8840 loss: 0.0139 lr: 0.005\n",
      "iteration: 8850 loss: 0.0124 lr: 0.005\n",
      "iteration: 8860 loss: 0.0104 lr: 0.005\n",
      "iteration: 8870 loss: 0.0122 lr: 0.005\n",
      "iteration: 8880 loss: 0.0116 lr: 0.005\n",
      "iteration: 8890 loss: 0.0134 lr: 0.005\n",
      "iteration: 8900 loss: 0.0115 lr: 0.005\n",
      "iteration: 8910 loss: 0.0108 lr: 0.005\n",
      "iteration: 8920 loss: 0.0075 lr: 0.005\n",
      "iteration: 8930 loss: 0.0076 lr: 0.005\n",
      "iteration: 8940 loss: 0.0121 lr: 0.005\n",
      "iteration: 8950 loss: 0.0122 lr: 0.005\n",
      "iteration: 8960 loss: 0.0104 lr: 0.005\n",
      "iteration: 8970 loss: 0.0115 lr: 0.005\n",
      "iteration: 8980 loss: 0.0091 lr: 0.005\n",
      "iteration: 8990 loss: 0.0110 lr: 0.005\n",
      "iteration: 9000 loss: 0.0103 lr: 0.005\n",
      "iteration: 9010 loss: 0.0103 lr: 0.005\n",
      "iteration: 9020 loss: 0.0106 lr: 0.005\n",
      "iteration: 9030 loss: 0.0135 lr: 0.005\n",
      "iteration: 9040 loss: 0.0091 lr: 0.005\n",
      "iteration: 9050 loss: 0.0098 lr: 0.005\n",
      "iteration: 9060 loss: 0.0091 lr: 0.005\n",
      "iteration: 9070 loss: 0.0101 lr: 0.005\n",
      "iteration: 9080 loss: 0.0139 lr: 0.005\n",
      "iteration: 9090 loss: 0.0080 lr: 0.005\n",
      "iteration: 9100 loss: 0.0113 lr: 0.005\n",
      "iteration: 9110 loss: 0.0102 lr: 0.005\n",
      "iteration: 9120 loss: 0.0106 lr: 0.005\n",
      "iteration: 9130 loss: 0.0083 lr: 0.005\n",
      "iteration: 9140 loss: 0.0091 lr: 0.005\n",
      "iteration: 9150 loss: 0.0075 lr: 0.005\n",
      "iteration: 9160 loss: 0.0106 lr: 0.005\n",
      "iteration: 9170 loss: 0.0113 lr: 0.005\n",
      "iteration: 9180 loss: 0.0093 lr: 0.005\n",
      "iteration: 9190 loss: 0.0113 lr: 0.005\n",
      "iteration: 9200 loss: 0.0076 lr: 0.005\n",
      "iteration: 9210 loss: 0.0073 lr: 0.005\n",
      "iteration: 9220 loss: 0.0090 lr: 0.005\n",
      "iteration: 9230 loss: 0.0109 lr: 0.005\n",
      "iteration: 9240 loss: 0.0139 lr: 0.005\n",
      "iteration: 9250 loss: 0.0125 lr: 0.005\n",
      "iteration: 9260 loss: 0.0095 lr: 0.005\n",
      "iteration: 9270 loss: 0.0088 lr: 0.005\n",
      "iteration: 9280 loss: 0.0135 lr: 0.005\n",
      "iteration: 9290 loss: 0.0129 lr: 0.005\n",
      "iteration: 9300 loss: 0.0125 lr: 0.005\n",
      "iteration: 9310 loss: 0.0115 lr: 0.005\n",
      "iteration: 9320 loss: 0.0080 lr: 0.005\n",
      "iteration: 9330 loss: 0.0147 lr: 0.005\n",
      "iteration: 9340 loss: 0.0112 lr: 0.005\n",
      "iteration: 9350 loss: 0.0106 lr: 0.005\n",
      "iteration: 9360 loss: 0.0139 lr: 0.005\n",
      "iteration: 9370 loss: 0.0103 lr: 0.005\n",
      "iteration: 9380 loss: 0.0122 lr: 0.005\n",
      "iteration: 9390 loss: 0.0134 lr: 0.005\n",
      "iteration: 9400 loss: 0.0132 lr: 0.005\n",
      "iteration: 9410 loss: 0.0077 lr: 0.005\n",
      "iteration: 9420 loss: 0.0088 lr: 0.005\n",
      "iteration: 9430 loss: 0.0090 lr: 0.005\n",
      "iteration: 9440 loss: 0.0089 lr: 0.005\n",
      "iteration: 9450 loss: 0.0121 lr: 0.005\n",
      "iteration: 9460 loss: 0.0081 lr: 0.005\n",
      "iteration: 9470 loss: 0.0115 lr: 0.005\n",
      "iteration: 9480 loss: 0.0076 lr: 0.005\n",
      "iteration: 9490 loss: 0.0108 lr: 0.005\n",
      "iteration: 9500 loss: 0.0097 lr: 0.005\n",
      "iteration: 9510 loss: 0.0064 lr: 0.005\n",
      "iteration: 9520 loss: 0.0106 lr: 0.005\n",
      "iteration: 9530 loss: 0.0089 lr: 0.005\n",
      "iteration: 9540 loss: 0.0087 lr: 0.005\n",
      "iteration: 9550 loss: 0.0107 lr: 0.005\n",
      "iteration: 9560 loss: 0.0069 lr: 0.005\n",
      "iteration: 9570 loss: 0.0107 lr: 0.005\n",
      "iteration: 9580 loss: 0.0072 lr: 0.005\n",
      "iteration: 9590 loss: 0.0105 lr: 0.005\n",
      "iteration: 9600 loss: 0.0097 lr: 0.005\n",
      "iteration: 9610 loss: 0.0094 lr: 0.005\n",
      "iteration: 9620 loss: 0.0094 lr: 0.005\n",
      "iteration: 9630 loss: 0.0107 lr: 0.005\n",
      "iteration: 9640 loss: 0.0118 lr: 0.005\n",
      "iteration: 9650 loss: 0.0116 lr: 0.005\n",
      "iteration: 9660 loss: 0.0097 lr: 0.005\n",
      "iteration: 9670 loss: 0.0105 lr: 0.005\n",
      "iteration: 9680 loss: 0.0104 lr: 0.005\n",
      "iteration: 9690 loss: 0.0136 lr: 0.005\n",
      "iteration: 9700 loss: 0.0089 lr: 0.005\n",
      "iteration: 9710 loss: 0.0095 lr: 0.005\n",
      "iteration: 9720 loss: 0.0070 lr: 0.005\n",
      "iteration: 9730 loss: 0.0115 lr: 0.005\n",
      "iteration: 9740 loss: 0.0108 lr: 0.005\n",
      "iteration: 9750 loss: 0.0101 lr: 0.005\n",
      "iteration: 9760 loss: 0.0094 lr: 0.005\n",
      "iteration: 9770 loss: 0.0081 lr: 0.005\n",
      "iteration: 9780 loss: 0.0115 lr: 0.005\n",
      "iteration: 9790 loss: 0.0091 lr: 0.005\n",
      "iteration: 9800 loss: 0.0090 lr: 0.005\n",
      "iteration: 9810 loss: 0.0132 lr: 0.005\n",
      "iteration: 9820 loss: 0.0084 lr: 0.005\n",
      "iteration: 9830 loss: 0.0106 lr: 0.005\n",
      "iteration: 9840 loss: 0.0102 lr: 0.005\n",
      "iteration: 9850 loss: 0.0081 lr: 0.005\n",
      "iteration: 9860 loss: 0.0070 lr: 0.005\n",
      "iteration: 9870 loss: 0.0084 lr: 0.005\n",
      "iteration: 9880 loss: 0.0109 lr: 0.005\n",
      "iteration: 9890 loss: 0.0102 lr: 0.005\n",
      "iteration: 9900 loss: 0.0112 lr: 0.005\n",
      "iteration: 9910 loss: 0.0085 lr: 0.005\n",
      "iteration: 9920 loss: 0.0117 lr: 0.005\n",
      "iteration: 9930 loss: 0.0126 lr: 0.005\n",
      "iteration: 9940 loss: 0.0093 lr: 0.005\n",
      "iteration: 9950 loss: 0.0128 lr: 0.005\n",
      "iteration: 9960 loss: 0.0069 lr: 0.005\n",
      "iteration: 9970 loss: 0.0078 lr: 0.005\n",
      "iteration: 9980 loss: 0.0087 lr: 0.005\n",
      "iteration: 9990 loss: 0.0071 lr: 0.005\n",
      "iteration: 10000 loss: 0.0091 lr: 0.005\n",
      "iteration: 10010 loss: 0.0148 lr: 0.02\n",
      "iteration: 10020 loss: 0.0176 lr: 0.02\n",
      "iteration: 10030 loss: 0.0149 lr: 0.02\n",
      "iteration: 10040 loss: 0.0151 lr: 0.02\n",
      "iteration: 10050 loss: 0.0146 lr: 0.02\n",
      "iteration: 10060 loss: 0.0162 lr: 0.02\n",
      "iteration: 10070 loss: 0.0144 lr: 0.02\n",
      "iteration: 10080 loss: 0.0162 lr: 0.02\n",
      "iteration: 10090 loss: 0.0125 lr: 0.02\n",
      "iteration: 10100 loss: 0.0110 lr: 0.02\n",
      "iteration: 10110 loss: 0.0158 lr: 0.02\n",
      "iteration: 10120 loss: 0.0180 lr: 0.02\n",
      "iteration: 10130 loss: 0.0204 lr: 0.02\n",
      "iteration: 10140 loss: 0.0172 lr: 0.02\n",
      "iteration: 10150 loss: 0.0180 lr: 0.02\n",
      "iteration: 10160 loss: 0.0155 lr: 0.02\n",
      "iteration: 10170 loss: 0.0095 lr: 0.02\n",
      "iteration: 10180 loss: 0.0158 lr: 0.02\n",
      "iteration: 10190 loss: 0.0124 lr: 0.02\n",
      "iteration: 10200 loss: 0.0184 lr: 0.02\n",
      "iteration: 10210 loss: 0.0150 lr: 0.02\n",
      "iteration: 10220 loss: 0.0137 lr: 0.02\n",
      "iteration: 10230 loss: 0.0130 lr: 0.02\n",
      "iteration: 10240 loss: 0.0181 lr: 0.02\n",
      "iteration: 10250 loss: 0.0166 lr: 0.02\n",
      "iteration: 10260 loss: 0.0204 lr: 0.02\n",
      "iteration: 10270 loss: 0.0122 lr: 0.02\n",
      "iteration: 10280 loss: 0.0217 lr: 0.02\n",
      "iteration: 10290 loss: 0.0151 lr: 0.02\n",
      "iteration: 10300 loss: 0.0186 lr: 0.02\n",
      "iteration: 10310 loss: 0.0140 lr: 0.02\n",
      "iteration: 10320 loss: 0.0107 lr: 0.02\n",
      "iteration: 10330 loss: 0.0214 lr: 0.02\n",
      "iteration: 10340 loss: 0.0144 lr: 0.02\n",
      "iteration: 10350 loss: 0.0116 lr: 0.02\n",
      "iteration: 10360 loss: 0.0132 lr: 0.02\n",
      "iteration: 10370 loss: 0.0129 lr: 0.02\n",
      "iteration: 10380 loss: 0.0151 lr: 0.02\n",
      "iteration: 10390 loss: 0.0131 lr: 0.02\n",
      "iteration: 10400 loss: 0.0153 lr: 0.02\n",
      "iteration: 10410 loss: 0.0168 lr: 0.02\n",
      "iteration: 10420 loss: 0.0154 lr: 0.02\n",
      "iteration: 10430 loss: 0.0164 lr: 0.02\n",
      "iteration: 10440 loss: 0.0140 lr: 0.02\n",
      "iteration: 10450 loss: 0.0135 lr: 0.02\n",
      "iteration: 10460 loss: 0.0138 lr: 0.02\n",
      "iteration: 10470 loss: 0.0192 lr: 0.02\n",
      "iteration: 10480 loss: 0.0173 lr: 0.02\n",
      "iteration: 10490 loss: 0.0121 lr: 0.02\n",
      "iteration: 10500 loss: 0.0138 lr: 0.02\n",
      "iteration: 10510 loss: 0.0160 lr: 0.02\n",
      "iteration: 10520 loss: 0.0156 lr: 0.02\n",
      "iteration: 10530 loss: 0.0113 lr: 0.02\n",
      "iteration: 10540 loss: 0.0112 lr: 0.02\n",
      "iteration: 10550 loss: 0.0135 lr: 0.02\n",
      "iteration: 10560 loss: 0.0116 lr: 0.02\n",
      "iteration: 10570 loss: 0.0170 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 10580 loss: 0.0108 lr: 0.02\n",
      "iteration: 10590 loss: 0.0115 lr: 0.02\n",
      "iteration: 10600 loss: 0.0129 lr: 0.02\n",
      "iteration: 10610 loss: 0.0150 lr: 0.02\n",
      "iteration: 10620 loss: 0.0151 lr: 0.02\n",
      "iteration: 10630 loss: 0.0139 lr: 0.02\n",
      "iteration: 10640 loss: 0.0124 lr: 0.02\n",
      "iteration: 10650 loss: 0.0164 lr: 0.02\n",
      "iteration: 10660 loss: 0.0156 lr: 0.02\n",
      "iteration: 10670 loss: 0.0161 lr: 0.02\n",
      "iteration: 10680 loss: 0.0135 lr: 0.02\n",
      "iteration: 10690 loss: 0.0130 lr: 0.02\n",
      "iteration: 10700 loss: 0.0152 lr: 0.02\n",
      "iteration: 10710 loss: 0.0160 lr: 0.02\n",
      "iteration: 10720 loss: 0.0138 lr: 0.02\n",
      "iteration: 10730 loss: 0.0156 lr: 0.02\n",
      "iteration: 10740 loss: 0.0136 lr: 0.02\n",
      "iteration: 10750 loss: 0.0151 lr: 0.02\n",
      "iteration: 10760 loss: 0.0132 lr: 0.02\n",
      "iteration: 10770 loss: 0.0143 lr: 0.02\n",
      "iteration: 10780 loss: 0.0143 lr: 0.02\n",
      "iteration: 10790 loss: 0.0112 lr: 0.02\n",
      "iteration: 10800 loss: 0.0116 lr: 0.02\n",
      "iteration: 10810 loss: 0.0107 lr: 0.02\n",
      "iteration: 10820 loss: 0.0131 lr: 0.02\n",
      "iteration: 10830 loss: 0.0159 lr: 0.02\n",
      "iteration: 10840 loss: 0.0174 lr: 0.02\n",
      "iteration: 10850 loss: 0.0124 lr: 0.02\n",
      "iteration: 10860 loss: 0.0149 lr: 0.02\n",
      "iteration: 10870 loss: 0.0140 lr: 0.02\n",
      "iteration: 10880 loss: 0.0159 lr: 0.02\n",
      "iteration: 10890 loss: 0.0151 lr: 0.02\n",
      "iteration: 10900 loss: 0.0153 lr: 0.02\n",
      "iteration: 10910 loss: 0.0148 lr: 0.02\n",
      "iteration: 10920 loss: 0.0138 lr: 0.02\n",
      "iteration: 10930 loss: 0.0125 lr: 0.02\n",
      "iteration: 10940 loss: 0.0151 lr: 0.02\n",
      "iteration: 10950 loss: 0.0133 lr: 0.02\n",
      "iteration: 10960 loss: 0.0125 lr: 0.02\n",
      "iteration: 10970 loss: 0.0099 lr: 0.02\n",
      "iteration: 10980 loss: 0.0126 lr: 0.02\n",
      "iteration: 10990 loss: 0.0124 lr: 0.02\n",
      "iteration: 11000 loss: 0.0123 lr: 0.02\n",
      "iteration: 11010 loss: 0.0196 lr: 0.02\n",
      "iteration: 11020 loss: 0.0123 lr: 0.02\n",
      "iteration: 11030 loss: 0.0106 lr: 0.02\n",
      "iteration: 11040 loss: 0.0091 lr: 0.02\n",
      "iteration: 11050 loss: 0.0126 lr: 0.02\n",
      "iteration: 11060 loss: 0.0120 lr: 0.02\n",
      "iteration: 11070 loss: 0.0125 lr: 0.02\n",
      "iteration: 11080 loss: 0.0138 lr: 0.02\n",
      "iteration: 11090 loss: 0.0125 lr: 0.02\n",
      "iteration: 11100 loss: 0.0123 lr: 0.02\n",
      "iteration: 11110 loss: 0.0152 lr: 0.02\n",
      "iteration: 11120 loss: 0.0129 lr: 0.02\n",
      "iteration: 11130 loss: 0.0160 lr: 0.02\n",
      "iteration: 11140 loss: 0.0136 lr: 0.02\n",
      "iteration: 11150 loss: 0.0160 lr: 0.02\n",
      "iteration: 11160 loss: 0.0119 lr: 0.02\n",
      "iteration: 11170 loss: 0.0142 lr: 0.02\n",
      "iteration: 11180 loss: 0.0134 lr: 0.02\n",
      "iteration: 11190 loss: 0.0124 lr: 0.02\n",
      "iteration: 11200 loss: 0.0153 lr: 0.02\n",
      "iteration: 11210 loss: 0.0114 lr: 0.02\n",
      "iteration: 11220 loss: 0.0117 lr: 0.02\n",
      "iteration: 11230 loss: 0.0126 lr: 0.02\n",
      "iteration: 11240 loss: 0.0119 lr: 0.02\n",
      "iteration: 11250 loss: 0.0129 lr: 0.02\n",
      "iteration: 11260 loss: 0.0111 lr: 0.02\n",
      "iteration: 11270 loss: 0.0120 lr: 0.02\n",
      "iteration: 11280 loss: 0.0124 lr: 0.02\n",
      "iteration: 11290 loss: 0.0125 lr: 0.02\n",
      "iteration: 11300 loss: 0.0091 lr: 0.02\n",
      "iteration: 11310 loss: 0.0113 lr: 0.02\n",
      "iteration: 11320 loss: 0.0122 lr: 0.02\n",
      "iteration: 11330 loss: 0.0154 lr: 0.02\n",
      "iteration: 11340 loss: 0.0129 lr: 0.02\n",
      "iteration: 11350 loss: 0.0139 lr: 0.02\n",
      "iteration: 11360 loss: 0.0150 lr: 0.02\n",
      "iteration: 11370 loss: 0.0146 lr: 0.02\n",
      "iteration: 11380 loss: 0.0145 lr: 0.02\n",
      "iteration: 11390 loss: 0.0111 lr: 0.02\n",
      "iteration: 11400 loss: 0.0142 lr: 0.02\n",
      "iteration: 11410 loss: 0.0130 lr: 0.02\n",
      "iteration: 11420 loss: 0.0137 lr: 0.02\n",
      "iteration: 11430 loss: 0.0116 lr: 0.02\n",
      "iteration: 11440 loss: 0.0109 lr: 0.02\n",
      "iteration: 11450 loss: 0.0142 lr: 0.02\n",
      "iteration: 11460 loss: 0.0184 lr: 0.02\n",
      "iteration: 11470 loss: 0.0126 lr: 0.02\n",
      "iteration: 11480 loss: 0.0112 lr: 0.02\n",
      "iteration: 11490 loss: 0.0120 lr: 0.02\n",
      "iteration: 11500 loss: 0.0139 lr: 0.02\n",
      "iteration: 11510 loss: 0.0099 lr: 0.02\n",
      "iteration: 11520 loss: 0.0110 lr: 0.02\n",
      "iteration: 11530 loss: 0.0131 lr: 0.02\n",
      "iteration: 11540 loss: 0.0176 lr: 0.02\n",
      "iteration: 11550 loss: 0.0162 lr: 0.02\n",
      "iteration: 11560 loss: 0.0137 lr: 0.02\n",
      "iteration: 11570 loss: 0.0085 lr: 0.02\n",
      "iteration: 11580 loss: 0.0138 lr: 0.02\n",
      "iteration: 11590 loss: 0.0176 lr: 0.02\n",
      "iteration: 11600 loss: 0.0127 lr: 0.02\n",
      "iteration: 11610 loss: 0.0172 lr: 0.02\n",
      "iteration: 11620 loss: 0.0128 lr: 0.02\n",
      "iteration: 11630 loss: 0.0091 lr: 0.02\n",
      "iteration: 11640 loss: 0.0116 lr: 0.02\n",
      "iteration: 11650 loss: 0.0135 lr: 0.02\n",
      "iteration: 11660 loss: 0.0151 lr: 0.02\n",
      "iteration: 11670 loss: 0.0145 lr: 0.02\n",
      "iteration: 11680 loss: 0.0112 lr: 0.02\n",
      "iteration: 11690 loss: 0.0117 lr: 0.02\n",
      "iteration: 11700 loss: 0.0127 lr: 0.02\n",
      "iteration: 11710 loss: 0.0097 lr: 0.02\n",
      "iteration: 11720 loss: 0.0147 lr: 0.02\n",
      "iteration: 11730 loss: 0.0114 lr: 0.02\n",
      "iteration: 11740 loss: 0.0117 lr: 0.02\n",
      "iteration: 11750 loss: 0.0126 lr: 0.02\n",
      "iteration: 11760 loss: 0.0113 lr: 0.02\n",
      "iteration: 11770 loss: 0.0104 lr: 0.02\n",
      "iteration: 11780 loss: 0.0150 lr: 0.02\n",
      "iteration: 11790 loss: 0.0128 lr: 0.02\n",
      "iteration: 11800 loss: 0.0114 lr: 0.02\n",
      "iteration: 11810 loss: 0.0115 lr: 0.02\n",
      "iteration: 11820 loss: 0.0202 lr: 0.02\n",
      "iteration: 11830 loss: 0.0115 lr: 0.02\n",
      "iteration: 11840 loss: 0.0146 lr: 0.02\n",
      "iteration: 11850 loss: 0.0082 lr: 0.02\n",
      "iteration: 11860 loss: 0.0118 lr: 0.02\n",
      "iteration: 11870 loss: 0.0139 lr: 0.02\n",
      "iteration: 11880 loss: 0.0103 lr: 0.02\n",
      "iteration: 11890 loss: 0.0131 lr: 0.02\n",
      "iteration: 11900 loss: 0.0118 lr: 0.02\n",
      "iteration: 11910 loss: 0.0130 lr: 0.02\n",
      "iteration: 11920 loss: 0.0116 lr: 0.02\n",
      "iteration: 11930 loss: 0.0165 lr: 0.02\n",
      "iteration: 11940 loss: 0.0119 lr: 0.02\n",
      "iteration: 11950 loss: 0.0143 lr: 0.02\n",
      "iteration: 11960 loss: 0.0107 lr: 0.02\n",
      "iteration: 11970 loss: 0.0128 lr: 0.02\n",
      "iteration: 11980 loss: 0.0147 lr: 0.02\n",
      "iteration: 11990 loss: 0.0142 lr: 0.02\n",
      "iteration: 12000 loss: 0.0130 lr: 0.02\n",
      "iteration: 12010 loss: 0.0133 lr: 0.02\n",
      "iteration: 12020 loss: 0.0108 lr: 0.02\n",
      "iteration: 12030 loss: 0.0105 lr: 0.02\n",
      "iteration: 12040 loss: 0.0138 lr: 0.02\n",
      "iteration: 12050 loss: 0.0125 lr: 0.02\n",
      "iteration: 12060 loss: 0.0125 lr: 0.02\n",
      "iteration: 12070 loss: 0.0121 lr: 0.02\n",
      "iteration: 12080 loss: 0.0123 lr: 0.02\n",
      "iteration: 12090 loss: 0.0130 lr: 0.02\n",
      "iteration: 12100 loss: 0.0149 lr: 0.02\n",
      "iteration: 12110 loss: 0.0134 lr: 0.02\n",
      "iteration: 12120 loss: 0.0139 lr: 0.02\n",
      "iteration: 12130 loss: 0.0108 lr: 0.02\n",
      "iteration: 12140 loss: 0.0108 lr: 0.02\n",
      "iteration: 12150 loss: 0.0083 lr: 0.02\n",
      "iteration: 12160 loss: 0.0119 lr: 0.02\n",
      "iteration: 12170 loss: 0.0135 lr: 0.02\n",
      "iteration: 12180 loss: 0.0120 lr: 0.02\n",
      "iteration: 12190 loss: 0.0125 lr: 0.02\n",
      "iteration: 12200 loss: 0.0105 lr: 0.02\n",
      "iteration: 12210 loss: 0.0135 lr: 0.02\n",
      "iteration: 12220 loss: 0.0127 lr: 0.02\n",
      "iteration: 12230 loss: 0.0148 lr: 0.02\n",
      "iteration: 12240 loss: 0.0088 lr: 0.02\n",
      "iteration: 12250 loss: 0.0138 lr: 0.02\n",
      "iteration: 12260 loss: 0.0138 lr: 0.02\n",
      "iteration: 12270 loss: 0.0109 lr: 0.02\n",
      "iteration: 12280 loss: 0.0126 lr: 0.02\n",
      "iteration: 12290 loss: 0.0103 lr: 0.02\n",
      "iteration: 12300 loss: 0.0078 lr: 0.02\n",
      "iteration: 12310 loss: 0.0105 lr: 0.02\n",
      "iteration: 12320 loss: 0.0106 lr: 0.02\n",
      "iteration: 12330 loss: 0.0082 lr: 0.02\n",
      "iteration: 12340 loss: 0.0114 lr: 0.02\n",
      "iteration: 12350 loss: 0.0130 lr: 0.02\n",
      "iteration: 12360 loss: 0.0121 lr: 0.02\n",
      "iteration: 12370 loss: 0.0070 lr: 0.02\n",
      "iteration: 12380 loss: 0.0132 lr: 0.02\n",
      "iteration: 12390 loss: 0.0102 lr: 0.02\n",
      "iteration: 12400 loss: 0.0169 lr: 0.02\n",
      "iteration: 12410 loss: 0.0148 lr: 0.02\n",
      "iteration: 12420 loss: 0.0140 lr: 0.02\n",
      "iteration: 12430 loss: 0.0140 lr: 0.02\n",
      "iteration: 12440 loss: 0.0158 lr: 0.02\n",
      "iteration: 12450 loss: 0.0114 lr: 0.02\n",
      "iteration: 12460 loss: 0.0091 lr: 0.02\n",
      "iteration: 12470 loss: 0.0149 lr: 0.02\n",
      "iteration: 12480 loss: 0.0138 lr: 0.02\n",
      "iteration: 12490 loss: 0.0098 lr: 0.02\n",
      "iteration: 12500 loss: 0.0107 lr: 0.02\n",
      "iteration: 12510 loss: 0.0142 lr: 0.02\n",
      "iteration: 12520 loss: 0.0101 lr: 0.02\n",
      "iteration: 12530 loss: 0.0124 lr: 0.02\n",
      "iteration: 12540 loss: 0.0109 lr: 0.02\n",
      "iteration: 12550 loss: 0.0118 lr: 0.02\n",
      "iteration: 12560 loss: 0.0133 lr: 0.02\n",
      "iteration: 12570 loss: 0.0128 lr: 0.02\n",
      "iteration: 12580 loss: 0.0125 lr: 0.02\n",
      "iteration: 12590 loss: 0.0136 lr: 0.02\n",
      "iteration: 12600 loss: 0.0115 lr: 0.02\n",
      "iteration: 12610 loss: 0.0172 lr: 0.02\n",
      "iteration: 12620 loss: 0.0131 lr: 0.02\n",
      "iteration: 12630 loss: 0.0103 lr: 0.02\n",
      "iteration: 12640 loss: 0.0101 lr: 0.02\n",
      "iteration: 12650 loss: 0.0113 lr: 0.02\n",
      "iteration: 12660 loss: 0.0107 lr: 0.02\n",
      "iteration: 12670 loss: 0.0084 lr: 0.02\n",
      "iteration: 12680 loss: 0.0118 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 12690 loss: 0.0092 lr: 0.02\n",
      "iteration: 12700 loss: 0.0156 lr: 0.02\n",
      "iteration: 12710 loss: 0.0153 lr: 0.02\n",
      "iteration: 12720 loss: 0.0108 lr: 0.02\n",
      "iteration: 12730 loss: 0.0112 lr: 0.02\n",
      "iteration: 12740 loss: 0.0108 lr: 0.02\n",
      "iteration: 12750 loss: 0.0118 lr: 0.02\n",
      "iteration: 12760 loss: 0.0134 lr: 0.02\n",
      "iteration: 12770 loss: 0.0115 lr: 0.02\n",
      "iteration: 12780 loss: 0.0149 lr: 0.02\n",
      "iteration: 12790 loss: 0.0092 lr: 0.02\n",
      "iteration: 12800 loss: 0.0069 lr: 0.02\n",
      "iteration: 12810 loss: 0.0151 lr: 0.02\n",
      "iteration: 12820 loss: 0.0153 lr: 0.02\n",
      "iteration: 12830 loss: 0.0125 lr: 0.02\n",
      "iteration: 12840 loss: 0.0126 lr: 0.02\n",
      "iteration: 12850 loss: 0.0098 lr: 0.02\n",
      "iteration: 12860 loss: 0.0082 lr: 0.02\n",
      "iteration: 12870 loss: 0.0131 lr: 0.02\n",
      "iteration: 12880 loss: 0.0143 lr: 0.02\n",
      "iteration: 12890 loss: 0.0161 lr: 0.02\n",
      "iteration: 12900 loss: 0.0139 lr: 0.02\n",
      "iteration: 12910 loss: 0.0091 lr: 0.02\n",
      "iteration: 12920 loss: 0.0107 lr: 0.02\n",
      "iteration: 12930 loss: 0.0121 lr: 0.02\n",
      "iteration: 12940 loss: 0.0104 lr: 0.02\n",
      "iteration: 12950 loss: 0.0117 lr: 0.02\n",
      "iteration: 12960 loss: 0.0125 lr: 0.02\n",
      "iteration: 12970 loss: 0.0087 lr: 0.02\n",
      "iteration: 12980 loss: 0.0113 lr: 0.02\n",
      "iteration: 12990 loss: 0.0104 lr: 0.02\n",
      "iteration: 13000 loss: 0.0087 lr: 0.02\n",
      "iteration: 13010 loss: 0.0078 lr: 0.02\n",
      "iteration: 13020 loss: 0.0116 lr: 0.02\n",
      "iteration: 13030 loss: 0.0139 lr: 0.02\n",
      "iteration: 13040 loss: 0.0121 lr: 0.02\n",
      "iteration: 13050 loss: 0.0132 lr: 0.02\n",
      "iteration: 13060 loss: 0.0113 lr: 0.02\n",
      "iteration: 13070 loss: 0.0099 lr: 0.02\n",
      "iteration: 13080 loss: 0.0117 lr: 0.02\n",
      "iteration: 13090 loss: 0.0126 lr: 0.02\n",
      "iteration: 13100 loss: 0.0171 lr: 0.02\n",
      "iteration: 13110 loss: 0.0147 lr: 0.02\n",
      "iteration: 13120 loss: 0.0113 lr: 0.02\n",
      "iteration: 13130 loss: 0.0092 lr: 0.02\n",
      "iteration: 13140 loss: 0.0098 lr: 0.02\n",
      "iteration: 13150 loss: 0.0084 lr: 0.02\n",
      "iteration: 13160 loss: 0.0109 lr: 0.02\n",
      "iteration: 13170 loss: 0.0125 lr: 0.02\n",
      "iteration: 13180 loss: 0.0120 lr: 0.02\n",
      "iteration: 13190 loss: 0.0098 lr: 0.02\n",
      "iteration: 13200 loss: 0.0132 lr: 0.02\n",
      "iteration: 13210 loss: 0.0128 lr: 0.02\n",
      "iteration: 13220 loss: 0.0103 lr: 0.02\n",
      "iteration: 13230 loss: 0.0103 lr: 0.02\n",
      "iteration: 13240 loss: 0.0127 lr: 0.02\n",
      "iteration: 13250 loss: 0.0120 lr: 0.02\n",
      "iteration: 13260 loss: 0.0100 lr: 0.02\n",
      "iteration: 13270 loss: 0.0124 lr: 0.02\n",
      "iteration: 13280 loss: 0.0099 lr: 0.02\n",
      "iteration: 13290 loss: 0.0126 lr: 0.02\n",
      "iteration: 13300 loss: 0.0115 lr: 0.02\n",
      "iteration: 13310 loss: 0.0106 lr: 0.02\n",
      "iteration: 13320 loss: 0.0107 lr: 0.02\n",
      "iteration: 13330 loss: 0.0104 lr: 0.02\n",
      "iteration: 13340 loss: 0.0096 lr: 0.02\n",
      "iteration: 13350 loss: 0.0099 lr: 0.02\n",
      "iteration: 13360 loss: 0.0129 lr: 0.02\n",
      "iteration: 13370 loss: 0.0099 lr: 0.02\n",
      "iteration: 13380 loss: 0.0142 lr: 0.02\n",
      "iteration: 13390 loss: 0.0094 lr: 0.02\n",
      "iteration: 13400 loss: 0.0098 lr: 0.02\n",
      "iteration: 13410 loss: 0.0101 lr: 0.02\n",
      "iteration: 13420 loss: 0.0100 lr: 0.02\n",
      "iteration: 13430 loss: 0.0132 lr: 0.02\n",
      "iteration: 13440 loss: 0.0131 lr: 0.02\n",
      "iteration: 13450 loss: 0.0137 lr: 0.02\n",
      "iteration: 13460 loss: 0.0109 lr: 0.02\n",
      "iteration: 13470 loss: 0.0128 lr: 0.02\n",
      "iteration: 13480 loss: 0.0127 lr: 0.02\n",
      "iteration: 13490 loss: 0.0110 lr: 0.02\n",
      "iteration: 13500 loss: 0.0142 lr: 0.02\n",
      "iteration: 13510 loss: 0.0131 lr: 0.02\n",
      "iteration: 13520 loss: 0.0098 lr: 0.02\n",
      "iteration: 13530 loss: 0.0096 lr: 0.02\n",
      "iteration: 13540 loss: 0.0112 lr: 0.02\n",
      "iteration: 13550 loss: 0.0112 lr: 0.02\n",
      "iteration: 13560 loss: 0.0104 lr: 0.02\n",
      "iteration: 13570 loss: 0.0130 lr: 0.02\n",
      "iteration: 13580 loss: 0.0094 lr: 0.02\n",
      "iteration: 13590 loss: 0.0139 lr: 0.02\n",
      "iteration: 13600 loss: 0.0071 lr: 0.02\n",
      "iteration: 13610 loss: 0.0091 lr: 0.02\n",
      "iteration: 13620 loss: 0.0093 lr: 0.02\n",
      "iteration: 13630 loss: 0.0105 lr: 0.02\n",
      "iteration: 13640 loss: 0.0097 lr: 0.02\n",
      "iteration: 13650 loss: 0.0116 lr: 0.02\n",
      "iteration: 13660 loss: 0.0099 lr: 0.02\n",
      "iteration: 13670 loss: 0.0120 lr: 0.02\n",
      "iteration: 13680 loss: 0.0071 lr: 0.02\n",
      "iteration: 13690 loss: 0.0138 lr: 0.02\n",
      "iteration: 13700 loss: 0.0138 lr: 0.02\n",
      "iteration: 13710 loss: 0.0122 lr: 0.02\n",
      "iteration: 13720 loss: 0.0111 lr: 0.02\n",
      "iteration: 13730 loss: 0.0104 lr: 0.02\n",
      "iteration: 13740 loss: 0.0107 lr: 0.02\n",
      "iteration: 13750 loss: 0.0084 lr: 0.02\n",
      "iteration: 13760 loss: 0.0104 lr: 0.02\n",
      "iteration: 13770 loss: 0.0101 lr: 0.02\n",
      "iteration: 13780 loss: 0.0096 lr: 0.02\n",
      "iteration: 13790 loss: 0.0083 lr: 0.02\n",
      "iteration: 13800 loss: 0.0116 lr: 0.02\n",
      "iteration: 13810 loss: 0.0123 lr: 0.02\n",
      "iteration: 13820 loss: 0.0106 lr: 0.02\n",
      "iteration: 13830 loss: 0.0124 lr: 0.02\n",
      "iteration: 13840 loss: 0.0108 lr: 0.02\n",
      "iteration: 13850 loss: 0.0122 lr: 0.02\n",
      "iteration: 13860 loss: 0.0118 lr: 0.02\n",
      "iteration: 13870 loss: 0.0150 lr: 0.02\n",
      "iteration: 13880 loss: 0.0136 lr: 0.02\n",
      "iteration: 13890 loss: 0.0101 lr: 0.02\n",
      "iteration: 13900 loss: 0.0125 lr: 0.02\n",
      "iteration: 13910 loss: 0.0084 lr: 0.02\n",
      "iteration: 13920 loss: 0.0113 lr: 0.02\n",
      "iteration: 13930 loss: 0.0110 lr: 0.02\n",
      "iteration: 13940 loss: 0.0099 lr: 0.02\n",
      "iteration: 13950 loss: 0.0098 lr: 0.02\n",
      "iteration: 13960 loss: 0.0086 lr: 0.02\n",
      "iteration: 13970 loss: 0.0083 lr: 0.02\n",
      "iteration: 13980 loss: 0.0107 lr: 0.02\n",
      "iteration: 13990 loss: 0.0095 lr: 0.02\n",
      "iteration: 14000 loss: 0.0104 lr: 0.02\n",
      "iteration: 14010 loss: 0.0096 lr: 0.02\n",
      "iteration: 14020 loss: 0.0134 lr: 0.02\n",
      "iteration: 14030 loss: 0.0123 lr: 0.02\n",
      "iteration: 14040 loss: 0.0115 lr: 0.02\n",
      "iteration: 14050 loss: 0.0127 lr: 0.02\n",
      "iteration: 14060 loss: 0.0140 lr: 0.02\n",
      "iteration: 14070 loss: 0.0124 lr: 0.02\n",
      "iteration: 14080 loss: 0.0140 lr: 0.02\n",
      "iteration: 14090 loss: 0.0098 lr: 0.02\n",
      "iteration: 14100 loss: 0.0076 lr: 0.02\n",
      "iteration: 14110 loss: 0.0090 lr: 0.02\n",
      "iteration: 14120 loss: 0.0098 lr: 0.02\n",
      "iteration: 14130 loss: 0.0103 lr: 0.02\n",
      "iteration: 14140 loss: 0.0115 lr: 0.02\n",
      "iteration: 14150 loss: 0.0076 lr: 0.02\n",
      "iteration: 14160 loss: 0.0127 lr: 0.02\n",
      "iteration: 14170 loss: 0.0124 lr: 0.02\n",
      "iteration: 14180 loss: 0.0121 lr: 0.02\n",
      "iteration: 14190 loss: 0.0132 lr: 0.02\n",
      "iteration: 14200 loss: 0.0096 lr: 0.02\n",
      "iteration: 14210 loss: 0.0139 lr: 0.02\n",
      "iteration: 14220 loss: 0.0110 lr: 0.02\n",
      "iteration: 14230 loss: 0.0088 lr: 0.02\n",
      "iteration: 14240 loss: 0.0146 lr: 0.02\n",
      "iteration: 14250 loss: 0.0112 lr: 0.02\n",
      "iteration: 14260 loss: 0.0085 lr: 0.02\n",
      "iteration: 14270 loss: 0.0114 lr: 0.02\n",
      "iteration: 14280 loss: 0.0097 lr: 0.02\n",
      "iteration: 14290 loss: 0.0115 lr: 0.02\n",
      "iteration: 14300 loss: 0.0122 lr: 0.02\n",
      "iteration: 14310 loss: 0.0105 lr: 0.02\n",
      "iteration: 14320 loss: 0.0143 lr: 0.02\n",
      "iteration: 14330 loss: 0.0123 lr: 0.02\n",
      "iteration: 14340 loss: 0.0118 lr: 0.02\n",
      "iteration: 14350 loss: 0.0122 lr: 0.02\n",
      "iteration: 14360 loss: 0.0106 lr: 0.02\n",
      "iteration: 14370 loss: 0.0097 lr: 0.02\n",
      "iteration: 14380 loss: 0.0126 lr: 0.02\n",
      "iteration: 14390 loss: 0.0112 lr: 0.02\n",
      "iteration: 14400 loss: 0.0098 lr: 0.02\n",
      "iteration: 14410 loss: 0.0090 lr: 0.02\n",
      "iteration: 14420 loss: 0.0098 lr: 0.02\n",
      "iteration: 14430 loss: 0.0107 lr: 0.02\n",
      "iteration: 14440 loss: 0.0111 lr: 0.02\n",
      "iteration: 14450 loss: 0.0074 lr: 0.02\n",
      "iteration: 14460 loss: 0.0143 lr: 0.02\n",
      "iteration: 14470 loss: 0.0082 lr: 0.02\n",
      "iteration: 14480 loss: 0.0109 lr: 0.02\n",
      "iteration: 14490 loss: 0.0120 lr: 0.02\n",
      "iteration: 14500 loss: 0.0132 lr: 0.02\n",
      "iteration: 14510 loss: 0.0079 lr: 0.02\n",
      "iteration: 14520 loss: 0.0134 lr: 0.02\n",
      "iteration: 14530 loss: 0.0109 lr: 0.02\n",
      "iteration: 14540 loss: 0.0103 lr: 0.02\n",
      "iteration: 14550 loss: 0.0136 lr: 0.02\n",
      "iteration: 14560 loss: 0.0101 lr: 0.02\n",
      "iteration: 14570 loss: 0.0125 lr: 0.02\n",
      "iteration: 14580 loss: 0.0176 lr: 0.02\n",
      "iteration: 14590 loss: 0.0096 lr: 0.02\n",
      "iteration: 14600 loss: 0.0105 lr: 0.02\n",
      "iteration: 14610 loss: 0.0112 lr: 0.02\n",
      "iteration: 14620 loss: 0.0138 lr: 0.02\n",
      "iteration: 14630 loss: 0.0099 lr: 0.02\n",
      "iteration: 14640 loss: 0.0117 lr: 0.02\n",
      "iteration: 14650 loss: 0.0111 lr: 0.02\n",
      "iteration: 14660 loss: 0.0089 lr: 0.02\n",
      "iteration: 14670 loss: 0.0119 lr: 0.02\n",
      "iteration: 14680 loss: 0.0119 lr: 0.02\n",
      "iteration: 14690 loss: 0.0122 lr: 0.02\n",
      "iteration: 14700 loss: 0.0120 lr: 0.02\n",
      "iteration: 14710 loss: 0.0119 lr: 0.02\n",
      "iteration: 14720 loss: 0.0112 lr: 0.02\n",
      "iteration: 14730 loss: 0.0088 lr: 0.02\n",
      "iteration: 14740 loss: 0.0140 lr: 0.02\n",
      "iteration: 14750 loss: 0.0105 lr: 0.02\n",
      "iteration: 14760 loss: 0.0118 lr: 0.02\n",
      "iteration: 14770 loss: 0.0121 lr: 0.02\n",
      "iteration: 14780 loss: 0.0150 lr: 0.02\n",
      "iteration: 14790 loss: 0.0089 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 14800 loss: 0.0125 lr: 0.02\n",
      "iteration: 14810 loss: 0.0110 lr: 0.02\n",
      "iteration: 14820 loss: 0.0097 lr: 0.02\n",
      "iteration: 14830 loss: 0.0116 lr: 0.02\n",
      "iteration: 14840 loss: 0.0098 lr: 0.02\n",
      "iteration: 14850 loss: 0.0075 lr: 0.02\n",
      "iteration: 14860 loss: 0.0110 lr: 0.02\n",
      "iteration: 14870 loss: 0.0112 lr: 0.02\n",
      "iteration: 14880 loss: 0.0083 lr: 0.02\n",
      "iteration: 14890 loss: 0.0112 lr: 0.02\n",
      "iteration: 14900 loss: 0.0086 lr: 0.02\n",
      "iteration: 14910 loss: 0.0092 lr: 0.02\n",
      "iteration: 14920 loss: 0.0103 lr: 0.02\n",
      "iteration: 14930 loss: 0.0098 lr: 0.02\n",
      "iteration: 14940 loss: 0.0101 lr: 0.02\n",
      "iteration: 14950 loss: 0.0129 lr: 0.02\n",
      "iteration: 14960 loss: 0.0130 lr: 0.02\n",
      "iteration: 14970 loss: 0.0087 lr: 0.02\n",
      "iteration: 14980 loss: 0.0097 lr: 0.02\n",
      "iteration: 14990 loss: 0.0101 lr: 0.02\n",
      "iteration: 15000 loss: 0.0120 lr: 0.02\n",
      "iteration: 15010 loss: 0.0091 lr: 0.02\n",
      "iteration: 15020 loss: 0.0081 lr: 0.02\n",
      "iteration: 15030 loss: 0.0162 lr: 0.02\n",
      "iteration: 15040 loss: 0.0091 lr: 0.02\n",
      "iteration: 15050 loss: 0.0130 lr: 0.02\n",
      "iteration: 15060 loss: 0.0117 lr: 0.02\n",
      "iteration: 15070 loss: 0.0107 lr: 0.02\n",
      "iteration: 15080 loss: 0.0085 lr: 0.02\n",
      "iteration: 15090 loss: 0.0121 lr: 0.02\n",
      "iteration: 15100 loss: 0.0108 lr: 0.02\n",
      "iteration: 15110 loss: 0.0115 lr: 0.02\n",
      "iteration: 15120 loss: 0.0070 lr: 0.02\n",
      "iteration: 15130 loss: 0.0111 lr: 0.02\n",
      "iteration: 15140 loss: 0.0126 lr: 0.02\n",
      "iteration: 15150 loss: 0.0103 lr: 0.02\n",
      "iteration: 15160 loss: 0.0079 lr: 0.02\n",
      "iteration: 15170 loss: 0.0109 lr: 0.02\n",
      "iteration: 15180 loss: 0.0112 lr: 0.02\n",
      "iteration: 15190 loss: 0.0107 lr: 0.02\n",
      "iteration: 15200 loss: 0.0099 lr: 0.02\n",
      "iteration: 15210 loss: 0.0105 lr: 0.02\n",
      "iteration: 15220 loss: 0.0094 lr: 0.02\n",
      "iteration: 15230 loss: 0.0111 lr: 0.02\n",
      "iteration: 15240 loss: 0.0122 lr: 0.02\n",
      "iteration: 15250 loss: 0.0076 lr: 0.02\n",
      "iteration: 15260 loss: 0.0100 lr: 0.02\n",
      "iteration: 15270 loss: 0.0085 lr: 0.02\n",
      "iteration: 15280 loss: 0.0104 lr: 0.02\n",
      "iteration: 15290 loss: 0.0091 lr: 0.02\n",
      "iteration: 15300 loss: 0.0089 lr: 0.02\n",
      "iteration: 15310 loss: 0.0104 lr: 0.02\n",
      "iteration: 15320 loss: 0.0129 lr: 0.02\n",
      "iteration: 15330 loss: 0.0078 lr: 0.02\n",
      "iteration: 15340 loss: 0.0077 lr: 0.02\n",
      "iteration: 15350 loss: 0.0117 lr: 0.02\n",
      "iteration: 15360 loss: 0.0111 lr: 0.02\n",
      "iteration: 15370 loss: 0.0096 lr: 0.02\n",
      "iteration: 15380 loss: 0.0130 lr: 0.02\n",
      "iteration: 15390 loss: 0.0093 lr: 0.02\n",
      "iteration: 15400 loss: 0.0134 lr: 0.02\n",
      "iteration: 15410 loss: 0.0096 lr: 0.02\n",
      "iteration: 15420 loss: 0.0078 lr: 0.02\n",
      "iteration: 15430 loss: 0.0121 lr: 0.02\n",
      "iteration: 15440 loss: 0.0102 lr: 0.02\n",
      "iteration: 15450 loss: 0.0104 lr: 0.02\n",
      "iteration: 15460 loss: 0.0110 lr: 0.02\n",
      "iteration: 15470 loss: 0.0089 lr: 0.02\n",
      "iteration: 15480 loss: 0.0088 lr: 0.02\n",
      "iteration: 15490 loss: 0.0088 lr: 0.02\n",
      "iteration: 15500 loss: 0.0087 lr: 0.02\n",
      "iteration: 15510 loss: 0.0103 lr: 0.02\n",
      "iteration: 15520 loss: 0.0109 lr: 0.02\n",
      "iteration: 15530 loss: 0.0118 lr: 0.02\n",
      "iteration: 15540 loss: 0.0128 lr: 0.02\n",
      "iteration: 15550 loss: 0.0091 lr: 0.02\n",
      "iteration: 15560 loss: 0.0121 lr: 0.02\n",
      "iteration: 15570 loss: 0.0073 lr: 0.02\n",
      "iteration: 15580 loss: 0.0063 lr: 0.02\n",
      "iteration: 15590 loss: 0.0110 lr: 0.02\n",
      "iteration: 15600 loss: 0.0097 lr: 0.02\n",
      "iteration: 15610 loss: 0.0156 lr: 0.02\n",
      "iteration: 15620 loss: 0.0103 lr: 0.02\n",
      "iteration: 15630 loss: 0.0100 lr: 0.02\n",
      "iteration: 15640 loss: 0.0097 lr: 0.02\n",
      "iteration: 15650 loss: 0.0148 lr: 0.02\n",
      "iteration: 15660 loss: 0.0093 lr: 0.02\n",
      "iteration: 15670 loss: 0.0097 lr: 0.02\n",
      "iteration: 15680 loss: 0.0123 lr: 0.02\n",
      "iteration: 15690 loss: 0.0091 lr: 0.02\n",
      "iteration: 15700 loss: 0.0097 lr: 0.02\n",
      "iteration: 15710 loss: 0.0090 lr: 0.02\n",
      "iteration: 15720 loss: 0.0117 lr: 0.02\n",
      "iteration: 15730 loss: 0.0095 lr: 0.02\n",
      "iteration: 15740 loss: 0.0115 lr: 0.02\n",
      "iteration: 15750 loss: 0.0067 lr: 0.02\n",
      "iteration: 15760 loss: 0.0109 lr: 0.02\n",
      "iteration: 15770 loss: 0.0127 lr: 0.02\n",
      "iteration: 15780 loss: 0.0076 lr: 0.02\n",
      "iteration: 15790 loss: 0.0091 lr: 0.02\n",
      "iteration: 15800 loss: 0.0086 lr: 0.02\n",
      "iteration: 15810 loss: 0.0112 lr: 0.02\n",
      "iteration: 15820 loss: 0.0087 lr: 0.02\n",
      "iteration: 15830 loss: 0.0119 lr: 0.02\n",
      "iteration: 15840 loss: 0.0112 lr: 0.02\n",
      "iteration: 15850 loss: 0.0142 lr: 0.02\n",
      "iteration: 15860 loss: 0.0075 lr: 0.02\n",
      "iteration: 15870 loss: 0.0102 lr: 0.02\n",
      "iteration: 15880 loss: 0.0093 lr: 0.02\n",
      "iteration: 15890 loss: 0.0092 lr: 0.02\n",
      "iteration: 15900 loss: 0.0089 lr: 0.02\n",
      "iteration: 15910 loss: 0.0098 lr: 0.02\n",
      "iteration: 15920 loss: 0.0089 lr: 0.02\n",
      "iteration: 15930 loss: 0.0097 lr: 0.02\n",
      "iteration: 15940 loss: 0.0077 lr: 0.02\n",
      "iteration: 15950 loss: 0.0068 lr: 0.02\n",
      "iteration: 15960 loss: 0.0094 lr: 0.02\n",
      "iteration: 15970 loss: 0.0079 lr: 0.02\n",
      "iteration: 15980 loss: 0.0127 lr: 0.02\n",
      "iteration: 15990 loss: 0.0109 lr: 0.02\n",
      "iteration: 16000 loss: 0.0123 lr: 0.02\n",
      "iteration: 16010 loss: 0.0108 lr: 0.02\n",
      "iteration: 16020 loss: 0.0110 lr: 0.02\n",
      "iteration: 16030 loss: 0.0072 lr: 0.02\n",
      "iteration: 16040 loss: 0.0145 lr: 0.02\n",
      "iteration: 16050 loss: 0.0093 lr: 0.02\n",
      "iteration: 16060 loss: 0.0096 lr: 0.02\n",
      "iteration: 16070 loss: 0.0092 lr: 0.02\n",
      "iteration: 16080 loss: 0.0083 lr: 0.02\n",
      "iteration: 16090 loss: 0.0115 lr: 0.02\n",
      "iteration: 16100 loss: 0.0101 lr: 0.02\n",
      "iteration: 16110 loss: 0.0093 lr: 0.02\n",
      "iteration: 16120 loss: 0.0081 lr: 0.02\n",
      "iteration: 16130 loss: 0.0091 lr: 0.02\n",
      "iteration: 16140 loss: 0.0086 lr: 0.02\n",
      "iteration: 16150 loss: 0.0091 lr: 0.02\n",
      "iteration: 16160 loss: 0.0081 lr: 0.02\n",
      "iteration: 16170 loss: 0.0107 lr: 0.02\n",
      "iteration: 16180 loss: 0.0086 lr: 0.02\n",
      "iteration: 16190 loss: 0.0115 lr: 0.02\n",
      "iteration: 16200 loss: 0.0108 lr: 0.02\n",
      "iteration: 16210 loss: 0.0133 lr: 0.02\n",
      "iteration: 16220 loss: 0.0079 lr: 0.02\n",
      "iteration: 16230 loss: 0.0084 lr: 0.02\n",
      "iteration: 16240 loss: 0.0123 lr: 0.02\n",
      "iteration: 16250 loss: 0.0140 lr: 0.02\n",
      "iteration: 16260 loss: 0.0096 lr: 0.02\n",
      "iteration: 16270 loss: 0.0094 lr: 0.02\n",
      "iteration: 16280 loss: 0.0073 lr: 0.02\n",
      "iteration: 16290 loss: 0.0090 lr: 0.02\n",
      "iteration: 16300 loss: 0.0107 lr: 0.02\n",
      "iteration: 16310 loss: 0.0084 lr: 0.02\n",
      "iteration: 16320 loss: 0.0113 lr: 0.02\n",
      "iteration: 16330 loss: 0.0100 lr: 0.02\n",
      "iteration: 16340 loss: 0.0097 lr: 0.02\n",
      "iteration: 16350 loss: 0.0083 lr: 0.02\n",
      "iteration: 16360 loss: 0.0087 lr: 0.02\n",
      "iteration: 16370 loss: 0.0077 lr: 0.02\n",
      "iteration: 16380 loss: 0.0098 lr: 0.02\n",
      "iteration: 16390 loss: 0.0069 lr: 0.02\n",
      "iteration: 16400 loss: 0.0110 lr: 0.02\n",
      "iteration: 16410 loss: 0.0079 lr: 0.02\n",
      "iteration: 16420 loss: 0.0109 lr: 0.02\n",
      "iteration: 16430 loss: 0.0091 lr: 0.02\n",
      "iteration: 16440 loss: 0.0078 lr: 0.02\n",
      "iteration: 16450 loss: 0.0114 lr: 0.02\n",
      "iteration: 16460 loss: 0.0072 lr: 0.02\n",
      "iteration: 16470 loss: 0.0070 lr: 0.02\n",
      "iteration: 16480 loss: 0.0109 lr: 0.02\n",
      "iteration: 16490 loss: 0.0085 lr: 0.02\n",
      "iteration: 16500 loss: 0.0084 lr: 0.02\n",
      "iteration: 16510 loss: 0.0074 lr: 0.02\n",
      "iteration: 16520 loss: 0.0094 lr: 0.02\n",
      "iteration: 16530 loss: 0.0100 lr: 0.02\n",
      "iteration: 16540 loss: 0.0113 lr: 0.02\n",
      "iteration: 16550 loss: 0.0106 lr: 0.02\n",
      "iteration: 16560 loss: 0.0080 lr: 0.02\n",
      "iteration: 16570 loss: 0.0087 lr: 0.02\n",
      "iteration: 16580 loss: 0.0109 lr: 0.02\n",
      "iteration: 16590 loss: 0.0108 lr: 0.02\n",
      "iteration: 16600 loss: 0.0127 lr: 0.02\n",
      "iteration: 16610 loss: 0.0107 lr: 0.02\n",
      "iteration: 16620 loss: 0.0077 lr: 0.02\n",
      "iteration: 16630 loss: 0.0076 lr: 0.02\n",
      "iteration: 16640 loss: 0.0119 lr: 0.02\n",
      "iteration: 16650 loss: 0.0074 lr: 0.02\n",
      "iteration: 16660 loss: 0.0133 lr: 0.02\n",
      "iteration: 16670 loss: 0.0103 lr: 0.02\n",
      "iteration: 16680 loss: 0.0109 lr: 0.02\n",
      "iteration: 16690 loss: 0.0096 lr: 0.02\n",
      "iteration: 16700 loss: 0.0079 lr: 0.02\n",
      "iteration: 16710 loss: 0.0078 lr: 0.02\n",
      "iteration: 16720 loss: 0.0107 lr: 0.02\n",
      "iteration: 16730 loss: 0.0080 lr: 0.02\n",
      "iteration: 16740 loss: 0.0112 lr: 0.02\n",
      "iteration: 16750 loss: 0.0107 lr: 0.02\n",
      "iteration: 16760 loss: 0.0083 lr: 0.02\n",
      "iteration: 16770 loss: 0.0066 lr: 0.02\n",
      "iteration: 16780 loss: 0.0091 lr: 0.02\n",
      "iteration: 16790 loss: 0.0106 lr: 0.02\n",
      "iteration: 16800 loss: 0.0105 lr: 0.02\n",
      "iteration: 16810 loss: 0.0094 lr: 0.02\n",
      "iteration: 16820 loss: 0.0112 lr: 0.02\n",
      "iteration: 16830 loss: 0.0100 lr: 0.02\n",
      "iteration: 16840 loss: 0.0081 lr: 0.02\n",
      "iteration: 16850 loss: 0.0082 lr: 0.02\n",
      "iteration: 16860 loss: 0.0094 lr: 0.02\n",
      "iteration: 16870 loss: 0.0113 lr: 0.02\n",
      "iteration: 16880 loss: 0.0092 lr: 0.02\n",
      "iteration: 16890 loss: 0.0107 lr: 0.02\n",
      "iteration: 16900 loss: 0.0095 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 16910 loss: 0.0092 lr: 0.02\n",
      "iteration: 16920 loss: 0.0079 lr: 0.02\n",
      "iteration: 16930 loss: 0.0079 lr: 0.02\n",
      "iteration: 16940 loss: 0.0085 lr: 0.02\n",
      "iteration: 16950 loss: 0.0082 lr: 0.02\n",
      "iteration: 16960 loss: 0.0102 lr: 0.02\n",
      "iteration: 16970 loss: 0.0107 lr: 0.02\n",
      "iteration: 16980 loss: 0.0113 lr: 0.02\n",
      "iteration: 16990 loss: 0.0091 lr: 0.02\n",
      "iteration: 17000 loss: 0.0070 lr: 0.02\n",
      "iteration: 17010 loss: 0.0107 lr: 0.02\n",
      "iteration: 17020 loss: 0.0095 lr: 0.02\n",
      "iteration: 17030 loss: 0.0093 lr: 0.02\n",
      "iteration: 17040 loss: 0.0080 lr: 0.02\n",
      "iteration: 17050 loss: 0.0149 lr: 0.02\n",
      "iteration: 17060 loss: 0.0105 lr: 0.02\n",
      "iteration: 17070 loss: 0.0127 lr: 0.02\n",
      "iteration: 17080 loss: 0.0098 lr: 0.02\n",
      "iteration: 17090 loss: 0.0069 lr: 0.02\n",
      "iteration: 17100 loss: 0.0108 lr: 0.02\n",
      "iteration: 17110 loss: 0.0105 lr: 0.02\n",
      "iteration: 17120 loss: 0.0095 lr: 0.02\n",
      "iteration: 17130 loss: 0.0083 lr: 0.02\n",
      "iteration: 17140 loss: 0.0107 lr: 0.02\n",
      "iteration: 17150 loss: 0.0116 lr: 0.02\n",
      "iteration: 17160 loss: 0.0089 lr: 0.02\n",
      "iteration: 17170 loss: 0.0091 lr: 0.02\n",
      "iteration: 17180 loss: 0.0101 lr: 0.02\n",
      "iteration: 17190 loss: 0.0090 lr: 0.02\n",
      "iteration: 17200 loss: 0.0107 lr: 0.02\n",
      "iteration: 17210 loss: 0.0075 lr: 0.02\n",
      "iteration: 17220 loss: 0.0094 lr: 0.02\n",
      "iteration: 17230 loss: 0.0105 lr: 0.02\n",
      "iteration: 17240 loss: 0.0073 lr: 0.02\n",
      "iteration: 17250 loss: 0.0112 lr: 0.02\n",
      "iteration: 17260 loss: 0.0097 lr: 0.02\n",
      "iteration: 17270 loss: 0.0101 lr: 0.02\n",
      "iteration: 17280 loss: 0.0069 lr: 0.02\n",
      "iteration: 17290 loss: 0.0097 lr: 0.02\n",
      "iteration: 17300 loss: 0.0076 lr: 0.02\n",
      "iteration: 17310 loss: 0.0093 lr: 0.02\n",
      "iteration: 17320 loss: 0.0061 lr: 0.02\n",
      "iteration: 17330 loss: 0.0120 lr: 0.02\n",
      "iteration: 17340 loss: 0.0076 lr: 0.02\n",
      "iteration: 17350 loss: 0.0090 lr: 0.02\n",
      "iteration: 17360 loss: 0.0066 lr: 0.02\n",
      "iteration: 17370 loss: 0.0070 lr: 0.02\n",
      "iteration: 17380 loss: 0.0070 lr: 0.02\n",
      "iteration: 17390 loss: 0.0123 lr: 0.02\n",
      "iteration: 17400 loss: 0.0100 lr: 0.02\n",
      "iteration: 17410 loss: 0.0067 lr: 0.02\n",
      "iteration: 17420 loss: 0.0093 lr: 0.02\n",
      "iteration: 17430 loss: 0.0109 lr: 0.02\n",
      "iteration: 17440 loss: 0.0078 lr: 0.02\n",
      "iteration: 17450 loss: 0.0135 lr: 0.02\n",
      "iteration: 17460 loss: 0.0091 lr: 0.02\n",
      "iteration: 17470 loss: 0.0082 lr: 0.02\n",
      "iteration: 17480 loss: 0.0070 lr: 0.02\n",
      "iteration: 17490 loss: 0.0076 lr: 0.02\n",
      "iteration: 17500 loss: 0.0084 lr: 0.02\n",
      "iteration: 17510 loss: 0.0105 lr: 0.02\n",
      "iteration: 17520 loss: 0.0113 lr: 0.02\n",
      "iteration: 17530 loss: 0.0094 lr: 0.02\n",
      "iteration: 17540 loss: 0.0114 lr: 0.02\n",
      "iteration: 17550 loss: 0.0079 lr: 0.02\n",
      "iteration: 17560 loss: 0.0094 lr: 0.02\n",
      "iteration: 17570 loss: 0.0081 lr: 0.02\n",
      "iteration: 17580 loss: 0.0108 lr: 0.02\n",
      "iteration: 17590 loss: 0.0054 lr: 0.02\n",
      "iteration: 17600 loss: 0.0072 lr: 0.02\n",
      "iteration: 17610 loss: 0.0103 lr: 0.02\n",
      "iteration: 17620 loss: 0.0089 lr: 0.02\n",
      "iteration: 17630 loss: 0.0098 lr: 0.02\n",
      "iteration: 17640 loss: 0.0082 lr: 0.02\n",
      "iteration: 17650 loss: 0.0091 lr: 0.02\n",
      "iteration: 17660 loss: 0.0102 lr: 0.02\n",
      "iteration: 17670 loss: 0.0091 lr: 0.02\n",
      "iteration: 17680 loss: 0.0081 lr: 0.02\n",
      "iteration: 17690 loss: 0.0104 lr: 0.02\n",
      "iteration: 17700 loss: 0.0083 lr: 0.02\n",
      "iteration: 17710 loss: 0.0088 lr: 0.02\n",
      "iteration: 17720 loss: 0.0117 lr: 0.02\n",
      "iteration: 17730 loss: 0.0120 lr: 0.02\n",
      "iteration: 17740 loss: 0.0096 lr: 0.02\n",
      "iteration: 17750 loss: 0.0125 lr: 0.02\n",
      "iteration: 17760 loss: 0.0096 lr: 0.02\n",
      "iteration: 17770 loss: 0.0092 lr: 0.02\n",
      "iteration: 17780 loss: 0.0057 lr: 0.02\n",
      "iteration: 17790 loss: 0.0104 lr: 0.02\n",
      "iteration: 17800 loss: 0.0104 lr: 0.02\n",
      "iteration: 17810 loss: 0.0111 lr: 0.02\n",
      "iteration: 17820 loss: 0.0095 lr: 0.02\n",
      "iteration: 17830 loss: 0.0095 lr: 0.02\n",
      "iteration: 17840 loss: 0.0088 lr: 0.02\n",
      "iteration: 17850 loss: 0.0104 lr: 0.02\n",
      "iteration: 17860 loss: 0.0090 lr: 0.02\n",
      "iteration: 17870 loss: 0.0086 lr: 0.02\n",
      "iteration: 17880 loss: 0.0105 lr: 0.02\n",
      "iteration: 17890 loss: 0.0083 lr: 0.02\n",
      "iteration: 17900 loss: 0.0078 lr: 0.02\n",
      "iteration: 17910 loss: 0.0081 lr: 0.02\n",
      "iteration: 17920 loss: 0.0074 lr: 0.02\n",
      "iteration: 17930 loss: 0.0089 lr: 0.02\n",
      "iteration: 17940 loss: 0.0072 lr: 0.02\n",
      "iteration: 17950 loss: 0.0090 lr: 0.02\n",
      "iteration: 17960 loss: 0.0110 lr: 0.02\n",
      "iteration: 17970 loss: 0.0061 lr: 0.02\n",
      "iteration: 17980 loss: 0.0101 lr: 0.02\n",
      "iteration: 17990 loss: 0.0063 lr: 0.02\n",
      "iteration: 18000 loss: 0.0086 lr: 0.02\n",
      "iteration: 18010 loss: 0.0059 lr: 0.02\n",
      "iteration: 18020 loss: 0.0071 lr: 0.02\n",
      "iteration: 18030 loss: 0.0097 lr: 0.02\n",
      "iteration: 18040 loss: 0.0067 lr: 0.02\n",
      "iteration: 18050 loss: 0.0139 lr: 0.02\n",
      "iteration: 18060 loss: 0.0084 lr: 0.02\n",
      "iteration: 18070 loss: 0.0090 lr: 0.02\n",
      "iteration: 18080 loss: 0.0104 lr: 0.02\n",
      "iteration: 18090 loss: 0.0105 lr: 0.02\n",
      "iteration: 18100 loss: 0.0051 lr: 0.02\n",
      "iteration: 18110 loss: 0.0114 lr: 0.02\n",
      "iteration: 18120 loss: 0.0063 lr: 0.02\n",
      "iteration: 18130 loss: 0.0074 lr: 0.02\n",
      "iteration: 18140 loss: 0.0095 lr: 0.02\n",
      "iteration: 18150 loss: 0.0079 lr: 0.02\n",
      "iteration: 18160 loss: 0.0143 lr: 0.02\n",
      "iteration: 18170 loss: 0.0084 lr: 0.02\n",
      "iteration: 18180 loss: 0.0108 lr: 0.02\n",
      "iteration: 18190 loss: 0.0060 lr: 0.02\n",
      "iteration: 18200 loss: 0.0076 lr: 0.02\n",
      "iteration: 18210 loss: 0.0078 lr: 0.02\n",
      "iteration: 18220 loss: 0.0077 lr: 0.02\n",
      "iteration: 18230 loss: 0.0087 lr: 0.02\n",
      "iteration: 18240 loss: 0.0061 lr: 0.02\n",
      "iteration: 18250 loss: 0.0062 lr: 0.02\n",
      "iteration: 18260 loss: 0.0098 lr: 0.02\n",
      "iteration: 18270 loss: 0.0084 lr: 0.02\n",
      "iteration: 18280 loss: 0.0136 lr: 0.02\n",
      "iteration: 18290 loss: 0.0086 lr: 0.02\n",
      "iteration: 18300 loss: 0.0084 lr: 0.02\n",
      "iteration: 18310 loss: 0.0092 lr: 0.02\n",
      "iteration: 18320 loss: 0.0096 lr: 0.02\n",
      "iteration: 18330 loss: 0.0113 lr: 0.02\n",
      "iteration: 18340 loss: 0.0072 lr: 0.02\n",
      "iteration: 18350 loss: 0.0121 lr: 0.02\n",
      "iteration: 18360 loss: 0.0081 lr: 0.02\n",
      "iteration: 18370 loss: 0.0107 lr: 0.02\n",
      "iteration: 18380 loss: 0.0095 lr: 0.02\n",
      "iteration: 18390 loss: 0.0093 lr: 0.02\n",
      "iteration: 18400 loss: 0.0114 lr: 0.02\n",
      "iteration: 18410 loss: 0.0081 lr: 0.02\n",
      "iteration: 18420 loss: 0.0079 lr: 0.02\n",
      "iteration: 18430 loss: 0.0085 lr: 0.02\n",
      "iteration: 18440 loss: 0.0079 lr: 0.02\n",
      "iteration: 18450 loss: 0.0082 lr: 0.02\n",
      "iteration: 18460 loss: 0.0088 lr: 0.02\n",
      "iteration: 18470 loss: 0.0103 lr: 0.02\n",
      "iteration: 18480 loss: 0.0085 lr: 0.02\n",
      "iteration: 18490 loss: 0.0081 lr: 0.02\n",
      "iteration: 18500 loss: 0.0066 lr: 0.02\n",
      "iteration: 18510 loss: 0.0084 lr: 0.02\n",
      "iteration: 18520 loss: 0.0107 lr: 0.02\n",
      "iteration: 18530 loss: 0.0076 lr: 0.02\n",
      "iteration: 18540 loss: 0.0114 lr: 0.02\n",
      "iteration: 18550 loss: 0.0110 lr: 0.02\n",
      "iteration: 18560 loss: 0.0073 lr: 0.02\n",
      "iteration: 18570 loss: 0.0072 lr: 0.02\n",
      "iteration: 18580 loss: 0.0050 lr: 0.02\n",
      "iteration: 18590 loss: 0.0086 lr: 0.02\n",
      "iteration: 18600 loss: 0.0085 lr: 0.02\n",
      "iteration: 18610 loss: 0.0061 lr: 0.02\n",
      "iteration: 18620 loss: 0.0096 lr: 0.02\n",
      "iteration: 18630 loss: 0.0113 lr: 0.02\n",
      "iteration: 18640 loss: 0.0100 lr: 0.02\n",
      "iteration: 18650 loss: 0.0102 lr: 0.02\n",
      "iteration: 18660 loss: 0.0058 lr: 0.02\n",
      "iteration: 18670 loss: 0.0095 lr: 0.02\n",
      "iteration: 18680 loss: 0.0071 lr: 0.02\n",
      "iteration: 18690 loss: 0.0122 lr: 0.02\n",
      "iteration: 18700 loss: 0.0087 lr: 0.02\n",
      "iteration: 18710 loss: 0.0085 lr: 0.02\n",
      "iteration: 18720 loss: 0.0085 lr: 0.02\n",
      "iteration: 18730 loss: 0.0091 lr: 0.02\n",
      "iteration: 18740 loss: 0.0107 lr: 0.02\n",
      "iteration: 18750 loss: 0.0077 lr: 0.02\n",
      "iteration: 18760 loss: 0.0106 lr: 0.02\n",
      "iteration: 18770 loss: 0.0078 lr: 0.02\n",
      "iteration: 18780 loss: 0.0121 lr: 0.02\n",
      "iteration: 18790 loss: 0.0107 lr: 0.02\n",
      "iteration: 18800 loss: 0.0093 lr: 0.02\n",
      "iteration: 18810 loss: 0.0089 lr: 0.02\n",
      "iteration: 18820 loss: 0.0118 lr: 0.02\n",
      "iteration: 18830 loss: 0.0063 lr: 0.02\n",
      "iteration: 18840 loss: 0.0096 lr: 0.02\n",
      "iteration: 18850 loss: 0.0079 lr: 0.02\n",
      "iteration: 18860 loss: 0.0080 lr: 0.02\n",
      "iteration: 18870 loss: 0.0089 lr: 0.02\n",
      "iteration: 18880 loss: 0.0099 lr: 0.02\n",
      "iteration: 18890 loss: 0.0078 lr: 0.02\n",
      "iteration: 18900 loss: 0.0080 lr: 0.02\n",
      "iteration: 18910 loss: 0.0117 lr: 0.02\n",
      "iteration: 18920 loss: 0.0105 lr: 0.02\n",
      "iteration: 18930 loss: 0.0102 lr: 0.02\n",
      "iteration: 18940 loss: 0.0089 lr: 0.02\n",
      "iteration: 18950 loss: 0.0115 lr: 0.02\n",
      "iteration: 18960 loss: 0.0080 lr: 0.02\n",
      "iteration: 18970 loss: 0.0076 lr: 0.02\n",
      "iteration: 18980 loss: 0.0103 lr: 0.02\n",
      "iteration: 18990 loss: 0.0068 lr: 0.02\n",
      "iteration: 19000 loss: 0.0078 lr: 0.02\n",
      "iteration: 19010 loss: 0.0104 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 19020 loss: 0.0083 lr: 0.02\n",
      "iteration: 19030 loss: 0.0066 lr: 0.02\n",
      "iteration: 19040 loss: 0.0125 lr: 0.02\n",
      "iteration: 19050 loss: 0.0089 lr: 0.02\n",
      "iteration: 19060 loss: 0.0096 lr: 0.02\n",
      "iteration: 19070 loss: 0.0099 lr: 0.02\n",
      "iteration: 19080 loss: 0.0109 lr: 0.02\n",
      "iteration: 19090 loss: 0.0105 lr: 0.02\n",
      "iteration: 19100 loss: 0.0106 lr: 0.02\n",
      "iteration: 19110 loss: 0.0130 lr: 0.02\n",
      "iteration: 19120 loss: 0.0105 lr: 0.02\n",
      "iteration: 19130 loss: 0.0119 lr: 0.02\n",
      "iteration: 19140 loss: 0.0094 lr: 0.02\n",
      "iteration: 19150 loss: 0.0080 lr: 0.02\n",
      "iteration: 19160 loss: 0.0072 lr: 0.02\n",
      "iteration: 19170 loss: 0.0086 lr: 0.02\n",
      "iteration: 19180 loss: 0.0082 lr: 0.02\n",
      "iteration: 19190 loss: 0.0068 lr: 0.02\n",
      "iteration: 19200 loss: 0.0105 lr: 0.02\n",
      "iteration: 19210 loss: 0.0112 lr: 0.02\n",
      "iteration: 19220 loss: 0.0068 lr: 0.02\n",
      "iteration: 19230 loss: 0.0087 lr: 0.02\n",
      "iteration: 19240 loss: 0.0110 lr: 0.02\n",
      "iteration: 19250 loss: 0.0112 lr: 0.02\n",
      "iteration: 19260 loss: 0.0082 lr: 0.02\n",
      "iteration: 19270 loss: 0.0092 lr: 0.02\n",
      "iteration: 19280 loss: 0.0106 lr: 0.02\n",
      "iteration: 19290 loss: 0.0100 lr: 0.02\n",
      "iteration: 19300 loss: 0.0099 lr: 0.02\n",
      "iteration: 19310 loss: 0.0088 lr: 0.02\n",
      "iteration: 19320 loss: 0.0077 lr: 0.02\n",
      "iteration: 19330 loss: 0.0072 lr: 0.02\n",
      "iteration: 19340 loss: 0.0103 lr: 0.02\n",
      "iteration: 19350 loss: 0.0100 lr: 0.02\n",
      "iteration: 19360 loss: 0.0111 lr: 0.02\n",
      "iteration: 19370 loss: 0.0117 lr: 0.02\n",
      "iteration: 19380 loss: 0.0095 lr: 0.02\n",
      "iteration: 19390 loss: 0.0099 lr: 0.02\n",
      "iteration: 19400 loss: 0.0083 lr: 0.02\n",
      "iteration: 19410 loss: 0.0104 lr: 0.02\n",
      "iteration: 19420 loss: 0.0110 lr: 0.02\n",
      "iteration: 19430 loss: 0.0093 lr: 0.02\n",
      "iteration: 19440 loss: 0.0117 lr: 0.02\n",
      "iteration: 19450 loss: 0.0094 lr: 0.02\n",
      "iteration: 19460 loss: 0.0103 lr: 0.02\n",
      "iteration: 19470 loss: 0.0085 lr: 0.02\n",
      "iteration: 19480 loss: 0.0062 lr: 0.02\n",
      "iteration: 19490 loss: 0.0072 lr: 0.02\n",
      "iteration: 19500 loss: 0.0133 lr: 0.02\n",
      "iteration: 19510 loss: 0.0090 lr: 0.02\n",
      "iteration: 19520 loss: 0.0080 lr: 0.02\n",
      "iteration: 19530 loss: 0.0079 lr: 0.02\n",
      "iteration: 19540 loss: 0.0101 lr: 0.02\n",
      "iteration: 19550 loss: 0.0077 lr: 0.02\n",
      "iteration: 19560 loss: 0.0104 lr: 0.02\n",
      "iteration: 19570 loss: 0.0082 lr: 0.02\n",
      "iteration: 19580 loss: 0.0058 lr: 0.02\n",
      "iteration: 19590 loss: 0.0122 lr: 0.02\n",
      "iteration: 19600 loss: 0.0100 lr: 0.02\n",
      "iteration: 19610 loss: 0.0091 lr: 0.02\n",
      "iteration: 19620 loss: 0.0063 lr: 0.02\n",
      "iteration: 19630 loss: 0.0094 lr: 0.02\n",
      "iteration: 19640 loss: 0.0104 lr: 0.02\n",
      "iteration: 19650 loss: 0.0070 lr: 0.02\n",
      "iteration: 19660 loss: 0.0068 lr: 0.02\n",
      "iteration: 19670 loss: 0.0082 lr: 0.02\n",
      "iteration: 19680 loss: 0.0080 lr: 0.02\n",
      "iteration: 19690 loss: 0.0097 lr: 0.02\n",
      "iteration: 19700 loss: 0.0060 lr: 0.02\n",
      "iteration: 19710 loss: 0.0070 lr: 0.02\n",
      "iteration: 19720 loss: 0.0098 lr: 0.02\n",
      "iteration: 19730 loss: 0.0082 lr: 0.02\n",
      "iteration: 19740 loss: 0.0081 lr: 0.02\n",
      "iteration: 19750 loss: 0.0065 lr: 0.02\n",
      "iteration: 19760 loss: 0.0091 lr: 0.02\n",
      "iteration: 19770 loss: 0.0064 lr: 0.02\n",
      "iteration: 19780 loss: 0.0085 lr: 0.02\n",
      "iteration: 19790 loss: 0.0094 lr: 0.02\n",
      "iteration: 19800 loss: 0.0122 lr: 0.02\n",
      "iteration: 19810 loss: 0.0092 lr: 0.02\n",
      "iteration: 19820 loss: 0.0088 lr: 0.02\n",
      "iteration: 19830 loss: 0.0075 lr: 0.02\n",
      "iteration: 19840 loss: 0.0089 lr: 0.02\n",
      "iteration: 19850 loss: 0.0076 lr: 0.02\n",
      "iteration: 19860 loss: 0.0105 lr: 0.02\n",
      "iteration: 19870 loss: 0.0126 lr: 0.02\n",
      "iteration: 19880 loss: 0.0101 lr: 0.02\n",
      "iteration: 19890 loss: 0.0072 lr: 0.02\n",
      "iteration: 19900 loss: 0.0098 lr: 0.02\n",
      "iteration: 19910 loss: 0.0108 lr: 0.02\n",
      "iteration: 19920 loss: 0.0096 lr: 0.02\n",
      "iteration: 19930 loss: 0.0096 lr: 0.02\n",
      "iteration: 19940 loss: 0.0067 lr: 0.02\n",
      "iteration: 19950 loss: 0.0068 lr: 0.02\n",
      "iteration: 19960 loss: 0.0081 lr: 0.02\n",
      "iteration: 19970 loss: 0.0083 lr: 0.02\n",
      "iteration: 19980 loss: 0.0083 lr: 0.02\n",
      "iteration: 19990 loss: 0.0087 lr: 0.02\n",
      "iteration: 20000 loss: 0.0080 lr: 0.02\n",
      "iteration: 20010 loss: 0.0061 lr: 0.02\n",
      "iteration: 20020 loss: 0.0075 lr: 0.02\n",
      "iteration: 20030 loss: 0.0075 lr: 0.02\n",
      "iteration: 20040 loss: 0.0090 lr: 0.02\n",
      "iteration: 20050 loss: 0.0079 lr: 0.02\n",
      "iteration: 20060 loss: 0.0068 lr: 0.02\n",
      "iteration: 20070 loss: 0.0070 lr: 0.02\n",
      "iteration: 20080 loss: 0.0117 lr: 0.02\n",
      "iteration: 20090 loss: 0.0103 lr: 0.02\n",
      "iteration: 20100 loss: 0.0129 lr: 0.02\n",
      "iteration: 20110 loss: 0.0106 lr: 0.02\n",
      "iteration: 20120 loss: 0.0103 lr: 0.02\n",
      "iteration: 20130 loss: 0.0076 lr: 0.02\n",
      "iteration: 20140 loss: 0.0077 lr: 0.02\n",
      "iteration: 20150 loss: 0.0079 lr: 0.02\n",
      "iteration: 20160 loss: 0.0076 lr: 0.02\n",
      "iteration: 20170 loss: 0.0073 lr: 0.02\n",
      "iteration: 20180 loss: 0.0076 lr: 0.02\n",
      "iteration: 20190 loss: 0.0099 lr: 0.02\n",
      "iteration: 20200 loss: 0.0069 lr: 0.02\n",
      "iteration: 20210 loss: 0.0113 lr: 0.02\n",
      "iteration: 20220 loss: 0.0074 lr: 0.02\n",
      "iteration: 20230 loss: 0.0067 lr: 0.02\n",
      "iteration: 20240 loss: 0.0074 lr: 0.02\n",
      "iteration: 20250 loss: 0.0075 lr: 0.02\n",
      "iteration: 20260 loss: 0.0096 lr: 0.02\n",
      "iteration: 20270 loss: 0.0091 lr: 0.02\n",
      "iteration: 20280 loss: 0.0099 lr: 0.02\n",
      "iteration: 20290 loss: 0.0101 lr: 0.02\n",
      "iteration: 20300 loss: 0.0085 lr: 0.02\n",
      "iteration: 20310 loss: 0.0095 lr: 0.02\n",
      "iteration: 20320 loss: 0.0091 lr: 0.02\n",
      "iteration: 20330 loss: 0.0099 lr: 0.02\n",
      "iteration: 20340 loss: 0.0099 lr: 0.02\n",
      "iteration: 20350 loss: 0.0069 lr: 0.02\n",
      "iteration: 20360 loss: 0.0082 lr: 0.02\n",
      "iteration: 20370 loss: 0.0100 lr: 0.02\n",
      "iteration: 20380 loss: 0.0119 lr: 0.02\n",
      "iteration: 20390 loss: 0.0093 lr: 0.02\n",
      "iteration: 20400 loss: 0.0069 lr: 0.02\n",
      "iteration: 20410 loss: 0.0103 lr: 0.02\n",
      "iteration: 20420 loss: 0.0089 lr: 0.02\n",
      "iteration: 20430 loss: 0.0059 lr: 0.02\n",
      "iteration: 20440 loss: 0.0076 lr: 0.02\n",
      "iteration: 20450 loss: 0.0115 lr: 0.02\n",
      "iteration: 20460 loss: 0.0072 lr: 0.02\n",
      "iteration: 20470 loss: 0.0074 lr: 0.02\n",
      "iteration: 20480 loss: 0.0102 lr: 0.02\n",
      "iteration: 20490 loss: 0.0084 lr: 0.02\n",
      "iteration: 20500 loss: 0.0080 lr: 0.02\n",
      "iteration: 20510 loss: 0.0074 lr: 0.02\n",
      "iteration: 20520 loss: 0.0098 lr: 0.02\n",
      "iteration: 20530 loss: 0.0102 lr: 0.02\n",
      "iteration: 20540 loss: 0.0101 lr: 0.02\n",
      "iteration: 20550 loss: 0.0110 lr: 0.02\n",
      "iteration: 20560 loss: 0.0078 lr: 0.02\n",
      "iteration: 20570 loss: 0.0080 lr: 0.02\n",
      "iteration: 20580 loss: 0.0099 lr: 0.02\n",
      "iteration: 20590 loss: 0.0101 lr: 0.02\n",
      "iteration: 20600 loss: 0.0093 lr: 0.02\n",
      "iteration: 20610 loss: 0.0106 lr: 0.02\n",
      "iteration: 20620 loss: 0.0083 lr: 0.02\n",
      "iteration: 20630 loss: 0.0067 lr: 0.02\n",
      "iteration: 20640 loss: 0.0079 lr: 0.02\n",
      "iteration: 20650 loss: 0.0088 lr: 0.02\n",
      "iteration: 20660 loss: 0.0087 lr: 0.02\n",
      "iteration: 20670 loss: 0.0082 lr: 0.02\n",
      "iteration: 20680 loss: 0.0093 lr: 0.02\n",
      "iteration: 20690 loss: 0.0074 lr: 0.02\n",
      "iteration: 20700 loss: 0.0080 lr: 0.02\n",
      "iteration: 20710 loss: 0.0069 lr: 0.02\n",
      "iteration: 20720 loss: 0.0061 lr: 0.02\n",
      "iteration: 20730 loss: 0.0110 lr: 0.02\n",
      "iteration: 20740 loss: 0.0080 lr: 0.02\n",
      "iteration: 20750 loss: 0.0076 lr: 0.02\n",
      "iteration: 20760 loss: 0.0077 lr: 0.02\n",
      "iteration: 20770 loss: 0.0075 lr: 0.02\n",
      "iteration: 20780 loss: 0.0079 lr: 0.02\n",
      "iteration: 20790 loss: 0.0092 lr: 0.02\n",
      "iteration: 20800 loss: 0.0083 lr: 0.02\n",
      "iteration: 20810 loss: 0.0075 lr: 0.02\n",
      "iteration: 20820 loss: 0.0104 lr: 0.02\n",
      "iteration: 20830 loss: 0.0107 lr: 0.02\n",
      "iteration: 20840 loss: 0.0091 lr: 0.02\n",
      "iteration: 20850 loss: 0.0107 lr: 0.02\n",
      "iteration: 20860 loss: 0.0079 lr: 0.02\n",
      "iteration: 20870 loss: 0.0083 lr: 0.02\n",
      "iteration: 20880 loss: 0.0090 lr: 0.02\n",
      "iteration: 20890 loss: 0.0085 lr: 0.02\n",
      "iteration: 20900 loss: 0.0073 lr: 0.02\n",
      "iteration: 20910 loss: 0.0057 lr: 0.02\n",
      "iteration: 20920 loss: 0.0077 lr: 0.02\n",
      "iteration: 20930 loss: 0.0062 lr: 0.02\n",
      "iteration: 20940 loss: 0.0085 lr: 0.02\n",
      "iteration: 20950 loss: 0.0103 lr: 0.02\n",
      "iteration: 20960 loss: 0.0085 lr: 0.02\n",
      "iteration: 20970 loss: 0.0086 lr: 0.02\n",
      "iteration: 20980 loss: 0.0091 lr: 0.02\n",
      "iteration: 20990 loss: 0.0102 lr: 0.02\n",
      "iteration: 21000 loss: 0.0064 lr: 0.02\n",
      "iteration: 21010 loss: 0.0086 lr: 0.02\n",
      "iteration: 21020 loss: 0.0077 lr: 0.02\n",
      "iteration: 21030 loss: 0.0097 lr: 0.02\n",
      "iteration: 21040 loss: 0.0056 lr: 0.02\n",
      "iteration: 21050 loss: 0.0064 lr: 0.02\n",
      "iteration: 21060 loss: 0.0090 lr: 0.02\n",
      "iteration: 21070 loss: 0.0108 lr: 0.02\n",
      "iteration: 21080 loss: 0.0081 lr: 0.02\n",
      "iteration: 21090 loss: 0.0119 lr: 0.02\n",
      "iteration: 21100 loss: 0.0100 lr: 0.02\n",
      "iteration: 21110 loss: 0.0071 lr: 0.02\n",
      "iteration: 21120 loss: 0.0079 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 21130 loss: 0.0090 lr: 0.02\n",
      "iteration: 21140 loss: 0.0079 lr: 0.02\n",
      "iteration: 21150 loss: 0.0086 lr: 0.02\n",
      "iteration: 21160 loss: 0.0072 lr: 0.02\n",
      "iteration: 21170 loss: 0.0093 lr: 0.02\n",
      "iteration: 21180 loss: 0.0157 lr: 0.02\n",
      "iteration: 21190 loss: 0.0101 lr: 0.02\n",
      "iteration: 21200 loss: 0.0081 lr: 0.02\n",
      "iteration: 21210 loss: 0.0063 lr: 0.02\n",
      "iteration: 21220 loss: 0.0079 lr: 0.02\n",
      "iteration: 21230 loss: 0.0056 lr: 0.02\n",
      "iteration: 21240 loss: 0.0069 lr: 0.02\n",
      "iteration: 21250 loss: 0.0074 lr: 0.02\n",
      "iteration: 21260 loss: 0.0077 lr: 0.02\n",
      "iteration: 21270 loss: 0.0095 lr: 0.02\n",
      "iteration: 21280 loss: 0.0099 lr: 0.02\n",
      "iteration: 21290 loss: 0.0079 lr: 0.02\n",
      "iteration: 21300 loss: 0.0065 lr: 0.02\n",
      "iteration: 21310 loss: 0.0090 lr: 0.02\n",
      "iteration: 21320 loss: 0.0106 lr: 0.02\n",
      "iteration: 21330 loss: 0.0089 lr: 0.02\n",
      "iteration: 21340 loss: 0.0057 lr: 0.02\n",
      "iteration: 21350 loss: 0.0075 lr: 0.02\n",
      "iteration: 21360 loss: 0.0081 lr: 0.02\n",
      "iteration: 21370 loss: 0.0077 lr: 0.02\n",
      "iteration: 21380 loss: 0.0082 lr: 0.02\n",
      "iteration: 21390 loss: 0.0060 lr: 0.02\n",
      "iteration: 21400 loss: 0.0081 lr: 0.02\n",
      "iteration: 21410 loss: 0.0128 lr: 0.02\n",
      "iteration: 21420 loss: 0.0062 lr: 0.02\n",
      "iteration: 21430 loss: 0.0079 lr: 0.02\n",
      "iteration: 21440 loss: 0.0070 lr: 0.02\n",
      "iteration: 21450 loss: 0.0098 lr: 0.02\n",
      "iteration: 21460 loss: 0.0083 lr: 0.02\n",
      "iteration: 21470 loss: 0.0098 lr: 0.02\n",
      "iteration: 21480 loss: 0.0092 lr: 0.02\n",
      "iteration: 21490 loss: 0.0080 lr: 0.02\n",
      "iteration: 21500 loss: 0.0071 lr: 0.02\n",
      "iteration: 21510 loss: 0.0071 lr: 0.02\n",
      "iteration: 21520 loss: 0.0086 lr: 0.02\n",
      "iteration: 21530 loss: 0.0094 lr: 0.02\n",
      "iteration: 21540 loss: 0.0075 lr: 0.02\n",
      "iteration: 21550 loss: 0.0100 lr: 0.02\n",
      "iteration: 21560 loss: 0.0075 lr: 0.02\n",
      "iteration: 21570 loss: 0.0105 lr: 0.02\n",
      "iteration: 21580 loss: 0.0087 lr: 0.02\n",
      "iteration: 21590 loss: 0.0066 lr: 0.02\n",
      "iteration: 21600 loss: 0.0075 lr: 0.02\n",
      "iteration: 21610 loss: 0.0079 lr: 0.02\n",
      "iteration: 21620 loss: 0.0072 lr: 0.02\n",
      "iteration: 21630 loss: 0.0072 lr: 0.02\n",
      "iteration: 21640 loss: 0.0065 lr: 0.02\n",
      "iteration: 21650 loss: 0.0080 lr: 0.02\n",
      "iteration: 21660 loss: 0.0057 lr: 0.02\n",
      "iteration: 21670 loss: 0.0078 lr: 0.02\n",
      "iteration: 21680 loss: 0.0104 lr: 0.02\n",
      "iteration: 21690 loss: 0.0102 lr: 0.02\n",
      "iteration: 21700 loss: 0.0074 lr: 0.02\n",
      "iteration: 21710 loss: 0.0093 lr: 0.02\n",
      "iteration: 21720 loss: 0.0086 lr: 0.02\n",
      "iteration: 21730 loss: 0.0083 lr: 0.02\n",
      "iteration: 21740 loss: 0.0053 lr: 0.02\n",
      "iteration: 21750 loss: 0.0111 lr: 0.02\n",
      "iteration: 21760 loss: 0.0061 lr: 0.02\n",
      "iteration: 21770 loss: 0.0069 lr: 0.02\n",
      "iteration: 21780 loss: 0.0067 lr: 0.02\n",
      "iteration: 21790 loss: 0.0098 lr: 0.02\n",
      "iteration: 21800 loss: 0.0090 lr: 0.02\n",
      "iteration: 21810 loss: 0.0081 lr: 0.02\n",
      "iteration: 21820 loss: 0.0067 lr: 0.02\n",
      "iteration: 21830 loss: 0.0066 lr: 0.02\n",
      "iteration: 21840 loss: 0.0096 lr: 0.02\n",
      "iteration: 21850 loss: 0.0097 lr: 0.02\n",
      "iteration: 21860 loss: 0.0090 lr: 0.02\n",
      "iteration: 21870 loss: 0.0070 lr: 0.02\n",
      "iteration: 21880 loss: 0.0096 lr: 0.02\n",
      "iteration: 21890 loss: 0.0100 lr: 0.02\n",
      "iteration: 21900 loss: 0.0078 lr: 0.02\n",
      "iteration: 21910 loss: 0.0067 lr: 0.02\n",
      "iteration: 21920 loss: 0.0068 lr: 0.02\n",
      "iteration: 21930 loss: 0.0111 lr: 0.02\n",
      "iteration: 21940 loss: 0.0062 lr: 0.02\n",
      "iteration: 21950 loss: 0.0078 lr: 0.02\n",
      "iteration: 21960 loss: 0.0080 lr: 0.02\n",
      "iteration: 21970 loss: 0.0100 lr: 0.02\n",
      "iteration: 21980 loss: 0.0066 lr: 0.02\n",
      "iteration: 21990 loss: 0.0072 lr: 0.02\n",
      "iteration: 22000 loss: 0.0085 lr: 0.02\n",
      "iteration: 22010 loss: 0.0120 lr: 0.02\n",
      "iteration: 22020 loss: 0.0072 lr: 0.02\n",
      "iteration: 22030 loss: 0.0088 lr: 0.02\n",
      "iteration: 22040 loss: 0.0057 lr: 0.02\n",
      "iteration: 22050 loss: 0.0114 lr: 0.02\n",
      "iteration: 22060 loss: 0.0087 lr: 0.02\n",
      "iteration: 22070 loss: 0.0090 lr: 0.02\n",
      "iteration: 22080 loss: 0.0053 lr: 0.02\n",
      "iteration: 22090 loss: 0.0080 lr: 0.02\n",
      "iteration: 22100 loss: 0.0085 lr: 0.02\n",
      "iteration: 22110 loss: 0.0063 lr: 0.02\n",
      "iteration: 22120 loss: 0.0081 lr: 0.02\n",
      "iteration: 22130 loss: 0.0062 lr: 0.02\n",
      "iteration: 22140 loss: 0.0088 lr: 0.02\n",
      "iteration: 22150 loss: 0.0069 lr: 0.02\n",
      "iteration: 22160 loss: 0.0059 lr: 0.02\n",
      "iteration: 22170 loss: 0.0084 lr: 0.02\n",
      "iteration: 22180 loss: 0.0084 lr: 0.02\n",
      "iteration: 22190 loss: 0.0091 lr: 0.02\n",
      "iteration: 22200 loss: 0.0088 lr: 0.02\n",
      "iteration: 22210 loss: 0.0089 lr: 0.02\n",
      "iteration: 22220 loss: 0.0084 lr: 0.02\n",
      "iteration: 22230 loss: 0.0082 lr: 0.02\n",
      "iteration: 22240 loss: 0.0049 lr: 0.02\n",
      "iteration: 22250 loss: 0.0079 lr: 0.02\n",
      "iteration: 22260 loss: 0.0097 lr: 0.02\n",
      "iteration: 22270 loss: 0.0061 lr: 0.02\n",
      "iteration: 22280 loss: 0.0075 lr: 0.02\n",
      "iteration: 22290 loss: 0.0096 lr: 0.02\n",
      "iteration: 22300 loss: 0.0067 lr: 0.02\n",
      "iteration: 22310 loss: 0.0071 lr: 0.02\n",
      "iteration: 22320 loss: 0.0083 lr: 0.02\n",
      "iteration: 22330 loss: 0.0097 lr: 0.02\n",
      "iteration: 22340 loss: 0.0097 lr: 0.02\n",
      "iteration: 22350 loss: 0.0081 lr: 0.02\n",
      "iteration: 22360 loss: 0.0100 lr: 0.02\n",
      "iteration: 22370 loss: 0.0075 lr: 0.02\n",
      "iteration: 22380 loss: 0.0087 lr: 0.02\n",
      "iteration: 22390 loss: 0.0096 lr: 0.02\n",
      "iteration: 22400 loss: 0.0085 lr: 0.02\n",
      "iteration: 22410 loss: 0.0081 lr: 0.02\n",
      "iteration: 22420 loss: 0.0086 lr: 0.02\n",
      "iteration: 22430 loss: 0.0105 lr: 0.02\n",
      "iteration: 22440 loss: 0.0055 lr: 0.02\n",
      "iteration: 22450 loss: 0.0060 lr: 0.02\n",
      "iteration: 22460 loss: 0.0113 lr: 0.02\n",
      "iteration: 22470 loss: 0.0102 lr: 0.02\n",
      "iteration: 22480 loss: 0.0100 lr: 0.02\n",
      "iteration: 22490 loss: 0.0086 lr: 0.02\n",
      "iteration: 22500 loss: 0.0074 lr: 0.02\n",
      "iteration: 22510 loss: 0.0080 lr: 0.02\n",
      "iteration: 22520 loss: 0.0089 lr: 0.02\n",
      "iteration: 22530 loss: 0.0078 lr: 0.02\n",
      "iteration: 22540 loss: 0.0082 lr: 0.02\n",
      "iteration: 22550 loss: 0.0116 lr: 0.02\n",
      "iteration: 22560 loss: 0.0098 lr: 0.02\n",
      "iteration: 22570 loss: 0.0073 lr: 0.02\n",
      "iteration: 22580 loss: 0.0087 lr: 0.02\n",
      "iteration: 22590 loss: 0.0061 lr: 0.02\n",
      "iteration: 22600 loss: 0.0073 lr: 0.02\n",
      "iteration: 22610 loss: 0.0091 lr: 0.02\n",
      "iteration: 22620 loss: 0.0072 lr: 0.02\n",
      "iteration: 22630 loss: 0.0075 lr: 0.02\n",
      "iteration: 22640 loss: 0.0078 lr: 0.02\n",
      "iteration: 22650 loss: 0.0104 lr: 0.02\n",
      "iteration: 22660 loss: 0.0076 lr: 0.02\n",
      "iteration: 22670 loss: 0.0078 lr: 0.02\n",
      "iteration: 22680 loss: 0.0120 lr: 0.02\n",
      "iteration: 22690 loss: 0.0073 lr: 0.02\n",
      "iteration: 22700 loss: 0.0096 lr: 0.02\n",
      "iteration: 22710 loss: 0.0095 lr: 0.02\n",
      "iteration: 22720 loss: 0.0081 lr: 0.02\n",
      "iteration: 22730 loss: 0.0086 lr: 0.02\n",
      "iteration: 22740 loss: 0.0096 lr: 0.02\n",
      "iteration: 22750 loss: 0.0107 lr: 0.02\n",
      "iteration: 22760 loss: 0.0060 lr: 0.02\n",
      "iteration: 22770 loss: 0.0105 lr: 0.02\n",
      "iteration: 22780 loss: 0.0073 lr: 0.02\n",
      "iteration: 22790 loss: 0.0081 lr: 0.02\n",
      "iteration: 22800 loss: 0.0094 lr: 0.02\n",
      "iteration: 22810 loss: 0.0089 lr: 0.02\n",
      "iteration: 22820 loss: 0.0081 lr: 0.02\n",
      "iteration: 22830 loss: 0.0082 lr: 0.02\n",
      "iteration: 22840 loss: 0.0065 lr: 0.02\n",
      "iteration: 22850 loss: 0.0073 lr: 0.02\n",
      "iteration: 22860 loss: 0.0062 lr: 0.02\n",
      "iteration: 22870 loss: 0.0046 lr: 0.02\n",
      "iteration: 22880 loss: 0.0114 lr: 0.02\n",
      "iteration: 22890 loss: 0.0095 lr: 0.02\n",
      "iteration: 22900 loss: 0.0076 lr: 0.02\n",
      "iteration: 22910 loss: 0.0086 lr: 0.02\n",
      "iteration: 22920 loss: 0.0075 lr: 0.02\n",
      "iteration: 22930 loss: 0.0107 lr: 0.02\n",
      "iteration: 22940 loss: 0.0100 lr: 0.02\n",
      "iteration: 22950 loss: 0.0102 lr: 0.02\n",
      "iteration: 22960 loss: 0.0081 lr: 0.02\n",
      "iteration: 22970 loss: 0.0072 lr: 0.02\n",
      "iteration: 22980 loss: 0.0052 lr: 0.02\n",
      "iteration: 22990 loss: 0.0076 lr: 0.02\n",
      "iteration: 23000 loss: 0.0062 lr: 0.02\n",
      "iteration: 23010 loss: 0.0084 lr: 0.02\n",
      "iteration: 23020 loss: 0.0081 lr: 0.02\n",
      "iteration: 23030 loss: 0.0061 lr: 0.02\n",
      "iteration: 23040 loss: 0.0075 lr: 0.02\n",
      "iteration: 23050 loss: 0.0083 lr: 0.02\n",
      "iteration: 23060 loss: 0.0112 lr: 0.02\n",
      "iteration: 23070 loss: 0.0080 lr: 0.02\n",
      "iteration: 23080 loss: 0.0090 lr: 0.02\n",
      "iteration: 23090 loss: 0.0084 lr: 0.02\n",
      "iteration: 23100 loss: 0.0094 lr: 0.02\n",
      "iteration: 23110 loss: 0.0103 lr: 0.02\n",
      "iteration: 23120 loss: 0.0081 lr: 0.02\n",
      "iteration: 23130 loss: 0.0056 lr: 0.02\n",
      "iteration: 23140 loss: 0.0087 lr: 0.02\n",
      "iteration: 23150 loss: 0.0078 lr: 0.02\n",
      "iteration: 23160 loss: 0.0123 lr: 0.02\n",
      "iteration: 23170 loss: 0.0066 lr: 0.02\n",
      "iteration: 23180 loss: 0.0080 lr: 0.02\n",
      "iteration: 23190 loss: 0.0079 lr: 0.02\n",
      "iteration: 23200 loss: 0.0123 lr: 0.02\n",
      "iteration: 23210 loss: 0.0123 lr: 0.02\n",
      "iteration: 23220 loss: 0.0078 lr: 0.02\n",
      "iteration: 23230 loss: 0.0104 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 23240 loss: 0.0088 lr: 0.02\n",
      "iteration: 23250 loss: 0.0101 lr: 0.02\n",
      "iteration: 23260 loss: 0.0077 lr: 0.02\n",
      "iteration: 23270 loss: 0.0083 lr: 0.02\n",
      "iteration: 23280 loss: 0.0066 lr: 0.02\n",
      "iteration: 23290 loss: 0.0066 lr: 0.02\n",
      "iteration: 23300 loss: 0.0098 lr: 0.02\n",
      "iteration: 23310 loss: 0.0055 lr: 0.02\n",
      "iteration: 23320 loss: 0.0089 lr: 0.02\n",
      "iteration: 23330 loss: 0.0092 lr: 0.02\n",
      "iteration: 23340 loss: 0.0138 lr: 0.02\n",
      "iteration: 23350 loss: 0.0101 lr: 0.02\n",
      "iteration: 23360 loss: 0.0067 lr: 0.02\n",
      "iteration: 23370 loss: 0.0071 lr: 0.02\n",
      "iteration: 23380 loss: 0.0112 lr: 0.02\n",
      "iteration: 23390 loss: 0.0081 lr: 0.02\n",
      "iteration: 23400 loss: 0.0071 lr: 0.02\n",
      "iteration: 23410 loss: 0.0092 lr: 0.02\n",
      "iteration: 23420 loss: 0.0102 lr: 0.02\n",
      "iteration: 23430 loss: 0.0106 lr: 0.02\n",
      "iteration: 23440 loss: 0.0086 lr: 0.02\n",
      "iteration: 23450 loss: 0.0058 lr: 0.02\n",
      "iteration: 23460 loss: 0.0073 lr: 0.02\n",
      "iteration: 23470 loss: 0.0091 lr: 0.02\n",
      "iteration: 23480 loss: 0.0070 lr: 0.02\n",
      "iteration: 23490 loss: 0.0082 lr: 0.02\n",
      "iteration: 23500 loss: 0.0088 lr: 0.02\n",
      "iteration: 23510 loss: 0.0070 lr: 0.02\n",
      "iteration: 23520 loss: 0.0076 lr: 0.02\n",
      "iteration: 23530 loss: 0.0059 lr: 0.02\n",
      "iteration: 23540 loss: 0.0069 lr: 0.02\n",
      "iteration: 23550 loss: 0.0074 lr: 0.02\n",
      "iteration: 23560 loss: 0.0081 lr: 0.02\n",
      "iteration: 23570 loss: 0.0083 lr: 0.02\n",
      "iteration: 23580 loss: 0.0071 lr: 0.02\n",
      "iteration: 23590 loss: 0.0084 lr: 0.02\n",
      "iteration: 23600 loss: 0.0082 lr: 0.02\n",
      "iteration: 23610 loss: 0.0076 lr: 0.02\n",
      "iteration: 23620 loss: 0.0122 lr: 0.02\n",
      "iteration: 23630 loss: 0.0065 lr: 0.02\n",
      "iteration: 23640 loss: 0.0098 lr: 0.02\n",
      "iteration: 23650 loss: 0.0105 lr: 0.02\n",
      "iteration: 23660 loss: 0.0078 lr: 0.02\n",
      "iteration: 23670 loss: 0.0077 lr: 0.02\n",
      "iteration: 23680 loss: 0.0079 lr: 0.02\n",
      "iteration: 23690 loss: 0.0050 lr: 0.02\n",
      "iteration: 23700 loss: 0.0076 lr: 0.02\n",
      "iteration: 23710 loss: 0.0082 lr: 0.02\n",
      "iteration: 23720 loss: 0.0051 lr: 0.02\n",
      "iteration: 23730 loss: 0.0067 lr: 0.02\n",
      "iteration: 23740 loss: 0.0086 lr: 0.02\n",
      "iteration: 23750 loss: 0.0070 lr: 0.02\n",
      "iteration: 23760 loss: 0.0079 lr: 0.02\n",
      "iteration: 23770 loss: 0.0103 lr: 0.02\n",
      "iteration: 23780 loss: 0.0073 lr: 0.02\n",
      "iteration: 23790 loss: 0.0088 lr: 0.02\n",
      "iteration: 23800 loss: 0.0096 lr: 0.02\n",
      "iteration: 23810 loss: 0.0093 lr: 0.02\n",
      "iteration: 23820 loss: 0.0104 lr: 0.02\n",
      "iteration: 23830 loss: 0.0069 lr: 0.02\n",
      "iteration: 23840 loss: 0.0084 lr: 0.02\n",
      "iteration: 23850 loss: 0.0068 lr: 0.02\n",
      "iteration: 23860 loss: 0.0067 lr: 0.02\n",
      "iteration: 23870 loss: 0.0059 lr: 0.02\n",
      "iteration: 23880 loss: 0.0073 lr: 0.02\n",
      "iteration: 23890 loss: 0.0095 lr: 0.02\n",
      "iteration: 23900 loss: 0.0074 lr: 0.02\n",
      "iteration: 23910 loss: 0.0103 lr: 0.02\n",
      "iteration: 23920 loss: 0.0082 lr: 0.02\n",
      "iteration: 23930 loss: 0.0084 lr: 0.02\n",
      "iteration: 23940 loss: 0.0100 lr: 0.02\n",
      "iteration: 23950 loss: 0.0084 lr: 0.02\n",
      "iteration: 23960 loss: 0.0052 lr: 0.02\n",
      "iteration: 23970 loss: 0.0100 lr: 0.02\n",
      "iteration: 23980 loss: 0.0085 lr: 0.02\n",
      "iteration: 23990 loss: 0.0078 lr: 0.02\n",
      "iteration: 24000 loss: 0.0113 lr: 0.02\n",
      "iteration: 24010 loss: 0.0111 lr: 0.02\n",
      "iteration: 24020 loss: 0.0075 lr: 0.02\n",
      "iteration: 24030 loss: 0.0081 lr: 0.02\n",
      "iteration: 24040 loss: 0.0106 lr: 0.02\n",
      "iteration: 24050 loss: 0.0077 lr: 0.02\n",
      "iteration: 24060 loss: 0.0075 lr: 0.02\n",
      "iteration: 24070 loss: 0.0064 lr: 0.02\n",
      "iteration: 24080 loss: 0.0074 lr: 0.02\n",
      "iteration: 24090 loss: 0.0063 lr: 0.02\n",
      "iteration: 24100 loss: 0.0067 lr: 0.02\n",
      "iteration: 24110 loss: 0.0079 lr: 0.02\n",
      "iteration: 24120 loss: 0.0060 lr: 0.02\n",
      "iteration: 24130 loss: 0.0052 lr: 0.02\n",
      "iteration: 24140 loss: 0.0088 lr: 0.02\n",
      "iteration: 24150 loss: 0.0076 lr: 0.02\n",
      "iteration: 24160 loss: 0.0079 lr: 0.02\n",
      "iteration: 24170 loss: 0.0068 lr: 0.02\n",
      "iteration: 24180 loss: 0.0054 lr: 0.02\n",
      "iteration: 24190 loss: 0.0077 lr: 0.02\n",
      "iteration: 24200 loss: 0.0108 lr: 0.02\n",
      "iteration: 24210 loss: 0.0088 lr: 0.02\n",
      "iteration: 24220 loss: 0.0080 lr: 0.02\n",
      "iteration: 24230 loss: 0.0091 lr: 0.02\n",
      "iteration: 24240 loss: 0.0071 lr: 0.02\n",
      "iteration: 24250 loss: 0.0068 lr: 0.02\n",
      "iteration: 24260 loss: 0.0105 lr: 0.02\n",
      "iteration: 24270 loss: 0.0085 lr: 0.02\n",
      "iteration: 24280 loss: 0.0061 lr: 0.02\n",
      "iteration: 24290 loss: 0.0103 lr: 0.02\n",
      "iteration: 24300 loss: 0.0063 lr: 0.02\n",
      "iteration: 24310 loss: 0.0073 lr: 0.02\n",
      "iteration: 24320 loss: 0.0069 lr: 0.02\n",
      "iteration: 24330 loss: 0.0096 lr: 0.02\n",
      "iteration: 24340 loss: 0.0061 lr: 0.02\n",
      "iteration: 24350 loss: 0.0084 lr: 0.02\n",
      "iteration: 24360 loss: 0.0073 lr: 0.02\n",
      "iteration: 24370 loss: 0.0101 lr: 0.02\n",
      "iteration: 24380 loss: 0.0073 lr: 0.02\n",
      "iteration: 24390 loss: 0.0055 lr: 0.02\n",
      "iteration: 24400 loss: 0.0089 lr: 0.02\n",
      "iteration: 24410 loss: 0.0063 lr: 0.02\n",
      "iteration: 24420 loss: 0.0064 lr: 0.02\n",
      "iteration: 24430 loss: 0.0087 lr: 0.02\n",
      "iteration: 24440 loss: 0.0083 lr: 0.02\n",
      "iteration: 24450 loss: 0.0097 lr: 0.02\n",
      "iteration: 24460 loss: 0.0100 lr: 0.02\n",
      "iteration: 24470 loss: 0.0093 lr: 0.02\n",
      "iteration: 24480 loss: 0.0076 lr: 0.02\n",
      "iteration: 24490 loss: 0.0061 lr: 0.02\n",
      "iteration: 24500 loss: 0.0081 lr: 0.02\n",
      "iteration: 24510 loss: 0.0087 lr: 0.02\n",
      "iteration: 24520 loss: 0.0091 lr: 0.02\n",
      "iteration: 24530 loss: 0.0093 lr: 0.02\n",
      "iteration: 24540 loss: 0.0084 lr: 0.02\n",
      "iteration: 24550 loss: 0.0068 lr: 0.02\n",
      "iteration: 24560 loss: 0.0084 lr: 0.02\n",
      "iteration: 24570 loss: 0.0075 lr: 0.02\n",
      "iteration: 24580 loss: 0.0084 lr: 0.02\n",
      "iteration: 24590 loss: 0.0096 lr: 0.02\n",
      "iteration: 24600 loss: 0.0057 lr: 0.02\n",
      "iteration: 24610 loss: 0.0063 lr: 0.02\n",
      "iteration: 24620 loss: 0.0083 lr: 0.02\n",
      "iteration: 24630 loss: 0.0116 lr: 0.02\n",
      "iteration: 24640 loss: 0.0085 lr: 0.02\n",
      "iteration: 24650 loss: 0.0077 lr: 0.02\n",
      "iteration: 24660 loss: 0.0074 lr: 0.02\n",
      "iteration: 24670 loss: 0.0068 lr: 0.02\n",
      "iteration: 24680 loss: 0.0074 lr: 0.02\n",
      "iteration: 24690 loss: 0.0082 lr: 0.02\n",
      "iteration: 24700 loss: 0.0093 lr: 0.02\n",
      "iteration: 24710 loss: 0.0072 lr: 0.02\n",
      "iteration: 24720 loss: 0.0061 lr: 0.02\n",
      "iteration: 24730 loss: 0.0072 lr: 0.02\n",
      "iteration: 24740 loss: 0.0100 lr: 0.02\n",
      "iteration: 24750 loss: 0.0055 lr: 0.02\n",
      "iteration: 24760 loss: 0.0077 lr: 0.02\n",
      "iteration: 24770 loss: 0.0091 lr: 0.02\n",
      "iteration: 24780 loss: 0.0068 lr: 0.02\n",
      "iteration: 24790 loss: 0.0123 lr: 0.02\n",
      "iteration: 24800 loss: 0.0067 lr: 0.02\n",
      "iteration: 24810 loss: 0.0080 lr: 0.02\n",
      "iteration: 24820 loss: 0.0058 lr: 0.02\n",
      "iteration: 24830 loss: 0.0069 lr: 0.02\n",
      "iteration: 24840 loss: 0.0070 lr: 0.02\n",
      "iteration: 24850 loss: 0.0083 lr: 0.02\n",
      "iteration: 24860 loss: 0.0058 lr: 0.02\n",
      "iteration: 24870 loss: 0.0081 lr: 0.02\n",
      "iteration: 24880 loss: 0.0054 lr: 0.02\n",
      "iteration: 24890 loss: 0.0063 lr: 0.02\n",
      "iteration: 24900 loss: 0.0061 lr: 0.02\n",
      "iteration: 24910 loss: 0.0082 lr: 0.02\n",
      "iteration: 24920 loss: 0.0101 lr: 0.02\n",
      "iteration: 24930 loss: 0.0090 lr: 0.02\n",
      "iteration: 24940 loss: 0.0084 lr: 0.02\n",
      "iteration: 24950 loss: 0.0060 lr: 0.02\n",
      "iteration: 24960 loss: 0.0100 lr: 0.02\n",
      "iteration: 24970 loss: 0.0078 lr: 0.02\n",
      "iteration: 24980 loss: 0.0075 lr: 0.02\n",
      "iteration: 24990 loss: 0.0076 lr: 0.02\n",
      "iteration: 25000 loss: 0.0086 lr: 0.02\n",
      "iteration: 25010 loss: 0.0063 lr: 0.02\n",
      "iteration: 25020 loss: 0.0082 lr: 0.02\n",
      "iteration: 25030 loss: 0.0049 lr: 0.02\n",
      "iteration: 25040 loss: 0.0075 lr: 0.02\n",
      "iteration: 25050 loss: 0.0090 lr: 0.02\n",
      "iteration: 25060 loss: 0.0068 lr: 0.02\n",
      "iteration: 25070 loss: 0.0073 lr: 0.02\n",
      "iteration: 25080 loss: 0.0082 lr: 0.02\n",
      "iteration: 25090 loss: 0.0080 lr: 0.02\n",
      "iteration: 25100 loss: 0.0057 lr: 0.02\n",
      "iteration: 25110 loss: 0.0071 lr: 0.02\n",
      "iteration: 25120 loss: 0.0079 lr: 0.02\n",
      "iteration: 25130 loss: 0.0064 lr: 0.02\n",
      "iteration: 25140 loss: 0.0083 lr: 0.02\n",
      "iteration: 25150 loss: 0.0042 lr: 0.02\n",
      "iteration: 25160 loss: 0.0081 lr: 0.02\n",
      "iteration: 25170 loss: 0.0066 lr: 0.02\n",
      "iteration: 25180 loss: 0.0086 lr: 0.02\n",
      "iteration: 25190 loss: 0.0105 lr: 0.02\n",
      "iteration: 25200 loss: 0.0070 lr: 0.02\n",
      "iteration: 25210 loss: 0.0081 lr: 0.02\n",
      "iteration: 25220 loss: 0.0070 lr: 0.02\n",
      "iteration: 25230 loss: 0.0070 lr: 0.02\n",
      "iteration: 25240 loss: 0.0094 lr: 0.02\n",
      "iteration: 25250 loss: 0.0074 lr: 0.02\n",
      "iteration: 25260 loss: 0.0073 lr: 0.02\n",
      "iteration: 25270 loss: 0.0064 lr: 0.02\n",
      "iteration: 25280 loss: 0.0077 lr: 0.02\n",
      "iteration: 25290 loss: 0.0065 lr: 0.02\n",
      "iteration: 25300 loss: 0.0076 lr: 0.02\n",
      "iteration: 25310 loss: 0.0075 lr: 0.02\n",
      "iteration: 25320 loss: 0.0076 lr: 0.02\n",
      "iteration: 25330 loss: 0.0102 lr: 0.02\n",
      "iteration: 25340 loss: 0.0088 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 25350 loss: 0.0086 lr: 0.02\n",
      "iteration: 25360 loss: 0.0061 lr: 0.02\n",
      "iteration: 25370 loss: 0.0079 lr: 0.02\n",
      "iteration: 25380 loss: 0.0069 lr: 0.02\n",
      "iteration: 25390 loss: 0.0096 lr: 0.02\n",
      "iteration: 25400 loss: 0.0056 lr: 0.02\n",
      "iteration: 25410 loss: 0.0084 lr: 0.02\n",
      "iteration: 25420 loss: 0.0124 lr: 0.02\n",
      "iteration: 25430 loss: 0.0091 lr: 0.02\n",
      "iteration: 25440 loss: 0.0067 lr: 0.02\n",
      "iteration: 25450 loss: 0.0069 lr: 0.02\n",
      "iteration: 25460 loss: 0.0083 lr: 0.02\n",
      "iteration: 25470 loss: 0.0070 lr: 0.02\n",
      "iteration: 25480 loss: 0.0097 lr: 0.02\n",
      "iteration: 25490 loss: 0.0054 lr: 0.02\n",
      "iteration: 25500 loss: 0.0073 lr: 0.02\n",
      "iteration: 25510 loss: 0.0068 lr: 0.02\n",
      "iteration: 25520 loss: 0.0060 lr: 0.02\n",
      "iteration: 25530 loss: 0.0072 lr: 0.02\n",
      "iteration: 25540 loss: 0.0071 lr: 0.02\n",
      "iteration: 25550 loss: 0.0081 lr: 0.02\n",
      "iteration: 25560 loss: 0.0082 lr: 0.02\n",
      "iteration: 25570 loss: 0.0060 lr: 0.02\n",
      "iteration: 25580 loss: 0.0096 lr: 0.02\n",
      "iteration: 25590 loss: 0.0095 lr: 0.02\n",
      "iteration: 25600 loss: 0.0081 lr: 0.02\n",
      "iteration: 25610 loss: 0.0083 lr: 0.02\n",
      "iteration: 25620 loss: 0.0074 lr: 0.02\n",
      "iteration: 25630 loss: 0.0086 lr: 0.02\n",
      "iteration: 25640 loss: 0.0086 lr: 0.02\n",
      "iteration: 25650 loss: 0.0071 lr: 0.02\n",
      "iteration: 25660 loss: 0.0084 lr: 0.02\n",
      "iteration: 25670 loss: 0.0097 lr: 0.02\n",
      "iteration: 25680 loss: 0.0097 lr: 0.02\n",
      "iteration: 25690 loss: 0.0086 lr: 0.02\n",
      "iteration: 25700 loss: 0.0078 lr: 0.02\n",
      "iteration: 25710 loss: 0.0066 lr: 0.02\n",
      "iteration: 25720 loss: 0.0101 lr: 0.02\n",
      "iteration: 25730 loss: 0.0068 lr: 0.02\n",
      "iteration: 25740 loss: 0.0086 lr: 0.02\n",
      "iteration: 25750 loss: 0.0091 lr: 0.02\n",
      "iteration: 25760 loss: 0.0073 lr: 0.02\n",
      "iteration: 25770 loss: 0.0070 lr: 0.02\n",
      "iteration: 25780 loss: 0.0064 lr: 0.02\n",
      "iteration: 25790 loss: 0.0070 lr: 0.02\n",
      "iteration: 25800 loss: 0.0079 lr: 0.02\n",
      "iteration: 25810 loss: 0.0092 lr: 0.02\n",
      "iteration: 25820 loss: 0.0050 lr: 0.02\n",
      "iteration: 25830 loss: 0.0082 lr: 0.02\n",
      "iteration: 25840 loss: 0.0055 lr: 0.02\n",
      "iteration: 25850 loss: 0.0065 lr: 0.02\n",
      "iteration: 25860 loss: 0.0064 lr: 0.02\n",
      "iteration: 25870 loss: 0.0064 lr: 0.02\n",
      "iteration: 25880 loss: 0.0088 lr: 0.02\n",
      "iteration: 25890 loss: 0.0080 lr: 0.02\n",
      "iteration: 25900 loss: 0.0074 lr: 0.02\n",
      "iteration: 25910 loss: 0.0082 lr: 0.02\n",
      "iteration: 25920 loss: 0.0082 lr: 0.02\n",
      "iteration: 25930 loss: 0.0048 lr: 0.02\n",
      "iteration: 25940 loss: 0.0097 lr: 0.02\n",
      "iteration: 25950 loss: 0.0069 lr: 0.02\n",
      "iteration: 25960 loss: 0.0051 lr: 0.02\n",
      "iteration: 25970 loss: 0.0091 lr: 0.02\n",
      "iteration: 25980 loss: 0.0062 lr: 0.02\n",
      "iteration: 25990 loss: 0.0070 lr: 0.02\n",
      "iteration: 26000 loss: 0.0058 lr: 0.02\n",
      "iteration: 26010 loss: 0.0068 lr: 0.02\n",
      "iteration: 26020 loss: 0.0077 lr: 0.02\n",
      "iteration: 26030 loss: 0.0089 lr: 0.02\n",
      "iteration: 26040 loss: 0.0074 lr: 0.02\n",
      "iteration: 26050 loss: 0.0096 lr: 0.02\n",
      "iteration: 26060 loss: 0.0050 lr: 0.02\n",
      "iteration: 26070 loss: 0.0061 lr: 0.02\n",
      "iteration: 26080 loss: 0.0045 lr: 0.02\n",
      "iteration: 26090 loss: 0.0060 lr: 0.02\n",
      "iteration: 26100 loss: 0.0068 lr: 0.02\n",
      "iteration: 26110 loss: 0.0064 lr: 0.02\n",
      "iteration: 26120 loss: 0.0057 lr: 0.02\n",
      "iteration: 26130 loss: 0.0078 lr: 0.02\n",
      "iteration: 26140 loss: 0.0073 lr: 0.02\n",
      "iteration: 26150 loss: 0.0067 lr: 0.02\n",
      "iteration: 26160 loss: 0.0071 lr: 0.02\n",
      "iteration: 26170 loss: 0.0082 lr: 0.02\n",
      "iteration: 26180 loss: 0.0065 lr: 0.02\n",
      "iteration: 26190 loss: 0.0086 lr: 0.02\n",
      "iteration: 26200 loss: 0.0050 lr: 0.02\n",
      "iteration: 26210 loss: 0.0048 lr: 0.02\n",
      "iteration: 26220 loss: 0.0055 lr: 0.02\n",
      "iteration: 26230 loss: 0.0082 lr: 0.02\n",
      "iteration: 26240 loss: 0.0082 lr: 0.02\n",
      "iteration: 26250 loss: 0.0099 lr: 0.02\n",
      "iteration: 26260 loss: 0.0054 lr: 0.02\n",
      "iteration: 26270 loss: 0.0046 lr: 0.02\n",
      "iteration: 26280 loss: 0.0082 lr: 0.02\n",
      "iteration: 26290 loss: 0.0049 lr: 0.02\n",
      "iteration: 26300 loss: 0.0066 lr: 0.02\n",
      "iteration: 26310 loss: 0.0074 lr: 0.02\n",
      "iteration: 26320 loss: 0.0049 lr: 0.02\n",
      "iteration: 26330 loss: 0.0081 lr: 0.02\n",
      "iteration: 26340 loss: 0.0088 lr: 0.02\n",
      "iteration: 26350 loss: 0.0062 lr: 0.02\n",
      "iteration: 26360 loss: 0.0061 lr: 0.02\n",
      "iteration: 26370 loss: 0.0066 lr: 0.02\n",
      "iteration: 26380 loss: 0.0052 lr: 0.02\n",
      "iteration: 26390 loss: 0.0076 lr: 0.02\n",
      "iteration: 26400 loss: 0.0066 lr: 0.02\n",
      "iteration: 26410 loss: 0.0075 lr: 0.02\n",
      "iteration: 26420 loss: 0.0071 lr: 0.02\n",
      "iteration: 26430 loss: 0.0078 lr: 0.02\n",
      "iteration: 26440 loss: 0.0059 lr: 0.02\n",
      "iteration: 26450 loss: 0.0076 lr: 0.02\n",
      "iteration: 26460 loss: 0.0092 lr: 0.02\n",
      "iteration: 26470 loss: 0.0113 lr: 0.02\n",
      "iteration: 26480 loss: 0.0082 lr: 0.02\n",
      "iteration: 26490 loss: 0.0087 lr: 0.02\n",
      "iteration: 26500 loss: 0.0067 lr: 0.02\n",
      "iteration: 26510 loss: 0.0074 lr: 0.02\n",
      "iteration: 26520 loss: 0.0064 lr: 0.02\n",
      "iteration: 26530 loss: 0.0076 lr: 0.02\n",
      "iteration: 26540 loss: 0.0081 lr: 0.02\n",
      "iteration: 26550 loss: 0.0071 lr: 0.02\n",
      "iteration: 26560 loss: 0.0070 lr: 0.02\n",
      "iteration: 26570 loss: 0.0073 lr: 0.02\n",
      "iteration: 26580 loss: 0.0109 lr: 0.02\n",
      "iteration: 26590 loss: 0.0081 lr: 0.02\n",
      "iteration: 26600 loss: 0.0075 lr: 0.02\n",
      "iteration: 26610 loss: 0.0079 lr: 0.02\n",
      "iteration: 26620 loss: 0.0075 lr: 0.02\n",
      "iteration: 26630 loss: 0.0060 lr: 0.02\n",
      "iteration: 26640 loss: 0.0084 lr: 0.02\n",
      "iteration: 26650 loss: 0.0083 lr: 0.02\n",
      "iteration: 26660 loss: 0.0092 lr: 0.02\n",
      "iteration: 26670 loss: 0.0067 lr: 0.02\n",
      "iteration: 26680 loss: 0.0095 lr: 0.02\n",
      "iteration: 26690 loss: 0.0068 lr: 0.02\n",
      "iteration: 26700 loss: 0.0087 lr: 0.02\n",
      "iteration: 26710 loss: 0.0067 lr: 0.02\n",
      "iteration: 26720 loss: 0.0056 lr: 0.02\n",
      "iteration: 26730 loss: 0.0071 lr: 0.02\n",
      "iteration: 26740 loss: 0.0072 lr: 0.02\n",
      "iteration: 26750 loss: 0.0067 lr: 0.02\n",
      "iteration: 26760 loss: 0.0066 lr: 0.02\n",
      "iteration: 26770 loss: 0.0064 lr: 0.02\n",
      "iteration: 26780 loss: 0.0078 lr: 0.02\n",
      "iteration: 26790 loss: 0.0103 lr: 0.02\n",
      "iteration: 26800 loss: 0.0065 lr: 0.02\n",
      "iteration: 26810 loss: 0.0098 lr: 0.02\n",
      "iteration: 26820 loss: 0.0057 lr: 0.02\n",
      "iteration: 26830 loss: 0.0061 lr: 0.02\n",
      "iteration: 26840 loss: 0.0111 lr: 0.02\n",
      "iteration: 26850 loss: 0.0062 lr: 0.02\n",
      "iteration: 26860 loss: 0.0097 lr: 0.02\n",
      "iteration: 26870 loss: 0.0062 lr: 0.02\n",
      "iteration: 26880 loss: 0.0091 lr: 0.02\n",
      "iteration: 26890 loss: 0.0067 lr: 0.02\n",
      "iteration: 26900 loss: 0.0068 lr: 0.02\n",
      "iteration: 26910 loss: 0.0059 lr: 0.02\n",
      "iteration: 26920 loss: 0.0077 lr: 0.02\n",
      "iteration: 26930 loss: 0.0108 lr: 0.02\n",
      "iteration: 26940 loss: 0.0060 lr: 0.02\n",
      "iteration: 26950 loss: 0.0084 lr: 0.02\n",
      "iteration: 26960 loss: 0.0072 lr: 0.02\n",
      "iteration: 26970 loss: 0.0053 lr: 0.02\n",
      "iteration: 26980 loss: 0.0072 lr: 0.02\n",
      "iteration: 26990 loss: 0.0060 lr: 0.02\n",
      "iteration: 27000 loss: 0.0071 lr: 0.02\n",
      "iteration: 27010 loss: 0.0083 lr: 0.02\n",
      "iteration: 27020 loss: 0.0047 lr: 0.02\n",
      "iteration: 27030 loss: 0.0078 lr: 0.02\n",
      "iteration: 27040 loss: 0.0054 lr: 0.02\n",
      "iteration: 27050 loss: 0.0109 lr: 0.02\n",
      "iteration: 27060 loss: 0.0079 lr: 0.02\n",
      "iteration: 27070 loss: 0.0076 lr: 0.02\n",
      "iteration: 27080 loss: 0.0080 lr: 0.02\n",
      "iteration: 27090 loss: 0.0063 lr: 0.02\n",
      "iteration: 27100 loss: 0.0084 lr: 0.02\n",
      "iteration: 27110 loss: 0.0080 lr: 0.02\n",
      "iteration: 27120 loss: 0.0089 lr: 0.02\n",
      "iteration: 27130 loss: 0.0057 lr: 0.02\n",
      "iteration: 27140 loss: 0.0084 lr: 0.02\n",
      "iteration: 27150 loss: 0.0068 lr: 0.02\n",
      "iteration: 27160 loss: 0.0059 lr: 0.02\n",
      "iteration: 27170 loss: 0.0063 lr: 0.02\n",
      "iteration: 27180 loss: 0.0063 lr: 0.02\n",
      "iteration: 27190 loss: 0.0054 lr: 0.02\n",
      "iteration: 27200 loss: 0.0091 lr: 0.02\n",
      "iteration: 27210 loss: 0.0077 lr: 0.02\n",
      "iteration: 27220 loss: 0.0072 lr: 0.02\n",
      "iteration: 27230 loss: 0.0079 lr: 0.02\n",
      "iteration: 27240 loss: 0.0072 lr: 0.02\n",
      "iteration: 27250 loss: 0.0080 lr: 0.02\n",
      "iteration: 27260 loss: 0.0070 lr: 0.02\n",
      "iteration: 27270 loss: 0.0083 lr: 0.02\n",
      "iteration: 27280 loss: 0.0056 lr: 0.02\n",
      "iteration: 27290 loss: 0.0075 lr: 0.02\n",
      "iteration: 27300 loss: 0.0096 lr: 0.02\n",
      "iteration: 27310 loss: 0.0088 lr: 0.02\n",
      "iteration: 27320 loss: 0.0095 lr: 0.02\n",
      "iteration: 27330 loss: 0.0060 lr: 0.02\n",
      "iteration: 27340 loss: 0.0071 lr: 0.02\n",
      "iteration: 27350 loss: 0.0054 lr: 0.02\n",
      "iteration: 27360 loss: 0.0067 lr: 0.02\n",
      "iteration: 27370 loss: 0.0081 lr: 0.02\n",
      "iteration: 27380 loss: 0.0054 lr: 0.02\n",
      "iteration: 27390 loss: 0.0092 lr: 0.02\n",
      "iteration: 27400 loss: 0.0051 lr: 0.02\n",
      "iteration: 27410 loss: 0.0095 lr: 0.02\n",
      "iteration: 27420 loss: 0.0077 lr: 0.02\n",
      "iteration: 27430 loss: 0.0069 lr: 0.02\n",
      "iteration: 27440 loss: 0.0077 lr: 0.02\n",
      "iteration: 27450 loss: 0.0049 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 27460 loss: 0.0060 lr: 0.02\n",
      "iteration: 27470 loss: 0.0070 lr: 0.02\n",
      "iteration: 27480 loss: 0.0081 lr: 0.02\n",
      "iteration: 27490 loss: 0.0057 lr: 0.02\n",
      "iteration: 27500 loss: 0.0057 lr: 0.02\n",
      "iteration: 27510 loss: 0.0077 lr: 0.02\n",
      "iteration: 27520 loss: 0.0073 lr: 0.02\n",
      "iteration: 27530 loss: 0.0089 lr: 0.02\n",
      "iteration: 27540 loss: 0.0108 lr: 0.02\n",
      "iteration: 27550 loss: 0.0080 lr: 0.02\n",
      "iteration: 27560 loss: 0.0082 lr: 0.02\n",
      "iteration: 27570 loss: 0.0075 lr: 0.02\n",
      "iteration: 27580 loss: 0.0074 lr: 0.02\n",
      "iteration: 27590 loss: 0.0076 lr: 0.02\n",
      "iteration: 27600 loss: 0.0080 lr: 0.02\n",
      "iteration: 27610 loss: 0.0080 lr: 0.02\n",
      "iteration: 27620 loss: 0.0052 lr: 0.02\n",
      "iteration: 27630 loss: 0.0075 lr: 0.02\n",
      "iteration: 27640 loss: 0.0060 lr: 0.02\n",
      "iteration: 27650 loss: 0.0071 lr: 0.02\n",
      "iteration: 27660 loss: 0.0072 lr: 0.02\n",
      "iteration: 27670 loss: 0.0057 lr: 0.02\n",
      "iteration: 27680 loss: 0.0098 lr: 0.02\n",
      "iteration: 27690 loss: 0.0109 lr: 0.02\n",
      "iteration: 27700 loss: 0.0056 lr: 0.02\n",
      "iteration: 27710 loss: 0.0079 lr: 0.02\n",
      "iteration: 27720 loss: 0.0062 lr: 0.02\n",
      "iteration: 27730 loss: 0.0067 lr: 0.02\n",
      "iteration: 27740 loss: 0.0073 lr: 0.02\n",
      "iteration: 27750 loss: 0.0053 lr: 0.02\n",
      "iteration: 27760 loss: 0.0072 lr: 0.02\n",
      "iteration: 27770 loss: 0.0065 lr: 0.02\n",
      "iteration: 27780 loss: 0.0103 lr: 0.02\n",
      "iteration: 27790 loss: 0.0105 lr: 0.02\n",
      "iteration: 27800 loss: 0.0099 lr: 0.02\n",
      "iteration: 27810 loss: 0.0059 lr: 0.02\n",
      "iteration: 27820 loss: 0.0094 lr: 0.02\n",
      "iteration: 27830 loss: 0.0104 lr: 0.02\n",
      "iteration: 27840 loss: 0.0052 lr: 0.02\n",
      "iteration: 27850 loss: 0.0050 lr: 0.02\n",
      "iteration: 27860 loss: 0.0072 lr: 0.02\n",
      "iteration: 27870 loss: 0.0061 lr: 0.02\n",
      "iteration: 27880 loss: 0.0079 lr: 0.02\n",
      "iteration: 27890 loss: 0.0116 lr: 0.02\n",
      "iteration: 27900 loss: 0.0054 lr: 0.02\n",
      "iteration: 27910 loss: 0.0063 lr: 0.02\n",
      "iteration: 27920 loss: 0.0054 lr: 0.02\n",
      "iteration: 27930 loss: 0.0067 lr: 0.02\n",
      "iteration: 27940 loss: 0.0041 lr: 0.02\n",
      "iteration: 27950 loss: 0.0065 lr: 0.02\n",
      "iteration: 27960 loss: 0.0069 lr: 0.02\n",
      "iteration: 27970 loss: 0.0068 lr: 0.02\n",
      "iteration: 27980 loss: 0.0058 lr: 0.02\n",
      "iteration: 27990 loss: 0.0078 lr: 0.02\n",
      "iteration: 28000 loss: 0.0071 lr: 0.02\n",
      "iteration: 28010 loss: 0.0066 lr: 0.02\n",
      "iteration: 28020 loss: 0.0079 lr: 0.02\n",
      "iteration: 28030 loss: 0.0094 lr: 0.02\n",
      "iteration: 28040 loss: 0.0080 lr: 0.02\n",
      "iteration: 28050 loss: 0.0053 lr: 0.02\n",
      "iteration: 28060 loss: 0.0052 lr: 0.02\n",
      "iteration: 28070 loss: 0.0061 lr: 0.02\n",
      "iteration: 28080 loss: 0.0070 lr: 0.02\n",
      "iteration: 28090 loss: 0.0069 lr: 0.02\n",
      "iteration: 28100 loss: 0.0110 lr: 0.02\n",
      "iteration: 28110 loss: 0.0068 lr: 0.02\n",
      "iteration: 28120 loss: 0.0091 lr: 0.02\n",
      "iteration: 28130 loss: 0.0083 lr: 0.02\n",
      "iteration: 28140 loss: 0.0061 lr: 0.02\n",
      "iteration: 28150 loss: 0.0071 lr: 0.02\n",
      "iteration: 28160 loss: 0.0059 lr: 0.02\n",
      "iteration: 28170 loss: 0.0052 lr: 0.02\n",
      "iteration: 28180 loss: 0.0076 lr: 0.02\n",
      "iteration: 28190 loss: 0.0057 lr: 0.02\n",
      "iteration: 28200 loss: 0.0073 lr: 0.02\n",
      "iteration: 28210 loss: 0.0077 lr: 0.02\n",
      "iteration: 28220 loss: 0.0063 lr: 0.02\n",
      "iteration: 28230 loss: 0.0080 lr: 0.02\n",
      "iteration: 28240 loss: 0.0071 lr: 0.02\n",
      "iteration: 28250 loss: 0.0076 lr: 0.02\n",
      "iteration: 28260 loss: 0.0081 lr: 0.02\n",
      "iteration: 28270 loss: 0.0085 lr: 0.02\n",
      "iteration: 28280 loss: 0.0066 lr: 0.02\n",
      "iteration: 28290 loss: 0.0090 lr: 0.02\n",
      "iteration: 28300 loss: 0.0057 lr: 0.02\n",
      "iteration: 28310 loss: 0.0057 lr: 0.02\n",
      "iteration: 28320 loss: 0.0078 lr: 0.02\n",
      "iteration: 28330 loss: 0.0089 lr: 0.02\n",
      "iteration: 28340 loss: 0.0068 lr: 0.02\n",
      "iteration: 28350 loss: 0.0087 lr: 0.02\n",
      "iteration: 28360 loss: 0.0086 lr: 0.02\n",
      "iteration: 28370 loss: 0.0087 lr: 0.02\n",
      "iteration: 28380 loss: 0.0081 lr: 0.02\n",
      "iteration: 28390 loss: 0.0073 lr: 0.02\n",
      "iteration: 28400 loss: 0.0053 lr: 0.02\n",
      "iteration: 28410 loss: 0.0054 lr: 0.02\n",
      "iteration: 28420 loss: 0.0065 lr: 0.02\n",
      "iteration: 28430 loss: 0.0085 lr: 0.02\n",
      "iteration: 28440 loss: 0.0123 lr: 0.02\n",
      "iteration: 28450 loss: 0.0070 lr: 0.02\n",
      "iteration: 28460 loss: 0.0105 lr: 0.02\n",
      "iteration: 28470 loss: 0.0049 lr: 0.02\n",
      "iteration: 28480 loss: 0.0056 lr: 0.02\n",
      "iteration: 28490 loss: 0.0084 lr: 0.02\n",
      "iteration: 28500 loss: 0.0067 lr: 0.02\n",
      "iteration: 28510 loss: 0.0077 lr: 0.02\n",
      "iteration: 28520 loss: 0.0064 lr: 0.02\n",
      "iteration: 28530 loss: 0.0058 lr: 0.02\n",
      "iteration: 28540 loss: 0.0078 lr: 0.02\n",
      "iteration: 28550 loss: 0.0069 lr: 0.02\n",
      "iteration: 28560 loss: 0.0057 lr: 0.02\n",
      "iteration: 28570 loss: 0.0073 lr: 0.02\n",
      "iteration: 28580 loss: 0.0053 lr: 0.02\n",
      "iteration: 28590 loss: 0.0046 lr: 0.02\n",
      "iteration: 28600 loss: 0.0078 lr: 0.02\n",
      "iteration: 28610 loss: 0.0074 lr: 0.02\n",
      "iteration: 28620 loss: 0.0052 lr: 0.02\n",
      "iteration: 28630 loss: 0.0051 lr: 0.02\n",
      "iteration: 28640 loss: 0.0063 lr: 0.02\n",
      "iteration: 28650 loss: 0.0053 lr: 0.02\n",
      "iteration: 28660 loss: 0.0075 lr: 0.02\n",
      "iteration: 28670 loss: 0.0071 lr: 0.02\n",
      "iteration: 28680 loss: 0.0050 lr: 0.02\n",
      "iteration: 28690 loss: 0.0085 lr: 0.02\n",
      "iteration: 28700 loss: 0.0069 lr: 0.02\n",
      "iteration: 28710 loss: 0.0066 lr: 0.02\n",
      "iteration: 28720 loss: 0.0083 lr: 0.02\n",
      "iteration: 28730 loss: 0.0071 lr: 0.02\n",
      "iteration: 28740 loss: 0.0078 lr: 0.02\n",
      "iteration: 28750 loss: 0.0100 lr: 0.02\n",
      "iteration: 28760 loss: 0.0083 lr: 0.02\n",
      "iteration: 28770 loss: 0.0056 lr: 0.02\n",
      "iteration: 28780 loss: 0.0064 lr: 0.02\n",
      "iteration: 28790 loss: 0.0104 lr: 0.02\n",
      "iteration: 28800 loss: 0.0062 lr: 0.02\n",
      "iteration: 28810 loss: 0.0066 lr: 0.02\n",
      "iteration: 28820 loss: 0.0118 lr: 0.02\n",
      "iteration: 28830 loss: 0.0049 lr: 0.02\n",
      "iteration: 28840 loss: 0.0075 lr: 0.02\n",
      "iteration: 28850 loss: 0.0077 lr: 0.02\n",
      "iteration: 28860 loss: 0.0061 lr: 0.02\n",
      "iteration: 28870 loss: 0.0078 lr: 0.02\n",
      "iteration: 28880 loss: 0.0085 lr: 0.02\n",
      "iteration: 28890 loss: 0.0104 lr: 0.02\n",
      "iteration: 28900 loss: 0.0073 lr: 0.02\n",
      "iteration: 28910 loss: 0.0049 lr: 0.02\n",
      "iteration: 28920 loss: 0.0068 lr: 0.02\n",
      "iteration: 28930 loss: 0.0088 lr: 0.02\n",
      "iteration: 28940 loss: 0.0067 lr: 0.02\n",
      "iteration: 28950 loss: 0.0084 lr: 0.02\n",
      "iteration: 28960 loss: 0.0052 lr: 0.02\n",
      "iteration: 28970 loss: 0.0074 lr: 0.02\n",
      "iteration: 28980 loss: 0.0050 lr: 0.02\n",
      "iteration: 28990 loss: 0.0048 lr: 0.02\n",
      "iteration: 29000 loss: 0.0081 lr: 0.02\n",
      "iteration: 29010 loss: 0.0063 lr: 0.02\n",
      "iteration: 29020 loss: 0.0060 lr: 0.02\n",
      "iteration: 29030 loss: 0.0048 lr: 0.02\n",
      "iteration: 29040 loss: 0.0073 lr: 0.02\n",
      "iteration: 29050 loss: 0.0088 lr: 0.02\n",
      "iteration: 29060 loss: 0.0077 lr: 0.02\n",
      "iteration: 29070 loss: 0.0063 lr: 0.02\n",
      "iteration: 29080 loss: 0.0081 lr: 0.02\n",
      "iteration: 29090 loss: 0.0096 lr: 0.02\n",
      "iteration: 29100 loss: 0.0080 lr: 0.02\n",
      "iteration: 29110 loss: 0.0089 lr: 0.02\n",
      "iteration: 29120 loss: 0.0065 lr: 0.02\n",
      "iteration: 29130 loss: 0.0068 lr: 0.02\n",
      "iteration: 29140 loss: 0.0038 lr: 0.02\n",
      "iteration: 29150 loss: 0.0072 lr: 0.02\n",
      "iteration: 29160 loss: 0.0077 lr: 0.02\n",
      "iteration: 29170 loss: 0.0065 lr: 0.02\n",
      "iteration: 29180 loss: 0.0069 lr: 0.02\n",
      "iteration: 29190 loss: 0.0066 lr: 0.02\n",
      "iteration: 29200 loss: 0.0069 lr: 0.02\n",
      "iteration: 29210 loss: 0.0072 lr: 0.02\n",
      "iteration: 29220 loss: 0.0055 lr: 0.02\n",
      "iteration: 29230 loss: 0.0062 lr: 0.02\n",
      "iteration: 29240 loss: 0.0061 lr: 0.02\n",
      "iteration: 29250 loss: 0.0062 lr: 0.02\n",
      "iteration: 29260 loss: 0.0071 lr: 0.02\n",
      "iteration: 29270 loss: 0.0064 lr: 0.02\n",
      "iteration: 29280 loss: 0.0069 lr: 0.02\n",
      "iteration: 29290 loss: 0.0060 lr: 0.02\n",
      "iteration: 29300 loss: 0.0077 lr: 0.02\n",
      "iteration: 29310 loss: 0.0082 lr: 0.02\n",
      "iteration: 29320 loss: 0.0067 lr: 0.02\n",
      "iteration: 29330 loss: 0.0074 lr: 0.02\n",
      "iteration: 29340 loss: 0.0050 lr: 0.02\n",
      "iteration: 29350 loss: 0.0046 lr: 0.02\n",
      "iteration: 29360 loss: 0.0078 lr: 0.02\n",
      "iteration: 29370 loss: 0.0070 lr: 0.02\n",
      "iteration: 29380 loss: 0.0054 lr: 0.02\n",
      "iteration: 29390 loss: 0.0074 lr: 0.02\n",
      "iteration: 29400 loss: 0.0085 lr: 0.02\n",
      "iteration: 29410 loss: 0.0076 lr: 0.02\n",
      "iteration: 29420 loss: 0.0065 lr: 0.02\n",
      "iteration: 29430 loss: 0.0077 lr: 0.02\n",
      "iteration: 29440 loss: 0.0063 lr: 0.02\n",
      "iteration: 29450 loss: 0.0067 lr: 0.02\n",
      "iteration: 29460 loss: 0.0061 lr: 0.02\n",
      "iteration: 29470 loss: 0.0049 lr: 0.02\n",
      "iteration: 29480 loss: 0.0068 lr: 0.02\n",
      "iteration: 29490 loss: 0.0042 lr: 0.02\n",
      "iteration: 29500 loss: 0.0071 lr: 0.02\n",
      "iteration: 29510 loss: 0.0065 lr: 0.02\n",
      "iteration: 29520 loss: 0.0067 lr: 0.02\n",
      "iteration: 29530 loss: 0.0083 lr: 0.02\n",
      "iteration: 29540 loss: 0.0069 lr: 0.02\n",
      "iteration: 29550 loss: 0.0075 lr: 0.02\n",
      "iteration: 29560 loss: 0.0077 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 29570 loss: 0.0074 lr: 0.02\n",
      "iteration: 29580 loss: 0.0062 lr: 0.02\n",
      "iteration: 29590 loss: 0.0057 lr: 0.02\n",
      "iteration: 29600 loss: 0.0054 lr: 0.02\n",
      "iteration: 29610 loss: 0.0061 lr: 0.02\n",
      "iteration: 29620 loss: 0.0069 lr: 0.02\n",
      "iteration: 29630 loss: 0.0077 lr: 0.02\n",
      "iteration: 29640 loss: 0.0077 lr: 0.02\n",
      "iteration: 29650 loss: 0.0087 lr: 0.02\n",
      "iteration: 29660 loss: 0.0075 lr: 0.02\n",
      "iteration: 29670 loss: 0.0102 lr: 0.02\n",
      "iteration: 29680 loss: 0.0095 lr: 0.02\n",
      "iteration: 29690 loss: 0.0084 lr: 0.02\n",
      "iteration: 29700 loss: 0.0088 lr: 0.02\n",
      "iteration: 29710 loss: 0.0094 lr: 0.02\n",
      "iteration: 29720 loss: 0.0071 lr: 0.02\n",
      "iteration: 29730 loss: 0.0060 lr: 0.02\n",
      "iteration: 29740 loss: 0.0070 lr: 0.02\n",
      "iteration: 29750 loss: 0.0073 lr: 0.02\n",
      "iteration: 29760 loss: 0.0077 lr: 0.02\n",
      "iteration: 29770 loss: 0.0071 lr: 0.02\n",
      "iteration: 29780 loss: 0.0091 lr: 0.02\n",
      "iteration: 29790 loss: 0.0079 lr: 0.02\n",
      "iteration: 29800 loss: 0.0065 lr: 0.02\n",
      "iteration: 29810 loss: 0.0077 lr: 0.02\n",
      "iteration: 29820 loss: 0.0062 lr: 0.02\n",
      "iteration: 29830 loss: 0.0053 lr: 0.02\n",
      "iteration: 29840 loss: 0.0067 lr: 0.02\n",
      "iteration: 29850 loss: 0.0068 lr: 0.02\n",
      "iteration: 29860 loss: 0.0082 lr: 0.02\n",
      "iteration: 29870 loss: 0.0050 lr: 0.02\n",
      "iteration: 29880 loss: 0.0046 lr: 0.02\n",
      "iteration: 29890 loss: 0.0057 lr: 0.02\n",
      "iteration: 29900 loss: 0.0075 lr: 0.02\n",
      "iteration: 29910 loss: 0.0044 lr: 0.02\n",
      "iteration: 29920 loss: 0.0085 lr: 0.02\n",
      "iteration: 29930 loss: 0.0061 lr: 0.02\n",
      "iteration: 29940 loss: 0.0075 lr: 0.02\n",
      "iteration: 29950 loss: 0.0069 lr: 0.02\n",
      "iteration: 29960 loss: 0.0065 lr: 0.02\n",
      "iteration: 29970 loss: 0.0071 lr: 0.02\n",
      "iteration: 29980 loss: 0.0053 lr: 0.02\n",
      "iteration: 29990 loss: 0.0076 lr: 0.02\n",
      "iteration: 30000 loss: 0.0090 lr: 0.02\n",
      "iteration: 30010 loss: 0.0054 lr: 0.02\n",
      "iteration: 30020 loss: 0.0064 lr: 0.02\n",
      "iteration: 30030 loss: 0.0064 lr: 0.02\n",
      "iteration: 30040 loss: 0.0059 lr: 0.02\n",
      "iteration: 30050 loss: 0.0065 lr: 0.02\n",
      "iteration: 30060 loss: 0.0074 lr: 0.02\n",
      "iteration: 30070 loss: 0.0075 lr: 0.02\n",
      "iteration: 30080 loss: 0.0075 lr: 0.02\n",
      "iteration: 30090 loss: 0.0101 lr: 0.02\n",
      "iteration: 30100 loss: 0.0065 lr: 0.02\n",
      "iteration: 30110 loss: 0.0097 lr: 0.02\n",
      "iteration: 30120 loss: 0.0073 lr: 0.02\n",
      "iteration: 30130 loss: 0.0072 lr: 0.02\n",
      "iteration: 30140 loss: 0.0059 lr: 0.02\n",
      "iteration: 30150 loss: 0.0062 lr: 0.02\n",
      "iteration: 30160 loss: 0.0070 lr: 0.02\n",
      "iteration: 30170 loss: 0.0068 lr: 0.02\n",
      "iteration: 30180 loss: 0.0053 lr: 0.02\n",
      "iteration: 30190 loss: 0.0049 lr: 0.02\n",
      "iteration: 30200 loss: 0.0046 lr: 0.02\n",
      "iteration: 30210 loss: 0.0052 lr: 0.02\n",
      "iteration: 30220 loss: 0.0077 lr: 0.02\n",
      "iteration: 30230 loss: 0.0052 lr: 0.02\n",
      "iteration: 30240 loss: 0.0064 lr: 0.02\n",
      "iteration: 30250 loss: 0.0049 lr: 0.02\n",
      "iteration: 30260 loss: 0.0049 lr: 0.02\n",
      "iteration: 30270 loss: 0.0072 lr: 0.02\n",
      "iteration: 30280 loss: 0.0079 lr: 0.02\n",
      "iteration: 30290 loss: 0.0075 lr: 0.02\n",
      "iteration: 30300 loss: 0.0056 lr: 0.02\n",
      "iteration: 30310 loss: 0.0056 lr: 0.02\n",
      "iteration: 30320 loss: 0.0091 lr: 0.02\n",
      "iteration: 30330 loss: 0.0062 lr: 0.02\n",
      "iteration: 30340 loss: 0.0074 lr: 0.02\n",
      "iteration: 30350 loss: 0.0070 lr: 0.02\n",
      "iteration: 30360 loss: 0.0044 lr: 0.02\n",
      "iteration: 30370 loss: 0.0085 lr: 0.02\n",
      "iteration: 30380 loss: 0.0068 lr: 0.02\n",
      "iteration: 30390 loss: 0.0099 lr: 0.02\n",
      "iteration: 30400 loss: 0.0077 lr: 0.02\n",
      "iteration: 30410 loss: 0.0067 lr: 0.02\n",
      "iteration: 30420 loss: 0.0066 lr: 0.02\n",
      "iteration: 30430 loss: 0.0055 lr: 0.02\n",
      "iteration: 30440 loss: 0.0061 lr: 0.02\n",
      "iteration: 30450 loss: 0.0070 lr: 0.02\n",
      "iteration: 30460 loss: 0.0070 lr: 0.02\n",
      "iteration: 30470 loss: 0.0056 lr: 0.02\n",
      "iteration: 30480 loss: 0.0078 lr: 0.02\n",
      "iteration: 30490 loss: 0.0067 lr: 0.02\n",
      "iteration: 30500 loss: 0.0055 lr: 0.02\n",
      "iteration: 30510 loss: 0.0060 lr: 0.02\n",
      "iteration: 30520 loss: 0.0043 lr: 0.02\n",
      "iteration: 30530 loss: 0.0057 lr: 0.02\n",
      "iteration: 30540 loss: 0.0061 lr: 0.02\n",
      "iteration: 30550 loss: 0.0088 lr: 0.02\n",
      "iteration: 30560 loss: 0.0064 lr: 0.02\n",
      "iteration: 30570 loss: 0.0055 lr: 0.02\n",
      "iteration: 30580 loss: 0.0080 lr: 0.02\n",
      "iteration: 30590 loss: 0.0057 lr: 0.02\n",
      "iteration: 30600 loss: 0.0084 lr: 0.02\n",
      "iteration: 30610 loss: 0.0054 lr: 0.02\n",
      "iteration: 30620 loss: 0.0092 lr: 0.02\n",
      "iteration: 30630 loss: 0.0063 lr: 0.02\n",
      "iteration: 30640 loss: 0.0095 lr: 0.02\n",
      "iteration: 30650 loss: 0.0049 lr: 0.02\n",
      "iteration: 30660 loss: 0.0060 lr: 0.02\n",
      "iteration: 30670 loss: 0.0061 lr: 0.02\n",
      "iteration: 30680 loss: 0.0068 lr: 0.02\n",
      "iteration: 30690 loss: 0.0081 lr: 0.02\n",
      "iteration: 30700 loss: 0.0091 lr: 0.02\n",
      "iteration: 30710 loss: 0.0072 lr: 0.02\n",
      "iteration: 30720 loss: 0.0075 lr: 0.02\n",
      "iteration: 30730 loss: 0.0073 lr: 0.02\n",
      "iteration: 30740 loss: 0.0065 lr: 0.02\n",
      "iteration: 30750 loss: 0.0066 lr: 0.02\n",
      "iteration: 30760 loss: 0.0069 lr: 0.02\n",
      "iteration: 30770 loss: 0.0088 lr: 0.02\n",
      "iteration: 30780 loss: 0.0072 lr: 0.02\n",
      "iteration: 30790 loss: 0.0070 lr: 0.02\n",
      "iteration: 30800 loss: 0.0053 lr: 0.02\n",
      "iteration: 30810 loss: 0.0059 lr: 0.02\n",
      "iteration: 30820 loss: 0.0074 lr: 0.02\n",
      "iteration: 30830 loss: 0.0081 lr: 0.02\n",
      "iteration: 30840 loss: 0.0054 lr: 0.02\n",
      "iteration: 30850 loss: 0.0079 lr: 0.02\n",
      "iteration: 30860 loss: 0.0082 lr: 0.02\n",
      "iteration: 30870 loss: 0.0071 lr: 0.02\n",
      "iteration: 30880 loss: 0.0091 lr: 0.02\n",
      "iteration: 30890 loss: 0.0086 lr: 0.02\n",
      "iteration: 30900 loss: 0.0059 lr: 0.02\n",
      "iteration: 30910 loss: 0.0069 lr: 0.02\n",
      "iteration: 30920 loss: 0.0041 lr: 0.02\n",
      "iteration: 30930 loss: 0.0085 lr: 0.02\n",
      "iteration: 30940 loss: 0.0087 lr: 0.02\n",
      "iteration: 30950 loss: 0.0062 lr: 0.02\n",
      "iteration: 30960 loss: 0.0066 lr: 0.02\n",
      "iteration: 30970 loss: 0.0076 lr: 0.02\n",
      "iteration: 30980 loss: 0.0070 lr: 0.02\n",
      "iteration: 30990 loss: 0.0051 lr: 0.02\n",
      "iteration: 31000 loss: 0.0078 lr: 0.02\n",
      "iteration: 31010 loss: 0.0059 lr: 0.02\n",
      "iteration: 31020 loss: 0.0052 lr: 0.02\n",
      "iteration: 31030 loss: 0.0048 lr: 0.02\n",
      "iteration: 31040 loss: 0.0045 lr: 0.02\n",
      "iteration: 31050 loss: 0.0066 lr: 0.02\n",
      "iteration: 31060 loss: 0.0067 lr: 0.02\n",
      "iteration: 31070 loss: 0.0062 lr: 0.02\n",
      "iteration: 31080 loss: 0.0080 lr: 0.02\n",
      "iteration: 31090 loss: 0.0086 lr: 0.02\n",
      "iteration: 31100 loss: 0.0066 lr: 0.02\n",
      "iteration: 31110 loss: 0.0078 lr: 0.02\n",
      "iteration: 31120 loss: 0.0062 lr: 0.02\n",
      "iteration: 31130 loss: 0.0063 lr: 0.02\n",
      "iteration: 31140 loss: 0.0060 lr: 0.02\n",
      "iteration: 31150 loss: 0.0050 lr: 0.02\n",
      "iteration: 31160 loss: 0.0064 lr: 0.02\n",
      "iteration: 31170 loss: 0.0070 lr: 0.02\n",
      "iteration: 31180 loss: 0.0061 lr: 0.02\n",
      "iteration: 31190 loss: 0.0064 lr: 0.02\n",
      "iteration: 31200 loss: 0.0058 lr: 0.02\n",
      "iteration: 31210 loss: 0.0072 lr: 0.02\n",
      "iteration: 31220 loss: 0.0053 lr: 0.02\n",
      "iteration: 31230 loss: 0.0050 lr: 0.02\n",
      "iteration: 31240 loss: 0.0081 lr: 0.02\n",
      "iteration: 31250 loss: 0.0055 lr: 0.02\n",
      "iteration: 31260 loss: 0.0058 lr: 0.02\n",
      "iteration: 31270 loss: 0.0055 lr: 0.02\n",
      "iteration: 31280 loss: 0.0051 lr: 0.02\n",
      "iteration: 31290 loss: 0.0056 lr: 0.02\n",
      "iteration: 31300 loss: 0.0071 lr: 0.02\n",
      "iteration: 31310 loss: 0.0089 lr: 0.02\n",
      "iteration: 31320 loss: 0.0073 lr: 0.02\n",
      "iteration: 31330 loss: 0.0075 lr: 0.02\n",
      "iteration: 31340 loss: 0.0054 lr: 0.02\n",
      "iteration: 31350 loss: 0.0059 lr: 0.02\n",
      "iteration: 31360 loss: 0.0075 lr: 0.02\n",
      "iteration: 31370 loss: 0.0054 lr: 0.02\n",
      "iteration: 31380 loss: 0.0067 lr: 0.02\n",
      "iteration: 31390 loss: 0.0078 lr: 0.02\n",
      "iteration: 31400 loss: 0.0076 lr: 0.02\n",
      "iteration: 31410 loss: 0.0070 lr: 0.02\n",
      "iteration: 31420 loss: 0.0064 lr: 0.02\n",
      "iteration: 31430 loss: 0.0053 lr: 0.02\n",
      "iteration: 31440 loss: 0.0067 lr: 0.02\n",
      "iteration: 31450 loss: 0.0058 lr: 0.02\n",
      "iteration: 31460 loss: 0.0061 lr: 0.02\n",
      "iteration: 31470 loss: 0.0070 lr: 0.02\n",
      "iteration: 31480 loss: 0.0060 lr: 0.02\n",
      "iteration: 31490 loss: 0.0053 lr: 0.02\n",
      "iteration: 31500 loss: 0.0070 lr: 0.02\n",
      "iteration: 31510 loss: 0.0070 lr: 0.02\n",
      "iteration: 31520 loss: 0.0062 lr: 0.02\n",
      "iteration: 31530 loss: 0.0058 lr: 0.02\n",
      "iteration: 31540 loss: 0.0072 lr: 0.02\n",
      "iteration: 31550 loss: 0.0055 lr: 0.02\n",
      "iteration: 31560 loss: 0.0076 lr: 0.02\n",
      "iteration: 31570 loss: 0.0044 lr: 0.02\n",
      "iteration: 31580 loss: 0.0052 lr: 0.02\n",
      "iteration: 31590 loss: 0.0092 lr: 0.02\n",
      "iteration: 31600 loss: 0.0074 lr: 0.02\n",
      "iteration: 31610 loss: 0.0104 lr: 0.02\n",
      "iteration: 31620 loss: 0.0061 lr: 0.02\n",
      "iteration: 31630 loss: 0.0064 lr: 0.02\n",
      "iteration: 31640 loss: 0.0060 lr: 0.02\n",
      "iteration: 31650 loss: 0.0058 lr: 0.02\n",
      "iteration: 31660 loss: 0.0068 lr: 0.02\n",
      "iteration: 31670 loss: 0.0072 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 31680 loss: 0.0061 lr: 0.02\n",
      "iteration: 31690 loss: 0.0049 lr: 0.02\n",
      "iteration: 31700 loss: 0.0051 lr: 0.02\n",
      "iteration: 31710 loss: 0.0126 lr: 0.02\n",
      "iteration: 31720 loss: 0.0086 lr: 0.02\n",
      "iteration: 31730 loss: 0.0063 lr: 0.02\n",
      "iteration: 31740 loss: 0.0063 lr: 0.02\n",
      "iteration: 31750 loss: 0.0080 lr: 0.02\n",
      "iteration: 31760 loss: 0.0065 lr: 0.02\n",
      "iteration: 31770 loss: 0.0071 lr: 0.02\n",
      "iteration: 31780 loss: 0.0065 lr: 0.02\n",
      "iteration: 31790 loss: 0.0050 lr: 0.02\n",
      "iteration: 31800 loss: 0.0070 lr: 0.02\n",
      "iteration: 31810 loss: 0.0057 lr: 0.02\n",
      "iteration: 31820 loss: 0.0062 lr: 0.02\n",
      "iteration: 31830 loss: 0.0079 lr: 0.02\n",
      "iteration: 31840 loss: 0.0054 lr: 0.02\n",
      "iteration: 31850 loss: 0.0067 lr: 0.02\n",
      "iteration: 31860 loss: 0.0045 lr: 0.02\n",
      "iteration: 31870 loss: 0.0053 lr: 0.02\n",
      "iteration: 31880 loss: 0.0057 lr: 0.02\n",
      "iteration: 31890 loss: 0.0061 lr: 0.02\n",
      "iteration: 31900 loss: 0.0073 lr: 0.02\n",
      "iteration: 31910 loss: 0.0063 lr: 0.02\n",
      "iteration: 31920 loss: 0.0059 lr: 0.02\n",
      "iteration: 31930 loss: 0.0083 lr: 0.02\n",
      "iteration: 31940 loss: 0.0075 lr: 0.02\n",
      "iteration: 31950 loss: 0.0063 lr: 0.02\n",
      "iteration: 31960 loss: 0.0047 lr: 0.02\n",
      "iteration: 31970 loss: 0.0068 lr: 0.02\n",
      "iteration: 31980 loss: 0.0055 lr: 0.02\n",
      "iteration: 31990 loss: 0.0066 lr: 0.02\n",
      "iteration: 32000 loss: 0.0074 lr: 0.02\n",
      "iteration: 32010 loss: 0.0044 lr: 0.02\n",
      "iteration: 32020 loss: 0.0052 lr: 0.02\n",
      "iteration: 32030 loss: 0.0082 lr: 0.02\n",
      "iteration: 32040 loss: 0.0061 lr: 0.02\n",
      "iteration: 32050 loss: 0.0048 lr: 0.02\n",
      "iteration: 32060 loss: 0.0067 lr: 0.02\n",
      "iteration: 32070 loss: 0.0064 lr: 0.02\n",
      "iteration: 32080 loss: 0.0065 lr: 0.02\n",
      "iteration: 32090 loss: 0.0059 lr: 0.02\n",
      "iteration: 32100 loss: 0.0065 lr: 0.02\n",
      "iteration: 32110 loss: 0.0078 lr: 0.02\n",
      "iteration: 32120 loss: 0.0061 lr: 0.02\n",
      "iteration: 32130 loss: 0.0076 lr: 0.02\n",
      "iteration: 32140 loss: 0.0049 lr: 0.02\n",
      "iteration: 32150 loss: 0.0062 lr: 0.02\n",
      "iteration: 32160 loss: 0.0077 lr: 0.02\n",
      "iteration: 32170 loss: 0.0045 lr: 0.02\n",
      "iteration: 32180 loss: 0.0059 lr: 0.02\n",
      "iteration: 32190 loss: 0.0086 lr: 0.02\n",
      "iteration: 32200 loss: 0.0076 lr: 0.02\n",
      "iteration: 32210 loss: 0.0061 lr: 0.02\n",
      "iteration: 32220 loss: 0.0076 lr: 0.02\n",
      "iteration: 32230 loss: 0.0070 lr: 0.02\n",
      "iteration: 32240 loss: 0.0061 lr: 0.02\n",
      "iteration: 32250 loss: 0.0053 lr: 0.02\n",
      "iteration: 32260 loss: 0.0051 lr: 0.02\n",
      "iteration: 32270 loss: 0.0087 lr: 0.02\n",
      "iteration: 32280 loss: 0.0064 lr: 0.02\n",
      "iteration: 32290 loss: 0.0058 lr: 0.02\n",
      "iteration: 32300 loss: 0.0077 lr: 0.02\n",
      "iteration: 32310 loss: 0.0049 lr: 0.02\n",
      "iteration: 32320 loss: 0.0050 lr: 0.02\n",
      "iteration: 32330 loss: 0.0057 lr: 0.02\n",
      "iteration: 32340 loss: 0.0069 lr: 0.02\n",
      "iteration: 32350 loss: 0.0075 lr: 0.02\n",
      "iteration: 32360 loss: 0.0085 lr: 0.02\n",
      "iteration: 32370 loss: 0.0062 lr: 0.02\n",
      "iteration: 32380 loss: 0.0080 lr: 0.02\n",
      "iteration: 32390 loss: 0.0064 lr: 0.02\n",
      "iteration: 32400 loss: 0.0059 lr: 0.02\n",
      "iteration: 32410 loss: 0.0051 lr: 0.02\n",
      "iteration: 32420 loss: 0.0066 lr: 0.02\n",
      "iteration: 32430 loss: 0.0056 lr: 0.02\n",
      "iteration: 32440 loss: 0.0091 lr: 0.02\n",
      "iteration: 32450 loss: 0.0082 lr: 0.02\n",
      "iteration: 32460 loss: 0.0073 lr: 0.02\n",
      "iteration: 32470 loss: 0.0071 lr: 0.02\n",
      "iteration: 32480 loss: 0.0059 lr: 0.02\n",
      "iteration: 32490 loss: 0.0058 lr: 0.02\n",
      "iteration: 32500 loss: 0.0062 lr: 0.02\n",
      "iteration: 32510 loss: 0.0049 lr: 0.02\n",
      "iteration: 32520 loss: 0.0052 lr: 0.02\n",
      "iteration: 32530 loss: 0.0041 lr: 0.02\n",
      "iteration: 32540 loss: 0.0057 lr: 0.02\n",
      "iteration: 32550 loss: 0.0071 lr: 0.02\n",
      "iteration: 32560 loss: 0.0060 lr: 0.02\n",
      "iteration: 32570 loss: 0.0072 lr: 0.02\n",
      "iteration: 32580 loss: 0.0070 lr: 0.02\n",
      "iteration: 32590 loss: 0.0052 lr: 0.02\n",
      "iteration: 32600 loss: 0.0071 lr: 0.02\n",
      "iteration: 32610 loss: 0.0073 lr: 0.02\n",
      "iteration: 32620 loss: 0.0079 lr: 0.02\n",
      "iteration: 32630 loss: 0.0060 lr: 0.02\n",
      "iteration: 32640 loss: 0.0049 lr: 0.02\n",
      "iteration: 32650 loss: 0.0090 lr: 0.02\n",
      "iteration: 32660 loss: 0.0063 lr: 0.02\n",
      "iteration: 32670 loss: 0.0086 lr: 0.02\n",
      "iteration: 32680 loss: 0.0048 lr: 0.02\n",
      "iteration: 32690 loss: 0.0069 lr: 0.02\n",
      "iteration: 32700 loss: 0.0060 lr: 0.02\n",
      "iteration: 32710 loss: 0.0062 lr: 0.02\n",
      "iteration: 32720 loss: 0.0070 lr: 0.02\n",
      "iteration: 32730 loss: 0.0091 lr: 0.02\n",
      "iteration: 32740 loss: 0.0053 lr: 0.02\n",
      "iteration: 32750 loss: 0.0053 lr: 0.02\n",
      "iteration: 32760 loss: 0.0068 lr: 0.02\n",
      "iteration: 32770 loss: 0.0076 lr: 0.02\n",
      "iteration: 32780 loss: 0.0076 lr: 0.02\n",
      "iteration: 32790 loss: 0.0063 lr: 0.02\n",
      "iteration: 32800 loss: 0.0077 lr: 0.02\n",
      "iteration: 32810 loss: 0.0058 lr: 0.02\n",
      "iteration: 32820 loss: 0.0066 lr: 0.02\n",
      "iteration: 32830 loss: 0.0081 lr: 0.02\n",
      "iteration: 32840 loss: 0.0070 lr: 0.02\n",
      "iteration: 32850 loss: 0.0077 lr: 0.02\n",
      "iteration: 32860 loss: 0.0061 lr: 0.02\n",
      "iteration: 32870 loss: 0.0054 lr: 0.02\n",
      "iteration: 32880 loss: 0.0060 lr: 0.02\n",
      "iteration: 32890 loss: 0.0061 lr: 0.02\n",
      "iteration: 32900 loss: 0.0064 lr: 0.02\n",
      "iteration: 32910 loss: 0.0059 lr: 0.02\n",
      "iteration: 32920 loss: 0.0070 lr: 0.02\n",
      "iteration: 32930 loss: 0.0053 lr: 0.02\n",
      "iteration: 32940 loss: 0.0063 lr: 0.02\n",
      "iteration: 32950 loss: 0.0073 lr: 0.02\n",
      "iteration: 32960 loss: 0.0061 lr: 0.02\n",
      "iteration: 32970 loss: 0.0069 lr: 0.02\n",
      "iteration: 32980 loss: 0.0064 lr: 0.02\n",
      "iteration: 32990 loss: 0.0081 lr: 0.02\n",
      "iteration: 33000 loss: 0.0067 lr: 0.02\n",
      "iteration: 33010 loss: 0.0063 lr: 0.02\n",
      "iteration: 33020 loss: 0.0051 lr: 0.02\n",
      "iteration: 33030 loss: 0.0064 lr: 0.02\n",
      "iteration: 33040 loss: 0.0071 lr: 0.02\n",
      "iteration: 33050 loss: 0.0051 lr: 0.02\n",
      "iteration: 33060 loss: 0.0052 lr: 0.02\n",
      "iteration: 33070 loss: 0.0056 lr: 0.02\n",
      "iteration: 33080 loss: 0.0051 lr: 0.02\n",
      "iteration: 33090 loss: 0.0071 lr: 0.02\n",
      "iteration: 33100 loss: 0.0049 lr: 0.02\n",
      "iteration: 33110 loss: 0.0043 lr: 0.02\n",
      "iteration: 33120 loss: 0.0043 lr: 0.02\n",
      "iteration: 33130 loss: 0.0078 lr: 0.02\n",
      "iteration: 33140 loss: 0.0083 lr: 0.02\n",
      "iteration: 33150 loss: 0.0053 lr: 0.02\n",
      "iteration: 33160 loss: 0.0055 lr: 0.02\n",
      "iteration: 33170 loss: 0.0052 lr: 0.02\n",
      "iteration: 33180 loss: 0.0072 lr: 0.02\n",
      "iteration: 33190 loss: 0.0075 lr: 0.02\n",
      "iteration: 33200 loss: 0.0079 lr: 0.02\n",
      "iteration: 33210 loss: 0.0070 lr: 0.02\n",
      "iteration: 33220 loss: 0.0059 lr: 0.02\n",
      "iteration: 33230 loss: 0.0098 lr: 0.02\n",
      "iteration: 33240 loss: 0.0067 lr: 0.02\n",
      "iteration: 33250 loss: 0.0057 lr: 0.02\n",
      "iteration: 33260 loss: 0.0045 lr: 0.02\n",
      "iteration: 33270 loss: 0.0092 lr: 0.02\n",
      "iteration: 33280 loss: 0.0065 lr: 0.02\n",
      "iteration: 33290 loss: 0.0054 lr: 0.02\n",
      "iteration: 33300 loss: 0.0034 lr: 0.02\n",
      "iteration: 33310 loss: 0.0082 lr: 0.02\n",
      "iteration: 33320 loss: 0.0041 lr: 0.02\n",
      "iteration: 33330 loss: 0.0050 lr: 0.02\n",
      "iteration: 33340 loss: 0.0067 lr: 0.02\n",
      "iteration: 33350 loss: 0.0052 lr: 0.02\n",
      "iteration: 33360 loss: 0.0064 lr: 0.02\n",
      "iteration: 33370 loss: 0.0052 lr: 0.02\n",
      "iteration: 33380 loss: 0.0045 lr: 0.02\n",
      "iteration: 33390 loss: 0.0079 lr: 0.02\n",
      "iteration: 33400 loss: 0.0051 lr: 0.02\n",
      "iteration: 33410 loss: 0.0071 lr: 0.02\n",
      "iteration: 33420 loss: 0.0045 lr: 0.02\n",
      "iteration: 33430 loss: 0.0052 lr: 0.02\n",
      "iteration: 33440 loss: 0.0056 lr: 0.02\n",
      "iteration: 33450 loss: 0.0064 lr: 0.02\n",
      "iteration: 33460 loss: 0.0061 lr: 0.02\n",
      "iteration: 33470 loss: 0.0091 lr: 0.02\n",
      "iteration: 33480 loss: 0.0058 lr: 0.02\n",
      "iteration: 33490 loss: 0.0077 lr: 0.02\n",
      "iteration: 33500 loss: 0.0043 lr: 0.02\n",
      "iteration: 33510 loss: 0.0056 lr: 0.02\n",
      "iteration: 33520 loss: 0.0037 lr: 0.02\n",
      "iteration: 33530 loss: 0.0060 lr: 0.02\n",
      "iteration: 33540 loss: 0.0066 lr: 0.02\n",
      "iteration: 33550 loss: 0.0062 lr: 0.02\n",
      "iteration: 33560 loss: 0.0049 lr: 0.02\n",
      "iteration: 33570 loss: 0.0081 lr: 0.02\n",
      "iteration: 33580 loss: 0.0044 lr: 0.02\n",
      "iteration: 33590 loss: 0.0091 lr: 0.02\n",
      "iteration: 33600 loss: 0.0059 lr: 0.02\n",
      "iteration: 33610 loss: 0.0044 lr: 0.02\n",
      "iteration: 33620 loss: 0.0075 lr: 0.02\n",
      "iteration: 33630 loss: 0.0073 lr: 0.02\n",
      "iteration: 33640 loss: 0.0085 lr: 0.02\n",
      "iteration: 33650 loss: 0.0059 lr: 0.02\n",
      "iteration: 33660 loss: 0.0080 lr: 0.02\n",
      "iteration: 33670 loss: 0.0068 lr: 0.02\n",
      "iteration: 33680 loss: 0.0044 lr: 0.02\n",
      "iteration: 33690 loss: 0.0049 lr: 0.02\n",
      "iteration: 33700 loss: 0.0047 lr: 0.02\n",
      "iteration: 33710 loss: 0.0078 lr: 0.02\n",
      "iteration: 33720 loss: 0.0054 lr: 0.02\n",
      "iteration: 33730 loss: 0.0071 lr: 0.02\n",
      "iteration: 33740 loss: 0.0068 lr: 0.02\n",
      "iteration: 33750 loss: 0.0065 lr: 0.02\n",
      "iteration: 33760 loss: 0.0046 lr: 0.02\n",
      "iteration: 33770 loss: 0.0067 lr: 0.02\n",
      "iteration: 33780 loss: 0.0061 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 33790 loss: 0.0054 lr: 0.02\n",
      "iteration: 33800 loss: 0.0066 lr: 0.02\n",
      "iteration: 33810 loss: 0.0068 lr: 0.02\n",
      "iteration: 33820 loss: 0.0072 lr: 0.02\n",
      "iteration: 33830 loss: 0.0058 lr: 0.02\n",
      "iteration: 33840 loss: 0.0056 lr: 0.02\n",
      "iteration: 33850 loss: 0.0060 lr: 0.02\n",
      "iteration: 33860 loss: 0.0042 lr: 0.02\n",
      "iteration: 33870 loss: 0.0066 lr: 0.02\n",
      "iteration: 33880 loss: 0.0082 lr: 0.02\n",
      "iteration: 33890 loss: 0.0058 lr: 0.02\n",
      "iteration: 33900 loss: 0.0085 lr: 0.02\n",
      "iteration: 33910 loss: 0.0068 lr: 0.02\n",
      "iteration: 33920 loss: 0.0077 lr: 0.02\n",
      "iteration: 33930 loss: 0.0076 lr: 0.02\n",
      "iteration: 33940 loss: 0.0079 lr: 0.02\n",
      "iteration: 33950 loss: 0.0036 lr: 0.02\n",
      "iteration: 33960 loss: 0.0059 lr: 0.02\n",
      "iteration: 33970 loss: 0.0054 lr: 0.02\n",
      "iteration: 33980 loss: 0.0061 lr: 0.02\n",
      "iteration: 33990 loss: 0.0057 lr: 0.02\n",
      "iteration: 34000 loss: 0.0064 lr: 0.02\n",
      "iteration: 34010 loss: 0.0058 lr: 0.02\n",
      "iteration: 34020 loss: 0.0059 lr: 0.02\n",
      "iteration: 34030 loss: 0.0083 lr: 0.02\n",
      "iteration: 34040 loss: 0.0056 lr: 0.02\n",
      "iteration: 34050 loss: 0.0081 lr: 0.02\n",
      "iteration: 34060 loss: 0.0058 lr: 0.02\n",
      "iteration: 34070 loss: 0.0065 lr: 0.02\n",
      "iteration: 34080 loss: 0.0072 lr: 0.02\n",
      "iteration: 34090 loss: 0.0081 lr: 0.02\n",
      "iteration: 34100 loss: 0.0046 lr: 0.02\n",
      "iteration: 34110 loss: 0.0053 lr: 0.02\n",
      "iteration: 34120 loss: 0.0045 lr: 0.02\n",
      "iteration: 34130 loss: 0.0050 lr: 0.02\n",
      "iteration: 34140 loss: 0.0077 lr: 0.02\n",
      "iteration: 34150 loss: 0.0053 lr: 0.02\n",
      "iteration: 34160 loss: 0.0051 lr: 0.02\n",
      "iteration: 34170 loss: 0.0037 lr: 0.02\n",
      "iteration: 34180 loss: 0.0066 lr: 0.02\n",
      "iteration: 34190 loss: 0.0055 lr: 0.02\n",
      "iteration: 34200 loss: 0.0051 lr: 0.02\n",
      "iteration: 34210 loss: 0.0051 lr: 0.02\n",
      "iteration: 34220 loss: 0.0075 lr: 0.02\n",
      "iteration: 34230 loss: 0.0040 lr: 0.02\n",
      "iteration: 34240 loss: 0.0054 lr: 0.02\n",
      "iteration: 34250 loss: 0.0060 lr: 0.02\n",
      "iteration: 34260 loss: 0.0055 lr: 0.02\n",
      "iteration: 34270 loss: 0.0055 lr: 0.02\n",
      "iteration: 34280 loss: 0.0091 lr: 0.02\n",
      "iteration: 34290 loss: 0.0078 lr: 0.02\n",
      "iteration: 34300 loss: 0.0058 lr: 0.02\n",
      "iteration: 34310 loss: 0.0061 lr: 0.02\n",
      "iteration: 34320 loss: 0.0059 lr: 0.02\n",
      "iteration: 34330 loss: 0.0078 lr: 0.02\n",
      "iteration: 34340 loss: 0.0050 lr: 0.02\n",
      "iteration: 34350 loss: 0.0051 lr: 0.02\n",
      "iteration: 34360 loss: 0.0105 lr: 0.02\n",
      "iteration: 34370 loss: 0.0047 lr: 0.02\n",
      "iteration: 34380 loss: 0.0052 lr: 0.02\n",
      "iteration: 34390 loss: 0.0067 lr: 0.02\n",
      "iteration: 34400 loss: 0.0070 lr: 0.02\n",
      "iteration: 34410 loss: 0.0071 lr: 0.02\n",
      "iteration: 34420 loss: 0.0069 lr: 0.02\n",
      "iteration: 34430 loss: 0.0070 lr: 0.02\n",
      "iteration: 34440 loss: 0.0056 lr: 0.02\n",
      "iteration: 34450 loss: 0.0053 lr: 0.02\n",
      "iteration: 34460 loss: 0.0061 lr: 0.02\n",
      "iteration: 34470 loss: 0.0066 lr: 0.02\n",
      "iteration: 34480 loss: 0.0076 lr: 0.02\n",
      "iteration: 34490 loss: 0.0052 lr: 0.02\n",
      "iteration: 34500 loss: 0.0053 lr: 0.02\n",
      "iteration: 34510 loss: 0.0054 lr: 0.02\n",
      "iteration: 34520 loss: 0.0061 lr: 0.02\n",
      "iteration: 34530 loss: 0.0067 lr: 0.02\n",
      "iteration: 34540 loss: 0.0051 lr: 0.02\n",
      "iteration: 34550 loss: 0.0056 lr: 0.02\n",
      "iteration: 34560 loss: 0.0080 lr: 0.02\n",
      "iteration: 34570 loss: 0.0054 lr: 0.02\n",
      "iteration: 34580 loss: 0.0081 lr: 0.02\n",
      "iteration: 34590 loss: 0.0083 lr: 0.02\n",
      "iteration: 34600 loss: 0.0069 lr: 0.02\n",
      "iteration: 34610 loss: 0.0063 lr: 0.02\n",
      "iteration: 34620 loss: 0.0084 lr: 0.02\n",
      "iteration: 34630 loss: 0.0061 lr: 0.02\n",
      "iteration: 34640 loss: 0.0061 lr: 0.02\n",
      "iteration: 34650 loss: 0.0100 lr: 0.02\n",
      "iteration: 34660 loss: 0.0062 lr: 0.02\n",
      "iteration: 34670 loss: 0.0077 lr: 0.02\n",
      "iteration: 34680 loss: 0.0063 lr: 0.02\n",
      "iteration: 34690 loss: 0.0060 lr: 0.02\n",
      "iteration: 34700 loss: 0.0075 lr: 0.02\n",
      "iteration: 34710 loss: 0.0049 lr: 0.02\n",
      "iteration: 34720 loss: 0.0072 lr: 0.02\n",
      "iteration: 34730 loss: 0.0054 lr: 0.02\n",
      "iteration: 34740 loss: 0.0065 lr: 0.02\n",
      "iteration: 34750 loss: 0.0097 lr: 0.02\n",
      "iteration: 34760 loss: 0.0079 lr: 0.02\n",
      "iteration: 34770 loss: 0.0062 lr: 0.02\n",
      "iteration: 34780 loss: 0.0079 lr: 0.02\n",
      "iteration: 34790 loss: 0.0077 lr: 0.02\n",
      "iteration: 34800 loss: 0.0093 lr: 0.02\n",
      "iteration: 34810 loss: 0.0055 lr: 0.02\n",
      "iteration: 34820 loss: 0.0066 lr: 0.02\n",
      "iteration: 34830 loss: 0.0066 lr: 0.02\n",
      "iteration: 34840 loss: 0.0052 lr: 0.02\n",
      "iteration: 34850 loss: 0.0047 lr: 0.02\n",
      "iteration: 34860 loss: 0.0079 lr: 0.02\n",
      "iteration: 34870 loss: 0.0065 lr: 0.02\n",
      "iteration: 34880 loss: 0.0057 lr: 0.02\n",
      "iteration: 34890 loss: 0.0069 lr: 0.02\n",
      "iteration: 34900 loss: 0.0071 lr: 0.02\n",
      "iteration: 34910 loss: 0.0041 lr: 0.02\n",
      "iteration: 34920 loss: 0.0064 lr: 0.02\n",
      "iteration: 34930 loss: 0.0072 lr: 0.02\n",
      "iteration: 34940 loss: 0.0052 lr: 0.02\n",
      "iteration: 34950 loss: 0.0045 lr: 0.02\n",
      "iteration: 34960 loss: 0.0052 lr: 0.02\n",
      "iteration: 34970 loss: 0.0052 lr: 0.02\n",
      "iteration: 34980 loss: 0.0061 lr: 0.02\n",
      "iteration: 34990 loss: 0.0056 lr: 0.02\n",
      "iteration: 35000 loss: 0.0054 lr: 0.02\n",
      "iteration: 35010 loss: 0.0073 lr: 0.02\n",
      "iteration: 35020 loss: 0.0059 lr: 0.02\n",
      "iteration: 35030 loss: 0.0056 lr: 0.02\n",
      "iteration: 35040 loss: 0.0071 lr: 0.02\n",
      "iteration: 35050 loss: 0.0079 lr: 0.02\n",
      "iteration: 35060 loss: 0.0080 lr: 0.02\n",
      "iteration: 35070 loss: 0.0046 lr: 0.02\n",
      "iteration: 35080 loss: 0.0059 lr: 0.02\n",
      "iteration: 35090 loss: 0.0053 lr: 0.02\n",
      "iteration: 35100 loss: 0.0074 lr: 0.02\n",
      "iteration: 35110 loss: 0.0064 lr: 0.02\n",
      "iteration: 35120 loss: 0.0091 lr: 0.02\n",
      "iteration: 35130 loss: 0.0063 lr: 0.02\n",
      "iteration: 35140 loss: 0.0048 lr: 0.02\n",
      "iteration: 35150 loss: 0.0054 lr: 0.02\n",
      "iteration: 35160 loss: 0.0059 lr: 0.02\n",
      "iteration: 35170 loss: 0.0064 lr: 0.02\n",
      "iteration: 35180 loss: 0.0069 lr: 0.02\n",
      "iteration: 35190 loss: 0.0071 lr: 0.02\n",
      "iteration: 35200 loss: 0.0048 lr: 0.02\n",
      "iteration: 35210 loss: 0.0089 lr: 0.02\n",
      "iteration: 35220 loss: 0.0072 lr: 0.02\n",
      "iteration: 35230 loss: 0.0061 lr: 0.02\n",
      "iteration: 35240 loss: 0.0084 lr: 0.02\n",
      "iteration: 35250 loss: 0.0079 lr: 0.02\n",
      "iteration: 35260 loss: 0.0049 lr: 0.02\n",
      "iteration: 35270 loss: 0.0056 lr: 0.02\n",
      "iteration: 35280 loss: 0.0049 lr: 0.02\n",
      "iteration: 35290 loss: 0.0042 lr: 0.02\n",
      "iteration: 35300 loss: 0.0071 lr: 0.02\n",
      "iteration: 35310 loss: 0.0061 lr: 0.02\n",
      "iteration: 35320 loss: 0.0094 lr: 0.02\n",
      "iteration: 35330 loss: 0.0088 lr: 0.02\n",
      "iteration: 35340 loss: 0.0080 lr: 0.02\n",
      "iteration: 35350 loss: 0.0061 lr: 0.02\n",
      "iteration: 35360 loss: 0.0051 lr: 0.02\n",
      "iteration: 35370 loss: 0.0098 lr: 0.02\n",
      "iteration: 35380 loss: 0.0061 lr: 0.02\n",
      "iteration: 35390 loss: 0.0054 lr: 0.02\n",
      "iteration: 35400 loss: 0.0066 lr: 0.02\n",
      "iteration: 35410 loss: 0.0081 lr: 0.02\n",
      "iteration: 35420 loss: 0.0051 lr: 0.02\n",
      "iteration: 35430 loss: 0.0096 lr: 0.02\n",
      "iteration: 35440 loss: 0.0069 lr: 0.02\n",
      "iteration: 35450 loss: 0.0068 lr: 0.02\n",
      "iteration: 35460 loss: 0.0055 lr: 0.02\n",
      "iteration: 35470 loss: 0.0056 lr: 0.02\n",
      "iteration: 35480 loss: 0.0054 lr: 0.02\n",
      "iteration: 35490 loss: 0.0047 lr: 0.02\n",
      "iteration: 35500 loss: 0.0074 lr: 0.02\n",
      "iteration: 35510 loss: 0.0068 lr: 0.02\n",
      "iteration: 35520 loss: 0.0039 lr: 0.02\n",
      "iteration: 35530 loss: 0.0068 lr: 0.02\n",
      "iteration: 35540 loss: 0.0080 lr: 0.02\n",
      "iteration: 35550 loss: 0.0062 lr: 0.02\n",
      "iteration: 35560 loss: 0.0072 lr: 0.02\n",
      "iteration: 35570 loss: 0.0056 lr: 0.02\n",
      "iteration: 35580 loss: 0.0056 lr: 0.02\n",
      "iteration: 35590 loss: 0.0086 lr: 0.02\n",
      "iteration: 35600 loss: 0.0069 lr: 0.02\n",
      "iteration: 35610 loss: 0.0079 lr: 0.02\n",
      "iteration: 35620 loss: 0.0040 lr: 0.02\n",
      "iteration: 35630 loss: 0.0077 lr: 0.02\n",
      "iteration: 35640 loss: 0.0075 lr: 0.02\n",
      "iteration: 35650 loss: 0.0053 lr: 0.02\n",
      "iteration: 35660 loss: 0.0050 lr: 0.02\n",
      "iteration: 35670 loss: 0.0062 lr: 0.02\n",
      "iteration: 35680 loss: 0.0057 lr: 0.02\n",
      "iteration: 35690 loss: 0.0069 lr: 0.02\n",
      "iteration: 35700 loss: 0.0060 lr: 0.02\n",
      "iteration: 35710 loss: 0.0052 lr: 0.02\n",
      "iteration: 35720 loss: 0.0050 lr: 0.02\n",
      "iteration: 35730 loss: 0.0072 lr: 0.02\n",
      "iteration: 35740 loss: 0.0048 lr: 0.02\n",
      "iteration: 35750 loss: 0.0065 lr: 0.02\n",
      "iteration: 35760 loss: 0.0085 lr: 0.02\n",
      "iteration: 35770 loss: 0.0065 lr: 0.02\n",
      "iteration: 35780 loss: 0.0050 lr: 0.02\n",
      "iteration: 35790 loss: 0.0047 lr: 0.02\n",
      "iteration: 35800 loss: 0.0059 lr: 0.02\n",
      "iteration: 35810 loss: 0.0071 lr: 0.02\n",
      "iteration: 35820 loss: 0.0091 lr: 0.02\n",
      "iteration: 35830 loss: 0.0067 lr: 0.02\n",
      "iteration: 35840 loss: 0.0090 lr: 0.02\n",
      "iteration: 35850 loss: 0.0061 lr: 0.02\n",
      "iteration: 35860 loss: 0.0065 lr: 0.02\n",
      "iteration: 35870 loss: 0.0073 lr: 0.02\n",
      "iteration: 35880 loss: 0.0054 lr: 0.02\n",
      "iteration: 35890 loss: 0.0060 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 35900 loss: 0.0063 lr: 0.02\n",
      "iteration: 35910 loss: 0.0065 lr: 0.02\n",
      "iteration: 35920 loss: 0.0050 lr: 0.02\n",
      "iteration: 35930 loss: 0.0057 lr: 0.02\n",
      "iteration: 35940 loss: 0.0059 lr: 0.02\n",
      "iteration: 35950 loss: 0.0057 lr: 0.02\n",
      "iteration: 35960 loss: 0.0056 lr: 0.02\n",
      "iteration: 35970 loss: 0.0105 lr: 0.02\n",
      "iteration: 35980 loss: 0.0074 lr: 0.02\n",
      "iteration: 35990 loss: 0.0059 lr: 0.02\n",
      "iteration: 36000 loss: 0.0057 lr: 0.02\n",
      "iteration: 36010 loss: 0.0065 lr: 0.02\n",
      "iteration: 36020 loss: 0.0083 lr: 0.02\n",
      "iteration: 36030 loss: 0.0064 lr: 0.02\n",
      "iteration: 36040 loss: 0.0079 lr: 0.02\n",
      "iteration: 36050 loss: 0.0073 lr: 0.02\n",
      "iteration: 36060 loss: 0.0075 lr: 0.02\n",
      "iteration: 36070 loss: 0.0068 lr: 0.02\n",
      "iteration: 36080 loss: 0.0049 lr: 0.02\n",
      "iteration: 36090 loss: 0.0063 lr: 0.02\n",
      "iteration: 36100 loss: 0.0042 lr: 0.02\n",
      "iteration: 36110 loss: 0.0080 lr: 0.02\n",
      "iteration: 36120 loss: 0.0060 lr: 0.02\n",
      "iteration: 36130 loss: 0.0067 lr: 0.02\n",
      "iteration: 36140 loss: 0.0061 lr: 0.02\n",
      "iteration: 36150 loss: 0.0041 lr: 0.02\n",
      "iteration: 36160 loss: 0.0062 lr: 0.02\n",
      "iteration: 36170 loss: 0.0062 lr: 0.02\n",
      "iteration: 36180 loss: 0.0080 lr: 0.02\n",
      "iteration: 36190 loss: 0.0062 lr: 0.02\n",
      "iteration: 36200 loss: 0.0057 lr: 0.02\n",
      "iteration: 36210 loss: 0.0071 lr: 0.02\n",
      "iteration: 36220 loss: 0.0075 lr: 0.02\n",
      "iteration: 36230 loss: 0.0071 lr: 0.02\n",
      "iteration: 36240 loss: 0.0083 lr: 0.02\n",
      "iteration: 36250 loss: 0.0040 lr: 0.02\n",
      "iteration: 36260 loss: 0.0059 lr: 0.02\n",
      "iteration: 36270 loss: 0.0060 lr: 0.02\n",
      "iteration: 36280 loss: 0.0066 lr: 0.02\n",
      "iteration: 36290 loss: 0.0051 lr: 0.02\n",
      "iteration: 36300 loss: 0.0050 lr: 0.02\n",
      "iteration: 36310 loss: 0.0079 lr: 0.02\n",
      "iteration: 36320 loss: 0.0068 lr: 0.02\n",
      "iteration: 36330 loss: 0.0086 lr: 0.02\n",
      "iteration: 36340 loss: 0.0053 lr: 0.02\n",
      "iteration: 36350 loss: 0.0068 lr: 0.02\n",
      "iteration: 36360 loss: 0.0095 lr: 0.02\n",
      "iteration: 36370 loss: 0.0067 lr: 0.02\n",
      "iteration: 36380 loss: 0.0081 lr: 0.02\n",
      "iteration: 36390 loss: 0.0042 lr: 0.02\n",
      "iteration: 36400 loss: 0.0055 lr: 0.02\n",
      "iteration: 36410 loss: 0.0044 lr: 0.02\n",
      "iteration: 36420 loss: 0.0063 lr: 0.02\n",
      "iteration: 36430 loss: 0.0054 lr: 0.02\n",
      "iteration: 36440 loss: 0.0045 lr: 0.02\n",
      "iteration: 36450 loss: 0.0057 lr: 0.02\n",
      "iteration: 36460 loss: 0.0047 lr: 0.02\n",
      "iteration: 36470 loss: 0.0074 lr: 0.02\n",
      "iteration: 36480 loss: 0.0074 lr: 0.02\n",
      "iteration: 36490 loss: 0.0054 lr: 0.02\n",
      "iteration: 36500 loss: 0.0043 lr: 0.02\n",
      "iteration: 36510 loss: 0.0056 lr: 0.02\n",
      "iteration: 36520 loss: 0.0051 lr: 0.02\n",
      "iteration: 36530 loss: 0.0063 lr: 0.02\n",
      "iteration: 36540 loss: 0.0053 lr: 0.02\n",
      "iteration: 36550 loss: 0.0050 lr: 0.02\n",
      "iteration: 36560 loss: 0.0084 lr: 0.02\n",
      "iteration: 36570 loss: 0.0046 lr: 0.02\n",
      "iteration: 36580 loss: 0.0064 lr: 0.02\n",
      "iteration: 36590 loss: 0.0065 lr: 0.02\n",
      "iteration: 36600 loss: 0.0040 lr: 0.02\n",
      "iteration: 36610 loss: 0.0071 lr: 0.02\n",
      "iteration: 36620 loss: 0.0059 lr: 0.02\n",
      "iteration: 36630 loss: 0.0044 lr: 0.02\n",
      "iteration: 36640 loss: 0.0056 lr: 0.02\n",
      "iteration: 36650 loss: 0.0092 lr: 0.02\n",
      "iteration: 36660 loss: 0.0071 lr: 0.02\n",
      "iteration: 36670 loss: 0.0058 lr: 0.02\n",
      "iteration: 36680 loss: 0.0062 lr: 0.02\n",
      "iteration: 36690 loss: 0.0069 lr: 0.02\n",
      "iteration: 36700 loss: 0.0052 lr: 0.02\n",
      "iteration: 36710 loss: 0.0054 lr: 0.02\n",
      "iteration: 36720 loss: 0.0072 lr: 0.02\n",
      "iteration: 36730 loss: 0.0070 lr: 0.02\n",
      "iteration: 36740 loss: 0.0079 lr: 0.02\n",
      "iteration: 36750 loss: 0.0064 lr: 0.02\n",
      "iteration: 36760 loss: 0.0096 lr: 0.02\n",
      "iteration: 36770 loss: 0.0083 lr: 0.02\n",
      "iteration: 36780 loss: 0.0066 lr: 0.02\n",
      "iteration: 36790 loss: 0.0056 lr: 0.02\n",
      "iteration: 36800 loss: 0.0052 lr: 0.02\n",
      "iteration: 36810 loss: 0.0070 lr: 0.02\n",
      "iteration: 36820 loss: 0.0058 lr: 0.02\n",
      "iteration: 36830 loss: 0.0063 lr: 0.02\n",
      "iteration: 36840 loss: 0.0065 lr: 0.02\n",
      "iteration: 36850 loss: 0.0055 lr: 0.02\n",
      "iteration: 36860 loss: 0.0072 lr: 0.02\n",
      "iteration: 36870 loss: 0.0081 lr: 0.02\n",
      "iteration: 36880 loss: 0.0060 lr: 0.02\n",
      "iteration: 36890 loss: 0.0056 lr: 0.02\n",
      "iteration: 36900 loss: 0.0054 lr: 0.02\n",
      "iteration: 36910 loss: 0.0050 lr: 0.02\n",
      "iteration: 36920 loss: 0.0052 lr: 0.02\n",
      "iteration: 36930 loss: 0.0058 lr: 0.02\n",
      "iteration: 36940 loss: 0.0063 lr: 0.02\n",
      "iteration: 36950 loss: 0.0059 lr: 0.02\n",
      "iteration: 36960 loss: 0.0050 lr: 0.02\n",
      "iteration: 36970 loss: 0.0055 lr: 0.02\n",
      "iteration: 36980 loss: 0.0051 lr: 0.02\n",
      "iteration: 36990 loss: 0.0051 lr: 0.02\n",
      "iteration: 37000 loss: 0.0040 lr: 0.02\n",
      "iteration: 37010 loss: 0.0065 lr: 0.02\n",
      "iteration: 37020 loss: 0.0043 lr: 0.02\n",
      "iteration: 37030 loss: 0.0070 lr: 0.02\n",
      "iteration: 37040 loss: 0.0059 lr: 0.02\n",
      "iteration: 37050 loss: 0.0045 lr: 0.02\n",
      "iteration: 37060 loss: 0.0074 lr: 0.02\n",
      "iteration: 37070 loss: 0.0070 lr: 0.02\n",
      "iteration: 37080 loss: 0.0074 lr: 0.02\n",
      "iteration: 37090 loss: 0.0058 lr: 0.02\n",
      "iteration: 37100 loss: 0.0072 lr: 0.02\n",
      "iteration: 37110 loss: 0.0087 lr: 0.02\n",
      "iteration: 37120 loss: 0.0061 lr: 0.02\n",
      "iteration: 37130 loss: 0.0049 lr: 0.02\n",
      "iteration: 37140 loss: 0.0074 lr: 0.02\n",
      "iteration: 37150 loss: 0.0060 lr: 0.02\n",
      "iteration: 37160 loss: 0.0052 lr: 0.02\n",
      "iteration: 37170 loss: 0.0059 lr: 0.02\n",
      "iteration: 37180 loss: 0.0046 lr: 0.02\n",
      "iteration: 37190 loss: 0.0053 lr: 0.02\n",
      "iteration: 37200 loss: 0.0065 lr: 0.02\n",
      "iteration: 37210 loss: 0.0053 lr: 0.02\n",
      "iteration: 37220 loss: 0.0056 lr: 0.02\n",
      "iteration: 37230 loss: 0.0065 lr: 0.02\n",
      "iteration: 37240 loss: 0.0061 lr: 0.02\n",
      "iteration: 37250 loss: 0.0053 lr: 0.02\n",
      "iteration: 37260 loss: 0.0064 lr: 0.02\n",
      "iteration: 37270 loss: 0.0057 lr: 0.02\n",
      "iteration: 37280 loss: 0.0068 lr: 0.02\n",
      "iteration: 37290 loss: 0.0114 lr: 0.02\n",
      "iteration: 37300 loss: 0.0073 lr: 0.02\n",
      "iteration: 37310 loss: 0.0054 lr: 0.02\n",
      "iteration: 37320 loss: 0.0059 lr: 0.02\n",
      "iteration: 37330 loss: 0.0082 lr: 0.02\n",
      "iteration: 37340 loss: 0.0080 lr: 0.02\n",
      "iteration: 37350 loss: 0.0048 lr: 0.02\n",
      "iteration: 37360 loss: 0.0066 lr: 0.02\n",
      "iteration: 37370 loss: 0.0061 lr: 0.02\n",
      "iteration: 37380 loss: 0.0066 lr: 0.02\n",
      "iteration: 37390 loss: 0.0059 lr: 0.02\n",
      "iteration: 37400 loss: 0.0069 lr: 0.02\n",
      "iteration: 37410 loss: 0.0049 lr: 0.02\n",
      "iteration: 37420 loss: 0.0069 lr: 0.02\n",
      "iteration: 37430 loss: 0.0052 lr: 0.02\n",
      "iteration: 37440 loss: 0.0088 lr: 0.02\n",
      "iteration: 37450 loss: 0.0052 lr: 0.02\n",
      "iteration: 37460 loss: 0.0088 lr: 0.02\n",
      "iteration: 37470 loss: 0.0078 lr: 0.02\n",
      "iteration: 37480 loss: 0.0053 lr: 0.02\n",
      "iteration: 37490 loss: 0.0084 lr: 0.02\n",
      "iteration: 37500 loss: 0.0114 lr: 0.02\n",
      "iteration: 37510 loss: 0.0059 lr: 0.02\n",
      "iteration: 37520 loss: 0.0083 lr: 0.02\n",
      "iteration: 37530 loss: 0.0048 lr: 0.02\n",
      "iteration: 37540 loss: 0.0058 lr: 0.02\n",
      "iteration: 37550 loss: 0.0060 lr: 0.02\n",
      "iteration: 37560 loss: 0.0076 lr: 0.02\n",
      "iteration: 37570 loss: 0.0108 lr: 0.02\n",
      "iteration: 37580 loss: 0.0064 lr: 0.02\n",
      "iteration: 37590 loss: 0.0051 lr: 0.02\n",
      "iteration: 37600 loss: 0.0076 lr: 0.02\n",
      "iteration: 37610 loss: 0.0079 lr: 0.02\n",
      "iteration: 37620 loss: 0.0055 lr: 0.02\n",
      "iteration: 37630 loss: 0.0080 lr: 0.02\n",
      "iteration: 37640 loss: 0.0064 lr: 0.02\n",
      "iteration: 37650 loss: 0.0059 lr: 0.02\n",
      "iteration: 37660 loss: 0.0066 lr: 0.02\n",
      "iteration: 37670 loss: 0.0063 lr: 0.02\n",
      "iteration: 37680 loss: 0.0057 lr: 0.02\n",
      "iteration: 37690 loss: 0.0063 lr: 0.02\n",
      "iteration: 37700 loss: 0.0041 lr: 0.02\n",
      "iteration: 37710 loss: 0.0069 lr: 0.02\n",
      "iteration: 37720 loss: 0.0058 lr: 0.02\n",
      "iteration: 37730 loss: 0.0067 lr: 0.02\n",
      "iteration: 37740 loss: 0.0060 lr: 0.02\n",
      "iteration: 37750 loss: 0.0054 lr: 0.02\n",
      "iteration: 37760 loss: 0.0067 lr: 0.02\n",
      "iteration: 37770 loss: 0.0069 lr: 0.02\n",
      "iteration: 37780 loss: 0.0066 lr: 0.02\n",
      "iteration: 37790 loss: 0.0065 lr: 0.02\n",
      "iteration: 37800 loss: 0.0048 lr: 0.02\n",
      "iteration: 37810 loss: 0.0070 lr: 0.02\n",
      "iteration: 37820 loss: 0.0072 lr: 0.02\n",
      "iteration: 37830 loss: 0.0067 lr: 0.02\n",
      "iteration: 37840 loss: 0.0067 lr: 0.02\n",
      "iteration: 37850 loss: 0.0049 lr: 0.02\n",
      "iteration: 37860 loss: 0.0049 lr: 0.02\n",
      "iteration: 37870 loss: 0.0042 lr: 0.02\n",
      "iteration: 37880 loss: 0.0062 lr: 0.02\n",
      "iteration: 37890 loss: 0.0077 lr: 0.02\n",
      "iteration: 37900 loss: 0.0062 lr: 0.02\n",
      "iteration: 37910 loss: 0.0049 lr: 0.02\n",
      "iteration: 37920 loss: 0.0057 lr: 0.02\n",
      "iteration: 37930 loss: 0.0044 lr: 0.02\n",
      "iteration: 37940 loss: 0.0050 lr: 0.02\n",
      "iteration: 37950 loss: 0.0060 lr: 0.02\n",
      "iteration: 37960 loss: 0.0047 lr: 0.02\n",
      "iteration: 37970 loss: 0.0050 lr: 0.02\n",
      "iteration: 37980 loss: 0.0045 lr: 0.02\n",
      "iteration: 37990 loss: 0.0041 lr: 0.02\n",
      "iteration: 38000 loss: 0.0051 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 38010 loss: 0.0065 lr: 0.02\n",
      "iteration: 38020 loss: 0.0057 lr: 0.02\n",
      "iteration: 38030 loss: 0.0068 lr: 0.02\n",
      "iteration: 38040 loss: 0.0054 lr: 0.02\n",
      "iteration: 38050 loss: 0.0048 lr: 0.02\n",
      "iteration: 38060 loss: 0.0052 lr: 0.02\n",
      "iteration: 38070 loss: 0.0074 lr: 0.02\n",
      "iteration: 38080 loss: 0.0091 lr: 0.02\n",
      "iteration: 38090 loss: 0.0073 lr: 0.02\n",
      "iteration: 38100 loss: 0.0074 lr: 0.02\n",
      "iteration: 38110 loss: 0.0071 lr: 0.02\n",
      "iteration: 38120 loss: 0.0069 lr: 0.02\n",
      "iteration: 38130 loss: 0.0084 lr: 0.02\n",
      "iteration: 38140 loss: 0.0055 lr: 0.02\n",
      "iteration: 38150 loss: 0.0072 lr: 0.02\n",
      "iteration: 38160 loss: 0.0066 lr: 0.02\n",
      "iteration: 38170 loss: 0.0055 lr: 0.02\n",
      "iteration: 38180 loss: 0.0046 lr: 0.02\n",
      "iteration: 38190 loss: 0.0056 lr: 0.02\n",
      "iteration: 38200 loss: 0.0071 lr: 0.02\n",
      "iteration: 38210 loss: 0.0072 lr: 0.02\n",
      "iteration: 38220 loss: 0.0053 lr: 0.02\n",
      "iteration: 38230 loss: 0.0056 lr: 0.02\n",
      "iteration: 38240 loss: 0.0042 lr: 0.02\n",
      "iteration: 38250 loss: 0.0060 lr: 0.02\n",
      "iteration: 38260 loss: 0.0046 lr: 0.02\n",
      "iteration: 38270 loss: 0.0048 lr: 0.02\n",
      "iteration: 38280 loss: 0.0050 lr: 0.02\n",
      "iteration: 38290 loss: 0.0052 lr: 0.02\n",
      "iteration: 38300 loss: 0.0067 lr: 0.02\n",
      "iteration: 38310 loss: 0.0062 lr: 0.02\n",
      "iteration: 38320 loss: 0.0043 lr: 0.02\n",
      "iteration: 38330 loss: 0.0066 lr: 0.02\n",
      "iteration: 38340 loss: 0.0071 lr: 0.02\n",
      "iteration: 38350 loss: 0.0041 lr: 0.02\n",
      "iteration: 38360 loss: 0.0079 lr: 0.02\n",
      "iteration: 38370 loss: 0.0059 lr: 0.02\n",
      "iteration: 38380 loss: 0.0050 lr: 0.02\n",
      "iteration: 38390 loss: 0.0073 lr: 0.02\n",
      "iteration: 38400 loss: 0.0073 lr: 0.02\n",
      "iteration: 38410 loss: 0.0069 lr: 0.02\n",
      "iteration: 38420 loss: 0.0069 lr: 0.02\n",
      "iteration: 38430 loss: 0.0053 lr: 0.02\n",
      "iteration: 38440 loss: 0.0067 lr: 0.02\n",
      "iteration: 38450 loss: 0.0058 lr: 0.02\n",
      "iteration: 38460 loss: 0.0073 lr: 0.02\n",
      "iteration: 38470 loss: 0.0070 lr: 0.02\n",
      "iteration: 38480 loss: 0.0081 lr: 0.02\n",
      "iteration: 38490 loss: 0.0060 lr: 0.02\n",
      "iteration: 38500 loss: 0.0051 lr: 0.02\n",
      "iteration: 38510 loss: 0.0045 lr: 0.02\n",
      "iteration: 38520 loss: 0.0100 lr: 0.02\n",
      "iteration: 38530 loss: 0.0080 lr: 0.02\n",
      "iteration: 38540 loss: 0.0073 lr: 0.02\n",
      "iteration: 38550 loss: 0.0057 lr: 0.02\n",
      "iteration: 38560 loss: 0.0040 lr: 0.02\n",
      "iteration: 38570 loss: 0.0073 lr: 0.02\n",
      "iteration: 38580 loss: 0.0062 lr: 0.02\n",
      "iteration: 38590 loss: 0.0060 lr: 0.02\n",
      "iteration: 38600 loss: 0.0059 lr: 0.02\n",
      "iteration: 38610 loss: 0.0051 lr: 0.02\n",
      "iteration: 38620 loss: 0.0063 lr: 0.02\n",
      "iteration: 38630 loss: 0.0069 lr: 0.02\n",
      "iteration: 38640 loss: 0.0091 lr: 0.02\n",
      "iteration: 38650 loss: 0.0077 lr: 0.02\n",
      "iteration: 38660 loss: 0.0049 lr: 0.02\n",
      "iteration: 38670 loss: 0.0067 lr: 0.02\n",
      "iteration: 38680 loss: 0.0075 lr: 0.02\n",
      "iteration: 38690 loss: 0.0042 lr: 0.02\n",
      "iteration: 38700 loss: 0.0047 lr: 0.02\n",
      "iteration: 38710 loss: 0.0078 lr: 0.02\n",
      "iteration: 38720 loss: 0.0045 lr: 0.02\n",
      "iteration: 38730 loss: 0.0045 lr: 0.02\n",
      "iteration: 38740 loss: 0.0062 lr: 0.02\n",
      "iteration: 38750 loss: 0.0056 lr: 0.02\n",
      "iteration: 38760 loss: 0.0052 lr: 0.02\n",
      "iteration: 38770 loss: 0.0060 lr: 0.02\n",
      "iteration: 38780 loss: 0.0060 lr: 0.02\n",
      "iteration: 38790 loss: 0.0058 lr: 0.02\n",
      "iteration: 38800 loss: 0.0051 lr: 0.02\n",
      "iteration: 38810 loss: 0.0070 lr: 0.02\n",
      "iteration: 38820 loss: 0.0082 lr: 0.02\n",
      "iteration: 38830 loss: 0.0066 lr: 0.02\n",
      "iteration: 38840 loss: 0.0054 lr: 0.02\n",
      "iteration: 38850 loss: 0.0071 lr: 0.02\n",
      "iteration: 38860 loss: 0.0060 lr: 0.02\n",
      "iteration: 38870 loss: 0.0059 lr: 0.02\n",
      "iteration: 38880 loss: 0.0052 lr: 0.02\n",
      "iteration: 38890 loss: 0.0051 lr: 0.02\n",
      "iteration: 38900 loss: 0.0037 lr: 0.02\n",
      "iteration: 38910 loss: 0.0050 lr: 0.02\n",
      "iteration: 38920 loss: 0.0045 lr: 0.02\n",
      "iteration: 38930 loss: 0.0074 lr: 0.02\n",
      "iteration: 38940 loss: 0.0037 lr: 0.02\n",
      "iteration: 38950 loss: 0.0061 lr: 0.02\n",
      "iteration: 38960 loss: 0.0072 lr: 0.02\n",
      "iteration: 38970 loss: 0.0063 lr: 0.02\n",
      "iteration: 38980 loss: 0.0045 lr: 0.02\n",
      "iteration: 38990 loss: 0.0065 lr: 0.02\n",
      "iteration: 39000 loss: 0.0053 lr: 0.02\n",
      "iteration: 39010 loss: 0.0058 lr: 0.02\n",
      "iteration: 39020 loss: 0.0047 lr: 0.02\n",
      "iteration: 39030 loss: 0.0049 lr: 0.02\n",
      "iteration: 39040 loss: 0.0046 lr: 0.02\n",
      "iteration: 39050 loss: 0.0049 lr: 0.02\n",
      "iteration: 39060 loss: 0.0047 lr: 0.02\n",
      "iteration: 39070 loss: 0.0099 lr: 0.02\n",
      "iteration: 39080 loss: 0.0052 lr: 0.02\n",
      "iteration: 39090 loss: 0.0068 lr: 0.02\n",
      "iteration: 39100 loss: 0.0071 lr: 0.02\n",
      "iteration: 39110 loss: 0.0041 lr: 0.02\n",
      "iteration: 39120 loss: 0.0062 lr: 0.02\n",
      "iteration: 39130 loss: 0.0050 lr: 0.02\n",
      "iteration: 39140 loss: 0.0074 lr: 0.02\n",
      "iteration: 39150 loss: 0.0063 lr: 0.02\n",
      "iteration: 39160 loss: 0.0046 lr: 0.02\n",
      "iteration: 39170 loss: 0.0061 lr: 0.02\n",
      "iteration: 39180 loss: 0.0054 lr: 0.02\n",
      "iteration: 39190 loss: 0.0058 lr: 0.02\n",
      "iteration: 39200 loss: 0.0057 lr: 0.02\n",
      "iteration: 39210 loss: 0.0061 lr: 0.02\n",
      "iteration: 39220 loss: 0.0078 lr: 0.02\n",
      "iteration: 39230 loss: 0.0050 lr: 0.02\n",
      "iteration: 39240 loss: 0.0056 lr: 0.02\n",
      "iteration: 39250 loss: 0.0069 lr: 0.02\n",
      "iteration: 39260 loss: 0.0034 lr: 0.02\n",
      "iteration: 39270 loss: 0.0053 lr: 0.02\n",
      "iteration: 39280 loss: 0.0039 lr: 0.02\n",
      "iteration: 39290 loss: 0.0044 lr: 0.02\n",
      "iteration: 39300 loss: 0.0045 lr: 0.02\n",
      "iteration: 39310 loss: 0.0067 lr: 0.02\n",
      "iteration: 39320 loss: 0.0068 lr: 0.02\n",
      "iteration: 39330 loss: 0.0080 lr: 0.02\n",
      "iteration: 39340 loss: 0.0048 lr: 0.02\n",
      "iteration: 39350 loss: 0.0039 lr: 0.02\n",
      "iteration: 39360 loss: 0.0058 lr: 0.02\n",
      "iteration: 39370 loss: 0.0066 lr: 0.02\n",
      "iteration: 39380 loss: 0.0067 lr: 0.02\n",
      "iteration: 39390 loss: 0.0077 lr: 0.02\n",
      "iteration: 39400 loss: 0.0067 lr: 0.02\n",
      "iteration: 39410 loss: 0.0086 lr: 0.02\n",
      "iteration: 39420 loss: 0.0061 lr: 0.02\n",
      "iteration: 39430 loss: 0.0065 lr: 0.02\n",
      "iteration: 39440 loss: 0.0050 lr: 0.02\n",
      "iteration: 39450 loss: 0.0057 lr: 0.02\n",
      "iteration: 39460 loss: 0.0074 lr: 0.02\n",
      "iteration: 39470 loss: 0.0081 lr: 0.02\n",
      "iteration: 39480 loss: 0.0070 lr: 0.02\n",
      "iteration: 39490 loss: 0.0048 lr: 0.02\n",
      "iteration: 39500 loss: 0.0048 lr: 0.02\n",
      "iteration: 39510 loss: 0.0056 lr: 0.02\n",
      "iteration: 39520 loss: 0.0094 lr: 0.02\n",
      "iteration: 39530 loss: 0.0058 lr: 0.02\n",
      "iteration: 39540 loss: 0.0085 lr: 0.02\n",
      "iteration: 39550 loss: 0.0114 lr: 0.02\n",
      "iteration: 39560 loss: 0.0086 lr: 0.02\n",
      "iteration: 39570 loss: 0.0058 lr: 0.02\n",
      "iteration: 39580 loss: 0.0063 lr: 0.02\n",
      "iteration: 39590 loss: 0.0057 lr: 0.02\n",
      "iteration: 39600 loss: 0.0068 lr: 0.02\n",
      "iteration: 39610 loss: 0.0051 lr: 0.02\n",
      "iteration: 39620 loss: 0.0065 lr: 0.02\n",
      "iteration: 39630 loss: 0.0063 lr: 0.02\n",
      "iteration: 39640 loss: 0.0056 lr: 0.02\n",
      "iteration: 39650 loss: 0.0058 lr: 0.02\n",
      "iteration: 39660 loss: 0.0080 lr: 0.02\n",
      "iteration: 39670 loss: 0.0069 lr: 0.02\n",
      "iteration: 39680 loss: 0.0063 lr: 0.02\n",
      "iteration: 39690 loss: 0.0064 lr: 0.02\n",
      "iteration: 39700 loss: 0.0056 lr: 0.02\n",
      "iteration: 39710 loss: 0.0043 lr: 0.02\n",
      "iteration: 39720 loss: 0.0044 lr: 0.02\n",
      "iteration: 39730 loss: 0.0066 lr: 0.02\n",
      "iteration: 39740 loss: 0.0066 lr: 0.02\n",
      "iteration: 39750 loss: 0.0053 lr: 0.02\n",
      "iteration: 39760 loss: 0.0075 lr: 0.02\n",
      "iteration: 39770 loss: 0.0058 lr: 0.02\n",
      "iteration: 39780 loss: 0.0044 lr: 0.02\n",
      "iteration: 39790 loss: 0.0043 lr: 0.02\n",
      "iteration: 39800 loss: 0.0053 lr: 0.02\n",
      "iteration: 39810 loss: 0.0069 lr: 0.02\n",
      "iteration: 39820 loss: 0.0043 lr: 0.02\n",
      "iteration: 39830 loss: 0.0050 lr: 0.02\n",
      "iteration: 39840 loss: 0.0052 lr: 0.02\n",
      "iteration: 39850 loss: 0.0050 lr: 0.02\n",
      "iteration: 39860 loss: 0.0063 lr: 0.02\n",
      "iteration: 39870 loss: 0.0053 lr: 0.02\n",
      "iteration: 39880 loss: 0.0041 lr: 0.02\n",
      "iteration: 39890 loss: 0.0066 lr: 0.02\n",
      "iteration: 39900 loss: 0.0055 lr: 0.02\n",
      "iteration: 39910 loss: 0.0043 lr: 0.02\n",
      "iteration: 39920 loss: 0.0048 lr: 0.02\n",
      "iteration: 39930 loss: 0.0057 lr: 0.02\n",
      "iteration: 39940 loss: 0.0061 lr: 0.02\n",
      "iteration: 39950 loss: 0.0057 lr: 0.02\n",
      "iteration: 39960 loss: 0.0059 lr: 0.02\n",
      "iteration: 39970 loss: 0.0069 lr: 0.02\n",
      "iteration: 39980 loss: 0.0064 lr: 0.02\n",
      "iteration: 39990 loss: 0.0059 lr: 0.02\n",
      "iteration: 40000 loss: 0.0058 lr: 0.02\n",
      "iteration: 40010 loss: 0.0063 lr: 0.02\n",
      "iteration: 40020 loss: 0.0060 lr: 0.02\n",
      "iteration: 40030 loss: 0.0045 lr: 0.02\n",
      "iteration: 40040 loss: 0.0051 lr: 0.02\n",
      "iteration: 40050 loss: 0.0057 lr: 0.02\n",
      "iteration: 40060 loss: 0.0041 lr: 0.02\n",
      "iteration: 40070 loss: 0.0049 lr: 0.02\n",
      "iteration: 40080 loss: 0.0071 lr: 0.02\n",
      "iteration: 40090 loss: 0.0043 lr: 0.02\n",
      "iteration: 40100 loss: 0.0055 lr: 0.02\n",
      "iteration: 40110 loss: 0.0070 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 40120 loss: 0.0057 lr: 0.02\n",
      "iteration: 40130 loss: 0.0076 lr: 0.02\n",
      "iteration: 40140 loss: 0.0043 lr: 0.02\n",
      "iteration: 40150 loss: 0.0074 lr: 0.02\n",
      "iteration: 40160 loss: 0.0059 lr: 0.02\n",
      "iteration: 40170 loss: 0.0047 lr: 0.02\n",
      "iteration: 40180 loss: 0.0074 lr: 0.02\n",
      "iteration: 40190 loss: 0.0039 lr: 0.02\n",
      "iteration: 40200 loss: 0.0046 lr: 0.02\n",
      "iteration: 40210 loss: 0.0054 lr: 0.02\n",
      "iteration: 40220 loss: 0.0055 lr: 0.02\n",
      "iteration: 40230 loss: 0.0058 lr: 0.02\n",
      "iteration: 40240 loss: 0.0050 lr: 0.02\n",
      "iteration: 40250 loss: 0.0090 lr: 0.02\n",
      "iteration: 40260 loss: 0.0057 lr: 0.02\n",
      "iteration: 40270 loss: 0.0061 lr: 0.02\n",
      "iteration: 40280 loss: 0.0064 lr: 0.02\n",
      "iteration: 40290 loss: 0.0051 lr: 0.02\n",
      "iteration: 40300 loss: 0.0053 lr: 0.02\n",
      "iteration: 40310 loss: 0.0075 lr: 0.02\n",
      "iteration: 40320 loss: 0.0046 lr: 0.02\n",
      "iteration: 40330 loss: 0.0056 lr: 0.02\n",
      "iteration: 40340 loss: 0.0062 lr: 0.02\n",
      "iteration: 40350 loss: 0.0070 lr: 0.02\n",
      "iteration: 40360 loss: 0.0046 lr: 0.02\n",
      "iteration: 40370 loss: 0.0066 lr: 0.02\n",
      "iteration: 40380 loss: 0.0070 lr: 0.02\n",
      "iteration: 40390 loss: 0.0058 lr: 0.02\n",
      "iteration: 40400 loss: 0.0044 lr: 0.02\n",
      "iteration: 40410 loss: 0.0050 lr: 0.02\n",
      "iteration: 40420 loss: 0.0056 lr: 0.02\n",
      "iteration: 40430 loss: 0.0075 lr: 0.02\n",
      "iteration: 40440 loss: 0.0051 lr: 0.02\n",
      "iteration: 40450 loss: 0.0048 lr: 0.02\n",
      "iteration: 40460 loss: 0.0054 lr: 0.02\n",
      "iteration: 40470 loss: 0.0039 lr: 0.02\n",
      "iteration: 40480 loss: 0.0060 lr: 0.02\n",
      "iteration: 40490 loss: 0.0065 lr: 0.02\n",
      "iteration: 40500 loss: 0.0049 lr: 0.02\n",
      "iteration: 40510 loss: 0.0066 lr: 0.02\n",
      "iteration: 40520 loss: 0.0064 lr: 0.02\n",
      "iteration: 40530 loss: 0.0052 lr: 0.02\n",
      "iteration: 40540 loss: 0.0090 lr: 0.02\n",
      "iteration: 40550 loss: 0.0066 lr: 0.02\n",
      "iteration: 40560 loss: 0.0052 lr: 0.02\n",
      "iteration: 40570 loss: 0.0064 lr: 0.02\n",
      "iteration: 40580 loss: 0.0049 lr: 0.02\n",
      "iteration: 40590 loss: 0.0042 lr: 0.02\n",
      "iteration: 40600 loss: 0.0059 lr: 0.02\n",
      "iteration: 40610 loss: 0.0050 lr: 0.02\n",
      "iteration: 40620 loss: 0.0059 lr: 0.02\n",
      "iteration: 40630 loss: 0.0051 lr: 0.02\n",
      "iteration: 40640 loss: 0.0076 lr: 0.02\n",
      "iteration: 40650 loss: 0.0066 lr: 0.02\n",
      "iteration: 40660 loss: 0.0039 lr: 0.02\n",
      "iteration: 40670 loss: 0.0057 lr: 0.02\n",
      "iteration: 40680 loss: 0.0068 lr: 0.02\n",
      "iteration: 40690 loss: 0.0090 lr: 0.02\n",
      "iteration: 40700 loss: 0.0069 lr: 0.02\n",
      "iteration: 40710 loss: 0.0057 lr: 0.02\n",
      "iteration: 40720 loss: 0.0070 lr: 0.02\n",
      "iteration: 40730 loss: 0.0073 lr: 0.02\n",
      "iteration: 40740 loss: 0.0042 lr: 0.02\n",
      "iteration: 40750 loss: 0.0058 lr: 0.02\n",
      "iteration: 40760 loss: 0.0068 lr: 0.02\n",
      "iteration: 40770 loss: 0.0043 lr: 0.02\n",
      "iteration: 40780 loss: 0.0051 lr: 0.02\n",
      "iteration: 40790 loss: 0.0055 lr: 0.02\n",
      "iteration: 40800 loss: 0.0062 lr: 0.02\n",
      "iteration: 40810 loss: 0.0043 lr: 0.02\n",
      "iteration: 40820 loss: 0.0048 lr: 0.02\n",
      "iteration: 40830 loss: 0.0047 lr: 0.02\n",
      "iteration: 40840 loss: 0.0065 lr: 0.02\n",
      "iteration: 40850 loss: 0.0053 lr: 0.02\n",
      "iteration: 40860 loss: 0.0048 lr: 0.02\n",
      "iteration: 40870 loss: 0.0043 lr: 0.02\n",
      "iteration: 40880 loss: 0.0039 lr: 0.02\n",
      "iteration: 40890 loss: 0.0048 lr: 0.02\n",
      "iteration: 40900 loss: 0.0076 lr: 0.02\n",
      "iteration: 40910 loss: 0.0042 lr: 0.02\n",
      "iteration: 40920 loss: 0.0070 lr: 0.02\n",
      "iteration: 40930 loss: 0.0079 lr: 0.02\n",
      "iteration: 40940 loss: 0.0052 lr: 0.02\n",
      "iteration: 40950 loss: 0.0068 lr: 0.02\n",
      "iteration: 40960 loss: 0.0050 lr: 0.02\n",
      "iteration: 40970 loss: 0.0040 lr: 0.02\n",
      "iteration: 40980 loss: 0.0061 lr: 0.02\n",
      "iteration: 40990 loss: 0.0043 lr: 0.02\n",
      "iteration: 41000 loss: 0.0054 lr: 0.02\n",
      "iteration: 41010 loss: 0.0089 lr: 0.02\n",
      "iteration: 41020 loss: 0.0062 lr: 0.02\n",
      "iteration: 41030 loss: 0.0055 lr: 0.02\n",
      "iteration: 41040 loss: 0.0054 lr: 0.02\n",
      "iteration: 41050 loss: 0.0061 lr: 0.02\n",
      "iteration: 41060 loss: 0.0044 lr: 0.02\n",
      "iteration: 41070 loss: 0.0055 lr: 0.02\n",
      "iteration: 41080 loss: 0.0049 lr: 0.02\n",
      "iteration: 41090 loss: 0.0053 lr: 0.02\n",
      "iteration: 41100 loss: 0.0051 lr: 0.02\n",
      "iteration: 41110 loss: 0.0075 lr: 0.02\n",
      "iteration: 41120 loss: 0.0041 lr: 0.02\n",
      "iteration: 41130 loss: 0.0051 lr: 0.02\n",
      "iteration: 41140 loss: 0.0055 lr: 0.02\n",
      "iteration: 41150 loss: 0.0059 lr: 0.02\n",
      "iteration: 41160 loss: 0.0073 lr: 0.02\n",
      "iteration: 41170 loss: 0.0062 lr: 0.02\n",
      "iteration: 41180 loss: 0.0059 lr: 0.02\n",
      "iteration: 41190 loss: 0.0048 lr: 0.02\n",
      "iteration: 41200 loss: 0.0061 lr: 0.02\n",
      "iteration: 41210 loss: 0.0067 lr: 0.02\n",
      "iteration: 41220 loss: 0.0054 lr: 0.02\n",
      "iteration: 41230 loss: 0.0064 lr: 0.02\n",
      "iteration: 41240 loss: 0.0056 lr: 0.02\n",
      "iteration: 41250 loss: 0.0071 lr: 0.02\n",
      "iteration: 41260 loss: 0.0050 lr: 0.02\n",
      "iteration: 41270 loss: 0.0047 lr: 0.02\n",
      "iteration: 41280 loss: 0.0048 lr: 0.02\n",
      "iteration: 41290 loss: 0.0068 lr: 0.02\n",
      "iteration: 41300 loss: 0.0055 lr: 0.02\n",
      "iteration: 41310 loss: 0.0051 lr: 0.02\n",
      "iteration: 41320 loss: 0.0050 lr: 0.02\n",
      "iteration: 41330 loss: 0.0065 lr: 0.02\n",
      "iteration: 41340 loss: 0.0076 lr: 0.02\n",
      "iteration: 41350 loss: 0.0069 lr: 0.02\n",
      "iteration: 41360 loss: 0.0062 lr: 0.02\n",
      "iteration: 41370 loss: 0.0057 lr: 0.02\n",
      "iteration: 41380 loss: 0.0064 lr: 0.02\n",
      "iteration: 41390 loss: 0.0040 lr: 0.02\n",
      "iteration: 41400 loss: 0.0056 lr: 0.02\n",
      "iteration: 41410 loss: 0.0048 lr: 0.02\n",
      "iteration: 41420 loss: 0.0046 lr: 0.02\n",
      "iteration: 41430 loss: 0.0063 lr: 0.02\n",
      "iteration: 41440 loss: 0.0088 lr: 0.02\n",
      "iteration: 41450 loss: 0.0060 lr: 0.02\n",
      "iteration: 41460 loss: 0.0040 lr: 0.02\n",
      "iteration: 41470 loss: 0.0048 lr: 0.02\n",
      "iteration: 41480 loss: 0.0050 lr: 0.02\n",
      "iteration: 41490 loss: 0.0055 lr: 0.02\n",
      "iteration: 41500 loss: 0.0090 lr: 0.02\n",
      "iteration: 41510 loss: 0.0045 lr: 0.02\n",
      "iteration: 41520 loss: 0.0048 lr: 0.02\n",
      "iteration: 41530 loss: 0.0055 lr: 0.02\n",
      "iteration: 41540 loss: 0.0042 lr: 0.02\n",
      "iteration: 41550 loss: 0.0074 lr: 0.02\n",
      "iteration: 41560 loss: 0.0045 lr: 0.02\n",
      "iteration: 41570 loss: 0.0060 lr: 0.02\n",
      "iteration: 41580 loss: 0.0046 lr: 0.02\n",
      "iteration: 41590 loss: 0.0051 lr: 0.02\n",
      "iteration: 41600 loss: 0.0056 lr: 0.02\n",
      "iteration: 41610 loss: 0.0047 lr: 0.02\n",
      "iteration: 41620 loss: 0.0053 lr: 0.02\n",
      "iteration: 41630 loss: 0.0077 lr: 0.02\n",
      "iteration: 41640 loss: 0.0083 lr: 0.02\n",
      "iteration: 41650 loss: 0.0063 lr: 0.02\n",
      "iteration: 41660 loss: 0.0053 lr: 0.02\n",
      "iteration: 41670 loss: 0.0067 lr: 0.02\n",
      "iteration: 41680 loss: 0.0070 lr: 0.02\n",
      "iteration: 41690 loss: 0.0057 lr: 0.02\n",
      "iteration: 41700 loss: 0.0047 lr: 0.02\n",
      "iteration: 41710 loss: 0.0039 lr: 0.02\n",
      "iteration: 41720 loss: 0.0037 lr: 0.02\n",
      "iteration: 41730 loss: 0.0053 lr: 0.02\n",
      "iteration: 41740 loss: 0.0062 lr: 0.02\n",
      "iteration: 41750 loss: 0.0043 lr: 0.02\n",
      "iteration: 41760 loss: 0.0049 lr: 0.02\n",
      "iteration: 41770 loss: 0.0037 lr: 0.02\n",
      "iteration: 41780 loss: 0.0048 lr: 0.02\n",
      "iteration: 41790 loss: 0.0087 lr: 0.02\n",
      "iteration: 41800 loss: 0.0063 lr: 0.02\n",
      "iteration: 41810 loss: 0.0061 lr: 0.02\n",
      "iteration: 41820 loss: 0.0065 lr: 0.02\n",
      "iteration: 41830 loss: 0.0065 lr: 0.02\n",
      "iteration: 41840 loss: 0.0043 lr: 0.02\n",
      "iteration: 41850 loss: 0.0059 lr: 0.02\n",
      "iteration: 41860 loss: 0.0058 lr: 0.02\n",
      "iteration: 41870 loss: 0.0043 lr: 0.02\n",
      "iteration: 41880 loss: 0.0067 lr: 0.02\n",
      "iteration: 41890 loss: 0.0048 lr: 0.02\n",
      "iteration: 41900 loss: 0.0062 lr: 0.02\n",
      "iteration: 41910 loss: 0.0073 lr: 0.02\n",
      "iteration: 41920 loss: 0.0066 lr: 0.02\n",
      "iteration: 41930 loss: 0.0058 lr: 0.02\n",
      "iteration: 41940 loss: 0.0081 lr: 0.02\n",
      "iteration: 41950 loss: 0.0080 lr: 0.02\n",
      "iteration: 41960 loss: 0.0054 lr: 0.02\n",
      "iteration: 41970 loss: 0.0051 lr: 0.02\n",
      "iteration: 41980 loss: 0.0061 lr: 0.02\n",
      "iteration: 41990 loss: 0.0066 lr: 0.02\n",
      "iteration: 42000 loss: 0.0046 lr: 0.02\n",
      "iteration: 42010 loss: 0.0045 lr: 0.02\n",
      "iteration: 42020 loss: 0.0041 lr: 0.02\n",
      "iteration: 42030 loss: 0.0059 lr: 0.02\n",
      "iteration: 42040 loss: 0.0078 lr: 0.02\n",
      "iteration: 42050 loss: 0.0048 lr: 0.02\n",
      "iteration: 42060 loss: 0.0065 lr: 0.02\n",
      "iteration: 42070 loss: 0.0060 lr: 0.02\n",
      "iteration: 42080 loss: 0.0054 lr: 0.02\n",
      "iteration: 42090 loss: 0.0069 lr: 0.02\n",
      "iteration: 42100 loss: 0.0056 lr: 0.02\n",
      "iteration: 42110 loss: 0.0053 lr: 0.02\n",
      "iteration: 42120 loss: 0.0051 lr: 0.02\n",
      "iteration: 42130 loss: 0.0052 lr: 0.02\n",
      "iteration: 42140 loss: 0.0061 lr: 0.02\n",
      "iteration: 42150 loss: 0.0046 lr: 0.02\n",
      "iteration: 42160 loss: 0.0062 lr: 0.02\n",
      "iteration: 42170 loss: 0.0066 lr: 0.02\n",
      "iteration: 42180 loss: 0.0061 lr: 0.02\n",
      "iteration: 42190 loss: 0.0047 lr: 0.02\n",
      "iteration: 42200 loss: 0.0067 lr: 0.02\n",
      "iteration: 42210 loss: 0.0048 lr: 0.02\n",
      "iteration: 42220 loss: 0.0059 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 42230 loss: 0.0067 lr: 0.02\n",
      "iteration: 42240 loss: 0.0051 lr: 0.02\n",
      "iteration: 42250 loss: 0.0054 lr: 0.02\n",
      "iteration: 42260 loss: 0.0046 lr: 0.02\n",
      "iteration: 42270 loss: 0.0048 lr: 0.02\n",
      "iteration: 42280 loss: 0.0087 lr: 0.02\n",
      "iteration: 42290 loss: 0.0052 lr: 0.02\n",
      "iteration: 42300 loss: 0.0036 lr: 0.02\n",
      "iteration: 42310 loss: 0.0072 lr: 0.02\n",
      "iteration: 42320 loss: 0.0062 lr: 0.02\n",
      "iteration: 42330 loss: 0.0067 lr: 0.02\n",
      "iteration: 42340 loss: 0.0097 lr: 0.02\n",
      "iteration: 42350 loss: 0.0079 lr: 0.02\n",
      "iteration: 42360 loss: 0.0057 lr: 0.02\n",
      "iteration: 42370 loss: 0.0059 lr: 0.02\n",
      "iteration: 42380 loss: 0.0058 lr: 0.02\n",
      "iteration: 42390 loss: 0.0056 lr: 0.02\n",
      "iteration: 42400 loss: 0.0046 lr: 0.02\n",
      "iteration: 42410 loss: 0.0038 lr: 0.02\n",
      "iteration: 42420 loss: 0.0037 lr: 0.02\n",
      "iteration: 42430 loss: 0.0050 lr: 0.02\n",
      "iteration: 42440 loss: 0.0043 lr: 0.02\n",
      "iteration: 42450 loss: 0.0038 lr: 0.02\n",
      "iteration: 42460 loss: 0.0065 lr: 0.02\n",
      "iteration: 42470 loss: 0.0051 lr: 0.02\n",
      "iteration: 42480 loss: 0.0054 lr: 0.02\n",
      "iteration: 42490 loss: 0.0079 lr: 0.02\n",
      "iteration: 42500 loss: 0.0055 lr: 0.02\n",
      "iteration: 42510 loss: 0.0040 lr: 0.02\n",
      "iteration: 42520 loss: 0.0042 lr: 0.02\n",
      "iteration: 42530 loss: 0.0063 lr: 0.02\n",
      "iteration: 42540 loss: 0.0062 lr: 0.02\n",
      "iteration: 42550 loss: 0.0063 lr: 0.02\n",
      "iteration: 42560 loss: 0.0075 lr: 0.02\n",
      "iteration: 42570 loss: 0.0049 lr: 0.02\n",
      "iteration: 42580 loss: 0.0048 lr: 0.02\n",
      "iteration: 42590 loss: 0.0056 lr: 0.02\n",
      "iteration: 42600 loss: 0.0066 lr: 0.02\n",
      "iteration: 42610 loss: 0.0060 lr: 0.02\n",
      "iteration: 42620 loss: 0.0045 lr: 0.02\n",
      "iteration: 42630 loss: 0.0053 lr: 0.02\n",
      "iteration: 42640 loss: 0.0064 lr: 0.02\n",
      "iteration: 42650 loss: 0.0052 lr: 0.02\n",
      "iteration: 42660 loss: 0.0053 lr: 0.02\n",
      "iteration: 42670 loss: 0.0083 lr: 0.02\n",
      "iteration: 42680 loss: 0.0057 lr: 0.02\n",
      "iteration: 42690 loss: 0.0051 lr: 0.02\n",
      "iteration: 42700 loss: 0.0069 lr: 0.02\n",
      "iteration: 42710 loss: 0.0047 lr: 0.02\n",
      "iteration: 42720 loss: 0.0056 lr: 0.02\n",
      "iteration: 42730 loss: 0.0057 lr: 0.02\n",
      "iteration: 42740 loss: 0.0092 lr: 0.02\n",
      "iteration: 42750 loss: 0.0069 lr: 0.02\n",
      "iteration: 42760 loss: 0.0053 lr: 0.02\n",
      "iteration: 42770 loss: 0.0040 lr: 0.02\n",
      "iteration: 42780 loss: 0.0088 lr: 0.02\n",
      "iteration: 42790 loss: 0.0054 lr: 0.02\n",
      "iteration: 42800 loss: 0.0065 lr: 0.02\n",
      "iteration: 42810 loss: 0.0042 lr: 0.02\n",
      "iteration: 42820 loss: 0.0052 lr: 0.02\n",
      "iteration: 42830 loss: 0.0046 lr: 0.02\n",
      "iteration: 42840 loss: 0.0058 lr: 0.02\n",
      "iteration: 42850 loss: 0.0070 lr: 0.02\n",
      "iteration: 42860 loss: 0.0065 lr: 0.02\n",
      "iteration: 42870 loss: 0.0074 lr: 0.02\n",
      "iteration: 42880 loss: 0.0054 lr: 0.02\n",
      "iteration: 42890 loss: 0.0060 lr: 0.02\n",
      "iteration: 42900 loss: 0.0048 lr: 0.02\n",
      "iteration: 42910 loss: 0.0076 lr: 0.02\n",
      "iteration: 42920 loss: 0.0050 lr: 0.02\n",
      "iteration: 42930 loss: 0.0058 lr: 0.02\n",
      "iteration: 42940 loss: 0.0087 lr: 0.02\n",
      "iteration: 42950 loss: 0.0063 lr: 0.02\n",
      "iteration: 42960 loss: 0.0058 lr: 0.02\n",
      "iteration: 42970 loss: 0.0049 lr: 0.02\n",
      "iteration: 42980 loss: 0.0047 lr: 0.02\n",
      "iteration: 42990 loss: 0.0056 lr: 0.02\n",
      "iteration: 43000 loss: 0.0046 lr: 0.02\n",
      "iteration: 43010 loss: 0.0040 lr: 0.02\n",
      "iteration: 43020 loss: 0.0038 lr: 0.02\n",
      "iteration: 43030 loss: 0.0042 lr: 0.02\n",
      "iteration: 43040 loss: 0.0040 lr: 0.02\n",
      "iteration: 43050 loss: 0.0057 lr: 0.02\n",
      "iteration: 43060 loss: 0.0036 lr: 0.02\n",
      "iteration: 43070 loss: 0.0064 lr: 0.02\n",
      "iteration: 43080 loss: 0.0044 lr: 0.02\n",
      "iteration: 43090 loss: 0.0069 lr: 0.02\n",
      "iteration: 43100 loss: 0.0067 lr: 0.02\n",
      "iteration: 43110 loss: 0.0055 lr: 0.02\n",
      "iteration: 43120 loss: 0.0049 lr: 0.02\n",
      "iteration: 43130 loss: 0.0062 lr: 0.02\n",
      "iteration: 43140 loss: 0.0051 lr: 0.02\n",
      "iteration: 43150 loss: 0.0063 lr: 0.02\n",
      "iteration: 43160 loss: 0.0065 lr: 0.02\n",
      "iteration: 43170 loss: 0.0081 lr: 0.02\n",
      "iteration: 43180 loss: 0.0060 lr: 0.02\n",
      "iteration: 43190 loss: 0.0046 lr: 0.02\n",
      "iteration: 43200 loss: 0.0050 lr: 0.02\n",
      "iteration: 43210 loss: 0.0059 lr: 0.02\n",
      "iteration: 43220 loss: 0.0069 lr: 0.02\n",
      "iteration: 43230 loss: 0.0047 lr: 0.02\n",
      "iteration: 43240 loss: 0.0067 lr: 0.02\n",
      "iteration: 43250 loss: 0.0041 lr: 0.02\n",
      "iteration: 43260 loss: 0.0048 lr: 0.02\n",
      "iteration: 43270 loss: 0.0063 lr: 0.02\n",
      "iteration: 43280 loss: 0.0051 lr: 0.02\n",
      "iteration: 43290 loss: 0.0050 lr: 0.02\n",
      "iteration: 43300 loss: 0.0047 lr: 0.02\n",
      "iteration: 43310 loss: 0.0093 lr: 0.02\n",
      "iteration: 43320 loss: 0.0050 lr: 0.02\n",
      "iteration: 43330 loss: 0.0034 lr: 0.02\n",
      "iteration: 43340 loss: 0.0048 lr: 0.02\n",
      "iteration: 43350 loss: 0.0059 lr: 0.02\n",
      "iteration: 43360 loss: 0.0042 lr: 0.02\n",
      "iteration: 43370 loss: 0.0068 lr: 0.02\n",
      "iteration: 43380 loss: 0.0072 lr: 0.02\n",
      "iteration: 43390 loss: 0.0076 lr: 0.02\n",
      "iteration: 43400 loss: 0.0064 lr: 0.02\n",
      "iteration: 43410 loss: 0.0083 lr: 0.02\n",
      "iteration: 43420 loss: 0.0053 lr: 0.02\n",
      "iteration: 43430 loss: 0.0068 lr: 0.02\n",
      "iteration: 43440 loss: 0.0070 lr: 0.02\n",
      "iteration: 43450 loss: 0.0051 lr: 0.02\n",
      "iteration: 43460 loss: 0.0061 lr: 0.02\n",
      "iteration: 43470 loss: 0.0049 lr: 0.02\n",
      "iteration: 43480 loss: 0.0036 lr: 0.02\n",
      "iteration: 43490 loss: 0.0070 lr: 0.02\n",
      "iteration: 43500 loss: 0.0046 lr: 0.02\n",
      "iteration: 43510 loss: 0.0051 lr: 0.02\n",
      "iteration: 43520 loss: 0.0054 lr: 0.02\n",
      "iteration: 43530 loss: 0.0044 lr: 0.02\n",
      "iteration: 43540 loss: 0.0039 lr: 0.02\n",
      "iteration: 43550 loss: 0.0045 lr: 0.02\n",
      "iteration: 43560 loss: 0.0035 lr: 0.02\n",
      "iteration: 43570 loss: 0.0054 lr: 0.02\n",
      "iteration: 43580 loss: 0.0064 lr: 0.02\n",
      "iteration: 43590 loss: 0.0037 lr: 0.02\n",
      "iteration: 43600 loss: 0.0073 lr: 0.02\n",
      "iteration: 43610 loss: 0.0104 lr: 0.02\n",
      "iteration: 43620 loss: 0.0067 lr: 0.02\n",
      "iteration: 43630 loss: 0.0060 lr: 0.02\n",
      "iteration: 43640 loss: 0.0054 lr: 0.02\n",
      "iteration: 43650 loss: 0.0096 lr: 0.02\n",
      "iteration: 43660 loss: 0.0049 lr: 0.02\n",
      "iteration: 43670 loss: 0.0055 lr: 0.02\n",
      "iteration: 43680 loss: 0.0044 lr: 0.02\n",
      "iteration: 43690 loss: 0.0044 lr: 0.02\n",
      "iteration: 43700 loss: 0.0047 lr: 0.02\n",
      "iteration: 43710 loss: 0.0045 lr: 0.02\n",
      "iteration: 43720 loss: 0.0055 lr: 0.02\n",
      "iteration: 43730 loss: 0.0050 lr: 0.02\n",
      "iteration: 43740 loss: 0.0054 lr: 0.02\n",
      "iteration: 43750 loss: 0.0072 lr: 0.02\n",
      "iteration: 43760 loss: 0.0058 lr: 0.02\n",
      "iteration: 43770 loss: 0.0063 lr: 0.02\n",
      "iteration: 43780 loss: 0.0036 lr: 0.02\n",
      "iteration: 43790 loss: 0.0054 lr: 0.02\n",
      "iteration: 43800 loss: 0.0053 lr: 0.02\n",
      "iteration: 43810 loss: 0.0068 lr: 0.02\n",
      "iteration: 43820 loss: 0.0048 lr: 0.02\n",
      "iteration: 43830 loss: 0.0043 lr: 0.02\n",
      "iteration: 43840 loss: 0.0052 lr: 0.02\n",
      "iteration: 43850 loss: 0.0045 lr: 0.02\n",
      "iteration: 43860 loss: 0.0053 lr: 0.02\n",
      "iteration: 43870 loss: 0.0053 lr: 0.02\n",
      "iteration: 43880 loss: 0.0066 lr: 0.02\n",
      "iteration: 43890 loss: 0.0046 lr: 0.02\n",
      "iteration: 43900 loss: 0.0071 lr: 0.02\n",
      "iteration: 43910 loss: 0.0045 lr: 0.02\n",
      "iteration: 43920 loss: 0.0075 lr: 0.02\n",
      "iteration: 43930 loss: 0.0048 lr: 0.02\n",
      "iteration: 43940 loss: 0.0069 lr: 0.02\n",
      "iteration: 43950 loss: 0.0053 lr: 0.02\n",
      "iteration: 43960 loss: 0.0044 lr: 0.02\n",
      "iteration: 43970 loss: 0.0055 lr: 0.02\n",
      "iteration: 43980 loss: 0.0061 lr: 0.02\n",
      "iteration: 43990 loss: 0.0067 lr: 0.02\n",
      "iteration: 44000 loss: 0.0051 lr: 0.02\n",
      "iteration: 44010 loss: 0.0060 lr: 0.02\n",
      "iteration: 44020 loss: 0.0047 lr: 0.02\n",
      "iteration: 44030 loss: 0.0047 lr: 0.02\n",
      "iteration: 44040 loss: 0.0060 lr: 0.02\n",
      "iteration: 44050 loss: 0.0057 lr: 0.02\n",
      "iteration: 44060 loss: 0.0068 lr: 0.02\n",
      "iteration: 44070 loss: 0.0058 lr: 0.02\n",
      "iteration: 44080 loss: 0.0040 lr: 0.02\n",
      "iteration: 44090 loss: 0.0053 lr: 0.02\n",
      "iteration: 44100 loss: 0.0058 lr: 0.02\n",
      "iteration: 44110 loss: 0.0040 lr: 0.02\n",
      "iteration: 44120 loss: 0.0041 lr: 0.02\n",
      "iteration: 44130 loss: 0.0066 lr: 0.02\n",
      "iteration: 44140 loss: 0.0055 lr: 0.02\n",
      "iteration: 44150 loss: 0.0048 lr: 0.02\n",
      "iteration: 44160 loss: 0.0064 lr: 0.02\n",
      "iteration: 44170 loss: 0.0041 lr: 0.02\n",
      "iteration: 44180 loss: 0.0055 lr: 0.02\n",
      "iteration: 44190 loss: 0.0046 lr: 0.02\n",
      "iteration: 44200 loss: 0.0081 lr: 0.02\n",
      "iteration: 44210 loss: 0.0052 lr: 0.02\n",
      "iteration: 44220 loss: 0.0053 lr: 0.02\n",
      "iteration: 44230 loss: 0.0038 lr: 0.02\n",
      "iteration: 44240 loss: 0.0034 lr: 0.02\n",
      "iteration: 44250 loss: 0.0063 lr: 0.02\n",
      "iteration: 44260 loss: 0.0060 lr: 0.02\n",
      "iteration: 44270 loss: 0.0059 lr: 0.02\n",
      "iteration: 44280 loss: 0.0045 lr: 0.02\n",
      "iteration: 44290 loss: 0.0046 lr: 0.02\n",
      "iteration: 44300 loss: 0.0048 lr: 0.02\n",
      "iteration: 44310 loss: 0.0084 lr: 0.02\n",
      "iteration: 44320 loss: 0.0064 lr: 0.02\n",
      "iteration: 44330 loss: 0.0053 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 44340 loss: 0.0059 lr: 0.02\n",
      "iteration: 44350 loss: 0.0050 lr: 0.02\n",
      "iteration: 44360 loss: 0.0051 lr: 0.02\n",
      "iteration: 44370 loss: 0.0042 lr: 0.02\n",
      "iteration: 44380 loss: 0.0062 lr: 0.02\n",
      "iteration: 44390 loss: 0.0061 lr: 0.02\n",
      "iteration: 44400 loss: 0.0061 lr: 0.02\n",
      "iteration: 44410 loss: 0.0052 lr: 0.02\n",
      "iteration: 44420 loss: 0.0052 lr: 0.02\n",
      "iteration: 44430 loss: 0.0051 lr: 0.02\n",
      "iteration: 44440 loss: 0.0045 lr: 0.02\n",
      "iteration: 44450 loss: 0.0054 lr: 0.02\n",
      "iteration: 44460 loss: 0.0062 lr: 0.02\n",
      "iteration: 44470 loss: 0.0051 lr: 0.02\n",
      "iteration: 44480 loss: 0.0056 lr: 0.02\n",
      "iteration: 44490 loss: 0.0044 lr: 0.02\n",
      "iteration: 44500 loss: 0.0054 lr: 0.02\n",
      "iteration: 44510 loss: 0.0041 lr: 0.02\n",
      "iteration: 44520 loss: 0.0056 lr: 0.02\n",
      "iteration: 44530 loss: 0.0038 lr: 0.02\n",
      "iteration: 44540 loss: 0.0043 lr: 0.02\n",
      "iteration: 44550 loss: 0.0072 lr: 0.02\n",
      "iteration: 44560 loss: 0.0051 lr: 0.02\n",
      "iteration: 44570 loss: 0.0048 lr: 0.02\n",
      "iteration: 44580 loss: 0.0042 lr: 0.02\n",
      "iteration: 44590 loss: 0.0055 lr: 0.02\n",
      "iteration: 44600 loss: 0.0058 lr: 0.02\n",
      "iteration: 44610 loss: 0.0062 lr: 0.02\n",
      "iteration: 44620 loss: 0.0056 lr: 0.02\n",
      "iteration: 44630 loss: 0.0055 lr: 0.02\n",
      "iteration: 44640 loss: 0.0046 lr: 0.02\n",
      "iteration: 44650 loss: 0.0051 lr: 0.02\n",
      "iteration: 44660 loss: 0.0070 lr: 0.02\n",
      "iteration: 44670 loss: 0.0053 lr: 0.02\n",
      "iteration: 44680 loss: 0.0051 lr: 0.02\n",
      "iteration: 44690 loss: 0.0038 lr: 0.02\n",
      "iteration: 44700 loss: 0.0056 lr: 0.02\n",
      "iteration: 44710 loss: 0.0046 lr: 0.02\n",
      "iteration: 44720 loss: 0.0051 lr: 0.02\n",
      "iteration: 44730 loss: 0.0047 lr: 0.02\n",
      "iteration: 44740 loss: 0.0053 lr: 0.02\n",
      "iteration: 44750 loss: 0.0054 lr: 0.02\n",
      "iteration: 44760 loss: 0.0048 lr: 0.02\n",
      "iteration: 44770 loss: 0.0054 lr: 0.02\n",
      "iteration: 44780 loss: 0.0056 lr: 0.02\n",
      "iteration: 44790 loss: 0.0043 lr: 0.02\n",
      "iteration: 44800 loss: 0.0049 lr: 0.02\n",
      "iteration: 44810 loss: 0.0039 lr: 0.02\n",
      "iteration: 44820 loss: 0.0064 lr: 0.02\n",
      "iteration: 44830 loss: 0.0057 lr: 0.02\n",
      "iteration: 44840 loss: 0.0064 lr: 0.02\n",
      "iteration: 44850 loss: 0.0055 lr: 0.02\n",
      "iteration: 44860 loss: 0.0057 lr: 0.02\n",
      "iteration: 44870 loss: 0.0063 lr: 0.02\n",
      "iteration: 44880 loss: 0.0038 lr: 0.02\n",
      "iteration: 44890 loss: 0.0046 lr: 0.02\n",
      "iteration: 44900 loss: 0.0059 lr: 0.02\n",
      "iteration: 44910 loss: 0.0037 lr: 0.02\n",
      "iteration: 44920 loss: 0.0060 lr: 0.02\n",
      "iteration: 44930 loss: 0.0069 lr: 0.02\n",
      "iteration: 44940 loss: 0.0059 lr: 0.02\n",
      "iteration: 44950 loss: 0.0049 lr: 0.02\n",
      "iteration: 44960 loss: 0.0063 lr: 0.02\n",
      "iteration: 44970 loss: 0.0053 lr: 0.02\n",
      "iteration: 44980 loss: 0.0058 lr: 0.02\n",
      "iteration: 44990 loss: 0.0069 lr: 0.02\n",
      "iteration: 45000 loss: 0.0050 lr: 0.02\n",
      "iteration: 45010 loss: 0.0056 lr: 0.02\n",
      "iteration: 45020 loss: 0.0046 lr: 0.02\n",
      "iteration: 45030 loss: 0.0037 lr: 0.02\n",
      "iteration: 45040 loss: 0.0053 lr: 0.02\n",
      "iteration: 45050 loss: 0.0054 lr: 0.02\n",
      "iteration: 45060 loss: 0.0037 lr: 0.02\n",
      "iteration: 45070 loss: 0.0055 lr: 0.02\n",
      "iteration: 45080 loss: 0.0076 lr: 0.02\n",
      "iteration: 45090 loss: 0.0038 lr: 0.02\n",
      "iteration: 45100 loss: 0.0054 lr: 0.02\n",
      "iteration: 45110 loss: 0.0042 lr: 0.02\n",
      "iteration: 45120 loss: 0.0055 lr: 0.02\n",
      "iteration: 45130 loss: 0.0071 lr: 0.02\n",
      "iteration: 45140 loss: 0.0050 lr: 0.02\n",
      "iteration: 45150 loss: 0.0065 lr: 0.02\n",
      "iteration: 45160 loss: 0.0079 lr: 0.02\n",
      "iteration: 45170 loss: 0.0060 lr: 0.02\n",
      "iteration: 45180 loss: 0.0075 lr: 0.02\n",
      "iteration: 45190 loss: 0.0040 lr: 0.02\n",
      "iteration: 45200 loss: 0.0065 lr: 0.02\n",
      "iteration: 45210 loss: 0.0047 lr: 0.02\n",
      "iteration: 45220 loss: 0.0036 lr: 0.02\n",
      "iteration: 45230 loss: 0.0043 lr: 0.02\n",
      "iteration: 45240 loss: 0.0067 lr: 0.02\n",
      "iteration: 45250 loss: 0.0042 lr: 0.02\n",
      "iteration: 45260 loss: 0.0038 lr: 0.02\n",
      "iteration: 45270 loss: 0.0069 lr: 0.02\n",
      "iteration: 45280 loss: 0.0046 lr: 0.02\n",
      "iteration: 45290 loss: 0.0089 lr: 0.02\n",
      "iteration: 45300 loss: 0.0038 lr: 0.02\n",
      "iteration: 45310 loss: 0.0058 lr: 0.02\n",
      "iteration: 45320 loss: 0.0043 lr: 0.02\n",
      "iteration: 45330 loss: 0.0046 lr: 0.02\n",
      "iteration: 45340 loss: 0.0052 lr: 0.02\n",
      "iteration: 45350 loss: 0.0058 lr: 0.02\n",
      "iteration: 45360 loss: 0.0065 lr: 0.02\n",
      "iteration: 45370 loss: 0.0069 lr: 0.02\n",
      "iteration: 45380 loss: 0.0070 lr: 0.02\n",
      "iteration: 45390 loss: 0.0059 lr: 0.02\n",
      "iteration: 45400 loss: 0.0044 lr: 0.02\n",
      "iteration: 45410 loss: 0.0052 lr: 0.02\n",
      "iteration: 45420 loss: 0.0040 lr: 0.02\n",
      "iteration: 45430 loss: 0.0057 lr: 0.02\n",
      "iteration: 45440 loss: 0.0061 lr: 0.02\n",
      "iteration: 45450 loss: 0.0051 lr: 0.02\n",
      "iteration: 45460 loss: 0.0062 lr: 0.02\n",
      "iteration: 45470 loss: 0.0049 lr: 0.02\n",
      "iteration: 45480 loss: 0.0061 lr: 0.02\n",
      "iteration: 45490 loss: 0.0045 lr: 0.02\n",
      "iteration: 45500 loss: 0.0038 lr: 0.02\n",
      "iteration: 45510 loss: 0.0055 lr: 0.02\n",
      "iteration: 45520 loss: 0.0057 lr: 0.02\n",
      "iteration: 45530 loss: 0.0052 lr: 0.02\n",
      "iteration: 45540 loss: 0.0046 lr: 0.02\n",
      "iteration: 45550 loss: 0.0047 lr: 0.02\n",
      "iteration: 45560 loss: 0.0055 lr: 0.02\n",
      "iteration: 45570 loss: 0.0056 lr: 0.02\n",
      "iteration: 45580 loss: 0.0043 lr: 0.02\n",
      "iteration: 45590 loss: 0.0051 lr: 0.02\n",
      "iteration: 45600 loss: 0.0047 lr: 0.02\n",
      "iteration: 45610 loss: 0.0049 lr: 0.02\n",
      "iteration: 45620 loss: 0.0035 lr: 0.02\n",
      "iteration: 45630 loss: 0.0051 lr: 0.02\n",
      "iteration: 45640 loss: 0.0071 lr: 0.02\n",
      "iteration: 45650 loss: 0.0042 lr: 0.02\n",
      "iteration: 45660 loss: 0.0044 lr: 0.02\n",
      "iteration: 45670 loss: 0.0048 lr: 0.02\n",
      "iteration: 45680 loss: 0.0070 lr: 0.02\n",
      "iteration: 45690 loss: 0.0048 lr: 0.02\n",
      "iteration: 45700 loss: 0.0051 lr: 0.02\n",
      "iteration: 45710 loss: 0.0055 lr: 0.02\n",
      "iteration: 45720 loss: 0.0056 lr: 0.02\n",
      "iteration: 45730 loss: 0.0049 lr: 0.02\n",
      "iteration: 45740 loss: 0.0054 lr: 0.02\n",
      "iteration: 45750 loss: 0.0072 lr: 0.02\n",
      "iteration: 45760 loss: 0.0038 lr: 0.02\n",
      "iteration: 45770 loss: 0.0048 lr: 0.02\n",
      "iteration: 45780 loss: 0.0063 lr: 0.02\n",
      "iteration: 45790 loss: 0.0061 lr: 0.02\n",
      "iteration: 45800 loss: 0.0051 lr: 0.02\n",
      "iteration: 45810 loss: 0.0060 lr: 0.02\n",
      "iteration: 45820 loss: 0.0074 lr: 0.02\n",
      "iteration: 45830 loss: 0.0044 lr: 0.02\n",
      "iteration: 45840 loss: 0.0052 lr: 0.02\n",
      "iteration: 45850 loss: 0.0040 lr: 0.02\n",
      "iteration: 45860 loss: 0.0064 lr: 0.02\n",
      "iteration: 45870 loss: 0.0069 lr: 0.02\n",
      "iteration: 45880 loss: 0.0062 lr: 0.02\n",
      "iteration: 45890 loss: 0.0090 lr: 0.02\n",
      "iteration: 45900 loss: 0.0057 lr: 0.02\n",
      "iteration: 45910 loss: 0.0050 lr: 0.02\n",
      "iteration: 45920 loss: 0.0050 lr: 0.02\n",
      "iteration: 45930 loss: 0.0076 lr: 0.02\n",
      "iteration: 45940 loss: 0.0055 lr: 0.02\n",
      "iteration: 45950 loss: 0.0063 lr: 0.02\n",
      "iteration: 45960 loss: 0.0061 lr: 0.02\n",
      "iteration: 45970 loss: 0.0058 lr: 0.02\n",
      "iteration: 45980 loss: 0.0052 lr: 0.02\n",
      "iteration: 45990 loss: 0.0054 lr: 0.02\n",
      "iteration: 46000 loss: 0.0050 lr: 0.02\n",
      "iteration: 46010 loss: 0.0063 lr: 0.02\n",
      "iteration: 46020 loss: 0.0058 lr: 0.02\n",
      "iteration: 46030 loss: 0.0074 lr: 0.02\n",
      "iteration: 46040 loss: 0.0058 lr: 0.02\n",
      "iteration: 46050 loss: 0.0042 lr: 0.02\n",
      "iteration: 46060 loss: 0.0050 lr: 0.02\n",
      "iteration: 46070 loss: 0.0088 lr: 0.02\n",
      "iteration: 46080 loss: 0.0058 lr: 0.02\n",
      "iteration: 46090 loss: 0.0057 lr: 0.02\n",
      "iteration: 46100 loss: 0.0053 lr: 0.02\n",
      "iteration: 46110 loss: 0.0047 lr: 0.02\n",
      "iteration: 46120 loss: 0.0052 lr: 0.02\n",
      "iteration: 46130 loss: 0.0060 lr: 0.02\n",
      "iteration: 46140 loss: 0.0050 lr: 0.02\n",
      "iteration: 46150 loss: 0.0070 lr: 0.02\n",
      "iteration: 46160 loss: 0.0047 lr: 0.02\n",
      "iteration: 46170 loss: 0.0053 lr: 0.02\n",
      "iteration: 46180 loss: 0.0050 lr: 0.02\n",
      "iteration: 46190 loss: 0.0050 lr: 0.02\n",
      "iteration: 46200 loss: 0.0050 lr: 0.02\n",
      "iteration: 46210 loss: 0.0044 lr: 0.02\n",
      "iteration: 46220 loss: 0.0037 lr: 0.02\n",
      "iteration: 46230 loss: 0.0033 lr: 0.02\n",
      "iteration: 46240 loss: 0.0049 lr: 0.02\n",
      "iteration: 46250 loss: 0.0081 lr: 0.02\n",
      "iteration: 46260 loss: 0.0046 lr: 0.02\n",
      "iteration: 46270 loss: 0.0042 lr: 0.02\n",
      "iteration: 46280 loss: 0.0061 lr: 0.02\n",
      "iteration: 46290 loss: 0.0066 lr: 0.02\n",
      "iteration: 46300 loss: 0.0040 lr: 0.02\n",
      "iteration: 46310 loss: 0.0064 lr: 0.02\n",
      "iteration: 46320 loss: 0.0039 lr: 0.02\n",
      "iteration: 46330 loss: 0.0065 lr: 0.02\n",
      "iteration: 46340 loss: 0.0060 lr: 0.02\n",
      "iteration: 46350 loss: 0.0040 lr: 0.02\n",
      "iteration: 46360 loss: 0.0055 lr: 0.02\n",
      "iteration: 46370 loss: 0.0051 lr: 0.02\n",
      "iteration: 46380 loss: 0.0036 lr: 0.02\n",
      "iteration: 46390 loss: 0.0034 lr: 0.02\n",
      "iteration: 46400 loss: 0.0037 lr: 0.02\n",
      "iteration: 46410 loss: 0.0065 lr: 0.02\n",
      "iteration: 46420 loss: 0.0056 lr: 0.02\n",
      "iteration: 46430 loss: 0.0045 lr: 0.02\n",
      "iteration: 46440 loss: 0.0062 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 46450 loss: 0.0040 lr: 0.02\n",
      "iteration: 46460 loss: 0.0046 lr: 0.02\n",
      "iteration: 46470 loss: 0.0054 lr: 0.02\n",
      "iteration: 46480 loss: 0.0058 lr: 0.02\n",
      "iteration: 46490 loss: 0.0062 lr: 0.02\n",
      "iteration: 46500 loss: 0.0051 lr: 0.02\n",
      "iteration: 46510 loss: 0.0051 lr: 0.02\n",
      "iteration: 46520 loss: 0.0048 lr: 0.02\n",
      "iteration: 46530 loss: 0.0064 lr: 0.02\n",
      "iteration: 46540 loss: 0.0042 lr: 0.02\n",
      "iteration: 46550 loss: 0.0049 lr: 0.02\n",
      "iteration: 46560 loss: 0.0055 lr: 0.02\n",
      "iteration: 46570 loss: 0.0062 lr: 0.02\n",
      "iteration: 46580 loss: 0.0051 lr: 0.02\n",
      "iteration: 46590 loss: 0.0051 lr: 0.02\n",
      "iteration: 46600 loss: 0.0046 lr: 0.02\n",
      "iteration: 46610 loss: 0.0062 lr: 0.02\n",
      "iteration: 46620 loss: 0.0037 lr: 0.02\n",
      "iteration: 46630 loss: 0.0056 lr: 0.02\n",
      "iteration: 46640 loss: 0.0042 lr: 0.02\n",
      "iteration: 46650 loss: 0.0055 lr: 0.02\n",
      "iteration: 46660 loss: 0.0047 lr: 0.02\n",
      "iteration: 46670 loss: 0.0052 lr: 0.02\n",
      "iteration: 46680 loss: 0.0062 lr: 0.02\n",
      "iteration: 46690 loss: 0.0055 lr: 0.02\n",
      "iteration: 46700 loss: 0.0060 lr: 0.02\n",
      "iteration: 46710 loss: 0.0053 lr: 0.02\n",
      "iteration: 46720 loss: 0.0066 lr: 0.02\n",
      "iteration: 46730 loss: 0.0061 lr: 0.02\n",
      "iteration: 46740 loss: 0.0063 lr: 0.02\n",
      "iteration: 46750 loss: 0.0069 lr: 0.02\n",
      "iteration: 46760 loss: 0.0045 lr: 0.02\n",
      "iteration: 46770 loss: 0.0044 lr: 0.02\n",
      "iteration: 46780 loss: 0.0033 lr: 0.02\n",
      "iteration: 46790 loss: 0.0059 lr: 0.02\n",
      "iteration: 46800 loss: 0.0052 lr: 0.02\n",
      "iteration: 46810 loss: 0.0042 lr: 0.02\n",
      "iteration: 46820 loss: 0.0051 lr: 0.02\n",
      "iteration: 46830 loss: 0.0058 lr: 0.02\n",
      "iteration: 46840 loss: 0.0065 lr: 0.02\n",
      "iteration: 46850 loss: 0.0054 lr: 0.02\n",
      "iteration: 46860 loss: 0.0047 lr: 0.02\n",
      "iteration: 46870 loss: 0.0045 lr: 0.02\n",
      "iteration: 46880 loss: 0.0050 lr: 0.02\n",
      "iteration: 46890 loss: 0.0064 lr: 0.02\n",
      "iteration: 46900 loss: 0.0062 lr: 0.02\n",
      "iteration: 46910 loss: 0.0037 lr: 0.02\n",
      "iteration: 46920 loss: 0.0054 lr: 0.02\n",
      "iteration: 46930 loss: 0.0071 lr: 0.02\n",
      "iteration: 46940 loss: 0.0054 lr: 0.02\n",
      "iteration: 46950 loss: 0.0059 lr: 0.02\n",
      "iteration: 46960 loss: 0.0050 lr: 0.02\n",
      "iteration: 46970 loss: 0.0054 lr: 0.02\n",
      "iteration: 46980 loss: 0.0064 lr: 0.02\n",
      "iteration: 46990 loss: 0.0047 lr: 0.02\n",
      "iteration: 47000 loss: 0.0052 lr: 0.02\n",
      "iteration: 47010 loss: 0.0044 lr: 0.02\n",
      "iteration: 47020 loss: 0.0056 lr: 0.02\n",
      "iteration: 47030 loss: 0.0046 lr: 0.02\n",
      "iteration: 47040 loss: 0.0063 lr: 0.02\n",
      "iteration: 47050 loss: 0.0050 lr: 0.02\n",
      "iteration: 47060 loss: 0.0061 lr: 0.02\n",
      "iteration: 47070 loss: 0.0055 lr: 0.02\n",
      "iteration: 47080 loss: 0.0043 lr: 0.02\n",
      "iteration: 47090 loss: 0.0050 lr: 0.02\n",
      "iteration: 47100 loss: 0.0058 lr: 0.02\n",
      "iteration: 47110 loss: 0.0066 lr: 0.02\n",
      "iteration: 47120 loss: 0.0073 lr: 0.02\n",
      "iteration: 47130 loss: 0.0050 lr: 0.02\n",
      "iteration: 47140 loss: 0.0081 lr: 0.02\n",
      "iteration: 47150 loss: 0.0041 lr: 0.02\n",
      "iteration: 47160 loss: 0.0067 lr: 0.02\n",
      "iteration: 47170 loss: 0.0052 lr: 0.02\n",
      "iteration: 47180 loss: 0.0049 lr: 0.02\n",
      "iteration: 47190 loss: 0.0039 lr: 0.02\n",
      "iteration: 47200 loss: 0.0041 lr: 0.02\n",
      "iteration: 47210 loss: 0.0039 lr: 0.02\n",
      "iteration: 47220 loss: 0.0039 lr: 0.02\n",
      "iteration: 47230 loss: 0.0050 lr: 0.02\n",
      "iteration: 47240 loss: 0.0053 lr: 0.02\n",
      "iteration: 47250 loss: 0.0044 lr: 0.02\n",
      "iteration: 47260 loss: 0.0063 lr: 0.02\n",
      "iteration: 47270 loss: 0.0043 lr: 0.02\n",
      "iteration: 47280 loss: 0.0059 lr: 0.02\n",
      "iteration: 47290 loss: 0.0063 lr: 0.02\n",
      "iteration: 47300 loss: 0.0057 lr: 0.02\n",
      "iteration: 47310 loss: 0.0052 lr: 0.02\n",
      "iteration: 47320 loss: 0.0047 lr: 0.02\n",
      "iteration: 47330 loss: 0.0036 lr: 0.02\n",
      "iteration: 47340 loss: 0.0052 lr: 0.02\n",
      "iteration: 47350 loss: 0.0047 lr: 0.02\n",
      "iteration: 47360 loss: 0.0059 lr: 0.02\n",
      "iteration: 47370 loss: 0.0048 lr: 0.02\n",
      "iteration: 47380 loss: 0.0063 lr: 0.02\n",
      "iteration: 47390 loss: 0.0058 lr: 0.02\n",
      "iteration: 47400 loss: 0.0058 lr: 0.02\n",
      "iteration: 47410 loss: 0.0049 lr: 0.02\n",
      "iteration: 47420 loss: 0.0065 lr: 0.02\n",
      "iteration: 47430 loss: 0.0057 lr: 0.02\n",
      "iteration: 47440 loss: 0.0059 lr: 0.02\n",
      "iteration: 47450 loss: 0.0049 lr: 0.02\n",
      "iteration: 47460 loss: 0.0078 lr: 0.02\n",
      "iteration: 47470 loss: 0.0049 lr: 0.02\n",
      "iteration: 47480 loss: 0.0071 lr: 0.02\n",
      "iteration: 47490 loss: 0.0065 lr: 0.02\n",
      "iteration: 47500 loss: 0.0095 lr: 0.02\n",
      "iteration: 47510 loss: 0.0069 lr: 0.02\n",
      "iteration: 47520 loss: 0.0054 lr: 0.02\n",
      "iteration: 47530 loss: 0.0050 lr: 0.02\n",
      "iteration: 47540 loss: 0.0050 lr: 0.02\n",
      "iteration: 47550 loss: 0.0073 lr: 0.02\n",
      "iteration: 47560 loss: 0.0056 lr: 0.02\n",
      "iteration: 47570 loss: 0.0053 lr: 0.02\n",
      "iteration: 47580 loss: 0.0034 lr: 0.02\n",
      "iteration: 47590 loss: 0.0045 lr: 0.02\n",
      "iteration: 47600 loss: 0.0048 lr: 0.02\n",
      "iteration: 47610 loss: 0.0047 lr: 0.02\n",
      "iteration: 47620 loss: 0.0043 lr: 0.02\n",
      "iteration: 47630 loss: 0.0059 lr: 0.02\n",
      "iteration: 47640 loss: 0.0061 lr: 0.02\n",
      "iteration: 47650 loss: 0.0059 lr: 0.02\n",
      "iteration: 47660 loss: 0.0049 lr: 0.02\n",
      "iteration: 47670 loss: 0.0046 lr: 0.02\n",
      "iteration: 47680 loss: 0.0047 lr: 0.02\n",
      "iteration: 47690 loss: 0.0057 lr: 0.02\n",
      "iteration: 47700 loss: 0.0057 lr: 0.02\n",
      "iteration: 47710 loss: 0.0074 lr: 0.02\n",
      "iteration: 47720 loss: 0.0044 lr: 0.02\n",
      "iteration: 47730 loss: 0.0061 lr: 0.02\n",
      "iteration: 47740 loss: 0.0047 lr: 0.02\n",
      "iteration: 47750 loss: 0.0053 lr: 0.02\n",
      "iteration: 47760 loss: 0.0044 lr: 0.02\n",
      "iteration: 47770 loss: 0.0063 lr: 0.02\n",
      "iteration: 47780 loss: 0.0064 lr: 0.02\n",
      "iteration: 47790 loss: 0.0043 lr: 0.02\n",
      "iteration: 47800 loss: 0.0057 lr: 0.02\n",
      "iteration: 47810 loss: 0.0056 lr: 0.02\n",
      "iteration: 47820 loss: 0.0042 lr: 0.02\n",
      "iteration: 47830 loss: 0.0063 lr: 0.02\n",
      "iteration: 47840 loss: 0.0058 lr: 0.02\n",
      "iteration: 47850 loss: 0.0043 lr: 0.02\n",
      "iteration: 47860 loss: 0.0077 lr: 0.02\n",
      "iteration: 47870 loss: 0.0062 lr: 0.02\n",
      "iteration: 47880 loss: 0.0040 lr: 0.02\n",
      "iteration: 47890 loss: 0.0069 lr: 0.02\n",
      "iteration: 47900 loss: 0.0048 lr: 0.02\n",
      "iteration: 47910 loss: 0.0055 lr: 0.02\n",
      "iteration: 47920 loss: 0.0071 lr: 0.02\n",
      "iteration: 47930 loss: 0.0055 lr: 0.02\n",
      "iteration: 47940 loss: 0.0054 lr: 0.02\n",
      "iteration: 47950 loss: 0.0042 lr: 0.02\n",
      "iteration: 47960 loss: 0.0055 lr: 0.02\n",
      "iteration: 47970 loss: 0.0059 lr: 0.02\n",
      "iteration: 47980 loss: 0.0055 lr: 0.02\n",
      "iteration: 47990 loss: 0.0040 lr: 0.02\n",
      "iteration: 48000 loss: 0.0041 lr: 0.02\n",
      "iteration: 48010 loss: 0.0054 lr: 0.02\n",
      "iteration: 48020 loss: 0.0043 lr: 0.02\n",
      "iteration: 48030 loss: 0.0053 lr: 0.02\n",
      "iteration: 48040 loss: 0.0072 lr: 0.02\n",
      "iteration: 48050 loss: 0.0055 lr: 0.02\n",
      "iteration: 48060 loss: 0.0043 lr: 0.02\n",
      "iteration: 48070 loss: 0.0053 lr: 0.02\n",
      "iteration: 48080 loss: 0.0049 lr: 0.02\n",
      "iteration: 48090 loss: 0.0056 lr: 0.02\n",
      "iteration: 48100 loss: 0.0091 lr: 0.02\n",
      "iteration: 48110 loss: 0.0066 lr: 0.02\n",
      "iteration: 48120 loss: 0.0058 lr: 0.02\n",
      "iteration: 48130 loss: 0.0062 lr: 0.02\n",
      "iteration: 48140 loss: 0.0046 lr: 0.02\n",
      "iteration: 48150 loss: 0.0047 lr: 0.02\n",
      "iteration: 48160 loss: 0.0055 lr: 0.02\n",
      "iteration: 48170 loss: 0.0039 lr: 0.02\n",
      "iteration: 48180 loss: 0.0045 lr: 0.02\n",
      "iteration: 48190 loss: 0.0088 lr: 0.02\n",
      "iteration: 48200 loss: 0.0037 lr: 0.02\n",
      "iteration: 48210 loss: 0.0066 lr: 0.02\n",
      "iteration: 48220 loss: 0.0067 lr: 0.02\n",
      "iteration: 48230 loss: 0.0056 lr: 0.02\n",
      "iteration: 48240 loss: 0.0051 lr: 0.02\n",
      "iteration: 48250 loss: 0.0073 lr: 0.02\n",
      "iteration: 48260 loss: 0.0044 lr: 0.02\n",
      "iteration: 48270 loss: 0.0057 lr: 0.02\n",
      "iteration: 48280 loss: 0.0048 lr: 0.02\n",
      "iteration: 48290 loss: 0.0065 lr: 0.02\n",
      "iteration: 48300 loss: 0.0045 lr: 0.02\n",
      "iteration: 48310 loss: 0.0070 lr: 0.02\n",
      "iteration: 48320 loss: 0.0062 lr: 0.02\n",
      "iteration: 48330 loss: 0.0049 lr: 0.02\n",
      "iteration: 48340 loss: 0.0032 lr: 0.02\n",
      "iteration: 48350 loss: 0.0063 lr: 0.02\n",
      "iteration: 48360 loss: 0.0045 lr: 0.02\n",
      "iteration: 48370 loss: 0.0039 lr: 0.02\n",
      "iteration: 48380 loss: 0.0088 lr: 0.02\n",
      "iteration: 48390 loss: 0.0073 lr: 0.02\n",
      "iteration: 48400 loss: 0.0038 lr: 0.02\n",
      "iteration: 48410 loss: 0.0060 lr: 0.02\n",
      "iteration: 48420 loss: 0.0057 lr: 0.02\n",
      "iteration: 48430 loss: 0.0055 lr: 0.02\n",
      "iteration: 48440 loss: 0.0063 lr: 0.02\n",
      "iteration: 48450 loss: 0.0041 lr: 0.02\n",
      "iteration: 48460 loss: 0.0041 lr: 0.02\n",
      "iteration: 48470 loss: 0.0064 lr: 0.02\n",
      "iteration: 48480 loss: 0.0058 lr: 0.02\n",
      "iteration: 48490 loss: 0.0070 lr: 0.02\n",
      "iteration: 48500 loss: 0.0070 lr: 0.02\n",
      "iteration: 48510 loss: 0.0047 lr: 0.02\n",
      "iteration: 48520 loss: 0.0054 lr: 0.02\n",
      "iteration: 48530 loss: 0.0068 lr: 0.02\n",
      "iteration: 48540 loss: 0.0045 lr: 0.02\n",
      "iteration: 48550 loss: 0.0054 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 48560 loss: 0.0042 lr: 0.02\n",
      "iteration: 48570 loss: 0.0057 lr: 0.02\n",
      "iteration: 48580 loss: 0.0057 lr: 0.02\n",
      "iteration: 48590 loss: 0.0066 lr: 0.02\n",
      "iteration: 48600 loss: 0.0035 lr: 0.02\n",
      "iteration: 48610 loss: 0.0056 lr: 0.02\n",
      "iteration: 48620 loss: 0.0047 lr: 0.02\n",
      "iteration: 48630 loss: 0.0046 lr: 0.02\n",
      "iteration: 48640 loss: 0.0047 lr: 0.02\n",
      "iteration: 48650 loss: 0.0067 lr: 0.02\n",
      "iteration: 48660 loss: 0.0049 lr: 0.02\n",
      "iteration: 48670 loss: 0.0066 lr: 0.02\n",
      "iteration: 48680 loss: 0.0037 lr: 0.02\n",
      "iteration: 48690 loss: 0.0043 lr: 0.02\n",
      "iteration: 48700 loss: 0.0049 lr: 0.02\n",
      "iteration: 48710 loss: 0.0043 lr: 0.02\n",
      "iteration: 48720 loss: 0.0051 lr: 0.02\n",
      "iteration: 48730 loss: 0.0083 lr: 0.02\n",
      "iteration: 48740 loss: 0.0067 lr: 0.02\n",
      "iteration: 48750 loss: 0.0052 lr: 0.02\n",
      "iteration: 48760 loss: 0.0070 lr: 0.02\n",
      "iteration: 48770 loss: 0.0062 lr: 0.02\n",
      "iteration: 48780 loss: 0.0049 lr: 0.02\n",
      "iteration: 48790 loss: 0.0076 lr: 0.02\n",
      "iteration: 48800 loss: 0.0052 lr: 0.02\n",
      "iteration: 48810 loss: 0.0050 lr: 0.02\n",
      "iteration: 48820 loss: 0.0065 lr: 0.02\n",
      "iteration: 48830 loss: 0.0056 lr: 0.02\n",
      "iteration: 48840 loss: 0.0036 lr: 0.02\n",
      "iteration: 48850 loss: 0.0037 lr: 0.02\n",
      "iteration: 48860 loss: 0.0047 lr: 0.02\n",
      "iteration: 48870 loss: 0.0041 lr: 0.02\n",
      "iteration: 48880 loss: 0.0038 lr: 0.02\n",
      "iteration: 48890 loss: 0.0072 lr: 0.02\n",
      "iteration: 48900 loss: 0.0075 lr: 0.02\n",
      "iteration: 48910 loss: 0.0037 lr: 0.02\n",
      "iteration: 48920 loss: 0.0055 lr: 0.02\n",
      "iteration: 48930 loss: 0.0053 lr: 0.02\n",
      "iteration: 48940 loss: 0.0043 lr: 0.02\n",
      "iteration: 48950 loss: 0.0056 lr: 0.02\n",
      "iteration: 48960 loss: 0.0037 lr: 0.02\n",
      "iteration: 48970 loss: 0.0053 lr: 0.02\n",
      "iteration: 48980 loss: 0.0052 lr: 0.02\n",
      "iteration: 48990 loss: 0.0042 lr: 0.02\n",
      "iteration: 49000 loss: 0.0040 lr: 0.02\n",
      "iteration: 49010 loss: 0.0059 lr: 0.02\n",
      "iteration: 49020 loss: 0.0052 lr: 0.02\n",
      "iteration: 49030 loss: 0.0045 lr: 0.02\n",
      "iteration: 49040 loss: 0.0072 lr: 0.02\n",
      "iteration: 49050 loss: 0.0065 lr: 0.02\n",
      "iteration: 49060 loss: 0.0058 lr: 0.02\n",
      "iteration: 49070 loss: 0.0061 lr: 0.02\n",
      "iteration: 49080 loss: 0.0051 lr: 0.02\n",
      "iteration: 49090 loss: 0.0046 lr: 0.02\n",
      "iteration: 49100 loss: 0.0045 lr: 0.02\n",
      "iteration: 49110 loss: 0.0068 lr: 0.02\n",
      "iteration: 49120 loss: 0.0051 lr: 0.02\n",
      "iteration: 49130 loss: 0.0062 lr: 0.02\n",
      "iteration: 49140 loss: 0.0045 lr: 0.02\n",
      "iteration: 49150 loss: 0.0059 lr: 0.02\n",
      "iteration: 49160 loss: 0.0051 lr: 0.02\n",
      "iteration: 49170 loss: 0.0055 lr: 0.02\n",
      "iteration: 49180 loss: 0.0045 lr: 0.02\n",
      "iteration: 49190 loss: 0.0041 lr: 0.02\n",
      "iteration: 49200 loss: 0.0032 lr: 0.02\n",
      "iteration: 49210 loss: 0.0042 lr: 0.02\n",
      "iteration: 49220 loss: 0.0048 lr: 0.02\n",
      "iteration: 49230 loss: 0.0040 lr: 0.02\n",
      "iteration: 49240 loss: 0.0089 lr: 0.02\n",
      "iteration: 49250 loss: 0.0044 lr: 0.02\n",
      "iteration: 49260 loss: 0.0044 lr: 0.02\n",
      "iteration: 49270 loss: 0.0044 lr: 0.02\n",
      "iteration: 49280 loss: 0.0060 lr: 0.02\n",
      "iteration: 49290 loss: 0.0043 lr: 0.02\n",
      "iteration: 49300 loss: 0.0045 lr: 0.02\n",
      "iteration: 49310 loss: 0.0055 lr: 0.02\n",
      "iteration: 49320 loss: 0.0053 lr: 0.02\n",
      "iteration: 49330 loss: 0.0048 lr: 0.02\n",
      "iteration: 49340 loss: 0.0046 lr: 0.02\n",
      "iteration: 49350 loss: 0.0037 lr: 0.02\n",
      "iteration: 49360 loss: 0.0073 lr: 0.02\n",
      "iteration: 49370 loss: 0.0043 lr: 0.02\n",
      "iteration: 49380 loss: 0.0045 lr: 0.02\n",
      "iteration: 49390 loss: 0.0046 lr: 0.02\n",
      "iteration: 49400 loss: 0.0073 lr: 0.02\n",
      "iteration: 49410 loss: 0.0064 lr: 0.02\n",
      "iteration: 49420 loss: 0.0047 lr: 0.02\n",
      "iteration: 49430 loss: 0.0038 lr: 0.02\n",
      "iteration: 49440 loss: 0.0045 lr: 0.02\n",
      "iteration: 49450 loss: 0.0063 lr: 0.02\n",
      "iteration: 49460 loss: 0.0061 lr: 0.02\n",
      "iteration: 49470 loss: 0.0036 lr: 0.02\n",
      "iteration: 49480 loss: 0.0046 lr: 0.02\n",
      "iteration: 49490 loss: 0.0067 lr: 0.02\n",
      "iteration: 49500 loss: 0.0063 lr: 0.02\n",
      "iteration: 49510 loss: 0.0059 lr: 0.02\n",
      "iteration: 49520 loss: 0.0042 lr: 0.02\n",
      "iteration: 49530 loss: 0.0046 lr: 0.02\n",
      "iteration: 49540 loss: 0.0049 lr: 0.02\n",
      "iteration: 49550 loss: 0.0057 lr: 0.02\n",
      "iteration: 49560 loss: 0.0047 lr: 0.02\n",
      "iteration: 49570 loss: 0.0052 lr: 0.02\n",
      "iteration: 49580 loss: 0.0032 lr: 0.02\n",
      "iteration: 49590 loss: 0.0056 lr: 0.02\n",
      "iteration: 49600 loss: 0.0031 lr: 0.02\n",
      "iteration: 49610 loss: 0.0044 lr: 0.02\n",
      "iteration: 49620 loss: 0.0040 lr: 0.02\n",
      "iteration: 49630 loss: 0.0035 lr: 0.02\n",
      "iteration: 49640 loss: 0.0033 lr: 0.02\n",
      "iteration: 49650 loss: 0.0046 lr: 0.02\n",
      "iteration: 49660 loss: 0.0051 lr: 0.02\n",
      "iteration: 49670 loss: 0.0042 lr: 0.02\n",
      "iteration: 49680 loss: 0.0060 lr: 0.02\n",
      "iteration: 49690 loss: 0.0059 lr: 0.02\n",
      "iteration: 49700 loss: 0.0068 lr: 0.02\n",
      "iteration: 49710 loss: 0.0041 lr: 0.02\n",
      "iteration: 49720 loss: 0.0063 lr: 0.02\n",
      "iteration: 49730 loss: 0.0045 lr: 0.02\n",
      "iteration: 49740 loss: 0.0038 lr: 0.02\n",
      "iteration: 49750 loss: 0.0055 lr: 0.02\n",
      "iteration: 49760 loss: 0.0040 lr: 0.02\n",
      "iteration: 49770 loss: 0.0049 lr: 0.02\n",
      "iteration: 49780 loss: 0.0053 lr: 0.02\n",
      "iteration: 49790 loss: 0.0030 lr: 0.02\n",
      "iteration: 49800 loss: 0.0051 lr: 0.02\n",
      "iteration: 49810 loss: 0.0045 lr: 0.02\n",
      "iteration: 49820 loss: 0.0048 lr: 0.02\n",
      "iteration: 49830 loss: 0.0038 lr: 0.02\n",
      "iteration: 49840 loss: 0.0048 lr: 0.02\n",
      "iteration: 49850 loss: 0.0037 lr: 0.02\n",
      "iteration: 49860 loss: 0.0049 lr: 0.02\n",
      "iteration: 49870 loss: 0.0048 lr: 0.02\n",
      "iteration: 49880 loss: 0.0045 lr: 0.02\n",
      "iteration: 49890 loss: 0.0038 lr: 0.02\n",
      "iteration: 49900 loss: 0.0049 lr: 0.02\n",
      "iteration: 49910 loss: 0.0046 lr: 0.02\n",
      "iteration: 49920 loss: 0.0047 lr: 0.02\n",
      "iteration: 49930 loss: 0.0043 lr: 0.02\n",
      "iteration: 49940 loss: 0.0050 lr: 0.02\n",
      "iteration: 49950 loss: 0.0053 lr: 0.02\n",
      "iteration: 49960 loss: 0.0058 lr: 0.02\n",
      "iteration: 49970 loss: 0.0055 lr: 0.02\n",
      "iteration: 49980 loss: 0.0092 lr: 0.02\n",
      "iteration: 49990 loss: 0.0063 lr: 0.02\n",
      "iteration: 50000 loss: 0.0076 lr: 0.02\n",
      "iteration: 50010 loss: 0.0058 lr: 0.02\n",
      "iteration: 50020 loss: 0.0084 lr: 0.02\n",
      "iteration: 50030 loss: 0.0046 lr: 0.02\n",
      "iteration: 50040 loss: 0.0046 lr: 0.02\n",
      "iteration: 50050 loss: 0.0072 lr: 0.02\n",
      "iteration: 50060 loss: 0.0038 lr: 0.02\n",
      "iteration: 50070 loss: 0.0057 lr: 0.02\n",
      "iteration: 50080 loss: 0.0049 lr: 0.02\n",
      "iteration: 50090 loss: 0.0049 lr: 0.02\n",
      "iteration: 50100 loss: 0.0041 lr: 0.02\n",
      "iteration: 50110 loss: 0.0049 lr: 0.02\n",
      "iteration: 50120 loss: 0.0059 lr: 0.02\n",
      "iteration: 50130 loss: 0.0049 lr: 0.02\n",
      "iteration: 50140 loss: 0.0058 lr: 0.02\n",
      "iteration: 50150 loss: 0.0036 lr: 0.02\n",
      "iteration: 50160 loss: 0.0045 lr: 0.02\n",
      "iteration: 50170 loss: 0.0065 lr: 0.02\n",
      "iteration: 50180 loss: 0.0038 lr: 0.02\n",
      "iteration: 50190 loss: 0.0041 lr: 0.02\n",
      "iteration: 50200 loss: 0.0054 lr: 0.02\n",
      "iteration: 50210 loss: 0.0055 lr: 0.02\n",
      "iteration: 50220 loss: 0.0033 lr: 0.02\n",
      "iteration: 50230 loss: 0.0076 lr: 0.02\n",
      "iteration: 50240 loss: 0.0070 lr: 0.02\n",
      "iteration: 50250 loss: 0.0038 lr: 0.02\n",
      "iteration: 50260 loss: 0.0066 lr: 0.02\n",
      "iteration: 50270 loss: 0.0051 lr: 0.02\n",
      "iteration: 50280 loss: 0.0059 lr: 0.02\n",
      "iteration: 50290 loss: 0.0051 lr: 0.02\n",
      "iteration: 50300 loss: 0.0070 lr: 0.02\n",
      "iteration: 50310 loss: 0.0044 lr: 0.02\n",
      "iteration: 50320 loss: 0.0054 lr: 0.02\n",
      "iteration: 50330 loss: 0.0054 lr: 0.02\n",
      "iteration: 50340 loss: 0.0038 lr: 0.02\n",
      "iteration: 50350 loss: 0.0037 lr: 0.02\n",
      "iteration: 50360 loss: 0.0046 lr: 0.02\n",
      "iteration: 50370 loss: 0.0044 lr: 0.02\n",
      "iteration: 50380 loss: 0.0052 lr: 0.02\n",
      "iteration: 50390 loss: 0.0044 lr: 0.02\n",
      "iteration: 50400 loss: 0.0042 lr: 0.02\n",
      "iteration: 50410 loss: 0.0057 lr: 0.02\n",
      "iteration: 50420 loss: 0.0051 lr: 0.02\n",
      "iteration: 50430 loss: 0.0050 lr: 0.02\n",
      "iteration: 50440 loss: 0.0056 lr: 0.02\n",
      "iteration: 50450 loss: 0.0048 lr: 0.02\n",
      "iteration: 50460 loss: 0.0042 lr: 0.02\n",
      "iteration: 50470 loss: 0.0038 lr: 0.02\n",
      "iteration: 50480 loss: 0.0063 lr: 0.02\n",
      "iteration: 50490 loss: 0.0049 lr: 0.02\n",
      "iteration: 50500 loss: 0.0051 lr: 0.02\n",
      "iteration: 50510 loss: 0.0076 lr: 0.02\n",
      "iteration: 50520 loss: 0.0049 lr: 0.02\n",
      "iteration: 50530 loss: 0.0038 lr: 0.02\n",
      "iteration: 50540 loss: 0.0058 lr: 0.02\n",
      "iteration: 50550 loss: 0.0082 lr: 0.02\n",
      "iteration: 50560 loss: 0.0070 lr: 0.02\n",
      "iteration: 50570 loss: 0.0039 lr: 0.02\n",
      "iteration: 50580 loss: 0.0065 lr: 0.02\n",
      "iteration: 50590 loss: 0.0073 lr: 0.02\n",
      "iteration: 50600 loss: 0.0044 lr: 0.02\n",
      "iteration: 50610 loss: 0.0044 lr: 0.02\n",
      "iteration: 50620 loss: 0.0040 lr: 0.02\n",
      "iteration: 50630 loss: 0.0058 lr: 0.02\n",
      "iteration: 50640 loss: 0.0049 lr: 0.02\n",
      "iteration: 50650 loss: 0.0069 lr: 0.02\n",
      "iteration: 50660 loss: 0.0053 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 50670 loss: 0.0047 lr: 0.02\n",
      "iteration: 50680 loss: 0.0049 lr: 0.02\n",
      "iteration: 50690 loss: 0.0057 lr: 0.02\n",
      "iteration: 50700 loss: 0.0053 lr: 0.02\n",
      "iteration: 50710 loss: 0.0056 lr: 0.02\n",
      "iteration: 50720 loss: 0.0050 lr: 0.02\n",
      "iteration: 50730 loss: 0.0053 lr: 0.02\n",
      "iteration: 50740 loss: 0.0045 lr: 0.02\n",
      "iteration: 50750 loss: 0.0052 lr: 0.02\n",
      "iteration: 50760 loss: 0.0044 lr: 0.02\n",
      "iteration: 50770 loss: 0.0036 lr: 0.02\n",
      "iteration: 50780 loss: 0.0043 lr: 0.02\n",
      "iteration: 50790 loss: 0.0048 lr: 0.02\n",
      "iteration: 50800 loss: 0.0058 lr: 0.02\n",
      "iteration: 50810 loss: 0.0051 lr: 0.02\n",
      "iteration: 50820 loss: 0.0053 lr: 0.02\n",
      "iteration: 50830 loss: 0.0042 lr: 0.02\n",
      "iteration: 50840 loss: 0.0046 lr: 0.02\n",
      "iteration: 50850 loss: 0.0035 lr: 0.02\n",
      "iteration: 50860 loss: 0.0042 lr: 0.02\n",
      "iteration: 50870 loss: 0.0041 lr: 0.02\n",
      "iteration: 50880 loss: 0.0055 lr: 0.02\n",
      "iteration: 50890 loss: 0.0047 lr: 0.02\n",
      "iteration: 50900 loss: 0.0042 lr: 0.02\n",
      "iteration: 50910 loss: 0.0040 lr: 0.02\n",
      "iteration: 50920 loss: 0.0037 lr: 0.02\n",
      "iteration: 50930 loss: 0.0044 lr: 0.02\n",
      "iteration: 50940 loss: 0.0047 lr: 0.02\n",
      "iteration: 50950 loss: 0.0070 lr: 0.02\n",
      "iteration: 50960 loss: 0.0067 lr: 0.02\n",
      "iteration: 50970 loss: 0.0050 lr: 0.02\n",
      "iteration: 50980 loss: 0.0063 lr: 0.02\n",
      "iteration: 50990 loss: 0.0048 lr: 0.02\n",
      "iteration: 51000 loss: 0.0047 lr: 0.02\n",
      "iteration: 51010 loss: 0.0037 lr: 0.02\n",
      "iteration: 51020 loss: 0.0042 lr: 0.02\n",
      "iteration: 51030 loss: 0.0054 lr: 0.02\n",
      "iteration: 51040 loss: 0.0044 lr: 0.02\n",
      "iteration: 51050 loss: 0.0063 lr: 0.02\n",
      "iteration: 51060 loss: 0.0052 lr: 0.02\n",
      "iteration: 51070 loss: 0.0035 lr: 0.02\n",
      "iteration: 51080 loss: 0.0031 lr: 0.02\n",
      "iteration: 51090 loss: 0.0064 lr: 0.02\n",
      "iteration: 51100 loss: 0.0040 lr: 0.02\n",
      "iteration: 51110 loss: 0.0047 lr: 0.02\n",
      "iteration: 51120 loss: 0.0065 lr: 0.02\n",
      "iteration: 51130 loss: 0.0075 lr: 0.02\n",
      "iteration: 51140 loss: 0.0041 lr: 0.02\n",
      "iteration: 51150 loss: 0.0045 lr: 0.02\n",
      "iteration: 51160 loss: 0.0055 lr: 0.02\n",
      "iteration: 51170 loss: 0.0044 lr: 0.02\n",
      "iteration: 51180 loss: 0.0046 lr: 0.02\n",
      "iteration: 51190 loss: 0.0043 lr: 0.02\n",
      "iteration: 51200 loss: 0.0032 lr: 0.02\n",
      "iteration: 51210 loss: 0.0038 lr: 0.02\n",
      "iteration: 51220 loss: 0.0043 lr: 0.02\n",
      "iteration: 51230 loss: 0.0050 lr: 0.02\n",
      "iteration: 51240 loss: 0.0056 lr: 0.02\n",
      "iteration: 51250 loss: 0.0051 lr: 0.02\n",
      "iteration: 51260 loss: 0.0051 lr: 0.02\n",
      "iteration: 51270 loss: 0.0052 lr: 0.02\n",
      "iteration: 51280 loss: 0.0061 lr: 0.02\n",
      "iteration: 51290 loss: 0.0063 lr: 0.02\n",
      "iteration: 51300 loss: 0.0061 lr: 0.02\n",
      "iteration: 51310 loss: 0.0040 lr: 0.02\n",
      "iteration: 51320 loss: 0.0047 lr: 0.02\n",
      "iteration: 51330 loss: 0.0069 lr: 0.02\n",
      "iteration: 51340 loss: 0.0029 lr: 0.02\n",
      "iteration: 51350 loss: 0.0054 lr: 0.02\n",
      "iteration: 51360 loss: 0.0047 lr: 0.02\n",
      "iteration: 51370 loss: 0.0053 lr: 0.02\n",
      "iteration: 51380 loss: 0.0032 lr: 0.02\n",
      "iteration: 51390 loss: 0.0038 lr: 0.02\n",
      "iteration: 51400 loss: 0.0069 lr: 0.02\n",
      "iteration: 51410 loss: 0.0049 lr: 0.02\n",
      "iteration: 51420 loss: 0.0072 lr: 0.02\n",
      "iteration: 51430 loss: 0.0048 lr: 0.02\n",
      "iteration: 51440 loss: 0.0048 lr: 0.02\n",
      "iteration: 51450 loss: 0.0048 lr: 0.02\n",
      "iteration: 51460 loss: 0.0051 lr: 0.02\n",
      "iteration: 51470 loss: 0.0058 lr: 0.02\n",
      "iteration: 51480 loss: 0.0044 lr: 0.02\n",
      "iteration: 51490 loss: 0.0047 lr: 0.02\n",
      "iteration: 51500 loss: 0.0060 lr: 0.02\n",
      "iteration: 51510 loss: 0.0045 lr: 0.02\n",
      "iteration: 51520 loss: 0.0052 lr: 0.02\n",
      "iteration: 51530 loss: 0.0035 lr: 0.02\n",
      "iteration: 51540 loss: 0.0072 lr: 0.02\n",
      "iteration: 51550 loss: 0.0034 lr: 0.02\n",
      "iteration: 51560 loss: 0.0042 lr: 0.02\n",
      "iteration: 51570 loss: 0.0052 lr: 0.02\n",
      "iteration: 51580 loss: 0.0058 lr: 0.02\n",
      "iteration: 51590 loss: 0.0042 lr: 0.02\n",
      "iteration: 51600 loss: 0.0052 lr: 0.02\n",
      "iteration: 51610 loss: 0.0061 lr: 0.02\n",
      "iteration: 51620 loss: 0.0040 lr: 0.02\n",
      "iteration: 51630 loss: 0.0055 lr: 0.02\n",
      "iteration: 51640 loss: 0.0036 lr: 0.02\n",
      "iteration: 51650 loss: 0.0047 lr: 0.02\n",
      "iteration: 51660 loss: 0.0051 lr: 0.02\n",
      "iteration: 51670 loss: 0.0049 lr: 0.02\n",
      "iteration: 51680 loss: 0.0043 lr: 0.02\n",
      "iteration: 51690 loss: 0.0051 lr: 0.02\n",
      "iteration: 51700 loss: 0.0039 lr: 0.02\n",
      "iteration: 51710 loss: 0.0044 lr: 0.02\n",
      "iteration: 51720 loss: 0.0039 lr: 0.02\n",
      "iteration: 51730 loss: 0.0049 lr: 0.02\n",
      "iteration: 51740 loss: 0.0064 lr: 0.02\n",
      "iteration: 51750 loss: 0.0044 lr: 0.02\n",
      "iteration: 51760 loss: 0.0048 lr: 0.02\n",
      "iteration: 51770 loss: 0.0052 lr: 0.02\n",
      "iteration: 51780 loss: 0.0061 lr: 0.02\n",
      "iteration: 51790 loss: 0.0055 lr: 0.02\n",
      "iteration: 51800 loss: 0.0071 lr: 0.02\n",
      "iteration: 51810 loss: 0.0045 lr: 0.02\n",
      "iteration: 51820 loss: 0.0059 lr: 0.02\n",
      "iteration: 51830 loss: 0.0037 lr: 0.02\n",
      "iteration: 51840 loss: 0.0055 lr: 0.02\n",
      "iteration: 51850 loss: 0.0044 lr: 0.02\n",
      "iteration: 51860 loss: 0.0046 lr: 0.02\n",
      "iteration: 51870 loss: 0.0045 lr: 0.02\n",
      "iteration: 51880 loss: 0.0059 lr: 0.02\n",
      "iteration: 51890 loss: 0.0052 lr: 0.02\n",
      "iteration: 51900 loss: 0.0058 lr: 0.02\n",
      "iteration: 51910 loss: 0.0053 lr: 0.02\n",
      "iteration: 51920 loss: 0.0057 lr: 0.02\n",
      "iteration: 51930 loss: 0.0050 lr: 0.02\n",
      "iteration: 51940 loss: 0.0046 lr: 0.02\n",
      "iteration: 51950 loss: 0.0044 lr: 0.02\n",
      "iteration: 51960 loss: 0.0054 lr: 0.02\n",
      "iteration: 51970 loss: 0.0061 lr: 0.02\n",
      "iteration: 51980 loss: 0.0051 lr: 0.02\n",
      "iteration: 51990 loss: 0.0069 lr: 0.02\n",
      "iteration: 52000 loss: 0.0056 lr: 0.02\n",
      "iteration: 52010 loss: 0.0065 lr: 0.02\n",
      "iteration: 52020 loss: 0.0047 lr: 0.02\n",
      "iteration: 52030 loss: 0.0045 lr: 0.02\n",
      "iteration: 52040 loss: 0.0051 lr: 0.02\n",
      "iteration: 52050 loss: 0.0042 lr: 0.02\n",
      "iteration: 52060 loss: 0.0052 lr: 0.02\n",
      "iteration: 52070 loss: 0.0047 lr: 0.02\n",
      "iteration: 52080 loss: 0.0054 lr: 0.02\n",
      "iteration: 52090 loss: 0.0074 lr: 0.02\n",
      "iteration: 52100 loss: 0.0039 lr: 0.02\n",
      "iteration: 52110 loss: 0.0040 lr: 0.02\n",
      "iteration: 52120 loss: 0.0041 lr: 0.02\n",
      "iteration: 52130 loss: 0.0045 lr: 0.02\n",
      "iteration: 52140 loss: 0.0063 lr: 0.02\n",
      "iteration: 52150 loss: 0.0054 lr: 0.02\n",
      "iteration: 52160 loss: 0.0034 lr: 0.02\n",
      "iteration: 52170 loss: 0.0048 lr: 0.02\n",
      "iteration: 52180 loss: 0.0055 lr: 0.02\n",
      "iteration: 52190 loss: 0.0069 lr: 0.02\n",
      "iteration: 52200 loss: 0.0049 lr: 0.02\n",
      "iteration: 52210 loss: 0.0052 lr: 0.02\n",
      "iteration: 52220 loss: 0.0047 lr: 0.02\n",
      "iteration: 52230 loss: 0.0054 lr: 0.02\n",
      "iteration: 52240 loss: 0.0061 lr: 0.02\n",
      "iteration: 52250 loss: 0.0040 lr: 0.02\n",
      "iteration: 52260 loss: 0.0055 lr: 0.02\n",
      "iteration: 52270 loss: 0.0045 lr: 0.02\n",
      "iteration: 52280 loss: 0.0055 lr: 0.02\n",
      "iteration: 52290 loss: 0.0042 lr: 0.02\n",
      "iteration: 52300 loss: 0.0039 lr: 0.02\n",
      "iteration: 52310 loss: 0.0073 lr: 0.02\n",
      "iteration: 52320 loss: 0.0053 lr: 0.02\n",
      "iteration: 52330 loss: 0.0058 lr: 0.02\n",
      "iteration: 52340 loss: 0.0034 lr: 0.02\n",
      "iteration: 52350 loss: 0.0050 lr: 0.02\n",
      "iteration: 52360 loss: 0.0064 lr: 0.02\n",
      "iteration: 52370 loss: 0.0058 lr: 0.02\n",
      "iteration: 52380 loss: 0.0043 lr: 0.02\n",
      "iteration: 52390 loss: 0.0060 lr: 0.02\n",
      "iteration: 52400 loss: 0.0053 lr: 0.02\n",
      "iteration: 52410 loss: 0.0076 lr: 0.02\n",
      "iteration: 52420 loss: 0.0056 lr: 0.02\n",
      "iteration: 52430 loss: 0.0048 lr: 0.02\n",
      "iteration: 52440 loss: 0.0065 lr: 0.02\n",
      "iteration: 52450 loss: 0.0057 lr: 0.02\n",
      "iteration: 52460 loss: 0.0038 lr: 0.02\n",
      "iteration: 52470 loss: 0.0045 lr: 0.02\n",
      "iteration: 52480 loss: 0.0037 lr: 0.02\n",
      "iteration: 52490 loss: 0.0046 lr: 0.02\n",
      "iteration: 52500 loss: 0.0040 lr: 0.02\n",
      "iteration: 52510 loss: 0.0036 lr: 0.02\n",
      "iteration: 52520 loss: 0.0041 lr: 0.02\n",
      "iteration: 52530 loss: 0.0060 lr: 0.02\n",
      "iteration: 52540 loss: 0.0039 lr: 0.02\n",
      "iteration: 52550 loss: 0.0043 lr: 0.02\n",
      "iteration: 52560 loss: 0.0040 lr: 0.02\n",
      "iteration: 52570 loss: 0.0053 lr: 0.02\n",
      "iteration: 52580 loss: 0.0037 lr: 0.02\n",
      "iteration: 52590 loss: 0.0055 lr: 0.02\n",
      "iteration: 52600 loss: 0.0048 lr: 0.02\n",
      "iteration: 52610 loss: 0.0048 lr: 0.02\n",
      "iteration: 52620 loss: 0.0051 lr: 0.02\n",
      "iteration: 52630 loss: 0.0050 lr: 0.02\n",
      "iteration: 52640 loss: 0.0054 lr: 0.02\n",
      "iteration: 52650 loss: 0.0036 lr: 0.02\n",
      "iteration: 52660 loss: 0.0047 lr: 0.02\n",
      "iteration: 52670 loss: 0.0031 lr: 0.02\n",
      "iteration: 52680 loss: 0.0049 lr: 0.02\n",
      "iteration: 52690 loss: 0.0036 lr: 0.02\n",
      "iteration: 52700 loss: 0.0059 lr: 0.02\n",
      "iteration: 52710 loss: 0.0037 lr: 0.02\n",
      "iteration: 52720 loss: 0.0051 lr: 0.02\n",
      "iteration: 52730 loss: 0.0047 lr: 0.02\n",
      "iteration: 52740 loss: 0.0029 lr: 0.02\n",
      "iteration: 52750 loss: 0.0038 lr: 0.02\n",
      "iteration: 52760 loss: 0.0053 lr: 0.02\n",
      "iteration: 52770 loss: 0.0043 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 52780 loss: 0.0053 lr: 0.02\n",
      "iteration: 52790 loss: 0.0059 lr: 0.02\n",
      "iteration: 52800 loss: 0.0045 lr: 0.02\n",
      "iteration: 52810 loss: 0.0039 lr: 0.02\n",
      "iteration: 52820 loss: 0.0034 lr: 0.02\n",
      "iteration: 52830 loss: 0.0046 lr: 0.02\n",
      "iteration: 52840 loss: 0.0038 lr: 0.02\n",
      "iteration: 52850 loss: 0.0053 lr: 0.02\n",
      "iteration: 52860 loss: 0.0036 lr: 0.02\n",
      "iteration: 52870 loss: 0.0040 lr: 0.02\n",
      "iteration: 52880 loss: 0.0052 lr: 0.02\n",
      "iteration: 52890 loss: 0.0064 lr: 0.02\n",
      "iteration: 52900 loss: 0.0042 lr: 0.02\n",
      "iteration: 52910 loss: 0.0034 lr: 0.02\n",
      "iteration: 52920 loss: 0.0058 lr: 0.02\n",
      "iteration: 52930 loss: 0.0046 lr: 0.02\n",
      "iteration: 52940 loss: 0.0041 lr: 0.02\n",
      "iteration: 52950 loss: 0.0051 lr: 0.02\n",
      "iteration: 52960 loss: 0.0095 lr: 0.02\n",
      "iteration: 52970 loss: 0.0049 lr: 0.02\n",
      "iteration: 52980 loss: 0.0045 lr: 0.02\n",
      "iteration: 52990 loss: 0.0056 lr: 0.02\n",
      "iteration: 53000 loss: 0.0049 lr: 0.02\n",
      "iteration: 53010 loss: 0.0055 lr: 0.02\n",
      "iteration: 53020 loss: 0.0052 lr: 0.02\n",
      "iteration: 53030 loss: 0.0051 lr: 0.02\n",
      "iteration: 53040 loss: 0.0070 lr: 0.02\n",
      "iteration: 53050 loss: 0.0052 lr: 0.02\n",
      "iteration: 53060 loss: 0.0058 lr: 0.02\n",
      "iteration: 53070 loss: 0.0063 lr: 0.02\n",
      "iteration: 53080 loss: 0.0039 lr: 0.02\n",
      "iteration: 53090 loss: 0.0062 lr: 0.02\n",
      "iteration: 53100 loss: 0.0045 lr: 0.02\n",
      "iteration: 53110 loss: 0.0039 lr: 0.02\n",
      "iteration: 53120 loss: 0.0033 lr: 0.02\n",
      "iteration: 53130 loss: 0.0054 lr: 0.02\n",
      "iteration: 53140 loss: 0.0051 lr: 0.02\n",
      "iteration: 53150 loss: 0.0067 lr: 0.02\n",
      "iteration: 53160 loss: 0.0060 lr: 0.02\n",
      "iteration: 53170 loss: 0.0050 lr: 0.02\n",
      "iteration: 53180 loss: 0.0074 lr: 0.02\n",
      "iteration: 53190 loss: 0.0051 lr: 0.02\n",
      "iteration: 53200 loss: 0.0058 lr: 0.02\n",
      "iteration: 53210 loss: 0.0036 lr: 0.02\n",
      "iteration: 53220 loss: 0.0049 lr: 0.02\n",
      "iteration: 53230 loss: 0.0043 lr: 0.02\n",
      "iteration: 53240 loss: 0.0050 lr: 0.02\n",
      "iteration: 53250 loss: 0.0065 lr: 0.02\n",
      "iteration: 53260 loss: 0.0058 lr: 0.02\n",
      "iteration: 53270 loss: 0.0042 lr: 0.02\n",
      "iteration: 53280 loss: 0.0061 lr: 0.02\n",
      "iteration: 53290 loss: 0.0038 lr: 0.02\n",
      "iteration: 53300 loss: 0.0051 lr: 0.02\n",
      "iteration: 53310 loss: 0.0051 lr: 0.02\n",
      "iteration: 53320 loss: 0.0058 lr: 0.02\n",
      "iteration: 53330 loss: 0.0058 lr: 0.02\n",
      "iteration: 53340 loss: 0.0054 lr: 0.02\n",
      "iteration: 53350 loss: 0.0061 lr: 0.02\n",
      "iteration: 53360 loss: 0.0039 lr: 0.02\n",
      "iteration: 53370 loss: 0.0042 lr: 0.02\n",
      "iteration: 53380 loss: 0.0070 lr: 0.02\n",
      "iteration: 53390 loss: 0.0043 lr: 0.02\n",
      "iteration: 53400 loss: 0.0062 lr: 0.02\n",
      "iteration: 53410 loss: 0.0057 lr: 0.02\n",
      "iteration: 53420 loss: 0.0077 lr: 0.02\n",
      "iteration: 53430 loss: 0.0050 lr: 0.02\n",
      "iteration: 53440 loss: 0.0073 lr: 0.02\n",
      "iteration: 53450 loss: 0.0040 lr: 0.02\n",
      "iteration: 53460 loss: 0.0062 lr: 0.02\n",
      "iteration: 53470 loss: 0.0077 lr: 0.02\n",
      "iteration: 53480 loss: 0.0052 lr: 0.02\n",
      "iteration: 53490 loss: 0.0051 lr: 0.02\n",
      "iteration: 53500 loss: 0.0051 lr: 0.02\n",
      "iteration: 53510 loss: 0.0045 lr: 0.02\n",
      "iteration: 53520 loss: 0.0046 lr: 0.02\n",
      "iteration: 53530 loss: 0.0051 lr: 0.02\n",
      "iteration: 53540 loss: 0.0050 lr: 0.02\n",
      "iteration: 53550 loss: 0.0052 lr: 0.02\n",
      "iteration: 53560 loss: 0.0032 lr: 0.02\n",
      "iteration: 53570 loss: 0.0049 lr: 0.02\n",
      "iteration: 53580 loss: 0.0047 lr: 0.02\n",
      "iteration: 53590 loss: 0.0042 lr: 0.02\n",
      "iteration: 53600 loss: 0.0044 lr: 0.02\n",
      "iteration: 53610 loss: 0.0053 lr: 0.02\n",
      "iteration: 53620 loss: 0.0049 lr: 0.02\n",
      "iteration: 53630 loss: 0.0056 lr: 0.02\n",
      "iteration: 53640 loss: 0.0048 lr: 0.02\n",
      "iteration: 53650 loss: 0.0051 lr: 0.02\n",
      "iteration: 53660 loss: 0.0046 lr: 0.02\n",
      "iteration: 53670 loss: 0.0038 lr: 0.02\n",
      "iteration: 53680 loss: 0.0041 lr: 0.02\n",
      "iteration: 53690 loss: 0.0042 lr: 0.02\n",
      "iteration: 53700 loss: 0.0039 lr: 0.02\n",
      "iteration: 53710 loss: 0.0042 lr: 0.02\n",
      "iteration: 53720 loss: 0.0048 lr: 0.02\n",
      "iteration: 53730 loss: 0.0035 lr: 0.02\n",
      "iteration: 53740 loss: 0.0043 lr: 0.02\n",
      "iteration: 53750 loss: 0.0041 lr: 0.02\n",
      "iteration: 53760 loss: 0.0059 lr: 0.02\n",
      "iteration: 53770 loss: 0.0040 lr: 0.02\n",
      "iteration: 53780 loss: 0.0065 lr: 0.02\n",
      "iteration: 53790 loss: 0.0041 lr: 0.02\n",
      "iteration: 53800 loss: 0.0039 lr: 0.02\n",
      "iteration: 53810 loss: 0.0044 lr: 0.02\n",
      "iteration: 53820 loss: 0.0049 lr: 0.02\n",
      "iteration: 53830 loss: 0.0040 lr: 0.02\n",
      "iteration: 53840 loss: 0.0028 lr: 0.02\n",
      "iteration: 53850 loss: 0.0046 lr: 0.02\n",
      "iteration: 53860 loss: 0.0047 lr: 0.02\n",
      "iteration: 53870 loss: 0.0052 lr: 0.02\n",
      "iteration: 53880 loss: 0.0054 lr: 0.02\n",
      "iteration: 53890 loss: 0.0052 lr: 0.02\n",
      "iteration: 53900 loss: 0.0055 lr: 0.02\n",
      "iteration: 53910 loss: 0.0041 lr: 0.02\n",
      "iteration: 53920 loss: 0.0036 lr: 0.02\n",
      "iteration: 53930 loss: 0.0046 lr: 0.02\n",
      "iteration: 53940 loss: 0.0037 lr: 0.02\n",
      "iteration: 53950 loss: 0.0048 lr: 0.02\n",
      "iteration: 53960 loss: 0.0048 lr: 0.02\n",
      "iteration: 53970 loss: 0.0052 lr: 0.02\n",
      "iteration: 53980 loss: 0.0052 lr: 0.02\n",
      "iteration: 53990 loss: 0.0064 lr: 0.02\n",
      "iteration: 54000 loss: 0.0055 lr: 0.02\n",
      "iteration: 54010 loss: 0.0049 lr: 0.02\n",
      "iteration: 54020 loss: 0.0040 lr: 0.02\n",
      "iteration: 54030 loss: 0.0050 lr: 0.02\n",
      "iteration: 54040 loss: 0.0043 lr: 0.02\n",
      "iteration: 54050 loss: 0.0054 lr: 0.02\n",
      "iteration: 54060 loss: 0.0034 lr: 0.02\n",
      "iteration: 54070 loss: 0.0048 lr: 0.02\n",
      "iteration: 54080 loss: 0.0043 lr: 0.02\n",
      "iteration: 54090 loss: 0.0046 lr: 0.02\n",
      "iteration: 54100 loss: 0.0043 lr: 0.02\n",
      "iteration: 54110 loss: 0.0040 lr: 0.02\n",
      "iteration: 54120 loss: 0.0041 lr: 0.02\n",
      "iteration: 54130 loss: 0.0051 lr: 0.02\n",
      "iteration: 54140 loss: 0.0050 lr: 0.02\n",
      "iteration: 54150 loss: 0.0050 lr: 0.02\n",
      "iteration: 54160 loss: 0.0062 lr: 0.02\n",
      "iteration: 54170 loss: 0.0051 lr: 0.02\n",
      "iteration: 54180 loss: 0.0042 lr: 0.02\n",
      "iteration: 54190 loss: 0.0050 lr: 0.02\n",
      "iteration: 54200 loss: 0.0044 lr: 0.02\n",
      "iteration: 54210 loss: 0.0044 lr: 0.02\n",
      "iteration: 54220 loss: 0.0058 lr: 0.02\n",
      "iteration: 54230 loss: 0.0034 lr: 0.02\n",
      "iteration: 54240 loss: 0.0050 lr: 0.02\n",
      "iteration: 54250 loss: 0.0061 lr: 0.02\n",
      "iteration: 54260 loss: 0.0049 lr: 0.02\n",
      "iteration: 54270 loss: 0.0054 lr: 0.02\n",
      "iteration: 54280 loss: 0.0042 lr: 0.02\n",
      "iteration: 54290 loss: 0.0054 lr: 0.02\n",
      "iteration: 54300 loss: 0.0033 lr: 0.02\n",
      "iteration: 54310 loss: 0.0040 lr: 0.02\n",
      "iteration: 54320 loss: 0.0032 lr: 0.02\n",
      "iteration: 54330 loss: 0.0053 lr: 0.02\n",
      "iteration: 54340 loss: 0.0054 lr: 0.02\n",
      "iteration: 54350 loss: 0.0042 lr: 0.02\n",
      "iteration: 54360 loss: 0.0047 lr: 0.02\n",
      "iteration: 54370 loss: 0.0046 lr: 0.02\n",
      "iteration: 54380 loss: 0.0041 lr: 0.02\n",
      "iteration: 54390 loss: 0.0032 lr: 0.02\n",
      "iteration: 54400 loss: 0.0033 lr: 0.02\n",
      "iteration: 54410 loss: 0.0051 lr: 0.02\n",
      "iteration: 54420 loss: 0.0056 lr: 0.02\n",
      "iteration: 54430 loss: 0.0031 lr: 0.02\n",
      "iteration: 54440 loss: 0.0034 lr: 0.02\n",
      "iteration: 54450 loss: 0.0050 lr: 0.02\n",
      "iteration: 54460 loss: 0.0061 lr: 0.02\n",
      "iteration: 54470 loss: 0.0052 lr: 0.02\n",
      "iteration: 54480 loss: 0.0041 lr: 0.02\n",
      "iteration: 54490 loss: 0.0035 lr: 0.02\n",
      "iteration: 54500 loss: 0.0044 lr: 0.02\n",
      "iteration: 54510 loss: 0.0036 lr: 0.02\n",
      "iteration: 54520 loss: 0.0055 lr: 0.02\n",
      "iteration: 54530 loss: 0.0056 lr: 0.02\n",
      "iteration: 54540 loss: 0.0051 lr: 0.02\n",
      "iteration: 54550 loss: 0.0045 lr: 0.02\n",
      "iteration: 54560 loss: 0.0048 lr: 0.02\n",
      "iteration: 54570 loss: 0.0044 lr: 0.02\n",
      "iteration: 54580 loss: 0.0057 lr: 0.02\n",
      "iteration: 54590 loss: 0.0047 lr: 0.02\n",
      "iteration: 54600 loss: 0.0054 lr: 0.02\n",
      "iteration: 54610 loss: 0.0060 lr: 0.02\n",
      "iteration: 54620 loss: 0.0040 lr: 0.02\n",
      "iteration: 54630 loss: 0.0051 lr: 0.02\n",
      "iteration: 54640 loss: 0.0055 lr: 0.02\n",
      "iteration: 54650 loss: 0.0037 lr: 0.02\n",
      "iteration: 54660 loss: 0.0039 lr: 0.02\n",
      "iteration: 54670 loss: 0.0062 lr: 0.02\n",
      "iteration: 54680 loss: 0.0046 lr: 0.02\n",
      "iteration: 54690 loss: 0.0039 lr: 0.02\n",
      "iteration: 54700 loss: 0.0036 lr: 0.02\n",
      "iteration: 54710 loss: 0.0052 lr: 0.02\n",
      "iteration: 54720 loss: 0.0044 lr: 0.02\n",
      "iteration: 54730 loss: 0.0038 lr: 0.02\n",
      "iteration: 54740 loss: 0.0033 lr: 0.02\n",
      "iteration: 54750 loss: 0.0046 lr: 0.02\n",
      "iteration: 54760 loss: 0.0047 lr: 0.02\n",
      "iteration: 54770 loss: 0.0054 lr: 0.02\n",
      "iteration: 54780 loss: 0.0037 lr: 0.02\n",
      "iteration: 54790 loss: 0.0046 lr: 0.02\n",
      "iteration: 54800 loss: 0.0051 lr: 0.02\n",
      "iteration: 54810 loss: 0.0044 lr: 0.02\n",
      "iteration: 54820 loss: 0.0042 lr: 0.02\n",
      "iteration: 54830 loss: 0.0041 lr: 0.02\n",
      "iteration: 54840 loss: 0.0051 lr: 0.02\n",
      "iteration: 54850 loss: 0.0044 lr: 0.02\n",
      "iteration: 54860 loss: 0.0054 lr: 0.02\n",
      "iteration: 54870 loss: 0.0039 lr: 0.02\n",
      "iteration: 54880 loss: 0.0042 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 54890 loss: 0.0037 lr: 0.02\n",
      "iteration: 54900 loss: 0.0043 lr: 0.02\n",
      "iteration: 54910 loss: 0.0047 lr: 0.02\n",
      "iteration: 54920 loss: 0.0043 lr: 0.02\n",
      "iteration: 54930 loss: 0.0048 lr: 0.02\n",
      "iteration: 54940 loss: 0.0042 lr: 0.02\n",
      "iteration: 54950 loss: 0.0039 lr: 0.02\n",
      "iteration: 54960 loss: 0.0044 lr: 0.02\n",
      "iteration: 54970 loss: 0.0056 lr: 0.02\n",
      "iteration: 54980 loss: 0.0051 lr: 0.02\n",
      "iteration: 54990 loss: 0.0048 lr: 0.02\n",
      "iteration: 55000 loss: 0.0056 lr: 0.02\n",
      "iteration: 55010 loss: 0.0085 lr: 0.02\n",
      "iteration: 55020 loss: 0.0057 lr: 0.02\n",
      "iteration: 55030 loss: 0.0049 lr: 0.02\n",
      "iteration: 55040 loss: 0.0053 lr: 0.02\n",
      "iteration: 55050 loss: 0.0033 lr: 0.02\n",
      "iteration: 55060 loss: 0.0059 lr: 0.02\n",
      "iteration: 55070 loss: 0.0057 lr: 0.02\n",
      "iteration: 55080 loss: 0.0036 lr: 0.02\n",
      "iteration: 55090 loss: 0.0037 lr: 0.02\n",
      "iteration: 55100 loss: 0.0064 lr: 0.02\n",
      "iteration: 55110 loss: 0.0048 lr: 0.02\n",
      "iteration: 55120 loss: 0.0033 lr: 0.02\n",
      "iteration: 55130 loss: 0.0041 lr: 0.02\n",
      "iteration: 55140 loss: 0.0053 lr: 0.02\n",
      "iteration: 55150 loss: 0.0036 lr: 0.02\n",
      "iteration: 55160 loss: 0.0040 lr: 0.02\n",
      "iteration: 55170 loss: 0.0043 lr: 0.02\n",
      "iteration: 55180 loss: 0.0057 lr: 0.02\n",
      "iteration: 55190 loss: 0.0042 lr: 0.02\n",
      "iteration: 55200 loss: 0.0049 lr: 0.02\n",
      "iteration: 55210 loss: 0.0043 lr: 0.02\n",
      "iteration: 55220 loss: 0.0048 lr: 0.02\n",
      "iteration: 55230 loss: 0.0051 lr: 0.02\n",
      "iteration: 55240 loss: 0.0045 lr: 0.02\n",
      "iteration: 55250 loss: 0.0052 lr: 0.02\n",
      "iteration: 55260 loss: 0.0034 lr: 0.02\n",
      "iteration: 55270 loss: 0.0038 lr: 0.02\n",
      "iteration: 55280 loss: 0.0038 lr: 0.02\n",
      "iteration: 55290 loss: 0.0055 lr: 0.02\n",
      "iteration: 55300 loss: 0.0054 lr: 0.02\n",
      "iteration: 55310 loss: 0.0059 lr: 0.02\n",
      "iteration: 55320 loss: 0.0041 lr: 0.02\n",
      "iteration: 55330 loss: 0.0061 lr: 0.02\n",
      "iteration: 55340 loss: 0.0045 lr: 0.02\n",
      "iteration: 55350 loss: 0.0058 lr: 0.02\n",
      "iteration: 55360 loss: 0.0056 lr: 0.02\n",
      "iteration: 55370 loss: 0.0057 lr: 0.02\n",
      "iteration: 55380 loss: 0.0050 lr: 0.02\n",
      "iteration: 55390 loss: 0.0041 lr: 0.02\n",
      "iteration: 55400 loss: 0.0068 lr: 0.02\n",
      "iteration: 55410 loss: 0.0066 lr: 0.02\n",
      "iteration: 55420 loss: 0.0041 lr: 0.02\n",
      "iteration: 55430 loss: 0.0059 lr: 0.02\n",
      "iteration: 55440 loss: 0.0050 lr: 0.02\n",
      "iteration: 55450 loss: 0.0054 lr: 0.02\n",
      "iteration: 55460 loss: 0.0062 lr: 0.02\n",
      "iteration: 55470 loss: 0.0043 lr: 0.02\n",
      "iteration: 55480 loss: 0.0041 lr: 0.02\n",
      "iteration: 55490 loss: 0.0048 lr: 0.02\n",
      "iteration: 55500 loss: 0.0040 lr: 0.02\n",
      "iteration: 55510 loss: 0.0068 lr: 0.02\n",
      "iteration: 55520 loss: 0.0055 lr: 0.02\n",
      "iteration: 55530 loss: 0.0060 lr: 0.02\n",
      "iteration: 55540 loss: 0.0049 lr: 0.02\n",
      "iteration: 55550 loss: 0.0049 lr: 0.02\n",
      "iteration: 55560 loss: 0.0046 lr: 0.02\n",
      "iteration: 55570 loss: 0.0074 lr: 0.02\n",
      "iteration: 55580 loss: 0.0045 lr: 0.02\n",
      "iteration: 55590 loss: 0.0048 lr: 0.02\n",
      "iteration: 55600 loss: 0.0034 lr: 0.02\n",
      "iteration: 55610 loss: 0.0032 lr: 0.02\n",
      "iteration: 55620 loss: 0.0040 lr: 0.02\n",
      "iteration: 55630 loss: 0.0030 lr: 0.02\n",
      "iteration: 55640 loss: 0.0051 lr: 0.02\n",
      "iteration: 55650 loss: 0.0045 lr: 0.02\n",
      "iteration: 55660 loss: 0.0056 lr: 0.02\n",
      "iteration: 55670 loss: 0.0044 lr: 0.02\n",
      "iteration: 55680 loss: 0.0042 lr: 0.02\n",
      "iteration: 55690 loss: 0.0049 lr: 0.02\n",
      "iteration: 55700 loss: 0.0051 lr: 0.02\n",
      "iteration: 55710 loss: 0.0040 lr: 0.02\n",
      "iteration: 55720 loss: 0.0047 lr: 0.02\n",
      "iteration: 55730 loss: 0.0032 lr: 0.02\n",
      "iteration: 55740 loss: 0.0040 lr: 0.02\n",
      "iteration: 55750 loss: 0.0055 lr: 0.02\n",
      "iteration: 55760 loss: 0.0057 lr: 0.02\n",
      "iteration: 55770 loss: 0.0062 lr: 0.02\n",
      "iteration: 55780 loss: 0.0040 lr: 0.02\n",
      "iteration: 55790 loss: 0.0051 lr: 0.02\n",
      "iteration: 55800 loss: 0.0065 lr: 0.02\n",
      "iteration: 55810 loss: 0.0037 lr: 0.02\n",
      "iteration: 55820 loss: 0.0038 lr: 0.02\n",
      "iteration: 55830 loss: 0.0048 lr: 0.02\n",
      "iteration: 55840 loss: 0.0052 lr: 0.02\n",
      "iteration: 55850 loss: 0.0040 lr: 0.02\n",
      "iteration: 55860 loss: 0.0061 lr: 0.02\n",
      "iteration: 55870 loss: 0.0054 lr: 0.02\n",
      "iteration: 55880 loss: 0.0032 lr: 0.02\n",
      "iteration: 55890 loss: 0.0061 lr: 0.02\n",
      "iteration: 55900 loss: 0.0049 lr: 0.02\n",
      "iteration: 55910 loss: 0.0047 lr: 0.02\n",
      "iteration: 55920 loss: 0.0045 lr: 0.02\n",
      "iteration: 55930 loss: 0.0040 lr: 0.02\n",
      "iteration: 55940 loss: 0.0046 lr: 0.02\n",
      "iteration: 55950 loss: 0.0058 lr: 0.02\n",
      "iteration: 55960 loss: 0.0049 lr: 0.02\n",
      "iteration: 55970 loss: 0.0061 lr: 0.02\n",
      "iteration: 55980 loss: 0.0052 lr: 0.02\n",
      "iteration: 55990 loss: 0.0055 lr: 0.02\n",
      "iteration: 56000 loss: 0.0044 lr: 0.02\n",
      "iteration: 56010 loss: 0.0042 lr: 0.02\n",
      "iteration: 56020 loss: 0.0048 lr: 0.02\n",
      "iteration: 56030 loss: 0.0045 lr: 0.02\n",
      "iteration: 56040 loss: 0.0059 lr: 0.02\n",
      "iteration: 56050 loss: 0.0040 lr: 0.02\n",
      "iteration: 56060 loss: 0.0033 lr: 0.02\n",
      "iteration: 56070 loss: 0.0035 lr: 0.02\n",
      "iteration: 56080 loss: 0.0043 lr: 0.02\n",
      "iteration: 56090 loss: 0.0053 lr: 0.02\n",
      "iteration: 56100 loss: 0.0047 lr: 0.02\n",
      "iteration: 56110 loss: 0.0057 lr: 0.02\n",
      "iteration: 56120 loss: 0.0046 lr: 0.02\n",
      "iteration: 56130 loss: 0.0041 lr: 0.02\n",
      "iteration: 56140 loss: 0.0036 lr: 0.02\n",
      "iteration: 56150 loss: 0.0048 lr: 0.02\n",
      "iteration: 56160 loss: 0.0056 lr: 0.02\n",
      "iteration: 56170 loss: 0.0048 lr: 0.02\n",
      "iteration: 56180 loss: 0.0063 lr: 0.02\n",
      "iteration: 56190 loss: 0.0048 lr: 0.02\n",
      "iteration: 56200 loss: 0.0055 lr: 0.02\n",
      "iteration: 56210 loss: 0.0044 lr: 0.02\n",
      "iteration: 56220 loss: 0.0040 lr: 0.02\n",
      "iteration: 56230 loss: 0.0050 lr: 0.02\n",
      "iteration: 56240 loss: 0.0045 lr: 0.02\n",
      "iteration: 56250 loss: 0.0035 lr: 0.02\n",
      "iteration: 56260 loss: 0.0060 lr: 0.02\n",
      "iteration: 56270 loss: 0.0044 lr: 0.02\n",
      "iteration: 56280 loss: 0.0045 lr: 0.02\n",
      "iteration: 56290 loss: 0.0048 lr: 0.02\n",
      "iteration: 56300 loss: 0.0045 lr: 0.02\n",
      "iteration: 56310 loss: 0.0047 lr: 0.02\n",
      "iteration: 56320 loss: 0.0058 lr: 0.02\n",
      "iteration: 56330 loss: 0.0048 lr: 0.02\n",
      "iteration: 56340 loss: 0.0034 lr: 0.02\n",
      "iteration: 56350 loss: 0.0052 lr: 0.02\n",
      "iteration: 56360 loss: 0.0052 lr: 0.02\n",
      "iteration: 56370 loss: 0.0035 lr: 0.02\n",
      "iteration: 56380 loss: 0.0054 lr: 0.02\n",
      "iteration: 56390 loss: 0.0047 lr: 0.02\n",
      "iteration: 56400 loss: 0.0046 lr: 0.02\n",
      "iteration: 56410 loss: 0.0047 lr: 0.02\n",
      "iteration: 56420 loss: 0.0053 lr: 0.02\n",
      "iteration: 56430 loss: 0.0049 lr: 0.02\n",
      "iteration: 56440 loss: 0.0052 lr: 0.02\n",
      "iteration: 56450 loss: 0.0045 lr: 0.02\n",
      "iteration: 56460 loss: 0.0040 lr: 0.02\n",
      "iteration: 56470 loss: 0.0048 lr: 0.02\n",
      "iteration: 56480 loss: 0.0045 lr: 0.02\n",
      "iteration: 56490 loss: 0.0058 lr: 0.02\n",
      "iteration: 56500 loss: 0.0030 lr: 0.02\n",
      "iteration: 56510 loss: 0.0059 lr: 0.02\n",
      "iteration: 56520 loss: 0.0041 lr: 0.02\n",
      "iteration: 56530 loss: 0.0040 lr: 0.02\n",
      "iteration: 56540 loss: 0.0035 lr: 0.02\n",
      "iteration: 56550 loss: 0.0052 lr: 0.02\n",
      "iteration: 56560 loss: 0.0040 lr: 0.02\n",
      "iteration: 56570 loss: 0.0054 lr: 0.02\n",
      "iteration: 56580 loss: 0.0035 lr: 0.02\n",
      "iteration: 56590 loss: 0.0043 lr: 0.02\n",
      "iteration: 56600 loss: 0.0048 lr: 0.02\n",
      "iteration: 56610 loss: 0.0051 lr: 0.02\n",
      "iteration: 56620 loss: 0.0056 lr: 0.02\n",
      "iteration: 56630 loss: 0.0052 lr: 0.02\n",
      "iteration: 56640 loss: 0.0055 lr: 0.02\n",
      "iteration: 56650 loss: 0.0045 lr: 0.02\n",
      "iteration: 56660 loss: 0.0044 lr: 0.02\n",
      "iteration: 56670 loss: 0.0071 lr: 0.02\n",
      "iteration: 56680 loss: 0.0057 lr: 0.02\n",
      "iteration: 56690 loss: 0.0053 lr: 0.02\n",
      "iteration: 56700 loss: 0.0045 lr: 0.02\n",
      "iteration: 56710 loss: 0.0044 lr: 0.02\n",
      "iteration: 56720 loss: 0.0069 lr: 0.02\n",
      "iteration: 56730 loss: 0.0039 lr: 0.02\n",
      "iteration: 56740 loss: 0.0041 lr: 0.02\n",
      "iteration: 56750 loss: 0.0061 lr: 0.02\n",
      "iteration: 56760 loss: 0.0049 lr: 0.02\n",
      "iteration: 56770 loss: 0.0062 lr: 0.02\n",
      "iteration: 56780 loss: 0.0046 lr: 0.02\n",
      "iteration: 56790 loss: 0.0053 lr: 0.02\n",
      "iteration: 56800 loss: 0.0040 lr: 0.02\n",
      "iteration: 56810 loss: 0.0050 lr: 0.02\n",
      "iteration: 56820 loss: 0.0034 lr: 0.02\n",
      "iteration: 56830 loss: 0.0054 lr: 0.02\n",
      "iteration: 56840 loss: 0.0048 lr: 0.02\n",
      "iteration: 56850 loss: 0.0040 lr: 0.02\n",
      "iteration: 56860 loss: 0.0056 lr: 0.02\n",
      "iteration: 56870 loss: 0.0042 lr: 0.02\n",
      "iteration: 56880 loss: 0.0040 lr: 0.02\n",
      "iteration: 56890 loss: 0.0039 lr: 0.02\n",
      "iteration: 56900 loss: 0.0042 lr: 0.02\n",
      "iteration: 56910 loss: 0.0046 lr: 0.02\n",
      "iteration: 56920 loss: 0.0054 lr: 0.02\n",
      "iteration: 56930 loss: 0.0035 lr: 0.02\n",
      "iteration: 56940 loss: 0.0042 lr: 0.02\n",
      "iteration: 56950 loss: 0.0051 lr: 0.02\n",
      "iteration: 56960 loss: 0.0055 lr: 0.02\n",
      "iteration: 56970 loss: 0.0040 lr: 0.02\n",
      "iteration: 56980 loss: 0.0040 lr: 0.02\n",
      "iteration: 56990 loss: 0.0057 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 57000 loss: 0.0052 lr: 0.02\n",
      "iteration: 57010 loss: 0.0055 lr: 0.02\n",
      "iteration: 57020 loss: 0.0046 lr: 0.02\n",
      "iteration: 57030 loss: 0.0038 lr: 0.02\n",
      "iteration: 57040 loss: 0.0038 lr: 0.02\n",
      "iteration: 57050 loss: 0.0054 lr: 0.02\n",
      "iteration: 57060 loss: 0.0038 lr: 0.02\n",
      "iteration: 57070 loss: 0.0075 lr: 0.02\n",
      "iteration: 57080 loss: 0.0053 lr: 0.02\n",
      "iteration: 57090 loss: 0.0053 lr: 0.02\n",
      "iteration: 57100 loss: 0.0046 lr: 0.02\n",
      "iteration: 57110 loss: 0.0040 lr: 0.02\n",
      "iteration: 57120 loss: 0.0050 lr: 0.02\n",
      "iteration: 57130 loss: 0.0036 lr: 0.02\n",
      "iteration: 57140 loss: 0.0033 lr: 0.02\n",
      "iteration: 57150 loss: 0.0039 lr: 0.02\n",
      "iteration: 57160 loss: 0.0031 lr: 0.02\n",
      "iteration: 57170 loss: 0.0038 lr: 0.02\n",
      "iteration: 57180 loss: 0.0057 lr: 0.02\n",
      "iteration: 57190 loss: 0.0039 lr: 0.02\n",
      "iteration: 57200 loss: 0.0047 lr: 0.02\n",
      "iteration: 57210 loss: 0.0056 lr: 0.02\n",
      "iteration: 57220 loss: 0.0045 lr: 0.02\n",
      "iteration: 57230 loss: 0.0023 lr: 0.02\n",
      "iteration: 57240 loss: 0.0051 lr: 0.02\n",
      "iteration: 57250 loss: 0.0036 lr: 0.02\n",
      "iteration: 57260 loss: 0.0047 lr: 0.02\n",
      "iteration: 57270 loss: 0.0057 lr: 0.02\n",
      "iteration: 57280 loss: 0.0055 lr: 0.02\n",
      "iteration: 57290 loss: 0.0055 lr: 0.02\n",
      "iteration: 57300 loss: 0.0039 lr: 0.02\n",
      "iteration: 57310 loss: 0.0040 lr: 0.02\n",
      "iteration: 57320 loss: 0.0043 lr: 0.02\n",
      "iteration: 57330 loss: 0.0050 lr: 0.02\n",
      "iteration: 57340 loss: 0.0047 lr: 0.02\n",
      "iteration: 57350 loss: 0.0039 lr: 0.02\n",
      "iteration: 57360 loss: 0.0044 lr: 0.02\n",
      "iteration: 57370 loss: 0.0048 lr: 0.02\n",
      "iteration: 57380 loss: 0.0039 lr: 0.02\n",
      "iteration: 57390 loss: 0.0053 lr: 0.02\n",
      "iteration: 57400 loss: 0.0046 lr: 0.02\n",
      "iteration: 57410 loss: 0.0051 lr: 0.02\n",
      "iteration: 57420 loss: 0.0039 lr: 0.02\n",
      "iteration: 57430 loss: 0.0041 lr: 0.02\n",
      "iteration: 57440 loss: 0.0052 lr: 0.02\n",
      "iteration: 57450 loss: 0.0046 lr: 0.02\n",
      "iteration: 57460 loss: 0.0034 lr: 0.02\n",
      "iteration: 57470 loss: 0.0035 lr: 0.02\n",
      "iteration: 57480 loss: 0.0039 lr: 0.02\n",
      "iteration: 57490 loss: 0.0034 lr: 0.02\n",
      "iteration: 57500 loss: 0.0048 lr: 0.02\n",
      "iteration: 57510 loss: 0.0046 lr: 0.02\n",
      "iteration: 57520 loss: 0.0041 lr: 0.02\n",
      "iteration: 57530 loss: 0.0034 lr: 0.02\n",
      "iteration: 57540 loss: 0.0046 lr: 0.02\n",
      "iteration: 57550 loss: 0.0039 lr: 0.02\n",
      "iteration: 57560 loss: 0.0049 lr: 0.02\n",
      "iteration: 57570 loss: 0.0052 lr: 0.02\n",
      "iteration: 57580 loss: 0.0063 lr: 0.02\n",
      "iteration: 57590 loss: 0.0051 lr: 0.02\n",
      "iteration: 57600 loss: 0.0043 lr: 0.02\n",
      "iteration: 57610 loss: 0.0055 lr: 0.02\n",
      "iteration: 57620 loss: 0.0042 lr: 0.02\n",
      "iteration: 57630 loss: 0.0048 lr: 0.02\n",
      "iteration: 57640 loss: 0.0042 lr: 0.02\n",
      "iteration: 57650 loss: 0.0065 lr: 0.02\n",
      "iteration: 57660 loss: 0.0058 lr: 0.02\n",
      "iteration: 57670 loss: 0.0046 lr: 0.02\n",
      "iteration: 57680 loss: 0.0046 lr: 0.02\n",
      "iteration: 57690 loss: 0.0027 lr: 0.02\n",
      "iteration: 57700 loss: 0.0059 lr: 0.02\n",
      "iteration: 57710 loss: 0.0036 lr: 0.02\n",
      "iteration: 57720 loss: 0.0046 lr: 0.02\n",
      "iteration: 57730 loss: 0.0049 lr: 0.02\n",
      "iteration: 57740 loss: 0.0037 lr: 0.02\n",
      "iteration: 57750 loss: 0.0043 lr: 0.02\n",
      "iteration: 57760 loss: 0.0057 lr: 0.02\n",
      "iteration: 57770 loss: 0.0055 lr: 0.02\n",
      "iteration: 57780 loss: 0.0044 lr: 0.02\n",
      "iteration: 57790 loss: 0.0054 lr: 0.02\n",
      "iteration: 57800 loss: 0.0045 lr: 0.02\n",
      "iteration: 57810 loss: 0.0043 lr: 0.02\n",
      "iteration: 57820 loss: 0.0033 lr: 0.02\n",
      "iteration: 57830 loss: 0.0054 lr: 0.02\n",
      "iteration: 57840 loss: 0.0039 lr: 0.02\n",
      "iteration: 57850 loss: 0.0075 lr: 0.02\n",
      "iteration: 57860 loss: 0.0064 lr: 0.02\n",
      "iteration: 57870 loss: 0.0036 lr: 0.02\n",
      "iteration: 57880 loss: 0.0036 lr: 0.02\n",
      "iteration: 57890 loss: 0.0057 lr: 0.02\n",
      "iteration: 57900 loss: 0.0051 lr: 0.02\n",
      "iteration: 57910 loss: 0.0060 lr: 0.02\n",
      "iteration: 57920 loss: 0.0044 lr: 0.02\n",
      "iteration: 57930 loss: 0.0063 lr: 0.02\n",
      "iteration: 57940 loss: 0.0052 lr: 0.02\n",
      "iteration: 57950 loss: 0.0046 lr: 0.02\n",
      "iteration: 57960 loss: 0.0042 lr: 0.02\n",
      "iteration: 57970 loss: 0.0048 lr: 0.02\n",
      "iteration: 57980 loss: 0.0051 lr: 0.02\n",
      "iteration: 57990 loss: 0.0040 lr: 0.02\n",
      "iteration: 58000 loss: 0.0044 lr: 0.02\n",
      "iteration: 58010 loss: 0.0037 lr: 0.02\n",
      "iteration: 58020 loss: 0.0044 lr: 0.02\n",
      "iteration: 58030 loss: 0.0032 lr: 0.02\n",
      "iteration: 58040 loss: 0.0052 lr: 0.02\n",
      "iteration: 58050 loss: 0.0045 lr: 0.02\n",
      "iteration: 58060 loss: 0.0028 lr: 0.02\n",
      "iteration: 58070 loss: 0.0030 lr: 0.02\n",
      "iteration: 58080 loss: 0.0069 lr: 0.02\n",
      "iteration: 58090 loss: 0.0044 lr: 0.02\n",
      "iteration: 58100 loss: 0.0061 lr: 0.02\n",
      "iteration: 58110 loss: 0.0058 lr: 0.02\n",
      "iteration: 58120 loss: 0.0079 lr: 0.02\n",
      "iteration: 58130 loss: 0.0041 lr: 0.02\n",
      "iteration: 58140 loss: 0.0045 lr: 0.02\n",
      "iteration: 58150 loss: 0.0032 lr: 0.02\n",
      "iteration: 58160 loss: 0.0044 lr: 0.02\n",
      "iteration: 58170 loss: 0.0054 lr: 0.02\n",
      "iteration: 58180 loss: 0.0042 lr: 0.02\n",
      "iteration: 58190 loss: 0.0042 lr: 0.02\n",
      "iteration: 58200 loss: 0.0043 lr: 0.02\n",
      "iteration: 58210 loss: 0.0057 lr: 0.02\n",
      "iteration: 58220 loss: 0.0050 lr: 0.02\n",
      "iteration: 58230 loss: 0.0047 lr: 0.02\n",
      "iteration: 58240 loss: 0.0048 lr: 0.02\n",
      "iteration: 58250 loss: 0.0037 lr: 0.02\n",
      "iteration: 58260 loss: 0.0051 lr: 0.02\n",
      "iteration: 58270 loss: 0.0045 lr: 0.02\n",
      "iteration: 58280 loss: 0.0049 lr: 0.02\n",
      "iteration: 58290 loss: 0.0044 lr: 0.02\n",
      "iteration: 58300 loss: 0.0051 lr: 0.02\n",
      "iteration: 58310 loss: 0.0036 lr: 0.02\n",
      "iteration: 58320 loss: 0.0058 lr: 0.02\n",
      "iteration: 58330 loss: 0.0048 lr: 0.02\n",
      "iteration: 58340 loss: 0.0030 lr: 0.02\n",
      "iteration: 58350 loss: 0.0044 lr: 0.02\n",
      "iteration: 58360 loss: 0.0044 lr: 0.02\n",
      "iteration: 58370 loss: 0.0044 lr: 0.02\n",
      "iteration: 58380 loss: 0.0033 lr: 0.02\n",
      "iteration: 58390 loss: 0.0048 lr: 0.02\n",
      "iteration: 58400 loss: 0.0056 lr: 0.02\n",
      "iteration: 58410 loss: 0.0028 lr: 0.02\n",
      "iteration: 58420 loss: 0.0050 lr: 0.02\n",
      "iteration: 58430 loss: 0.0047 lr: 0.02\n",
      "iteration: 58440 loss: 0.0045 lr: 0.02\n",
      "iteration: 58450 loss: 0.0038 lr: 0.02\n",
      "iteration: 58460 loss: 0.0040 lr: 0.02\n",
      "iteration: 58470 loss: 0.0068 lr: 0.02\n",
      "iteration: 58480 loss: 0.0065 lr: 0.02\n",
      "iteration: 58490 loss: 0.0049 lr: 0.02\n",
      "iteration: 58500 loss: 0.0036 lr: 0.02\n",
      "iteration: 58510 loss: 0.0055 lr: 0.02\n",
      "iteration: 58520 loss: 0.0055 lr: 0.02\n",
      "iteration: 58530 loss: 0.0057 lr: 0.02\n",
      "iteration: 58540 loss: 0.0038 lr: 0.02\n",
      "iteration: 58550 loss: 0.0046 lr: 0.02\n",
      "iteration: 58560 loss: 0.0051 lr: 0.02\n",
      "iteration: 58570 loss: 0.0044 lr: 0.02\n",
      "iteration: 58580 loss: 0.0053 lr: 0.02\n",
      "iteration: 58590 loss: 0.0037 lr: 0.02\n",
      "iteration: 58600 loss: 0.0041 lr: 0.02\n",
      "iteration: 58610 loss: 0.0041 lr: 0.02\n",
      "iteration: 58620 loss: 0.0044 lr: 0.02\n",
      "iteration: 58630 loss: 0.0033 lr: 0.02\n",
      "iteration: 58640 loss: 0.0033 lr: 0.02\n",
      "iteration: 58650 loss: 0.0053 lr: 0.02\n",
      "iteration: 58660 loss: 0.0039 lr: 0.02\n",
      "iteration: 58670 loss: 0.0044 lr: 0.02\n",
      "iteration: 58680 loss: 0.0052 lr: 0.02\n",
      "iteration: 58690 loss: 0.0045 lr: 0.02\n",
      "iteration: 58700 loss: 0.0037 lr: 0.02\n",
      "iteration: 58710 loss: 0.0042 lr: 0.02\n",
      "iteration: 58720 loss: 0.0042 lr: 0.02\n",
      "iteration: 58730 loss: 0.0041 lr: 0.02\n",
      "iteration: 58740 loss: 0.0055 lr: 0.02\n",
      "iteration: 58750 loss: 0.0041 lr: 0.02\n",
      "iteration: 58760 loss: 0.0068 lr: 0.02\n",
      "iteration: 58770 loss: 0.0038 lr: 0.02\n",
      "iteration: 58780 loss: 0.0061 lr: 0.02\n",
      "iteration: 58790 loss: 0.0051 lr: 0.02\n",
      "iteration: 58800 loss: 0.0031 lr: 0.02\n",
      "iteration: 58810 loss: 0.0032 lr: 0.02\n",
      "iteration: 58820 loss: 0.0040 lr: 0.02\n",
      "iteration: 58830 loss: 0.0043 lr: 0.02\n",
      "iteration: 58840 loss: 0.0035 lr: 0.02\n",
      "iteration: 58850 loss: 0.0035 lr: 0.02\n",
      "iteration: 58860 loss: 0.0049 lr: 0.02\n",
      "iteration: 58870 loss: 0.0052 lr: 0.02\n",
      "iteration: 58880 loss: 0.0036 lr: 0.02\n",
      "iteration: 58890 loss: 0.0053 lr: 0.02\n",
      "iteration: 58900 loss: 0.0064 lr: 0.02\n",
      "iteration: 58910 loss: 0.0056 lr: 0.02\n",
      "iteration: 58920 loss: 0.0047 lr: 0.02\n",
      "iteration: 58930 loss: 0.0050 lr: 0.02\n",
      "iteration: 58940 loss: 0.0049 lr: 0.02\n",
      "iteration: 58950 loss: 0.0047 lr: 0.02\n",
      "iteration: 58960 loss: 0.0039 lr: 0.02\n",
      "iteration: 58970 loss: 0.0043 lr: 0.02\n",
      "iteration: 58980 loss: 0.0040 lr: 0.02\n",
      "iteration: 58990 loss: 0.0034 lr: 0.02\n",
      "iteration: 59000 loss: 0.0067 lr: 0.02\n",
      "iteration: 59010 loss: 0.0059 lr: 0.02\n",
      "iteration: 59020 loss: 0.0054 lr: 0.02\n",
      "iteration: 59030 loss: 0.0043 lr: 0.02\n",
      "iteration: 59040 loss: 0.0049 lr: 0.02\n",
      "iteration: 59050 loss: 0.0055 lr: 0.02\n",
      "iteration: 59060 loss: 0.0039 lr: 0.02\n",
      "iteration: 59070 loss: 0.0059 lr: 0.02\n",
      "iteration: 59080 loss: 0.0052 lr: 0.02\n",
      "iteration: 59090 loss: 0.0047 lr: 0.02\n",
      "iteration: 59100 loss: 0.0035 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 59110 loss: 0.0037 lr: 0.02\n",
      "iteration: 59120 loss: 0.0046 lr: 0.02\n",
      "iteration: 59130 loss: 0.0045 lr: 0.02\n",
      "iteration: 59140 loss: 0.0031 lr: 0.02\n",
      "iteration: 59150 loss: 0.0038 lr: 0.02\n",
      "iteration: 59160 loss: 0.0047 lr: 0.02\n",
      "iteration: 59170 loss: 0.0038 lr: 0.02\n",
      "iteration: 59180 loss: 0.0053 lr: 0.02\n",
      "iteration: 59190 loss: 0.0038 lr: 0.02\n",
      "iteration: 59200 loss: 0.0045 lr: 0.02\n",
      "iteration: 59210 loss: 0.0043 lr: 0.02\n",
      "iteration: 59220 loss: 0.0043 lr: 0.02\n",
      "iteration: 59230 loss: 0.0036 lr: 0.02\n",
      "iteration: 59240 loss: 0.0038 lr: 0.02\n",
      "iteration: 59250 loss: 0.0043 lr: 0.02\n",
      "iteration: 59260 loss: 0.0050 lr: 0.02\n",
      "iteration: 59270 loss: 0.0044 lr: 0.02\n",
      "iteration: 59280 loss: 0.0042 lr: 0.02\n",
      "iteration: 59290 loss: 0.0043 lr: 0.02\n",
      "iteration: 59300 loss: 0.0047 lr: 0.02\n",
      "iteration: 59310 loss: 0.0031 lr: 0.02\n",
      "iteration: 59320 loss: 0.0031 lr: 0.02\n",
      "iteration: 59330 loss: 0.0044 lr: 0.02\n",
      "iteration: 59340 loss: 0.0038 lr: 0.02\n",
      "iteration: 59350 loss: 0.0046 lr: 0.02\n",
      "iteration: 59360 loss: 0.0042 lr: 0.02\n",
      "iteration: 59370 loss: 0.0031 lr: 0.02\n",
      "iteration: 59380 loss: 0.0029 lr: 0.02\n",
      "iteration: 59390 loss: 0.0032 lr: 0.02\n",
      "iteration: 59400 loss: 0.0052 lr: 0.02\n",
      "iteration: 59410 loss: 0.0045 lr: 0.02\n",
      "iteration: 59420 loss: 0.0034 lr: 0.02\n",
      "iteration: 59430 loss: 0.0048 lr: 0.02\n",
      "iteration: 59440 loss: 0.0051 lr: 0.02\n",
      "iteration: 59450 loss: 0.0040 lr: 0.02\n",
      "iteration: 59460 loss: 0.0083 lr: 0.02\n",
      "iteration: 59470 loss: 0.0032 lr: 0.02\n",
      "iteration: 59480 loss: 0.0036 lr: 0.02\n",
      "iteration: 59490 loss: 0.0050 lr: 0.02\n",
      "iteration: 59500 loss: 0.0041 lr: 0.02\n",
      "iteration: 59510 loss: 0.0044 lr: 0.02\n",
      "iteration: 59520 loss: 0.0041 lr: 0.02\n",
      "iteration: 59530 loss: 0.0033 lr: 0.02\n",
      "iteration: 59540 loss: 0.0051 lr: 0.02\n",
      "iteration: 59550 loss: 0.0039 lr: 0.02\n",
      "iteration: 59560 loss: 0.0039 lr: 0.02\n",
      "iteration: 59570 loss: 0.0046 lr: 0.02\n",
      "iteration: 59580 loss: 0.0043 lr: 0.02\n",
      "iteration: 59590 loss: 0.0052 lr: 0.02\n",
      "iteration: 59600 loss: 0.0036 lr: 0.02\n",
      "iteration: 59610 loss: 0.0045 lr: 0.02\n",
      "iteration: 59620 loss: 0.0051 lr: 0.02\n",
      "iteration: 59630 loss: 0.0043 lr: 0.02\n",
      "iteration: 59640 loss: 0.0036 lr: 0.02\n",
      "iteration: 59650 loss: 0.0055 lr: 0.02\n",
      "iteration: 59660 loss: 0.0050 lr: 0.02\n",
      "iteration: 59670 loss: 0.0067 lr: 0.02\n",
      "iteration: 59680 loss: 0.0041 lr: 0.02\n",
      "iteration: 59690 loss: 0.0059 lr: 0.02\n",
      "iteration: 59700 loss: 0.0063 lr: 0.02\n",
      "iteration: 59710 loss: 0.0045 lr: 0.02\n",
      "iteration: 59720 loss: 0.0037 lr: 0.02\n",
      "iteration: 59730 loss: 0.0046 lr: 0.02\n",
      "iteration: 59740 loss: 0.0039 lr: 0.02\n",
      "iteration: 59750 loss: 0.0032 lr: 0.02\n",
      "iteration: 59760 loss: 0.0048 lr: 0.02\n",
      "iteration: 59770 loss: 0.0052 lr: 0.02\n",
      "iteration: 59780 loss: 0.0067 lr: 0.02\n",
      "iteration: 59790 loss: 0.0052 lr: 0.02\n",
      "iteration: 59800 loss: 0.0050 lr: 0.02\n",
      "iteration: 59810 loss: 0.0040 lr: 0.02\n",
      "iteration: 59820 loss: 0.0044 lr: 0.02\n",
      "iteration: 59830 loss: 0.0048 lr: 0.02\n",
      "iteration: 59840 loss: 0.0061 lr: 0.02\n",
      "iteration: 59850 loss: 0.0052 lr: 0.02\n",
      "iteration: 59860 loss: 0.0046 lr: 0.02\n",
      "iteration: 59870 loss: 0.0054 lr: 0.02\n",
      "iteration: 59880 loss: 0.0065 lr: 0.02\n",
      "iteration: 59890 loss: 0.0051 lr: 0.02\n",
      "iteration: 59900 loss: 0.0038 lr: 0.02\n",
      "iteration: 59910 loss: 0.0074 lr: 0.02\n",
      "iteration: 59920 loss: 0.0032 lr: 0.02\n",
      "iteration: 59930 loss: 0.0075 lr: 0.02\n",
      "iteration: 59940 loss: 0.0038 lr: 0.02\n",
      "iteration: 59950 loss: 0.0034 lr: 0.02\n",
      "iteration: 59960 loss: 0.0043 lr: 0.02\n",
      "iteration: 59970 loss: 0.0045 lr: 0.02\n",
      "iteration: 59980 loss: 0.0043 lr: 0.02\n",
      "iteration: 59990 loss: 0.0035 lr: 0.02\n",
      "iteration: 60000 loss: 0.0040 lr: 0.02\n",
      "iteration: 60010 loss: 0.0056 lr: 0.02\n",
      "iteration: 60020 loss: 0.0036 lr: 0.02\n",
      "iteration: 60030 loss: 0.0054 lr: 0.02\n",
      "iteration: 60040 loss: 0.0055 lr: 0.02\n",
      "iteration: 60050 loss: 0.0040 lr: 0.02\n",
      "iteration: 60060 loss: 0.0037 lr: 0.02\n",
      "iteration: 60070 loss: 0.0044 lr: 0.02\n",
      "iteration: 60080 loss: 0.0050 lr: 0.02\n",
      "iteration: 60090 loss: 0.0046 lr: 0.02\n",
      "iteration: 60100 loss: 0.0058 lr: 0.02\n",
      "iteration: 60110 loss: 0.0036 lr: 0.02\n",
      "iteration: 60120 loss: 0.0032 lr: 0.02\n",
      "iteration: 60130 loss: 0.0034 lr: 0.02\n",
      "iteration: 60140 loss: 0.0029 lr: 0.02\n",
      "iteration: 60150 loss: 0.0053 lr: 0.02\n",
      "iteration: 60160 loss: 0.0062 lr: 0.02\n",
      "iteration: 60170 loss: 0.0039 lr: 0.02\n",
      "iteration: 60180 loss: 0.0046 lr: 0.02\n",
      "iteration: 60190 loss: 0.0044 lr: 0.02\n",
      "iteration: 60200 loss: 0.0061 lr: 0.02\n",
      "iteration: 60210 loss: 0.0035 lr: 0.02\n",
      "iteration: 60220 loss: 0.0047 lr: 0.02\n",
      "iteration: 60230 loss: 0.0049 lr: 0.02\n",
      "iteration: 60240 loss: 0.0065 lr: 0.02\n",
      "iteration: 60250 loss: 0.0046 lr: 0.02\n",
      "iteration: 60260 loss: 0.0043 lr: 0.02\n",
      "iteration: 60270 loss: 0.0039 lr: 0.02\n",
      "iteration: 60280 loss: 0.0053 lr: 0.02\n",
      "iteration: 60290 loss: 0.0052 lr: 0.02\n",
      "iteration: 60300 loss: 0.0045 lr: 0.02\n",
      "iteration: 60310 loss: 0.0046 lr: 0.02\n",
      "iteration: 60320 loss: 0.0041 lr: 0.02\n",
      "iteration: 60330 loss: 0.0036 lr: 0.02\n",
      "iteration: 60340 loss: 0.0038 lr: 0.02\n",
      "iteration: 60350 loss: 0.0053 lr: 0.02\n",
      "iteration: 60360 loss: 0.0032 lr: 0.02\n",
      "iteration: 60370 loss: 0.0043 lr: 0.02\n",
      "iteration: 60380 loss: 0.0051 lr: 0.02\n",
      "iteration: 60390 loss: 0.0043 lr: 0.02\n",
      "iteration: 60400 loss: 0.0048 lr: 0.02\n",
      "iteration: 60410 loss: 0.0046 lr: 0.02\n",
      "iteration: 60420 loss: 0.0054 lr: 0.02\n",
      "iteration: 60430 loss: 0.0050 lr: 0.02\n",
      "iteration: 60440 loss: 0.0058 lr: 0.02\n",
      "iteration: 60450 loss: 0.0041 lr: 0.02\n",
      "iteration: 60460 loss: 0.0036 lr: 0.02\n",
      "iteration: 60470 loss: 0.0052 lr: 0.02\n",
      "iteration: 60480 loss: 0.0053 lr: 0.02\n",
      "iteration: 60490 loss: 0.0045 lr: 0.02\n",
      "iteration: 60500 loss: 0.0052 lr: 0.02\n",
      "iteration: 60510 loss: 0.0063 lr: 0.02\n",
      "iteration: 60520 loss: 0.0034 lr: 0.02\n",
      "iteration: 60530 loss: 0.0062 lr: 0.02\n",
      "iteration: 60540 loss: 0.0044 lr: 0.02\n",
      "iteration: 60550 loss: 0.0049 lr: 0.02\n",
      "iteration: 60560 loss: 0.0030 lr: 0.02\n",
      "iteration: 60570 loss: 0.0048 lr: 0.02\n",
      "iteration: 60580 loss: 0.0035 lr: 0.02\n",
      "iteration: 60590 loss: 0.0051 lr: 0.02\n",
      "iteration: 60600 loss: 0.0049 lr: 0.02\n",
      "iteration: 60610 loss: 0.0031 lr: 0.02\n",
      "iteration: 60620 loss: 0.0058 lr: 0.02\n",
      "iteration: 60630 loss: 0.0060 lr: 0.02\n",
      "iteration: 60640 loss: 0.0045 lr: 0.02\n",
      "iteration: 60650 loss: 0.0036 lr: 0.02\n",
      "iteration: 60660 loss: 0.0031 lr: 0.02\n",
      "iteration: 60670 loss: 0.0029 lr: 0.02\n",
      "iteration: 60680 loss: 0.0042 lr: 0.02\n",
      "iteration: 60690 loss: 0.0056 lr: 0.02\n",
      "iteration: 60700 loss: 0.0046 lr: 0.02\n",
      "iteration: 60710 loss: 0.0032 lr: 0.02\n",
      "iteration: 60720 loss: 0.0038 lr: 0.02\n",
      "iteration: 60730 loss: 0.0048 lr: 0.02\n",
      "iteration: 60740 loss: 0.0053 lr: 0.02\n",
      "iteration: 60750 loss: 0.0044 lr: 0.02\n",
      "iteration: 60760 loss: 0.0036 lr: 0.02\n",
      "iteration: 60770 loss: 0.0046 lr: 0.02\n",
      "iteration: 60780 loss: 0.0065 lr: 0.02\n",
      "iteration: 60790 loss: 0.0058 lr: 0.02\n",
      "iteration: 60800 loss: 0.0052 lr: 0.02\n",
      "iteration: 60810 loss: 0.0056 lr: 0.02\n",
      "iteration: 60820 loss: 0.0034 lr: 0.02\n",
      "iteration: 60830 loss: 0.0041 lr: 0.02\n",
      "iteration: 60840 loss: 0.0038 lr: 0.02\n",
      "iteration: 60850 loss: 0.0043 lr: 0.02\n",
      "iteration: 60860 loss: 0.0045 lr: 0.02\n",
      "iteration: 60870 loss: 0.0036 lr: 0.02\n",
      "iteration: 60880 loss: 0.0041 lr: 0.02\n",
      "iteration: 60890 loss: 0.0040 lr: 0.02\n",
      "iteration: 60900 loss: 0.0042 lr: 0.02\n",
      "iteration: 60910 loss: 0.0037 lr: 0.02\n",
      "iteration: 60920 loss: 0.0043 lr: 0.02\n",
      "iteration: 60930 loss: 0.0067 lr: 0.02\n",
      "iteration: 60940 loss: 0.0059 lr: 0.02\n",
      "iteration: 60950 loss: 0.0043 lr: 0.02\n",
      "iteration: 60960 loss: 0.0041 lr: 0.02\n",
      "iteration: 60970 loss: 0.0049 lr: 0.02\n",
      "iteration: 60980 loss: 0.0037 lr: 0.02\n",
      "iteration: 60990 loss: 0.0040 lr: 0.02\n",
      "iteration: 61000 loss: 0.0050 lr: 0.02\n",
      "iteration: 61010 loss: 0.0063 lr: 0.02\n",
      "iteration: 61020 loss: 0.0037 lr: 0.02\n",
      "iteration: 61030 loss: 0.0051 lr: 0.02\n",
      "iteration: 61040 loss: 0.0034 lr: 0.02\n",
      "iteration: 61050 loss: 0.0057 lr: 0.02\n",
      "iteration: 61060 loss: 0.0047 lr: 0.02\n",
      "iteration: 61070 loss: 0.0048 lr: 0.02\n",
      "iteration: 61080 loss: 0.0034 lr: 0.02\n",
      "iteration: 61090 loss: 0.0032 lr: 0.02\n",
      "iteration: 61100 loss: 0.0059 lr: 0.02\n",
      "iteration: 61110 loss: 0.0045 lr: 0.02\n",
      "iteration: 61120 loss: 0.0050 lr: 0.02\n",
      "iteration: 61130 loss: 0.0046 lr: 0.02\n",
      "iteration: 61140 loss: 0.0039 lr: 0.02\n",
      "iteration: 61150 loss: 0.0042 lr: 0.02\n",
      "iteration: 61160 loss: 0.0046 lr: 0.02\n",
      "iteration: 61170 loss: 0.0043 lr: 0.02\n",
      "iteration: 61180 loss: 0.0068 lr: 0.02\n",
      "iteration: 61190 loss: 0.0035 lr: 0.02\n",
      "iteration: 61200 loss: 0.0052 lr: 0.02\n",
      "iteration: 61210 loss: 0.0048 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 61220 loss: 0.0065 lr: 0.02\n",
      "iteration: 61230 loss: 0.0039 lr: 0.02\n",
      "iteration: 61240 loss: 0.0045 lr: 0.02\n",
      "iteration: 61250 loss: 0.0043 lr: 0.02\n",
      "iteration: 61260 loss: 0.0048 lr: 0.02\n",
      "iteration: 61270 loss: 0.0056 lr: 0.02\n",
      "iteration: 61280 loss: 0.0049 lr: 0.02\n",
      "iteration: 61290 loss: 0.0059 lr: 0.02\n",
      "iteration: 61300 loss: 0.0055 lr: 0.02\n",
      "iteration: 61310 loss: 0.0071 lr: 0.02\n",
      "iteration: 61320 loss: 0.0058 lr: 0.02\n",
      "iteration: 61330 loss: 0.0048 lr: 0.02\n",
      "iteration: 61340 loss: 0.0062 lr: 0.02\n",
      "iteration: 61350 loss: 0.0042 lr: 0.02\n",
      "iteration: 61360 loss: 0.0037 lr: 0.02\n",
      "iteration: 61370 loss: 0.0039 lr: 0.02\n",
      "iteration: 61380 loss: 0.0044 lr: 0.02\n",
      "iteration: 61390 loss: 0.0050 lr: 0.02\n",
      "iteration: 61400 loss: 0.0037 lr: 0.02\n",
      "iteration: 61410 loss: 0.0055 lr: 0.02\n",
      "iteration: 61420 loss: 0.0066 lr: 0.02\n",
      "iteration: 61430 loss: 0.0042 lr: 0.02\n",
      "iteration: 61440 loss: 0.0039 lr: 0.02\n",
      "iteration: 61450 loss: 0.0045 lr: 0.02\n",
      "iteration: 61460 loss: 0.0035 lr: 0.02\n",
      "iteration: 61470 loss: 0.0034 lr: 0.02\n",
      "iteration: 61480 loss: 0.0033 lr: 0.02\n",
      "iteration: 61490 loss: 0.0038 lr: 0.02\n",
      "iteration: 61500 loss: 0.0038 lr: 0.02\n",
      "iteration: 61510 loss: 0.0031 lr: 0.02\n",
      "iteration: 61520 loss: 0.0048 lr: 0.02\n",
      "iteration: 61530 loss: 0.0036 lr: 0.02\n",
      "iteration: 61540 loss: 0.0047 lr: 0.02\n",
      "iteration: 61550 loss: 0.0037 lr: 0.02\n",
      "iteration: 61560 loss: 0.0063 lr: 0.02\n",
      "iteration: 61570 loss: 0.0061 lr: 0.02\n",
      "iteration: 61580 loss: 0.0043 lr: 0.02\n",
      "iteration: 61590 loss: 0.0063 lr: 0.02\n",
      "iteration: 61600 loss: 0.0063 lr: 0.02\n",
      "iteration: 61610 loss: 0.0051 lr: 0.02\n",
      "iteration: 61620 loss: 0.0042 lr: 0.02\n",
      "iteration: 61630 loss: 0.0044 lr: 0.02\n",
      "iteration: 61640 loss: 0.0044 lr: 0.02\n",
      "iteration: 61650 loss: 0.0030 lr: 0.02\n",
      "iteration: 61660 loss: 0.0029 lr: 0.02\n",
      "iteration: 61670 loss: 0.0032 lr: 0.02\n",
      "iteration: 61680 loss: 0.0052 lr: 0.02\n",
      "iteration: 61690 loss: 0.0039 lr: 0.02\n",
      "iteration: 61700 loss: 0.0049 lr: 0.02\n",
      "iteration: 61710 loss: 0.0046 lr: 0.02\n",
      "iteration: 61720 loss: 0.0049 lr: 0.02\n",
      "iteration: 61730 loss: 0.0039 lr: 0.02\n",
      "iteration: 61740 loss: 0.0042 lr: 0.02\n",
      "iteration: 61750 loss: 0.0043 lr: 0.02\n",
      "iteration: 61760 loss: 0.0035 lr: 0.02\n",
      "iteration: 61770 loss: 0.0035 lr: 0.02\n",
      "iteration: 61780 loss: 0.0046 lr: 0.02\n",
      "iteration: 61790 loss: 0.0052 lr: 0.02\n",
      "iteration: 61800 loss: 0.0051 lr: 0.02\n",
      "iteration: 61810 loss: 0.0039 lr: 0.02\n",
      "iteration: 61820 loss: 0.0059 lr: 0.02\n",
      "iteration: 61830 loss: 0.0051 lr: 0.02\n",
      "iteration: 61840 loss: 0.0050 lr: 0.02\n",
      "iteration: 61850 loss: 0.0035 lr: 0.02\n",
      "iteration: 61860 loss: 0.0049 lr: 0.02\n",
      "iteration: 61870 loss: 0.0045 lr: 0.02\n",
      "iteration: 61880 loss: 0.0036 lr: 0.02\n",
      "iteration: 61890 loss: 0.0036 lr: 0.02\n",
      "iteration: 61900 loss: 0.0042 lr: 0.02\n",
      "iteration: 61910 loss: 0.0036 lr: 0.02\n",
      "iteration: 61920 loss: 0.0059 lr: 0.02\n",
      "iteration: 61930 loss: 0.0040 lr: 0.02\n",
      "iteration: 61940 loss: 0.0036 lr: 0.02\n",
      "iteration: 61950 loss: 0.0035 lr: 0.02\n",
      "iteration: 61960 loss: 0.0028 lr: 0.02\n",
      "iteration: 61970 loss: 0.0046 lr: 0.02\n",
      "iteration: 61980 loss: 0.0041 lr: 0.02\n",
      "iteration: 61990 loss: 0.0041 lr: 0.02\n",
      "iteration: 62000 loss: 0.0043 lr: 0.02\n",
      "iteration: 62010 loss: 0.0035 lr: 0.02\n",
      "iteration: 62020 loss: 0.0033 lr: 0.02\n",
      "iteration: 62030 loss: 0.0037 lr: 0.02\n",
      "iteration: 62040 loss: 0.0057 lr: 0.02\n",
      "iteration: 62050 loss: 0.0039 lr: 0.02\n",
      "iteration: 62060 loss: 0.0037 lr: 0.02\n",
      "iteration: 62070 loss: 0.0035 lr: 0.02\n",
      "iteration: 62080 loss: 0.0035 lr: 0.02\n",
      "iteration: 62090 loss: 0.0075 lr: 0.02\n",
      "iteration: 62100 loss: 0.0039 lr: 0.02\n",
      "iteration: 62110 loss: 0.0035 lr: 0.02\n",
      "iteration: 62120 loss: 0.0034 lr: 0.02\n",
      "iteration: 62130 loss: 0.0035 lr: 0.02\n",
      "iteration: 62140 loss: 0.0056 lr: 0.02\n",
      "iteration: 62150 loss: 0.0046 lr: 0.02\n",
      "iteration: 62160 loss: 0.0042 lr: 0.02\n",
      "iteration: 62170 loss: 0.0040 lr: 0.02\n",
      "iteration: 62180 loss: 0.0040 lr: 0.02\n",
      "iteration: 62190 loss: 0.0045 lr: 0.02\n",
      "iteration: 62200 loss: 0.0057 lr: 0.02\n",
      "iteration: 62210 loss: 0.0048 lr: 0.02\n",
      "iteration: 62220 loss: 0.0031 lr: 0.02\n",
      "iteration: 62230 loss: 0.0052 lr: 0.02\n",
      "iteration: 62240 loss: 0.0048 lr: 0.02\n",
      "iteration: 62250 loss: 0.0035 lr: 0.02\n",
      "iteration: 62260 loss: 0.0033 lr: 0.02\n",
      "iteration: 62270 loss: 0.0036 lr: 0.02\n",
      "iteration: 62280 loss: 0.0043 lr: 0.02\n",
      "iteration: 62290 loss: 0.0056 lr: 0.02\n",
      "iteration: 62300 loss: 0.0047 lr: 0.02\n",
      "iteration: 62310 loss: 0.0046 lr: 0.02\n",
      "iteration: 62320 loss: 0.0041 lr: 0.02\n",
      "iteration: 62330 loss: 0.0031 lr: 0.02\n",
      "iteration: 62340 loss: 0.0036 lr: 0.02\n",
      "iteration: 62350 loss: 0.0029 lr: 0.02\n",
      "iteration: 62360 loss: 0.0046 lr: 0.02\n",
      "iteration: 62370 loss: 0.0038 lr: 0.02\n",
      "iteration: 62380 loss: 0.0041 lr: 0.02\n",
      "iteration: 62390 loss: 0.0030 lr: 0.02\n",
      "iteration: 62400 loss: 0.0058 lr: 0.02\n",
      "iteration: 62410 loss: 0.0066 lr: 0.02\n",
      "iteration: 62420 loss: 0.0042 lr: 0.02\n",
      "iteration: 62430 loss: 0.0040 lr: 0.02\n",
      "iteration: 62440 loss: 0.0047 lr: 0.02\n",
      "iteration: 62450 loss: 0.0034 lr: 0.02\n",
      "iteration: 62460 loss: 0.0033 lr: 0.02\n",
      "iteration: 62470 loss: 0.0054 lr: 0.02\n",
      "iteration: 62480 loss: 0.0056 lr: 0.02\n",
      "iteration: 62490 loss: 0.0058 lr: 0.02\n",
      "iteration: 62500 loss: 0.0059 lr: 0.02\n",
      "iteration: 62510 loss: 0.0034 lr: 0.02\n",
      "iteration: 62520 loss: 0.0045 lr: 0.02\n",
      "iteration: 62530 loss: 0.0027 lr: 0.02\n",
      "iteration: 62540 loss: 0.0035 lr: 0.02\n",
      "iteration: 62550 loss: 0.0051 lr: 0.02\n",
      "iteration: 62560 loss: 0.0041 lr: 0.02\n",
      "iteration: 62570 loss: 0.0030 lr: 0.02\n",
      "iteration: 62580 loss: 0.0042 lr: 0.02\n",
      "iteration: 62590 loss: 0.0052 lr: 0.02\n",
      "iteration: 62600 loss: 0.0044 lr: 0.02\n",
      "iteration: 62610 loss: 0.0069 lr: 0.02\n",
      "iteration: 62620 loss: 0.0060 lr: 0.02\n",
      "iteration: 62630 loss: 0.0034 lr: 0.02\n",
      "iteration: 62640 loss: 0.0055 lr: 0.02\n",
      "iteration: 62650 loss: 0.0046 lr: 0.02\n",
      "iteration: 62660 loss: 0.0048 lr: 0.02\n",
      "iteration: 62670 loss: 0.0032 lr: 0.02\n",
      "iteration: 62680 loss: 0.0042 lr: 0.02\n",
      "iteration: 62690 loss: 0.0049 lr: 0.02\n",
      "iteration: 62700 loss: 0.0033 lr: 0.02\n",
      "iteration: 62710 loss: 0.0032 lr: 0.02\n",
      "iteration: 62720 loss: 0.0040 lr: 0.02\n",
      "iteration: 62730 loss: 0.0044 lr: 0.02\n",
      "iteration: 62740 loss: 0.0043 lr: 0.02\n",
      "iteration: 62750 loss: 0.0050 lr: 0.02\n",
      "iteration: 62760 loss: 0.0053 lr: 0.02\n",
      "iteration: 62770 loss: 0.0037 lr: 0.02\n",
      "iteration: 62780 loss: 0.0038 lr: 0.02\n",
      "iteration: 62790 loss: 0.0046 lr: 0.02\n",
      "iteration: 62800 loss: 0.0034 lr: 0.02\n",
      "iteration: 62810 loss: 0.0050 lr: 0.02\n",
      "iteration: 62820 loss: 0.0034 lr: 0.02\n",
      "iteration: 62830 loss: 0.0055 lr: 0.02\n",
      "iteration: 62840 loss: 0.0039 lr: 0.02\n",
      "iteration: 62850 loss: 0.0047 lr: 0.02\n",
      "iteration: 62860 loss: 0.0037 lr: 0.02\n",
      "iteration: 62870 loss: 0.0036 lr: 0.02\n",
      "iteration: 62880 loss: 0.0045 lr: 0.02\n",
      "iteration: 62890 loss: 0.0052 lr: 0.02\n",
      "iteration: 62900 loss: 0.0048 lr: 0.02\n",
      "iteration: 62910 loss: 0.0059 lr: 0.02\n",
      "iteration: 62920 loss: 0.0040 lr: 0.02\n",
      "iteration: 62930 loss: 0.0043 lr: 0.02\n",
      "iteration: 62940 loss: 0.0039 lr: 0.02\n",
      "iteration: 62950 loss: 0.0048 lr: 0.02\n",
      "iteration: 62960 loss: 0.0041 lr: 0.02\n",
      "iteration: 62970 loss: 0.0053 lr: 0.02\n",
      "iteration: 62980 loss: 0.0040 lr: 0.02\n",
      "iteration: 62990 loss: 0.0041 lr: 0.02\n",
      "iteration: 63000 loss: 0.0044 lr: 0.02\n",
      "iteration: 63010 loss: 0.0042 lr: 0.02\n",
      "iteration: 63020 loss: 0.0037 lr: 0.02\n",
      "iteration: 63030 loss: 0.0044 lr: 0.02\n",
      "iteration: 63040 loss: 0.0039 lr: 0.02\n",
      "iteration: 63050 loss: 0.0036 lr: 0.02\n",
      "iteration: 63060 loss: 0.0066 lr: 0.02\n",
      "iteration: 63070 loss: 0.0045 lr: 0.02\n",
      "iteration: 63080 loss: 0.0060 lr: 0.02\n",
      "iteration: 63090 loss: 0.0047 lr: 0.02\n",
      "iteration: 63100 loss: 0.0044 lr: 0.02\n",
      "iteration: 63110 loss: 0.0062 lr: 0.02\n",
      "iteration: 63120 loss: 0.0040 lr: 0.02\n",
      "iteration: 63130 loss: 0.0057 lr: 0.02\n",
      "iteration: 63140 loss: 0.0040 lr: 0.02\n",
      "iteration: 63150 loss: 0.0049 lr: 0.02\n",
      "iteration: 63160 loss: 0.0032 lr: 0.02\n",
      "iteration: 63170 loss: 0.0039 lr: 0.02\n",
      "iteration: 63180 loss: 0.0047 lr: 0.02\n",
      "iteration: 63190 loss: 0.0043 lr: 0.02\n",
      "iteration: 63200 loss: 0.0063 lr: 0.02\n",
      "iteration: 63210 loss: 0.0045 lr: 0.02\n",
      "iteration: 63220 loss: 0.0039 lr: 0.02\n",
      "iteration: 63230 loss: 0.0034 lr: 0.02\n",
      "iteration: 63240 loss: 0.0065 lr: 0.02\n",
      "iteration: 63250 loss: 0.0068 lr: 0.02\n",
      "iteration: 63260 loss: 0.0039 lr: 0.02\n",
      "iteration: 63270 loss: 0.0044 lr: 0.02\n",
      "iteration: 63280 loss: 0.0034 lr: 0.02\n",
      "iteration: 63290 loss: 0.0037 lr: 0.02\n",
      "iteration: 63300 loss: 0.0035 lr: 0.02\n",
      "iteration: 63310 loss: 0.0043 lr: 0.02\n",
      "iteration: 63320 loss: 0.0079 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 63330 loss: 0.0038 lr: 0.02\n",
      "iteration: 63340 loss: 0.0045 lr: 0.02\n",
      "iteration: 63350 loss: 0.0039 lr: 0.02\n",
      "iteration: 63360 loss: 0.0033 lr: 0.02\n",
      "iteration: 63370 loss: 0.0048 lr: 0.02\n",
      "iteration: 63380 loss: 0.0038 lr: 0.02\n",
      "iteration: 63390 loss: 0.0049 lr: 0.02\n",
      "iteration: 63400 loss: 0.0059 lr: 0.02\n",
      "iteration: 63410 loss: 0.0053 lr: 0.02\n",
      "iteration: 63420 loss: 0.0050 lr: 0.02\n",
      "iteration: 63430 loss: 0.0040 lr: 0.02\n",
      "iteration: 63440 loss: 0.0040 lr: 0.02\n",
      "iteration: 63450 loss: 0.0036 lr: 0.02\n",
      "iteration: 63460 loss: 0.0051 lr: 0.02\n",
      "iteration: 63470 loss: 0.0042 lr: 0.02\n",
      "iteration: 63480 loss: 0.0068 lr: 0.02\n",
      "iteration: 63490 loss: 0.0056 lr: 0.02\n",
      "iteration: 63500 loss: 0.0044 lr: 0.02\n",
      "iteration: 63510 loss: 0.0051 lr: 0.02\n",
      "iteration: 63520 loss: 0.0060 lr: 0.02\n",
      "iteration: 63530 loss: 0.0068 lr: 0.02\n",
      "iteration: 63540 loss: 0.0049 lr: 0.02\n",
      "iteration: 63550 loss: 0.0048 lr: 0.02\n",
      "iteration: 63560 loss: 0.0039 lr: 0.02\n",
      "iteration: 63570 loss: 0.0047 lr: 0.02\n",
      "iteration: 63580 loss: 0.0042 lr: 0.02\n",
      "iteration: 63590 loss: 0.0039 lr: 0.02\n",
      "iteration: 63600 loss: 0.0040 lr: 0.02\n",
      "iteration: 63610 loss: 0.0044 lr: 0.02\n",
      "iteration: 63620 loss: 0.0040 lr: 0.02\n",
      "iteration: 63630 loss: 0.0046 lr: 0.02\n",
      "iteration: 63640 loss: 0.0045 lr: 0.02\n",
      "iteration: 63650 loss: 0.0058 lr: 0.02\n",
      "iteration: 63660 loss: 0.0039 lr: 0.02\n",
      "iteration: 63670 loss: 0.0040 lr: 0.02\n",
      "iteration: 63680 loss: 0.0047 lr: 0.02\n",
      "iteration: 63690 loss: 0.0058 lr: 0.02\n",
      "iteration: 63700 loss: 0.0038 lr: 0.02\n",
      "iteration: 63710 loss: 0.0058 lr: 0.02\n",
      "iteration: 63720 loss: 0.0044 lr: 0.02\n",
      "iteration: 63730 loss: 0.0037 lr: 0.02\n",
      "iteration: 63740 loss: 0.0054 lr: 0.02\n",
      "iteration: 63750 loss: 0.0036 lr: 0.02\n",
      "iteration: 63760 loss: 0.0048 lr: 0.02\n",
      "iteration: 63770 loss: 0.0043 lr: 0.02\n",
      "iteration: 63780 loss: 0.0040 lr: 0.02\n",
      "iteration: 63790 loss: 0.0044 lr: 0.02\n",
      "iteration: 63800 loss: 0.0046 lr: 0.02\n",
      "iteration: 63810 loss: 0.0046 lr: 0.02\n",
      "iteration: 63820 loss: 0.0059 lr: 0.02\n",
      "iteration: 63830 loss: 0.0051 lr: 0.02\n",
      "iteration: 63840 loss: 0.0037 lr: 0.02\n",
      "iteration: 63850 loss: 0.0044 lr: 0.02\n",
      "iteration: 63860 loss: 0.0053 lr: 0.02\n",
      "iteration: 63870 loss: 0.0037 lr: 0.02\n",
      "iteration: 63880 loss: 0.0040 lr: 0.02\n",
      "iteration: 63890 loss: 0.0026 lr: 0.02\n",
      "iteration: 63900 loss: 0.0061 lr: 0.02\n",
      "iteration: 63910 loss: 0.0053 lr: 0.02\n",
      "iteration: 63920 loss: 0.0047 lr: 0.02\n",
      "iteration: 63930 loss: 0.0046 lr: 0.02\n",
      "iteration: 63940 loss: 0.0037 lr: 0.02\n",
      "iteration: 63950 loss: 0.0047 lr: 0.02\n",
      "iteration: 63960 loss: 0.0026 lr: 0.02\n",
      "iteration: 63970 loss: 0.0052 lr: 0.02\n",
      "iteration: 63980 loss: 0.0036 lr: 0.02\n",
      "iteration: 63990 loss: 0.0051 lr: 0.02\n",
      "iteration: 64000 loss: 0.0056 lr: 0.02\n",
      "iteration: 64010 loss: 0.0035 lr: 0.02\n",
      "iteration: 64020 loss: 0.0048 lr: 0.02\n",
      "iteration: 64030 loss: 0.0086 lr: 0.02\n",
      "iteration: 64040 loss: 0.0061 lr: 0.02\n",
      "iteration: 64050 loss: 0.0062 lr: 0.02\n",
      "iteration: 64060 loss: 0.0037 lr: 0.02\n",
      "iteration: 64070 loss: 0.0054 lr: 0.02\n",
      "iteration: 64080 loss: 0.0038 lr: 0.02\n",
      "iteration: 64090 loss: 0.0071 lr: 0.02\n",
      "iteration: 64100 loss: 0.0034 lr: 0.02\n",
      "iteration: 64110 loss: 0.0064 lr: 0.02\n",
      "iteration: 64120 loss: 0.0047 lr: 0.02\n",
      "iteration: 64130 loss: 0.0063 lr: 0.02\n",
      "iteration: 64140 loss: 0.0047 lr: 0.02\n",
      "iteration: 64150 loss: 0.0049 lr: 0.02\n",
      "iteration: 64160 loss: 0.0063 lr: 0.02\n",
      "iteration: 64170 loss: 0.0048 lr: 0.02\n",
      "iteration: 64180 loss: 0.0043 lr: 0.02\n",
      "iteration: 64190 loss: 0.0044 lr: 0.02\n",
      "iteration: 64200 loss: 0.0034 lr: 0.02\n",
      "iteration: 64210 loss: 0.0027 lr: 0.02\n",
      "iteration: 64220 loss: 0.0043 lr: 0.02\n",
      "iteration: 64230 loss: 0.0043 lr: 0.02\n",
      "iteration: 64240 loss: 0.0041 lr: 0.02\n",
      "iteration: 64250 loss: 0.0035 lr: 0.02\n",
      "iteration: 64260 loss: 0.0046 lr: 0.02\n",
      "iteration: 64270 loss: 0.0045 lr: 0.02\n",
      "iteration: 64280 loss: 0.0040 lr: 0.02\n",
      "iteration: 64290 loss: 0.0055 lr: 0.02\n",
      "iteration: 64300 loss: 0.0059 lr: 0.02\n",
      "iteration: 64310 loss: 0.0054 lr: 0.02\n",
      "iteration: 64320 loss: 0.0060 lr: 0.02\n",
      "iteration: 64330 loss: 0.0036 lr: 0.02\n",
      "iteration: 64340 loss: 0.0044 lr: 0.02\n",
      "iteration: 64350 loss: 0.0044 lr: 0.02\n",
      "iteration: 64360 loss: 0.0048 lr: 0.02\n",
      "iteration: 64370 loss: 0.0041 lr: 0.02\n",
      "iteration: 64380 loss: 0.0034 lr: 0.02\n",
      "iteration: 64390 loss: 0.0049 lr: 0.02\n",
      "iteration: 64400 loss: 0.0047 lr: 0.02\n",
      "iteration: 64410 loss: 0.0048 lr: 0.02\n",
      "iteration: 64420 loss: 0.0045 lr: 0.02\n",
      "iteration: 64430 loss: 0.0047 lr: 0.02\n",
      "iteration: 64440 loss: 0.0035 lr: 0.02\n",
      "iteration: 64450 loss: 0.0056 lr: 0.02\n",
      "iteration: 64460 loss: 0.0045 lr: 0.02\n",
      "iteration: 64470 loss: 0.0058 lr: 0.02\n",
      "iteration: 64480 loss: 0.0040 lr: 0.02\n",
      "iteration: 64490 loss: 0.0043 lr: 0.02\n",
      "iteration: 64500 loss: 0.0035 lr: 0.02\n",
      "iteration: 64510 loss: 0.0038 lr: 0.02\n",
      "iteration: 64520 loss: 0.0048 lr: 0.02\n",
      "iteration: 64530 loss: 0.0041 lr: 0.02\n",
      "iteration: 64540 loss: 0.0033 lr: 0.02\n",
      "iteration: 64550 loss: 0.0041 lr: 0.02\n",
      "iteration: 64560 loss: 0.0042 lr: 0.02\n",
      "iteration: 64570 loss: 0.0028 lr: 0.02\n",
      "iteration: 64580 loss: 0.0031 lr: 0.02\n",
      "iteration: 64590 loss: 0.0040 lr: 0.02\n",
      "iteration: 64600 loss: 0.0038 lr: 0.02\n",
      "iteration: 64610 loss: 0.0034 lr: 0.02\n",
      "iteration: 64620 loss: 0.0036 lr: 0.02\n",
      "iteration: 64630 loss: 0.0034 lr: 0.02\n",
      "iteration: 64640 loss: 0.0047 lr: 0.02\n",
      "iteration: 64650 loss: 0.0037 lr: 0.02\n",
      "iteration: 64660 loss: 0.0057 lr: 0.02\n",
      "iteration: 64670 loss: 0.0051 lr: 0.02\n",
      "iteration: 64680 loss: 0.0054 lr: 0.02\n",
      "iteration: 64690 loss: 0.0061 lr: 0.02\n",
      "iteration: 64700 loss: 0.0044 lr: 0.02\n",
      "iteration: 64710 loss: 0.0045 lr: 0.02\n",
      "iteration: 64720 loss: 0.0054 lr: 0.02\n",
      "iteration: 64730 loss: 0.0041 lr: 0.02\n",
      "iteration: 64740 loss: 0.0055 lr: 0.02\n",
      "iteration: 64750 loss: 0.0047 lr: 0.02\n",
      "iteration: 64760 loss: 0.0039 lr: 0.02\n",
      "iteration: 64770 loss: 0.0049 lr: 0.02\n",
      "iteration: 64780 loss: 0.0051 lr: 0.02\n",
      "iteration: 64790 loss: 0.0043 lr: 0.02\n",
      "iteration: 64800 loss: 0.0052 lr: 0.02\n",
      "iteration: 64810 loss: 0.0039 lr: 0.02\n",
      "iteration: 64820 loss: 0.0047 lr: 0.02\n",
      "iteration: 64830 loss: 0.0033 lr: 0.02\n",
      "iteration: 64840 loss: 0.0035 lr: 0.02\n",
      "iteration: 64850 loss: 0.0047 lr: 0.02\n",
      "iteration: 64860 loss: 0.0045 lr: 0.02\n",
      "iteration: 64870 loss: 0.0041 lr: 0.02\n",
      "iteration: 64880 loss: 0.0045 lr: 0.02\n",
      "iteration: 64890 loss: 0.0040 lr: 0.02\n",
      "iteration: 64900 loss: 0.0049 lr: 0.02\n",
      "iteration: 64910 loss: 0.0051 lr: 0.02\n",
      "iteration: 64920 loss: 0.0044 lr: 0.02\n",
      "iteration: 64930 loss: 0.0036 lr: 0.02\n",
      "iteration: 64940 loss: 0.0042 lr: 0.02\n",
      "iteration: 64950 loss: 0.0040 lr: 0.02\n",
      "iteration: 64960 loss: 0.0043 lr: 0.02\n",
      "iteration: 64970 loss: 0.0043 lr: 0.02\n",
      "iteration: 64980 loss: 0.0049 lr: 0.02\n",
      "iteration: 64990 loss: 0.0046 lr: 0.02\n",
      "iteration: 65000 loss: 0.0045 lr: 0.02\n",
      "iteration: 65010 loss: 0.0027 lr: 0.02\n",
      "iteration: 65020 loss: 0.0034 lr: 0.02\n",
      "iteration: 65030 loss: 0.0036 lr: 0.02\n",
      "iteration: 65040 loss: 0.0042 lr: 0.02\n",
      "iteration: 65050 loss: 0.0042 lr: 0.02\n",
      "iteration: 65060 loss: 0.0048 lr: 0.02\n",
      "iteration: 65070 loss: 0.0048 lr: 0.02\n",
      "iteration: 65080 loss: 0.0047 lr: 0.02\n",
      "iteration: 65090 loss: 0.0055 lr: 0.02\n",
      "iteration: 65100 loss: 0.0048 lr: 0.02\n",
      "iteration: 65110 loss: 0.0037 lr: 0.02\n",
      "iteration: 65120 loss: 0.0055 lr: 0.02\n",
      "iteration: 65130 loss: 0.0049 lr: 0.02\n",
      "iteration: 65140 loss: 0.0058 lr: 0.02\n",
      "iteration: 65150 loss: 0.0047 lr: 0.02\n",
      "iteration: 65160 loss: 0.0037 lr: 0.02\n",
      "iteration: 65170 loss: 0.0037 lr: 0.02\n",
      "iteration: 65180 loss: 0.0037 lr: 0.02\n",
      "iteration: 65190 loss: 0.0047 lr: 0.02\n",
      "iteration: 65200 loss: 0.0054 lr: 0.02\n",
      "iteration: 65210 loss: 0.0034 lr: 0.02\n",
      "iteration: 65220 loss: 0.0036 lr: 0.02\n",
      "iteration: 65230 loss: 0.0039 lr: 0.02\n",
      "iteration: 65240 loss: 0.0042 lr: 0.02\n",
      "iteration: 65250 loss: 0.0051 lr: 0.02\n",
      "iteration: 65260 loss: 0.0050 lr: 0.02\n",
      "iteration: 65270 loss: 0.0055 lr: 0.02\n",
      "iteration: 65280 loss: 0.0068 lr: 0.02\n",
      "iteration: 65290 loss: 0.0037 lr: 0.02\n",
      "iteration: 65300 loss: 0.0047 lr: 0.02\n",
      "iteration: 65310 loss: 0.0052 lr: 0.02\n",
      "iteration: 65320 loss: 0.0048 lr: 0.02\n",
      "iteration: 65330 loss: 0.0044 lr: 0.02\n",
      "iteration: 65340 loss: 0.0045 lr: 0.02\n",
      "iteration: 65350 loss: 0.0055 lr: 0.02\n",
      "iteration: 65360 loss: 0.0050 lr: 0.02\n",
      "iteration: 65370 loss: 0.0043 lr: 0.02\n",
      "iteration: 65380 loss: 0.0036 lr: 0.02\n",
      "iteration: 65390 loss: 0.0055 lr: 0.02\n",
      "iteration: 65400 loss: 0.0044 lr: 0.02\n",
      "iteration: 65410 loss: 0.0044 lr: 0.02\n",
      "iteration: 65420 loss: 0.0031 lr: 0.02\n",
      "iteration: 65430 loss: 0.0070 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 65440 loss: 0.0057 lr: 0.02\n",
      "iteration: 65450 loss: 0.0038 lr: 0.02\n",
      "iteration: 65460 loss: 0.0057 lr: 0.02\n",
      "iteration: 65470 loss: 0.0047 lr: 0.02\n",
      "iteration: 65480 loss: 0.0034 lr: 0.02\n",
      "iteration: 65490 loss: 0.0041 lr: 0.02\n",
      "iteration: 65500 loss: 0.0047 lr: 0.02\n",
      "iteration: 65510 loss: 0.0040 lr: 0.02\n",
      "iteration: 65520 loss: 0.0035 lr: 0.02\n",
      "iteration: 65530 loss: 0.0051 lr: 0.02\n",
      "iteration: 65540 loss: 0.0038 lr: 0.02\n",
      "iteration: 65550 loss: 0.0037 lr: 0.02\n",
      "iteration: 65560 loss: 0.0035 lr: 0.02\n",
      "iteration: 65570 loss: 0.0034 lr: 0.02\n",
      "iteration: 65580 loss: 0.0058 lr: 0.02\n",
      "iteration: 65590 loss: 0.0036 lr: 0.02\n",
      "iteration: 65600 loss: 0.0047 lr: 0.02\n",
      "iteration: 65610 loss: 0.0069 lr: 0.02\n",
      "iteration: 65620 loss: 0.0046 lr: 0.02\n",
      "iteration: 65630 loss: 0.0034 lr: 0.02\n",
      "iteration: 65640 loss: 0.0068 lr: 0.02\n",
      "iteration: 65650 loss: 0.0035 lr: 0.02\n",
      "iteration: 65660 loss: 0.0051 lr: 0.02\n",
      "iteration: 65670 loss: 0.0049 lr: 0.02\n",
      "iteration: 65680 loss: 0.0077 lr: 0.02\n",
      "iteration: 65690 loss: 0.0047 lr: 0.02\n",
      "iteration: 65700 loss: 0.0031 lr: 0.02\n",
      "iteration: 65710 loss: 0.0043 lr: 0.02\n",
      "iteration: 65720 loss: 0.0058 lr: 0.02\n",
      "iteration: 65730 loss: 0.0039 lr: 0.02\n",
      "iteration: 65740 loss: 0.0042 lr: 0.02\n",
      "iteration: 65750 loss: 0.0053 lr: 0.02\n",
      "iteration: 65760 loss: 0.0037 lr: 0.02\n",
      "iteration: 65770 loss: 0.0039 lr: 0.02\n",
      "iteration: 65780 loss: 0.0043 lr: 0.02\n",
      "iteration: 65790 loss: 0.0044 lr: 0.02\n",
      "iteration: 65800 loss: 0.0046 lr: 0.02\n",
      "iteration: 65810 loss: 0.0038 lr: 0.02\n",
      "iteration: 65820 loss: 0.0054 lr: 0.02\n",
      "iteration: 65830 loss: 0.0043 lr: 0.02\n",
      "iteration: 65840 loss: 0.0045 lr: 0.02\n",
      "iteration: 65850 loss: 0.0038 lr: 0.02\n",
      "iteration: 65860 loss: 0.0038 lr: 0.02\n",
      "iteration: 65870 loss: 0.0047 lr: 0.02\n",
      "iteration: 65880 loss: 0.0044 lr: 0.02\n",
      "iteration: 65890 loss: 0.0044 lr: 0.02\n",
      "iteration: 65900 loss: 0.0035 lr: 0.02\n",
      "iteration: 65910 loss: 0.0060 lr: 0.02\n",
      "iteration: 65920 loss: 0.0043 lr: 0.02\n",
      "iteration: 65930 loss: 0.0053 lr: 0.02\n",
      "iteration: 65940 loss: 0.0034 lr: 0.02\n",
      "iteration: 65950 loss: 0.0049 lr: 0.02\n",
      "iteration: 65960 loss: 0.0033 lr: 0.02\n",
      "iteration: 65970 loss: 0.0049 lr: 0.02\n",
      "iteration: 65980 loss: 0.0050 lr: 0.02\n",
      "iteration: 65990 loss: 0.0046 lr: 0.02\n",
      "iteration: 66000 loss: 0.0048 lr: 0.02\n",
      "iteration: 66010 loss: 0.0040 lr: 0.02\n",
      "iteration: 66020 loss: 0.0047 lr: 0.02\n",
      "iteration: 66030 loss: 0.0034 lr: 0.02\n",
      "iteration: 66040 loss: 0.0044 lr: 0.02\n",
      "iteration: 66050 loss: 0.0046 lr: 0.02\n",
      "iteration: 66060 loss: 0.0043 lr: 0.02\n",
      "iteration: 66070 loss: 0.0040 lr: 0.02\n",
      "iteration: 66080 loss: 0.0047 lr: 0.02\n",
      "iteration: 66090 loss: 0.0057 lr: 0.02\n",
      "iteration: 66100 loss: 0.0044 lr: 0.02\n",
      "iteration: 66110 loss: 0.0039 lr: 0.02\n",
      "iteration: 66120 loss: 0.0040 lr: 0.02\n",
      "iteration: 66130 loss: 0.0038 lr: 0.02\n",
      "iteration: 66140 loss: 0.0034 lr: 0.02\n",
      "iteration: 66150 loss: 0.0048 lr: 0.02\n",
      "iteration: 66160 loss: 0.0027 lr: 0.02\n",
      "iteration: 66170 loss: 0.0045 lr: 0.02\n",
      "iteration: 66180 loss: 0.0050 lr: 0.02\n",
      "iteration: 66190 loss: 0.0042 lr: 0.02\n",
      "iteration: 66200 loss: 0.0047 lr: 0.02\n",
      "iteration: 66210 loss: 0.0031 lr: 0.02\n",
      "iteration: 66220 loss: 0.0055 lr: 0.02\n",
      "iteration: 66230 loss: 0.0031 lr: 0.02\n",
      "iteration: 66240 loss: 0.0037 lr: 0.02\n",
      "iteration: 66250 loss: 0.0039 lr: 0.02\n",
      "iteration: 66260 loss: 0.0047 lr: 0.02\n",
      "iteration: 66270 loss: 0.0043 lr: 0.02\n",
      "iteration: 66280 loss: 0.0059 lr: 0.02\n",
      "iteration: 66290 loss: 0.0044 lr: 0.02\n",
      "iteration: 66300 loss: 0.0050 lr: 0.02\n",
      "iteration: 66310 loss: 0.0040 lr: 0.02\n",
      "iteration: 66320 loss: 0.0041 lr: 0.02\n",
      "iteration: 66330 loss: 0.0041 lr: 0.02\n",
      "iteration: 66340 loss: 0.0046 lr: 0.02\n",
      "iteration: 66350 loss: 0.0028 lr: 0.02\n",
      "iteration: 66360 loss: 0.0044 lr: 0.02\n",
      "iteration: 66370 loss: 0.0031 lr: 0.02\n",
      "iteration: 66380 loss: 0.0045 lr: 0.02\n",
      "iteration: 66390 loss: 0.0038 lr: 0.02\n",
      "iteration: 66400 loss: 0.0039 lr: 0.02\n",
      "iteration: 66410 loss: 0.0037 lr: 0.02\n",
      "iteration: 66420 loss: 0.0044 lr: 0.02\n",
      "iteration: 66430 loss: 0.0049 lr: 0.02\n",
      "iteration: 66440 loss: 0.0036 lr: 0.02\n",
      "iteration: 66450 loss: 0.0029 lr: 0.02\n",
      "iteration: 66460 loss: 0.0039 lr: 0.02\n",
      "iteration: 66470 loss: 0.0040 lr: 0.02\n",
      "iteration: 66480 loss: 0.0075 lr: 0.02\n",
      "iteration: 66490 loss: 0.0051 lr: 0.02\n",
      "iteration: 66500 loss: 0.0060 lr: 0.02\n",
      "iteration: 66510 loss: 0.0043 lr: 0.02\n",
      "iteration: 66520 loss: 0.0046 lr: 0.02\n",
      "iteration: 66530 loss: 0.0030 lr: 0.02\n",
      "iteration: 66540 loss: 0.0033 lr: 0.02\n",
      "iteration: 66550 loss: 0.0052 lr: 0.02\n",
      "iteration: 66560 loss: 0.0035 lr: 0.02\n",
      "iteration: 66570 loss: 0.0043 lr: 0.02\n",
      "iteration: 66580 loss: 0.0051 lr: 0.02\n",
      "iteration: 66590 loss: 0.0037 lr: 0.02\n",
      "iteration: 66600 loss: 0.0039 lr: 0.02\n",
      "iteration: 66610 loss: 0.0045 lr: 0.02\n",
      "iteration: 66620 loss: 0.0034 lr: 0.02\n",
      "iteration: 66630 loss: 0.0064 lr: 0.02\n",
      "iteration: 66640 loss: 0.0046 lr: 0.02\n",
      "iteration: 66650 loss: 0.0035 lr: 0.02\n",
      "iteration: 66660 loss: 0.0046 lr: 0.02\n",
      "iteration: 66670 loss: 0.0049 lr: 0.02\n",
      "iteration: 66680 loss: 0.0036 lr: 0.02\n",
      "iteration: 66690 loss: 0.0041 lr: 0.02\n",
      "iteration: 66700 loss: 0.0060 lr: 0.02\n",
      "iteration: 66710 loss: 0.0043 lr: 0.02\n",
      "iteration: 66720 loss: 0.0032 lr: 0.02\n",
      "iteration: 66730 loss: 0.0035 lr: 0.02\n",
      "iteration: 66740 loss: 0.0031 lr: 0.02\n",
      "iteration: 66750 loss: 0.0048 lr: 0.02\n",
      "iteration: 66760 loss: 0.0046 lr: 0.02\n",
      "iteration: 66770 loss: 0.0037 lr: 0.02\n",
      "iteration: 66780 loss: 0.0029 lr: 0.02\n",
      "iteration: 66790 loss: 0.0061 lr: 0.02\n",
      "iteration: 66800 loss: 0.0049 lr: 0.02\n",
      "iteration: 66810 loss: 0.0053 lr: 0.02\n",
      "iteration: 66820 loss: 0.0050 lr: 0.02\n",
      "iteration: 66830 loss: 0.0040 lr: 0.02\n",
      "iteration: 66840 loss: 0.0056 lr: 0.02\n",
      "iteration: 66850 loss: 0.0036 lr: 0.02\n",
      "iteration: 66860 loss: 0.0038 lr: 0.02\n",
      "iteration: 66870 loss: 0.0052 lr: 0.02\n",
      "iteration: 66880 loss: 0.0034 lr: 0.02\n",
      "iteration: 66890 loss: 0.0035 lr: 0.02\n",
      "iteration: 66900 loss: 0.0048 lr: 0.02\n",
      "iteration: 66910 loss: 0.0038 lr: 0.02\n",
      "iteration: 66920 loss: 0.0049 lr: 0.02\n",
      "iteration: 66930 loss: 0.0036 lr: 0.02\n",
      "iteration: 66940 loss: 0.0048 lr: 0.02\n",
      "iteration: 66950 loss: 0.0043 lr: 0.02\n",
      "iteration: 66960 loss: 0.0041 lr: 0.02\n",
      "iteration: 66970 loss: 0.0034 lr: 0.02\n",
      "iteration: 66980 loss: 0.0042 lr: 0.02\n",
      "iteration: 66990 loss: 0.0034 lr: 0.02\n",
      "iteration: 67000 loss: 0.0043 lr: 0.02\n",
      "iteration: 67010 loss: 0.0031 lr: 0.02\n",
      "iteration: 67020 loss: 0.0034 lr: 0.02\n",
      "iteration: 67030 loss: 0.0037 lr: 0.02\n",
      "iteration: 67040 loss: 0.0041 lr: 0.02\n",
      "iteration: 67050 loss: 0.0029 lr: 0.02\n",
      "iteration: 67060 loss: 0.0054 lr: 0.02\n",
      "iteration: 67070 loss: 0.0041 lr: 0.02\n",
      "iteration: 67080 loss: 0.0055 lr: 0.02\n",
      "iteration: 67090 loss: 0.0089 lr: 0.02\n",
      "iteration: 67100 loss: 0.0048 lr: 0.02\n",
      "iteration: 67110 loss: 0.0053 lr: 0.02\n",
      "iteration: 67120 loss: 0.0040 lr: 0.02\n",
      "iteration: 67130 loss: 0.0033 lr: 0.02\n",
      "iteration: 67140 loss: 0.0041 lr: 0.02\n",
      "iteration: 67150 loss: 0.0044 lr: 0.02\n",
      "iteration: 67160 loss: 0.0051 lr: 0.02\n",
      "iteration: 67170 loss: 0.0046 lr: 0.02\n",
      "iteration: 67180 loss: 0.0036 lr: 0.02\n",
      "iteration: 67190 loss: 0.0031 lr: 0.02\n",
      "iteration: 67200 loss: 0.0048 lr: 0.02\n",
      "iteration: 67210 loss: 0.0049 lr: 0.02\n",
      "iteration: 67220 loss: 0.0045 lr: 0.02\n",
      "iteration: 67230 loss: 0.0042 lr: 0.02\n",
      "iteration: 67240 loss: 0.0037 lr: 0.02\n",
      "iteration: 67250 loss: 0.0036 lr: 0.02\n",
      "iteration: 67260 loss: 0.0040 lr: 0.02\n",
      "iteration: 67270 loss: 0.0038 lr: 0.02\n",
      "iteration: 67280 loss: 0.0032 lr: 0.02\n",
      "iteration: 67290 loss: 0.0043 lr: 0.02\n",
      "iteration: 67300 loss: 0.0034 lr: 0.02\n",
      "iteration: 67310 loss: 0.0037 lr: 0.02\n",
      "iteration: 67320 loss: 0.0039 lr: 0.02\n",
      "iteration: 67330 loss: 0.0060 lr: 0.02\n",
      "iteration: 67340 loss: 0.0051 lr: 0.02\n",
      "iteration: 67350 loss: 0.0065 lr: 0.02\n",
      "iteration: 67360 loss: 0.0040 lr: 0.02\n",
      "iteration: 67370 loss: 0.0045 lr: 0.02\n",
      "iteration: 67380 loss: 0.0044 lr: 0.02\n",
      "iteration: 67390 loss: 0.0031 lr: 0.02\n",
      "iteration: 67400 loss: 0.0029 lr: 0.02\n",
      "iteration: 67410 loss: 0.0057 lr: 0.02\n",
      "iteration: 67420 loss: 0.0035 lr: 0.02\n",
      "iteration: 67430 loss: 0.0038 lr: 0.02\n",
      "iteration: 67440 loss: 0.0040 lr: 0.02\n",
      "iteration: 67450 loss: 0.0035 lr: 0.02\n",
      "iteration: 67460 loss: 0.0031 lr: 0.02\n",
      "iteration: 67470 loss: 0.0049 lr: 0.02\n",
      "iteration: 67480 loss: 0.0049 lr: 0.02\n",
      "iteration: 67490 loss: 0.0050 lr: 0.02\n",
      "iteration: 67500 loss: 0.0034 lr: 0.02\n",
      "iteration: 67510 loss: 0.0030 lr: 0.02\n",
      "iteration: 67520 loss: 0.0042 lr: 0.02\n",
      "iteration: 67530 loss: 0.0046 lr: 0.02\n",
      "iteration: 67540 loss: 0.0033 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 67550 loss: 0.0043 lr: 0.02\n",
      "iteration: 67560 loss: 0.0052 lr: 0.02\n",
      "iteration: 67570 loss: 0.0042 lr: 0.02\n",
      "iteration: 67580 loss: 0.0030 lr: 0.02\n",
      "iteration: 67590 loss: 0.0040 lr: 0.02\n",
      "iteration: 67600 loss: 0.0067 lr: 0.02\n",
      "iteration: 67610 loss: 0.0052 lr: 0.02\n",
      "iteration: 67620 loss: 0.0029 lr: 0.02\n",
      "iteration: 67630 loss: 0.0033 lr: 0.02\n",
      "iteration: 67640 loss: 0.0037 lr: 0.02\n",
      "iteration: 67650 loss: 0.0042 lr: 0.02\n",
      "iteration: 67660 loss: 0.0031 lr: 0.02\n",
      "iteration: 67670 loss: 0.0025 lr: 0.02\n",
      "iteration: 67680 loss: 0.0047 lr: 0.02\n",
      "iteration: 67690 loss: 0.0041 lr: 0.02\n",
      "iteration: 67700 loss: 0.0055 lr: 0.02\n",
      "iteration: 67710 loss: 0.0035 lr: 0.02\n",
      "iteration: 67720 loss: 0.0064 lr: 0.02\n",
      "iteration: 67730 loss: 0.0053 lr: 0.02\n",
      "iteration: 67740 loss: 0.0048 lr: 0.02\n",
      "iteration: 67750 loss: 0.0041 lr: 0.02\n",
      "iteration: 67760 loss: 0.0047 lr: 0.02\n",
      "iteration: 67770 loss: 0.0032 lr: 0.02\n",
      "iteration: 67780 loss: 0.0043 lr: 0.02\n",
      "iteration: 67790 loss: 0.0064 lr: 0.02\n",
      "iteration: 67800 loss: 0.0051 lr: 0.02\n",
      "iteration: 67810 loss: 0.0043 lr: 0.02\n",
      "iteration: 67820 loss: 0.0035 lr: 0.02\n",
      "iteration: 67830 loss: 0.0034 lr: 0.02\n",
      "iteration: 67840 loss: 0.0050 lr: 0.02\n",
      "iteration: 67850 loss: 0.0040 lr: 0.02\n",
      "iteration: 67860 loss: 0.0038 lr: 0.02\n",
      "iteration: 67870 loss: 0.0036 lr: 0.02\n",
      "iteration: 67880 loss: 0.0044 lr: 0.02\n",
      "iteration: 67890 loss: 0.0028 lr: 0.02\n",
      "iteration: 67900 loss: 0.0034 lr: 0.02\n",
      "iteration: 67910 loss: 0.0032 lr: 0.02\n",
      "iteration: 67920 loss: 0.0057 lr: 0.02\n",
      "iteration: 67930 loss: 0.0040 lr: 0.02\n",
      "iteration: 67940 loss: 0.0081 lr: 0.02\n",
      "iteration: 67950 loss: 0.0039 lr: 0.02\n",
      "iteration: 67960 loss: 0.0047 lr: 0.02\n",
      "iteration: 67970 loss: 0.0038 lr: 0.02\n",
      "iteration: 67980 loss: 0.0041 lr: 0.02\n",
      "iteration: 67990 loss: 0.0051 lr: 0.02\n",
      "iteration: 68000 loss: 0.0037 lr: 0.02\n",
      "iteration: 68010 loss: 0.0033 lr: 0.02\n",
      "iteration: 68020 loss: 0.0035 lr: 0.02\n",
      "iteration: 68030 loss: 0.0052 lr: 0.02\n",
      "iteration: 68040 loss: 0.0036 lr: 0.02\n",
      "iteration: 68050 loss: 0.0030 lr: 0.02\n",
      "iteration: 68060 loss: 0.0051 lr: 0.02\n",
      "iteration: 68070 loss: 0.0043 lr: 0.02\n",
      "iteration: 68080 loss: 0.0047 lr: 0.02\n",
      "iteration: 68090 loss: 0.0066 lr: 0.02\n",
      "iteration: 68100 loss: 0.0040 lr: 0.02\n",
      "iteration: 68110 loss: 0.0049 lr: 0.02\n",
      "iteration: 68120 loss: 0.0048 lr: 0.02\n",
      "iteration: 68130 loss: 0.0047 lr: 0.02\n",
      "iteration: 68140 loss: 0.0042 lr: 0.02\n",
      "iteration: 68150 loss: 0.0044 lr: 0.02\n",
      "iteration: 68160 loss: 0.0034 lr: 0.02\n",
      "iteration: 68170 loss: 0.0059 lr: 0.02\n",
      "iteration: 68180 loss: 0.0030 lr: 0.02\n",
      "iteration: 68190 loss: 0.0054 lr: 0.02\n",
      "iteration: 68200 loss: 0.0032 lr: 0.02\n",
      "iteration: 68210 loss: 0.0038 lr: 0.02\n",
      "iteration: 68220 loss: 0.0037 lr: 0.02\n",
      "iteration: 68230 loss: 0.0039 lr: 0.02\n",
      "iteration: 68240 loss: 0.0041 lr: 0.02\n",
      "iteration: 68250 loss: 0.0039 lr: 0.02\n",
      "iteration: 68260 loss: 0.0042 lr: 0.02\n",
      "iteration: 68270 loss: 0.0049 lr: 0.02\n",
      "iteration: 68280 loss: 0.0036 lr: 0.02\n",
      "iteration: 68290 loss: 0.0048 lr: 0.02\n",
      "iteration: 68300 loss: 0.0048 lr: 0.02\n",
      "iteration: 68310 loss: 0.0034 lr: 0.02\n",
      "iteration: 68320 loss: 0.0038 lr: 0.02\n",
      "iteration: 68330 loss: 0.0047 lr: 0.02\n",
      "iteration: 68340 loss: 0.0037 lr: 0.02\n",
      "iteration: 68350 loss: 0.0050 lr: 0.02\n",
      "iteration: 68360 loss: 0.0040 lr: 0.02\n",
      "iteration: 68370 loss: 0.0034 lr: 0.02\n",
      "iteration: 68380 loss: 0.0049 lr: 0.02\n",
      "iteration: 68390 loss: 0.0038 lr: 0.02\n",
      "iteration: 68400 loss: 0.0038 lr: 0.02\n",
      "iteration: 68410 loss: 0.0038 lr: 0.02\n",
      "iteration: 68420 loss: 0.0037 lr: 0.02\n",
      "iteration: 68430 loss: 0.0039 lr: 0.02\n",
      "iteration: 68440 loss: 0.0027 lr: 0.02\n",
      "iteration: 68450 loss: 0.0034 lr: 0.02\n",
      "iteration: 68460 loss: 0.0047 lr: 0.02\n",
      "iteration: 68470 loss: 0.0033 lr: 0.02\n",
      "iteration: 68480 loss: 0.0034 lr: 0.02\n",
      "iteration: 68490 loss: 0.0039 lr: 0.02\n",
      "iteration: 68500 loss: 0.0039 lr: 0.02\n",
      "iteration: 68510 loss: 0.0036 lr: 0.02\n",
      "iteration: 68520 loss: 0.0034 lr: 0.02\n",
      "iteration: 68530 loss: 0.0034 lr: 0.02\n",
      "iteration: 68540 loss: 0.0032 lr: 0.02\n",
      "iteration: 68550 loss: 0.0033 lr: 0.02\n",
      "iteration: 68560 loss: 0.0040 lr: 0.02\n",
      "iteration: 68570 loss: 0.0038 lr: 0.02\n",
      "iteration: 68580 loss: 0.0041 lr: 0.02\n",
      "iteration: 68590 loss: 0.0034 lr: 0.02\n",
      "iteration: 68600 loss: 0.0030 lr: 0.02\n",
      "iteration: 68610 loss: 0.0046 lr: 0.02\n",
      "iteration: 68620 loss: 0.0042 lr: 0.02\n",
      "iteration: 68630 loss: 0.0040 lr: 0.02\n",
      "iteration: 68640 loss: 0.0040 lr: 0.02\n",
      "iteration: 68650 loss: 0.0046 lr: 0.02\n",
      "iteration: 68660 loss: 0.0048 lr: 0.02\n",
      "iteration: 68670 loss: 0.0048 lr: 0.02\n",
      "iteration: 68680 loss: 0.0037 lr: 0.02\n",
      "iteration: 68690 loss: 0.0036 lr: 0.02\n",
      "iteration: 68700 loss: 0.0037 lr: 0.02\n",
      "iteration: 68710 loss: 0.0042 lr: 0.02\n",
      "iteration: 68720 loss: 0.0037 lr: 0.02\n",
      "iteration: 68730 loss: 0.0053 lr: 0.02\n",
      "iteration: 68740 loss: 0.0039 lr: 0.02\n",
      "iteration: 68750 loss: 0.0041 lr: 0.02\n",
      "iteration: 68760 loss: 0.0050 lr: 0.02\n",
      "iteration: 68770 loss: 0.0035 lr: 0.02\n",
      "iteration: 68780 loss: 0.0035 lr: 0.02\n",
      "iteration: 68790 loss: 0.0032 lr: 0.02\n",
      "iteration: 68800 loss: 0.0052 lr: 0.02\n",
      "iteration: 68810 loss: 0.0034 lr: 0.02\n",
      "iteration: 68820 loss: 0.0032 lr: 0.02\n",
      "iteration: 68830 loss: 0.0032 lr: 0.02\n",
      "iteration: 68840 loss: 0.0041 lr: 0.02\n",
      "iteration: 68850 loss: 0.0034 lr: 0.02\n",
      "iteration: 68860 loss: 0.0034 lr: 0.02\n",
      "iteration: 68870 loss: 0.0035 lr: 0.02\n",
      "iteration: 68880 loss: 0.0064 lr: 0.02\n",
      "iteration: 68890 loss: 0.0038 lr: 0.02\n",
      "iteration: 68900 loss: 0.0030 lr: 0.02\n",
      "iteration: 68910 loss: 0.0039 lr: 0.02\n",
      "iteration: 68920 loss: 0.0031 lr: 0.02\n",
      "iteration: 68930 loss: 0.0038 lr: 0.02\n",
      "iteration: 68940 loss: 0.0041 lr: 0.02\n",
      "iteration: 68950 loss: 0.0047 lr: 0.02\n",
      "iteration: 68960 loss: 0.0038 lr: 0.02\n",
      "iteration: 68970 loss: 0.0033 lr: 0.02\n",
      "iteration: 68980 loss: 0.0046 lr: 0.02\n",
      "iteration: 68990 loss: 0.0030 lr: 0.02\n",
      "iteration: 69000 loss: 0.0031 lr: 0.02\n",
      "iteration: 69010 loss: 0.0051 lr: 0.02\n",
      "iteration: 69020 loss: 0.0050 lr: 0.02\n",
      "iteration: 69030 loss: 0.0057 lr: 0.02\n",
      "iteration: 69040 loss: 0.0046 lr: 0.02\n",
      "iteration: 69050 loss: 0.0026 lr: 0.02\n",
      "iteration: 69060 loss: 0.0034 lr: 0.02\n",
      "iteration: 69070 loss: 0.0036 lr: 0.02\n",
      "iteration: 69080 loss: 0.0039 lr: 0.02\n",
      "iteration: 69090 loss: 0.0029 lr: 0.02\n",
      "iteration: 69100 loss: 0.0032 lr: 0.02\n",
      "iteration: 69110 loss: 0.0046 lr: 0.02\n",
      "iteration: 69120 loss: 0.0046 lr: 0.02\n",
      "iteration: 69130 loss: 0.0032 lr: 0.02\n",
      "iteration: 69140 loss: 0.0056 lr: 0.02\n",
      "iteration: 69150 loss: 0.0033 lr: 0.02\n",
      "iteration: 69160 loss: 0.0041 lr: 0.02\n",
      "iteration: 69170 loss: 0.0035 lr: 0.02\n",
      "iteration: 69180 loss: 0.0037 lr: 0.02\n",
      "iteration: 69190 loss: 0.0045 lr: 0.02\n",
      "iteration: 69200 loss: 0.0035 lr: 0.02\n",
      "iteration: 69210 loss: 0.0052 lr: 0.02\n",
      "iteration: 69220 loss: 0.0036 lr: 0.02\n",
      "iteration: 69230 loss: 0.0061 lr: 0.02\n",
      "iteration: 69240 loss: 0.0036 lr: 0.02\n",
      "iteration: 69250 loss: 0.0054 lr: 0.02\n",
      "iteration: 69260 loss: 0.0031 lr: 0.02\n",
      "iteration: 69270 loss: 0.0066 lr: 0.02\n",
      "iteration: 69280 loss: 0.0036 lr: 0.02\n",
      "iteration: 69290 loss: 0.0045 lr: 0.02\n",
      "iteration: 69300 loss: 0.0044 lr: 0.02\n",
      "iteration: 69310 loss: 0.0051 lr: 0.02\n",
      "iteration: 69320 loss: 0.0036 lr: 0.02\n",
      "iteration: 69330 loss: 0.0037 lr: 0.02\n",
      "iteration: 69340 loss: 0.0037 lr: 0.02\n",
      "iteration: 69350 loss: 0.0032 lr: 0.02\n",
      "iteration: 69360 loss: 0.0031 lr: 0.02\n",
      "iteration: 69370 loss: 0.0040 lr: 0.02\n",
      "iteration: 69380 loss: 0.0055 lr: 0.02\n",
      "iteration: 69390 loss: 0.0036 lr: 0.02\n",
      "iteration: 69400 loss: 0.0049 lr: 0.02\n",
      "iteration: 69410 loss: 0.0034 lr: 0.02\n",
      "iteration: 69420 loss: 0.0031 lr: 0.02\n",
      "iteration: 69430 loss: 0.0042 lr: 0.02\n",
      "iteration: 69440 loss: 0.0043 lr: 0.02\n",
      "iteration: 69450 loss: 0.0062 lr: 0.02\n",
      "iteration: 69460 loss: 0.0041 lr: 0.02\n",
      "iteration: 69470 loss: 0.0028 lr: 0.02\n",
      "iteration: 69480 loss: 0.0027 lr: 0.02\n",
      "iteration: 69490 loss: 0.0037 lr: 0.02\n",
      "iteration: 69500 loss: 0.0041 lr: 0.02\n",
      "iteration: 69510 loss: 0.0046 lr: 0.02\n",
      "iteration: 69520 loss: 0.0032 lr: 0.02\n",
      "iteration: 69530 loss: 0.0030 lr: 0.02\n",
      "iteration: 69540 loss: 0.0032 lr: 0.02\n",
      "iteration: 69550 loss: 0.0036 lr: 0.02\n",
      "iteration: 69560 loss: 0.0031 lr: 0.02\n",
      "iteration: 69570 loss: 0.0041 lr: 0.02\n",
      "iteration: 69580 loss: 0.0037 lr: 0.02\n",
      "iteration: 69590 loss: 0.0040 lr: 0.02\n",
      "iteration: 69600 loss: 0.0045 lr: 0.02\n",
      "iteration: 69610 loss: 0.0030 lr: 0.02\n",
      "iteration: 69620 loss: 0.0039 lr: 0.02\n",
      "iteration: 69630 loss: 0.0043 lr: 0.02\n",
      "iteration: 69640 loss: 0.0040 lr: 0.02\n",
      "iteration: 69650 loss: 0.0035 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 69660 loss: 0.0036 lr: 0.02\n",
      "iteration: 69670 loss: 0.0037 lr: 0.02\n",
      "iteration: 69680 loss: 0.0045 lr: 0.02\n",
      "iteration: 69690 loss: 0.0034 lr: 0.02\n",
      "iteration: 69700 loss: 0.0039 lr: 0.02\n",
      "iteration: 69710 loss: 0.0032 lr: 0.02\n",
      "iteration: 69720 loss: 0.0027 lr: 0.02\n",
      "iteration: 69730 loss: 0.0034 lr: 0.02\n",
      "iteration: 69740 loss: 0.0051 lr: 0.02\n",
      "iteration: 69750 loss: 0.0043 lr: 0.02\n",
      "iteration: 69760 loss: 0.0033 lr: 0.02\n",
      "iteration: 69770 loss: 0.0058 lr: 0.02\n",
      "iteration: 69780 loss: 0.0037 lr: 0.02\n",
      "iteration: 69790 loss: 0.0040 lr: 0.02\n",
      "iteration: 69800 loss: 0.0038 lr: 0.02\n",
      "iteration: 69810 loss: 0.0047 lr: 0.02\n",
      "iteration: 69820 loss: 0.0039 lr: 0.02\n",
      "iteration: 69830 loss: 0.0040 lr: 0.02\n",
      "iteration: 69840 loss: 0.0032 lr: 0.02\n",
      "iteration: 69850 loss: 0.0032 lr: 0.02\n",
      "iteration: 69860 loss: 0.0039 lr: 0.02\n",
      "iteration: 69870 loss: 0.0056 lr: 0.02\n",
      "iteration: 69880 loss: 0.0029 lr: 0.02\n",
      "iteration: 69890 loss: 0.0043 lr: 0.02\n",
      "iteration: 69900 loss: 0.0031 lr: 0.02\n",
      "iteration: 69910 loss: 0.0032 lr: 0.02\n",
      "iteration: 69920 loss: 0.0041 lr: 0.02\n",
      "iteration: 69930 loss: 0.0027 lr: 0.02\n",
      "iteration: 69940 loss: 0.0040 lr: 0.02\n",
      "iteration: 69950 loss: 0.0034 lr: 0.02\n",
      "iteration: 69960 loss: 0.0039 lr: 0.02\n",
      "iteration: 69970 loss: 0.0046 lr: 0.02\n",
      "iteration: 69980 loss: 0.0038 lr: 0.02\n",
      "iteration: 69990 loss: 0.0048 lr: 0.02\n",
      "iteration: 70000 loss: 0.0029 lr: 0.02\n",
      "iteration: 70010 loss: 0.0042 lr: 0.02\n",
      "iteration: 70020 loss: 0.0044 lr: 0.02\n",
      "iteration: 70030 loss: 0.0051 lr: 0.02\n",
      "iteration: 70040 loss: 0.0041 lr: 0.02\n",
      "iteration: 70050 loss: 0.0037 lr: 0.02\n",
      "iteration: 70060 loss: 0.0034 lr: 0.02\n",
      "iteration: 70070 loss: 0.0041 lr: 0.02\n",
      "iteration: 70080 loss: 0.0042 lr: 0.02\n",
      "iteration: 70090 loss: 0.0036 lr: 0.02\n",
      "iteration: 70100 loss: 0.0058 lr: 0.02\n",
      "iteration: 70110 loss: 0.0041 lr: 0.02\n",
      "iteration: 70120 loss: 0.0046 lr: 0.02\n",
      "iteration: 70130 loss: 0.0047 lr: 0.02\n",
      "iteration: 70140 loss: 0.0039 lr: 0.02\n",
      "iteration: 70150 loss: 0.0056 lr: 0.02\n",
      "iteration: 70160 loss: 0.0039 lr: 0.02\n",
      "iteration: 70170 loss: 0.0045 lr: 0.02\n",
      "iteration: 70180 loss: 0.0045 lr: 0.02\n",
      "iteration: 70190 loss: 0.0044 lr: 0.02\n",
      "iteration: 70200 loss: 0.0039 lr: 0.02\n",
      "iteration: 70210 loss: 0.0038 lr: 0.02\n",
      "iteration: 70220 loss: 0.0041 lr: 0.02\n",
      "iteration: 70230 loss: 0.0027 lr: 0.02\n",
      "iteration: 70240 loss: 0.0045 lr: 0.02\n",
      "iteration: 70250 loss: 0.0037 lr: 0.02\n",
      "iteration: 70260 loss: 0.0037 lr: 0.02\n",
      "iteration: 70270 loss: 0.0035 lr: 0.02\n",
      "iteration: 70280 loss: 0.0031 lr: 0.02\n",
      "iteration: 70290 loss: 0.0041 lr: 0.02\n",
      "iteration: 70300 loss: 0.0037 lr: 0.02\n",
      "iteration: 70310 loss: 0.0041 lr: 0.02\n",
      "iteration: 70320 loss: 0.0039 lr: 0.02\n",
      "iteration: 70330 loss: 0.0044 lr: 0.02\n",
      "iteration: 70340 loss: 0.0032 lr: 0.02\n",
      "iteration: 70350 loss: 0.0058 lr: 0.02\n",
      "iteration: 70360 loss: 0.0042 lr: 0.02\n",
      "iteration: 70370 loss: 0.0044 lr: 0.02\n",
      "iteration: 70380 loss: 0.0093 lr: 0.02\n",
      "iteration: 70390 loss: 0.0040 lr: 0.02\n",
      "iteration: 70400 loss: 0.0037 lr: 0.02\n",
      "iteration: 70410 loss: 0.0031 lr: 0.02\n",
      "iteration: 70420 loss: 0.0035 lr: 0.02\n",
      "iteration: 70430 loss: 0.0030 lr: 0.02\n",
      "iteration: 70440 loss: 0.0063 lr: 0.02\n",
      "iteration: 70450 loss: 0.0046 lr: 0.02\n",
      "iteration: 70460 loss: 0.0038 lr: 0.02\n",
      "iteration: 70470 loss: 0.0041 lr: 0.02\n",
      "iteration: 70480 loss: 0.0037 lr: 0.02\n",
      "iteration: 70490 loss: 0.0067 lr: 0.02\n",
      "iteration: 70500 loss: 0.0054 lr: 0.02\n",
      "iteration: 70510 loss: 0.0032 lr: 0.02\n",
      "iteration: 70520 loss: 0.0067 lr: 0.02\n",
      "iteration: 70530 loss: 0.0044 lr: 0.02\n",
      "iteration: 70540 loss: 0.0035 lr: 0.02\n",
      "iteration: 70550 loss: 0.0046 lr: 0.02\n",
      "iteration: 70560 loss: 0.0044 lr: 0.02\n",
      "iteration: 70570 loss: 0.0037 lr: 0.02\n",
      "iteration: 70580 loss: 0.0035 lr: 0.02\n",
      "iteration: 70590 loss: 0.0048 lr: 0.02\n",
      "iteration: 70600 loss: 0.0046 lr: 0.02\n",
      "iteration: 70610 loss: 0.0037 lr: 0.02\n",
      "iteration: 70620 loss: 0.0047 lr: 0.02\n",
      "iteration: 70630 loss: 0.0034 lr: 0.02\n",
      "iteration: 70640 loss: 0.0026 lr: 0.02\n",
      "iteration: 70650 loss: 0.0029 lr: 0.02\n",
      "iteration: 70660 loss: 0.0032 lr: 0.02\n",
      "iteration: 70670 loss: 0.0041 lr: 0.02\n",
      "iteration: 70680 loss: 0.0051 lr: 0.02\n",
      "iteration: 70690 loss: 0.0036 lr: 0.02\n",
      "iteration: 70700 loss: 0.0041 lr: 0.02\n",
      "iteration: 70710 loss: 0.0040 lr: 0.02\n",
      "iteration: 70720 loss: 0.0052 lr: 0.02\n",
      "iteration: 70730 loss: 0.0046 lr: 0.02\n",
      "iteration: 70740 loss: 0.0035 lr: 0.02\n",
      "iteration: 70750 loss: 0.0037 lr: 0.02\n",
      "iteration: 70760 loss: 0.0035 lr: 0.02\n",
      "iteration: 70770 loss: 0.0032 lr: 0.02\n",
      "iteration: 70780 loss: 0.0027 lr: 0.02\n",
      "iteration: 70790 loss: 0.0037 lr: 0.02\n",
      "iteration: 70800 loss: 0.0034 lr: 0.02\n",
      "iteration: 70810 loss: 0.0036 lr: 0.02\n",
      "iteration: 70820 loss: 0.0047 lr: 0.02\n",
      "iteration: 70830 loss: 0.0036 lr: 0.02\n",
      "iteration: 70840 loss: 0.0032 lr: 0.02\n",
      "iteration: 70850 loss: 0.0047 lr: 0.02\n",
      "iteration: 70860 loss: 0.0042 lr: 0.02\n",
      "iteration: 70870 loss: 0.0032 lr: 0.02\n",
      "iteration: 70880 loss: 0.0040 lr: 0.02\n",
      "iteration: 70890 loss: 0.0035 lr: 0.02\n",
      "iteration: 70900 loss: 0.0027 lr: 0.02\n",
      "iteration: 70910 loss: 0.0045 lr: 0.02\n",
      "iteration: 70920 loss: 0.0040 lr: 0.02\n",
      "iteration: 70930 loss: 0.0038 lr: 0.02\n",
      "iteration: 70940 loss: 0.0028 lr: 0.02\n",
      "iteration: 70950 loss: 0.0036 lr: 0.02\n",
      "iteration: 70960 loss: 0.0028 lr: 0.02\n",
      "iteration: 70970 loss: 0.0032 lr: 0.02\n",
      "iteration: 70980 loss: 0.0034 lr: 0.02\n",
      "iteration: 70990 loss: 0.0053 lr: 0.02\n",
      "iteration: 71000 loss: 0.0032 lr: 0.02\n",
      "iteration: 71010 loss: 0.0058 lr: 0.02\n",
      "iteration: 71020 loss: 0.0052 lr: 0.02\n",
      "iteration: 71030 loss: 0.0043 lr: 0.02\n",
      "iteration: 71040 loss: 0.0057 lr: 0.02\n",
      "iteration: 71050 loss: 0.0041 lr: 0.02\n",
      "iteration: 71060 loss: 0.0028 lr: 0.02\n",
      "iteration: 71070 loss: 0.0042 lr: 0.02\n",
      "iteration: 71080 loss: 0.0039 lr: 0.02\n",
      "iteration: 71090 loss: 0.0048 lr: 0.02\n",
      "iteration: 71100 loss: 0.0043 lr: 0.02\n",
      "iteration: 71110 loss: 0.0040 lr: 0.02\n",
      "iteration: 71120 loss: 0.0053 lr: 0.02\n",
      "iteration: 71130 loss: 0.0044 lr: 0.02\n",
      "iteration: 71140 loss: 0.0033 lr: 0.02\n",
      "iteration: 71150 loss: 0.0046 lr: 0.02\n",
      "iteration: 71160 loss: 0.0042 lr: 0.02\n",
      "iteration: 71170 loss: 0.0045 lr: 0.02\n",
      "iteration: 71180 loss: 0.0038 lr: 0.02\n",
      "iteration: 71190 loss: 0.0050 lr: 0.02\n",
      "iteration: 71200 loss: 0.0047 lr: 0.02\n",
      "iteration: 71210 loss: 0.0032 lr: 0.02\n",
      "iteration: 71220 loss: 0.0036 lr: 0.02\n",
      "iteration: 71230 loss: 0.0037 lr: 0.02\n",
      "iteration: 71240 loss: 0.0032 lr: 0.02\n",
      "iteration: 71250 loss: 0.0026 lr: 0.02\n",
      "iteration: 71260 loss: 0.0030 lr: 0.02\n",
      "iteration: 71270 loss: 0.0036 lr: 0.02\n",
      "iteration: 71280 loss: 0.0038 lr: 0.02\n",
      "iteration: 71290 loss: 0.0078 lr: 0.02\n",
      "iteration: 71300 loss: 0.0034 lr: 0.02\n",
      "iteration: 71310 loss: 0.0028 lr: 0.02\n",
      "iteration: 71320 loss: 0.0036 lr: 0.02\n",
      "iteration: 71330 loss: 0.0039 lr: 0.02\n",
      "iteration: 71340 loss: 0.0032 lr: 0.02\n",
      "iteration: 71350 loss: 0.0056 lr: 0.02\n",
      "iteration: 71360 loss: 0.0039 lr: 0.02\n",
      "iteration: 71370 loss: 0.0041 lr: 0.02\n",
      "iteration: 71380 loss: 0.0041 lr: 0.02\n",
      "iteration: 71390 loss: 0.0038 lr: 0.02\n",
      "iteration: 71400 loss: 0.0034 lr: 0.02\n",
      "iteration: 71410 loss: 0.0038 lr: 0.02\n",
      "iteration: 71420 loss: 0.0030 lr: 0.02\n",
      "iteration: 71430 loss: 0.0035 lr: 0.02\n",
      "iteration: 71440 loss: 0.0029 lr: 0.02\n",
      "iteration: 71450 loss: 0.0060 lr: 0.02\n",
      "iteration: 71460 loss: 0.0041 lr: 0.02\n",
      "iteration: 71470 loss: 0.0041 lr: 0.02\n",
      "iteration: 71480 loss: 0.0038 lr: 0.02\n",
      "iteration: 71490 loss: 0.0045 lr: 0.02\n",
      "iteration: 71500 loss: 0.0029 lr: 0.02\n",
      "iteration: 71510 loss: 0.0035 lr: 0.02\n",
      "iteration: 71520 loss: 0.0036 lr: 0.02\n",
      "iteration: 71530 loss: 0.0030 lr: 0.02\n",
      "iteration: 71540 loss: 0.0029 lr: 0.02\n",
      "iteration: 71550 loss: 0.0046 lr: 0.02\n",
      "iteration: 71560 loss: 0.0038 lr: 0.02\n",
      "iteration: 71570 loss: 0.0036 lr: 0.02\n",
      "iteration: 71580 loss: 0.0029 lr: 0.02\n",
      "iteration: 71590 loss: 0.0047 lr: 0.02\n",
      "iteration: 71600 loss: 0.0029 lr: 0.02\n",
      "iteration: 71610 loss: 0.0029 lr: 0.02\n",
      "iteration: 71620 loss: 0.0049 lr: 0.02\n",
      "iteration: 71630 loss: 0.0038 lr: 0.02\n",
      "iteration: 71640 loss: 0.0025 lr: 0.02\n",
      "iteration: 71650 loss: 0.0054 lr: 0.02\n",
      "iteration: 71660 loss: 0.0040 lr: 0.02\n",
      "iteration: 71670 loss: 0.0026 lr: 0.02\n",
      "iteration: 71680 loss: 0.0049 lr: 0.02\n",
      "iteration: 71690 loss: 0.0029 lr: 0.02\n",
      "iteration: 71700 loss: 0.0042 lr: 0.02\n",
      "iteration: 71710 loss: 0.0061 lr: 0.02\n",
      "iteration: 71720 loss: 0.0033 lr: 0.02\n",
      "iteration: 71730 loss: 0.0041 lr: 0.02\n",
      "iteration: 71740 loss: 0.0054 lr: 0.02\n",
      "iteration: 71750 loss: 0.0038 lr: 0.02\n",
      "iteration: 71760 loss: 0.0034 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 71770 loss: 0.0034 lr: 0.02\n",
      "iteration: 71780 loss: 0.0044 lr: 0.02\n",
      "iteration: 71790 loss: 0.0031 lr: 0.02\n",
      "iteration: 71800 loss: 0.0037 lr: 0.02\n",
      "iteration: 71810 loss: 0.0043 lr: 0.02\n",
      "iteration: 71820 loss: 0.0024 lr: 0.02\n",
      "iteration: 71830 loss: 0.0031 lr: 0.02\n",
      "iteration: 71840 loss: 0.0029 lr: 0.02\n",
      "iteration: 71850 loss: 0.0031 lr: 0.02\n",
      "iteration: 71860 loss: 0.0028 lr: 0.02\n",
      "iteration: 71870 loss: 0.0056 lr: 0.02\n",
      "iteration: 71880 loss: 0.0033 lr: 0.02\n",
      "iteration: 71890 loss: 0.0035 lr: 0.02\n",
      "iteration: 71900 loss: 0.0053 lr: 0.02\n",
      "iteration: 71910 loss: 0.0061 lr: 0.02\n",
      "iteration: 71920 loss: 0.0054 lr: 0.02\n",
      "iteration: 71930 loss: 0.0038 lr: 0.02\n",
      "iteration: 71940 loss: 0.0041 lr: 0.02\n",
      "iteration: 71950 loss: 0.0040 lr: 0.02\n",
      "iteration: 71960 loss: 0.0035 lr: 0.02\n",
      "iteration: 71970 loss: 0.0047 lr: 0.02\n",
      "iteration: 71980 loss: 0.0059 lr: 0.02\n",
      "iteration: 71990 loss: 0.0055 lr: 0.02\n",
      "iteration: 72000 loss: 0.0034 lr: 0.02\n",
      "iteration: 72010 loss: 0.0047 lr: 0.02\n",
      "iteration: 72020 loss: 0.0046 lr: 0.02\n",
      "iteration: 72030 loss: 0.0031 lr: 0.02\n",
      "iteration: 72040 loss: 0.0050 lr: 0.02\n",
      "iteration: 72050 loss: 0.0038 lr: 0.02\n",
      "iteration: 72060 loss: 0.0042 lr: 0.02\n",
      "iteration: 72070 loss: 0.0037 lr: 0.02\n",
      "iteration: 72080 loss: 0.0047 lr: 0.02\n",
      "iteration: 72090 loss: 0.0041 lr: 0.02\n",
      "iteration: 72100 loss: 0.0038 lr: 0.02\n",
      "iteration: 72110 loss: 0.0055 lr: 0.02\n",
      "iteration: 72120 loss: 0.0037 lr: 0.02\n",
      "iteration: 72130 loss: 0.0043 lr: 0.02\n",
      "iteration: 72140 loss: 0.0040 lr: 0.02\n",
      "iteration: 72150 loss: 0.0039 lr: 0.02\n",
      "iteration: 72160 loss: 0.0044 lr: 0.02\n",
      "iteration: 72170 loss: 0.0048 lr: 0.02\n",
      "iteration: 72180 loss: 0.0047 lr: 0.02\n",
      "iteration: 72190 loss: 0.0036 lr: 0.02\n",
      "iteration: 72200 loss: 0.0043 lr: 0.02\n",
      "iteration: 72210 loss: 0.0038 lr: 0.02\n",
      "iteration: 72220 loss: 0.0044 lr: 0.02\n",
      "iteration: 72230 loss: 0.0038 lr: 0.02\n",
      "iteration: 72240 loss: 0.0051 lr: 0.02\n",
      "iteration: 72250 loss: 0.0044 lr: 0.02\n",
      "iteration: 72260 loss: 0.0032 lr: 0.02\n",
      "iteration: 72270 loss: 0.0040 lr: 0.02\n",
      "iteration: 72280 loss: 0.0037 lr: 0.02\n",
      "iteration: 72290 loss: 0.0053 lr: 0.02\n",
      "iteration: 72300 loss: 0.0032 lr: 0.02\n",
      "iteration: 72310 loss: 0.0042 lr: 0.02\n",
      "iteration: 72320 loss: 0.0031 lr: 0.02\n",
      "iteration: 72330 loss: 0.0032 lr: 0.02\n",
      "iteration: 72340 loss: 0.0043 lr: 0.02\n",
      "iteration: 72350 loss: 0.0045 lr: 0.02\n",
      "iteration: 72360 loss: 0.0035 lr: 0.02\n",
      "iteration: 72370 loss: 0.0030 lr: 0.02\n",
      "iteration: 72380 loss: 0.0055 lr: 0.02\n",
      "iteration: 72390 loss: 0.0030 lr: 0.02\n",
      "iteration: 72400 loss: 0.0044 lr: 0.02\n",
      "iteration: 72410 loss: 0.0062 lr: 0.02\n",
      "iteration: 72420 loss: 0.0058 lr: 0.02\n",
      "iteration: 72430 loss: 0.0040 lr: 0.02\n",
      "iteration: 72440 loss: 0.0038 lr: 0.02\n",
      "iteration: 72450 loss: 0.0063 lr: 0.02\n",
      "iteration: 72460 loss: 0.0046 lr: 0.02\n",
      "iteration: 72470 loss: 0.0051 lr: 0.02\n",
      "iteration: 72480 loss: 0.0043 lr: 0.02\n",
      "iteration: 72490 loss: 0.0035 lr: 0.02\n",
      "iteration: 72500 loss: 0.0027 lr: 0.02\n",
      "iteration: 72510 loss: 0.0035 lr: 0.02\n",
      "iteration: 72520 loss: 0.0042 lr: 0.02\n",
      "iteration: 72530 loss: 0.0022 lr: 0.02\n",
      "iteration: 72540 loss: 0.0028 lr: 0.02\n",
      "iteration: 72550 loss: 0.0026 lr: 0.02\n",
      "iteration: 72560 loss: 0.0039 lr: 0.02\n",
      "iteration: 72570 loss: 0.0038 lr: 0.02\n",
      "iteration: 72580 loss: 0.0037 lr: 0.02\n",
      "iteration: 72590 loss: 0.0039 lr: 0.02\n",
      "iteration: 72600 loss: 0.0040 lr: 0.02\n",
      "iteration: 72610 loss: 0.0048 lr: 0.02\n",
      "iteration: 72620 loss: 0.0044 lr: 0.02\n",
      "iteration: 72630 loss: 0.0045 lr: 0.02\n",
      "iteration: 72640 loss: 0.0042 lr: 0.02\n",
      "iteration: 72650 loss: 0.0031 lr: 0.02\n",
      "iteration: 72660 loss: 0.0035 lr: 0.02\n",
      "iteration: 72670 loss: 0.0025 lr: 0.02\n",
      "iteration: 72680 loss: 0.0029 lr: 0.02\n",
      "iteration: 72690 loss: 0.0038 lr: 0.02\n",
      "iteration: 72700 loss: 0.0034 lr: 0.02\n",
      "iteration: 72710 loss: 0.0042 lr: 0.02\n",
      "iteration: 72720 loss: 0.0042 lr: 0.02\n",
      "iteration: 72730 loss: 0.0041 lr: 0.02\n",
      "iteration: 72740 loss: 0.0042 lr: 0.02\n",
      "iteration: 72750 loss: 0.0031 lr: 0.02\n",
      "iteration: 72760 loss: 0.0052 lr: 0.02\n",
      "iteration: 72770 loss: 0.0048 lr: 0.02\n",
      "iteration: 72780 loss: 0.0034 lr: 0.02\n",
      "iteration: 72790 loss: 0.0043 lr: 0.02\n",
      "iteration: 72800 loss: 0.0033 lr: 0.02\n",
      "iteration: 72810 loss: 0.0044 lr: 0.02\n",
      "iteration: 72820 loss: 0.0033 lr: 0.02\n",
      "iteration: 72830 loss: 0.0043 lr: 0.02\n",
      "iteration: 72840 loss: 0.0040 lr: 0.02\n",
      "iteration: 72850 loss: 0.0035 lr: 0.02\n",
      "iteration: 72860 loss: 0.0035 lr: 0.02\n",
      "iteration: 72870 loss: 0.0036 lr: 0.02\n",
      "iteration: 72880 loss: 0.0043 lr: 0.02\n",
      "iteration: 72890 loss: 0.0054 lr: 0.02\n",
      "iteration: 72900 loss: 0.0063 lr: 0.02\n",
      "iteration: 72910 loss: 0.0043 lr: 0.02\n",
      "iteration: 72920 loss: 0.0032 lr: 0.02\n",
      "iteration: 72930 loss: 0.0029 lr: 0.02\n",
      "iteration: 72940 loss: 0.0038 lr: 0.02\n",
      "iteration: 72950 loss: 0.0034 lr: 0.02\n",
      "iteration: 72960 loss: 0.0034 lr: 0.02\n",
      "iteration: 72970 loss: 0.0030 lr: 0.02\n",
      "iteration: 72980 loss: 0.0033 lr: 0.02\n",
      "iteration: 72990 loss: 0.0033 lr: 0.02\n",
      "iteration: 73000 loss: 0.0044 lr: 0.02\n",
      "iteration: 73010 loss: 0.0036 lr: 0.02\n",
      "iteration: 73020 loss: 0.0027 lr: 0.02\n",
      "iteration: 73030 loss: 0.0029 lr: 0.02\n",
      "iteration: 73040 loss: 0.0033 lr: 0.02\n",
      "iteration: 73050 loss: 0.0054 lr: 0.02\n",
      "iteration: 73060 loss: 0.0036 lr: 0.02\n",
      "iteration: 73070 loss: 0.0036 lr: 0.02\n",
      "iteration: 73080 loss: 0.0036 lr: 0.02\n",
      "iteration: 73090 loss: 0.0050 lr: 0.02\n",
      "iteration: 73100 loss: 0.0034 lr: 0.02\n",
      "iteration: 73110 loss: 0.0040 lr: 0.02\n",
      "iteration: 73120 loss: 0.0041 lr: 0.02\n",
      "iteration: 73130 loss: 0.0051 lr: 0.02\n",
      "iteration: 73140 loss: 0.0027 lr: 0.02\n",
      "iteration: 73150 loss: 0.0038 lr: 0.02\n",
      "iteration: 73160 loss: 0.0051 lr: 0.02\n",
      "iteration: 73170 loss: 0.0039 lr: 0.02\n",
      "iteration: 73180 loss: 0.0039 lr: 0.02\n",
      "iteration: 73190 loss: 0.0041 lr: 0.02\n",
      "iteration: 73200 loss: 0.0032 lr: 0.02\n",
      "iteration: 73210 loss: 0.0048 lr: 0.02\n",
      "iteration: 73220 loss: 0.0035 lr: 0.02\n",
      "iteration: 73230 loss: 0.0029 lr: 0.02\n",
      "iteration: 73240 loss: 0.0040 lr: 0.02\n",
      "iteration: 73250 loss: 0.0037 lr: 0.02\n",
      "iteration: 73260 loss: 0.0040 lr: 0.02\n",
      "iteration: 73270 loss: 0.0042 lr: 0.02\n",
      "iteration: 73280 loss: 0.0031 lr: 0.02\n",
      "iteration: 73290 loss: 0.0029 lr: 0.02\n",
      "iteration: 73300 loss: 0.0041 lr: 0.02\n",
      "iteration: 73310 loss: 0.0047 lr: 0.02\n",
      "iteration: 73320 loss: 0.0038 lr: 0.02\n",
      "iteration: 73330 loss: 0.0038 lr: 0.02\n",
      "iteration: 73340 loss: 0.0035 lr: 0.02\n",
      "iteration: 73350 loss: 0.0031 lr: 0.02\n",
      "iteration: 73360 loss: 0.0048 lr: 0.02\n",
      "iteration: 73370 loss: 0.0042 lr: 0.02\n",
      "iteration: 73380 loss: 0.0041 lr: 0.02\n",
      "iteration: 73390 loss: 0.0043 lr: 0.02\n",
      "iteration: 73400 loss: 0.0037 lr: 0.02\n",
      "iteration: 73410 loss: 0.0045 lr: 0.02\n",
      "iteration: 73420 loss: 0.0040 lr: 0.02\n",
      "iteration: 73430 loss: 0.0037 lr: 0.02\n",
      "iteration: 73440 loss: 0.0033 lr: 0.02\n",
      "iteration: 73450 loss: 0.0042 lr: 0.02\n",
      "iteration: 73460 loss: 0.0061 lr: 0.02\n",
      "iteration: 73470 loss: 0.0048 lr: 0.02\n",
      "iteration: 73480 loss: 0.0037 lr: 0.02\n",
      "iteration: 73490 loss: 0.0040 lr: 0.02\n",
      "iteration: 73500 loss: 0.0045 lr: 0.02\n",
      "iteration: 73510 loss: 0.0032 lr: 0.02\n",
      "iteration: 73520 loss: 0.0028 lr: 0.02\n",
      "iteration: 73530 loss: 0.0037 lr: 0.02\n",
      "iteration: 73540 loss: 0.0047 lr: 0.02\n",
      "iteration: 73550 loss: 0.0036 lr: 0.02\n",
      "iteration: 73560 loss: 0.0053 lr: 0.02\n",
      "iteration: 73570 loss: 0.0037 lr: 0.02\n",
      "iteration: 73580 loss: 0.0053 lr: 0.02\n",
      "iteration: 73590 loss: 0.0051 lr: 0.02\n",
      "iteration: 73600 loss: 0.0031 lr: 0.02\n",
      "iteration: 73610 loss: 0.0035 lr: 0.02\n",
      "iteration: 73620 loss: 0.0042 lr: 0.02\n",
      "iteration: 73630 loss: 0.0034 lr: 0.02\n",
      "iteration: 73640 loss: 0.0043 lr: 0.02\n",
      "iteration: 73650 loss: 0.0048 lr: 0.02\n",
      "iteration: 73660 loss: 0.0037 lr: 0.02\n",
      "iteration: 73670 loss: 0.0033 lr: 0.02\n",
      "iteration: 73680 loss: 0.0038 lr: 0.02\n",
      "iteration: 73690 loss: 0.0047 lr: 0.02\n",
      "iteration: 73700 loss: 0.0040 lr: 0.02\n",
      "iteration: 73710 loss: 0.0046 lr: 0.02\n",
      "iteration: 73720 loss: 0.0057 lr: 0.02\n",
      "iteration: 73730 loss: 0.0043 lr: 0.02\n",
      "iteration: 73740 loss: 0.0033 lr: 0.02\n",
      "iteration: 73750 loss: 0.0052 lr: 0.02\n",
      "iteration: 73760 loss: 0.0035 lr: 0.02\n",
      "iteration: 73770 loss: 0.0036 lr: 0.02\n",
      "iteration: 73780 loss: 0.0027 lr: 0.02\n",
      "iteration: 73790 loss: 0.0042 lr: 0.02\n",
      "iteration: 73800 loss: 0.0034 lr: 0.02\n",
      "iteration: 73810 loss: 0.0032 lr: 0.02\n",
      "iteration: 73820 loss: 0.0038 lr: 0.02\n",
      "iteration: 73830 loss: 0.0052 lr: 0.02\n",
      "iteration: 73840 loss: 0.0057 lr: 0.02\n",
      "iteration: 73850 loss: 0.0044 lr: 0.02\n",
      "iteration: 73860 loss: 0.0031 lr: 0.02\n",
      "iteration: 73870 loss: 0.0032 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 73880 loss: 0.0039 lr: 0.02\n",
      "iteration: 73890 loss: 0.0026 lr: 0.02\n",
      "iteration: 73900 loss: 0.0036 lr: 0.02\n",
      "iteration: 73910 loss: 0.0044 lr: 0.02\n",
      "iteration: 73920 loss: 0.0028 lr: 0.02\n",
      "iteration: 73930 loss: 0.0046 lr: 0.02\n",
      "iteration: 73940 loss: 0.0042 lr: 0.02\n",
      "iteration: 73950 loss: 0.0049 lr: 0.02\n",
      "iteration: 73960 loss: 0.0026 lr: 0.02\n",
      "iteration: 73970 loss: 0.0033 lr: 0.02\n",
      "iteration: 73980 loss: 0.0036 lr: 0.02\n",
      "iteration: 73990 loss: 0.0027 lr: 0.02\n",
      "iteration: 74000 loss: 0.0049 lr: 0.02\n",
      "iteration: 74010 loss: 0.0027 lr: 0.02\n",
      "iteration: 74020 loss: 0.0032 lr: 0.02\n",
      "iteration: 74030 loss: 0.0047 lr: 0.02\n",
      "iteration: 74040 loss: 0.0037 lr: 0.02\n",
      "iteration: 74050 loss: 0.0082 lr: 0.02\n",
      "iteration: 74060 loss: 0.0040 lr: 0.02\n",
      "iteration: 74070 loss: 0.0026 lr: 0.02\n",
      "iteration: 74080 loss: 0.0039 lr: 0.02\n",
      "iteration: 74090 loss: 0.0031 lr: 0.02\n",
      "iteration: 74100 loss: 0.0046 lr: 0.02\n",
      "iteration: 74110 loss: 0.0038 lr: 0.02\n",
      "iteration: 74120 loss: 0.0034 lr: 0.02\n",
      "iteration: 74130 loss: 0.0035 lr: 0.02\n",
      "iteration: 74140 loss: 0.0031 lr: 0.02\n",
      "iteration: 74150 loss: 0.0040 lr: 0.02\n",
      "iteration: 74160 loss: 0.0035 lr: 0.02\n",
      "iteration: 74170 loss: 0.0049 lr: 0.02\n",
      "iteration: 74180 loss: 0.0029 lr: 0.02\n",
      "iteration: 74190 loss: 0.0036 lr: 0.02\n",
      "iteration: 74200 loss: 0.0052 lr: 0.02\n",
      "iteration: 74210 loss: 0.0038 lr: 0.02\n",
      "iteration: 74220 loss: 0.0051 lr: 0.02\n",
      "iteration: 74230 loss: 0.0031 lr: 0.02\n",
      "iteration: 74240 loss: 0.0035 lr: 0.02\n",
      "iteration: 74250 loss: 0.0045 lr: 0.02\n",
      "iteration: 74260 loss: 0.0049 lr: 0.02\n",
      "iteration: 74270 loss: 0.0048 lr: 0.02\n",
      "iteration: 74280 loss: 0.0040 lr: 0.02\n",
      "iteration: 74290 loss: 0.0032 lr: 0.02\n",
      "iteration: 74300 loss: 0.0051 lr: 0.02\n",
      "iteration: 74310 loss: 0.0036 lr: 0.02\n",
      "iteration: 74320 loss: 0.0041 lr: 0.02\n",
      "iteration: 74330 loss: 0.0032 lr: 0.02\n",
      "iteration: 74340 loss: 0.0047 lr: 0.02\n",
      "iteration: 74350 loss: 0.0044 lr: 0.02\n",
      "iteration: 74360 loss: 0.0050 lr: 0.02\n",
      "iteration: 74370 loss: 0.0056 lr: 0.02\n",
      "iteration: 74380 loss: 0.0043 lr: 0.02\n",
      "iteration: 74390 loss: 0.0064 lr: 0.02\n",
      "iteration: 74400 loss: 0.0032 lr: 0.02\n",
      "iteration: 74410 loss: 0.0039 lr: 0.02\n",
      "iteration: 74420 loss: 0.0032 lr: 0.02\n",
      "iteration: 74430 loss: 0.0042 lr: 0.02\n",
      "iteration: 74440 loss: 0.0025 lr: 0.02\n",
      "iteration: 74450 loss: 0.0038 lr: 0.02\n",
      "iteration: 74460 loss: 0.0048 lr: 0.02\n",
      "iteration: 74470 loss: 0.0050 lr: 0.02\n",
      "iteration: 74480 loss: 0.0048 lr: 0.02\n",
      "iteration: 74490 loss: 0.0042 lr: 0.02\n",
      "iteration: 74500 loss: 0.0037 lr: 0.02\n",
      "iteration: 74510 loss: 0.0046 lr: 0.02\n",
      "iteration: 74520 loss: 0.0035 lr: 0.02\n",
      "iteration: 74530 loss: 0.0038 lr: 0.02\n",
      "iteration: 74540 loss: 0.0029 lr: 0.02\n",
      "iteration: 74550 loss: 0.0051 lr: 0.02\n",
      "iteration: 74560 loss: 0.0044 lr: 0.02\n",
      "iteration: 74570 loss: 0.0034 lr: 0.02\n",
      "iteration: 74580 loss: 0.0062 lr: 0.02\n",
      "iteration: 74590 loss: 0.0043 lr: 0.02\n",
      "iteration: 74600 loss: 0.0039 lr: 0.02\n",
      "iteration: 74610 loss: 0.0046 lr: 0.02\n",
      "iteration: 74620 loss: 0.0041 lr: 0.02\n",
      "iteration: 74630 loss: 0.0044 lr: 0.02\n",
      "iteration: 74640 loss: 0.0043 lr: 0.02\n",
      "iteration: 74650 loss: 0.0031 lr: 0.02\n",
      "iteration: 74660 loss: 0.0047 lr: 0.02\n",
      "iteration: 74670 loss: 0.0047 lr: 0.02\n",
      "iteration: 74680 loss: 0.0043 lr: 0.02\n",
      "iteration: 74690 loss: 0.0052 lr: 0.02\n",
      "iteration: 74700 loss: 0.0038 lr: 0.02\n",
      "iteration: 74710 loss: 0.0036 lr: 0.02\n",
      "iteration: 74720 loss: 0.0042 lr: 0.02\n",
      "iteration: 74730 loss: 0.0035 lr: 0.02\n",
      "iteration: 74740 loss: 0.0035 lr: 0.02\n",
      "iteration: 74750 loss: 0.0029 lr: 0.02\n",
      "iteration: 74760 loss: 0.0039 lr: 0.02\n",
      "iteration: 74770 loss: 0.0029 lr: 0.02\n",
      "iteration: 74780 loss: 0.0034 lr: 0.02\n",
      "iteration: 74790 loss: 0.0031 lr: 0.02\n",
      "iteration: 74800 loss: 0.0053 lr: 0.02\n",
      "iteration: 74810 loss: 0.0034 lr: 0.02\n",
      "iteration: 74820 loss: 0.0048 lr: 0.02\n",
      "iteration: 74830 loss: 0.0047 lr: 0.02\n",
      "iteration: 74840 loss: 0.0037 lr: 0.02\n",
      "iteration: 74850 loss: 0.0030 lr: 0.02\n",
      "iteration: 74860 loss: 0.0035 lr: 0.02\n",
      "iteration: 74870 loss: 0.0037 lr: 0.02\n",
      "iteration: 74880 loss: 0.0038 lr: 0.02\n",
      "iteration: 74890 loss: 0.0042 lr: 0.02\n",
      "iteration: 74900 loss: 0.0060 lr: 0.02\n",
      "iteration: 74910 loss: 0.0049 lr: 0.02\n",
      "iteration: 74920 loss: 0.0031 lr: 0.02\n",
      "iteration: 74930 loss: 0.0034 lr: 0.02\n",
      "iteration: 74940 loss: 0.0039 lr: 0.02\n",
      "iteration: 74950 loss: 0.0044 lr: 0.02\n",
      "iteration: 74960 loss: 0.0047 lr: 0.02\n",
      "iteration: 74970 loss: 0.0061 lr: 0.02\n",
      "iteration: 74980 loss: 0.0036 lr: 0.02\n",
      "iteration: 74990 loss: 0.0033 lr: 0.02\n",
      "iteration: 75000 loss: 0.0042 lr: 0.02\n",
      "iteration: 75010 loss: 0.0033 lr: 0.02\n",
      "iteration: 75020 loss: 0.0031 lr: 0.02\n",
      "iteration: 75030 loss: 0.0029 lr: 0.02\n",
      "iteration: 75040 loss: 0.0037 lr: 0.02\n",
      "iteration: 75050 loss: 0.0028 lr: 0.02\n",
      "iteration: 75060 loss: 0.0046 lr: 0.02\n",
      "iteration: 75070 loss: 0.0040 lr: 0.02\n",
      "iteration: 75080 loss: 0.0043 lr: 0.02\n",
      "iteration: 75090 loss: 0.0045 lr: 0.02\n",
      "iteration: 75100 loss: 0.0039 lr: 0.02\n",
      "iteration: 75110 loss: 0.0035 lr: 0.02\n",
      "iteration: 75120 loss: 0.0043 lr: 0.02\n",
      "iteration: 75130 loss: 0.0039 lr: 0.02\n",
      "iteration: 75140 loss: 0.0035 lr: 0.02\n",
      "iteration: 75150 loss: 0.0050 lr: 0.02\n",
      "iteration: 75160 loss: 0.0039 lr: 0.02\n",
      "iteration: 75170 loss: 0.0050 lr: 0.02\n",
      "iteration: 75180 loss: 0.0037 lr: 0.02\n",
      "iteration: 75190 loss: 0.0039 lr: 0.02\n",
      "iteration: 75200 loss: 0.0048 lr: 0.02\n",
      "iteration: 75210 loss: 0.0046 lr: 0.02\n",
      "iteration: 75220 loss: 0.0032 lr: 0.02\n",
      "iteration: 75230 loss: 0.0032 lr: 0.02\n",
      "iteration: 75240 loss: 0.0045 lr: 0.02\n",
      "iteration: 75250 loss: 0.0033 lr: 0.02\n",
      "iteration: 75260 loss: 0.0043 lr: 0.02\n",
      "iteration: 75270 loss: 0.0040 lr: 0.02\n",
      "iteration: 75280 loss: 0.0036 lr: 0.02\n",
      "iteration: 75290 loss: 0.0046 lr: 0.02\n",
      "iteration: 75300 loss: 0.0030 lr: 0.02\n",
      "iteration: 75310 loss: 0.0058 lr: 0.02\n",
      "iteration: 75320 loss: 0.0030 lr: 0.02\n",
      "iteration: 75330 loss: 0.0036 lr: 0.02\n",
      "iteration: 75340 loss: 0.0048 lr: 0.02\n",
      "iteration: 75350 loss: 0.0036 lr: 0.02\n",
      "iteration: 75360 loss: 0.0036 lr: 0.02\n",
      "iteration: 75370 loss: 0.0038 lr: 0.02\n",
      "iteration: 75380 loss: 0.0033 lr: 0.02\n",
      "iteration: 75390 loss: 0.0037 lr: 0.02\n",
      "iteration: 75400 loss: 0.0041 lr: 0.02\n",
      "iteration: 75410 loss: 0.0033 lr: 0.02\n",
      "iteration: 75420 loss: 0.0038 lr: 0.02\n",
      "iteration: 75430 loss: 0.0030 lr: 0.02\n",
      "iteration: 75440 loss: 0.0032 lr: 0.02\n",
      "iteration: 75450 loss: 0.0037 lr: 0.02\n",
      "iteration: 75460 loss: 0.0032 lr: 0.02\n",
      "iteration: 75470 loss: 0.0050 lr: 0.02\n",
      "iteration: 75480 loss: 0.0032 lr: 0.02\n",
      "iteration: 75490 loss: 0.0041 lr: 0.02\n",
      "iteration: 75500 loss: 0.0061 lr: 0.02\n",
      "iteration: 75510 loss: 0.0043 lr: 0.02\n",
      "iteration: 75520 loss: 0.0025 lr: 0.02\n",
      "iteration: 75530 loss: 0.0047 lr: 0.02\n",
      "iteration: 75540 loss: 0.0038 lr: 0.02\n",
      "iteration: 75550 loss: 0.0037 lr: 0.02\n",
      "iteration: 75560 loss: 0.0032 lr: 0.02\n",
      "iteration: 75570 loss: 0.0045 lr: 0.02\n",
      "iteration: 75580 loss: 0.0033 lr: 0.02\n",
      "iteration: 75590 loss: 0.0034 lr: 0.02\n",
      "iteration: 75600 loss: 0.0029 lr: 0.02\n",
      "iteration: 75610 loss: 0.0043 lr: 0.02\n",
      "iteration: 75620 loss: 0.0031 lr: 0.02\n",
      "iteration: 75630 loss: 0.0034 lr: 0.02\n",
      "iteration: 75640 loss: 0.0036 lr: 0.02\n",
      "iteration: 75650 loss: 0.0046 lr: 0.02\n",
      "iteration: 75660 loss: 0.0038 lr: 0.02\n",
      "iteration: 75670 loss: 0.0036 lr: 0.02\n",
      "iteration: 75680 loss: 0.0047 lr: 0.02\n",
      "iteration: 75690 loss: 0.0041 lr: 0.02\n",
      "iteration: 75700 loss: 0.0044 lr: 0.02\n",
      "iteration: 75710 loss: 0.0038 lr: 0.02\n",
      "iteration: 75720 loss: 0.0037 lr: 0.02\n",
      "iteration: 75730 loss: 0.0027 lr: 0.02\n",
      "iteration: 75740 loss: 0.0037 lr: 0.02\n",
      "iteration: 75750 loss: 0.0041 lr: 0.02\n",
      "iteration: 75760 loss: 0.0048 lr: 0.02\n",
      "iteration: 75770 loss: 0.0034 lr: 0.02\n",
      "iteration: 75780 loss: 0.0044 lr: 0.02\n",
      "iteration: 75790 loss: 0.0030 lr: 0.02\n",
      "iteration: 75800 loss: 0.0035 lr: 0.02\n",
      "iteration: 75810 loss: 0.0034 lr: 0.02\n",
      "iteration: 75820 loss: 0.0070 lr: 0.02\n",
      "iteration: 75830 loss: 0.0043 lr: 0.02\n",
      "iteration: 75840 loss: 0.0042 lr: 0.02\n",
      "iteration: 75850 loss: 0.0027 lr: 0.02\n",
      "iteration: 75860 loss: 0.0033 lr: 0.02\n",
      "iteration: 75870 loss: 0.0027 lr: 0.02\n",
      "iteration: 75880 loss: 0.0044 lr: 0.02\n",
      "iteration: 75890 loss: 0.0044 lr: 0.02\n",
      "iteration: 75900 loss: 0.0037 lr: 0.02\n",
      "iteration: 75910 loss: 0.0035 lr: 0.02\n",
      "iteration: 75920 loss: 0.0059 lr: 0.02\n",
      "iteration: 75930 loss: 0.0037 lr: 0.02\n",
      "iteration: 75940 loss: 0.0045 lr: 0.02\n",
      "iteration: 75950 loss: 0.0040 lr: 0.02\n",
      "iteration: 75960 loss: 0.0033 lr: 0.02\n",
      "iteration: 75970 loss: 0.0040 lr: 0.02\n",
      "iteration: 75980 loss: 0.0035 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 75990 loss: 0.0026 lr: 0.02\n",
      "iteration: 76000 loss: 0.0030 lr: 0.02\n",
      "iteration: 76010 loss: 0.0042 lr: 0.02\n",
      "iteration: 76020 loss: 0.0035 lr: 0.02\n",
      "iteration: 76030 loss: 0.0042 lr: 0.02\n",
      "iteration: 76040 loss: 0.0039 lr: 0.02\n",
      "iteration: 76050 loss: 0.0045 lr: 0.02\n",
      "iteration: 76060 loss: 0.0042 lr: 0.02\n",
      "iteration: 76070 loss: 0.0035 lr: 0.02\n",
      "iteration: 76080 loss: 0.0064 lr: 0.02\n",
      "iteration: 76090 loss: 0.0038 lr: 0.02\n",
      "iteration: 76100 loss: 0.0047 lr: 0.02\n",
      "iteration: 76110 loss: 0.0039 lr: 0.02\n",
      "iteration: 76120 loss: 0.0047 lr: 0.02\n",
      "iteration: 76130 loss: 0.0042 lr: 0.02\n",
      "iteration: 76140 loss: 0.0044 lr: 0.02\n",
      "iteration: 76150 loss: 0.0047 lr: 0.02\n",
      "iteration: 76160 loss: 0.0058 lr: 0.02\n",
      "iteration: 76170 loss: 0.0034 lr: 0.02\n",
      "iteration: 76180 loss: 0.0059 lr: 0.02\n",
      "iteration: 76190 loss: 0.0039 lr: 0.02\n",
      "iteration: 76200 loss: 0.0041 lr: 0.02\n",
      "iteration: 76210 loss: 0.0023 lr: 0.02\n",
      "iteration: 76220 loss: 0.0032 lr: 0.02\n",
      "iteration: 76230 loss: 0.0029 lr: 0.02\n",
      "iteration: 76240 loss: 0.0027 lr: 0.02\n",
      "iteration: 76250 loss: 0.0040 lr: 0.02\n",
      "iteration: 76260 loss: 0.0031 lr: 0.02\n",
      "iteration: 76270 loss: 0.0034 lr: 0.02\n",
      "iteration: 76280 loss: 0.0039 lr: 0.02\n",
      "iteration: 76290 loss: 0.0045 lr: 0.02\n",
      "iteration: 76300 loss: 0.0047 lr: 0.02\n",
      "iteration: 76310 loss: 0.0031 lr: 0.02\n",
      "iteration: 76320 loss: 0.0039 lr: 0.02\n",
      "iteration: 76330 loss: 0.0042 lr: 0.02\n",
      "iteration: 76340 loss: 0.0037 lr: 0.02\n",
      "iteration: 76350 loss: 0.0032 lr: 0.02\n",
      "iteration: 76360 loss: 0.0027 lr: 0.02\n",
      "iteration: 76370 loss: 0.0039 lr: 0.02\n",
      "iteration: 76380 loss: 0.0041 lr: 0.02\n",
      "iteration: 76390 loss: 0.0055 lr: 0.02\n",
      "iteration: 76400 loss: 0.0055 lr: 0.02\n",
      "iteration: 76410 loss: 0.0042 lr: 0.02\n",
      "iteration: 76420 loss: 0.0034 lr: 0.02\n",
      "iteration: 76430 loss: 0.0034 lr: 0.02\n",
      "iteration: 76440 loss: 0.0038 lr: 0.02\n",
      "iteration: 76450 loss: 0.0039 lr: 0.02\n",
      "iteration: 76460 loss: 0.0029 lr: 0.02\n",
      "iteration: 76470 loss: 0.0044 lr: 0.02\n",
      "iteration: 76480 loss: 0.0034 lr: 0.02\n",
      "iteration: 76490 loss: 0.0036 lr: 0.02\n",
      "iteration: 76500 loss: 0.0065 lr: 0.02\n",
      "iteration: 76510 loss: 0.0038 lr: 0.02\n",
      "iteration: 76520 loss: 0.0041 lr: 0.02\n",
      "iteration: 76530 loss: 0.0035 lr: 0.02\n",
      "iteration: 76540 loss: 0.0042 lr: 0.02\n",
      "iteration: 76550 loss: 0.0044 lr: 0.02\n",
      "iteration: 76560 loss: 0.0034 lr: 0.02\n",
      "iteration: 76570 loss: 0.0036 lr: 0.02\n",
      "iteration: 76580 loss: 0.0031 lr: 0.02\n",
      "iteration: 76590 loss: 0.0036 lr: 0.02\n",
      "iteration: 76600 loss: 0.0036 lr: 0.02\n",
      "iteration: 76610 loss: 0.0036 lr: 0.02\n",
      "iteration: 76620 loss: 0.0027 lr: 0.02\n",
      "iteration: 76630 loss: 0.0044 lr: 0.02\n",
      "iteration: 76640 loss: 0.0042 lr: 0.02\n",
      "iteration: 76650 loss: 0.0038 lr: 0.02\n",
      "iteration: 76660 loss: 0.0038 lr: 0.02\n",
      "iteration: 76670 loss: 0.0028 lr: 0.02\n",
      "iteration: 76680 loss: 0.0040 lr: 0.02\n",
      "iteration: 76690 loss: 0.0040 lr: 0.02\n",
      "iteration: 76700 loss: 0.0042 lr: 0.02\n",
      "iteration: 76710 loss: 0.0046 lr: 0.02\n",
      "iteration: 76720 loss: 0.0038 lr: 0.02\n",
      "iteration: 76730 loss: 0.0031 lr: 0.02\n",
      "iteration: 76740 loss: 0.0042 lr: 0.02\n",
      "iteration: 76750 loss: 0.0030 lr: 0.02\n",
      "iteration: 76760 loss: 0.0040 lr: 0.02\n",
      "iteration: 76770 loss: 0.0041 lr: 0.02\n",
      "iteration: 76780 loss: 0.0048 lr: 0.02\n",
      "iteration: 76790 loss: 0.0033 lr: 0.02\n",
      "iteration: 76800 loss: 0.0042 lr: 0.02\n",
      "iteration: 76810 loss: 0.0045 lr: 0.02\n",
      "iteration: 76820 loss: 0.0034 lr: 0.02\n",
      "iteration: 76830 loss: 0.0037 lr: 0.02\n",
      "iteration: 76840 loss: 0.0031 lr: 0.02\n",
      "iteration: 76850 loss: 0.0037 lr: 0.02\n",
      "iteration: 76860 loss: 0.0031 lr: 0.02\n",
      "iteration: 76870 loss: 0.0050 lr: 0.02\n",
      "iteration: 76880 loss: 0.0043 lr: 0.02\n",
      "iteration: 76890 loss: 0.0031 lr: 0.02\n",
      "iteration: 76900 loss: 0.0027 lr: 0.02\n",
      "iteration: 76910 loss: 0.0059 lr: 0.02\n",
      "iteration: 76920 loss: 0.0053 lr: 0.02\n",
      "iteration: 76930 loss: 0.0028 lr: 0.02\n",
      "iteration: 76940 loss: 0.0031 lr: 0.02\n",
      "iteration: 76950 loss: 0.0034 lr: 0.02\n",
      "iteration: 76960 loss: 0.0035 lr: 0.02\n",
      "iteration: 76970 loss: 0.0041 lr: 0.02\n",
      "iteration: 76980 loss: 0.0031 lr: 0.02\n",
      "iteration: 76990 loss: 0.0042 lr: 0.02\n",
      "iteration: 77000 loss: 0.0041 lr: 0.02\n",
      "iteration: 77010 loss: 0.0046 lr: 0.02\n",
      "iteration: 77020 loss: 0.0042 lr: 0.02\n",
      "iteration: 77030 loss: 0.0028 lr: 0.02\n",
      "iteration: 77040 loss: 0.0034 lr: 0.02\n",
      "iteration: 77050 loss: 0.0042 lr: 0.02\n",
      "iteration: 77060 loss: 0.0032 lr: 0.02\n",
      "iteration: 77070 loss: 0.0041 lr: 0.02\n",
      "iteration: 77080 loss: 0.0028 lr: 0.02\n",
      "iteration: 77090 loss: 0.0037 lr: 0.02\n",
      "iteration: 77100 loss: 0.0044 lr: 0.02\n",
      "iteration: 77110 loss: 0.0034 lr: 0.02\n",
      "iteration: 77120 loss: 0.0036 lr: 0.02\n",
      "iteration: 77130 loss: 0.0040 lr: 0.02\n",
      "iteration: 77140 loss: 0.0041 lr: 0.02\n",
      "iteration: 77150 loss: 0.0039 lr: 0.02\n",
      "iteration: 77160 loss: 0.0049 lr: 0.02\n",
      "iteration: 77170 loss: 0.0041 lr: 0.02\n",
      "iteration: 77180 loss: 0.0038 lr: 0.02\n",
      "iteration: 77190 loss: 0.0039 lr: 0.02\n",
      "iteration: 77200 loss: 0.0034 lr: 0.02\n",
      "iteration: 77210 loss: 0.0047 lr: 0.02\n",
      "iteration: 77220 loss: 0.0042 lr: 0.02\n",
      "iteration: 77230 loss: 0.0035 lr: 0.02\n",
      "iteration: 77240 loss: 0.0030 lr: 0.02\n",
      "iteration: 77250 loss: 0.0044 lr: 0.02\n",
      "iteration: 77260 loss: 0.0042 lr: 0.02\n",
      "iteration: 77270 loss: 0.0032 lr: 0.02\n",
      "iteration: 77280 loss: 0.0039 lr: 0.02\n",
      "iteration: 77290 loss: 0.0043 lr: 0.02\n",
      "iteration: 77300 loss: 0.0029 lr: 0.02\n",
      "iteration: 77310 loss: 0.0044 lr: 0.02\n",
      "iteration: 77320 loss: 0.0055 lr: 0.02\n",
      "iteration: 77330 loss: 0.0047 lr: 0.02\n",
      "iteration: 77340 loss: 0.0052 lr: 0.02\n",
      "iteration: 77350 loss: 0.0032 lr: 0.02\n",
      "iteration: 77360 loss: 0.0053 lr: 0.02\n",
      "iteration: 77370 loss: 0.0034 lr: 0.02\n",
      "iteration: 77380 loss: 0.0042 lr: 0.02\n",
      "iteration: 77390 loss: 0.0032 lr: 0.02\n",
      "iteration: 77400 loss: 0.0040 lr: 0.02\n",
      "iteration: 77410 loss: 0.0034 lr: 0.02\n",
      "iteration: 77420 loss: 0.0031 lr: 0.02\n",
      "iteration: 77430 loss: 0.0037 lr: 0.02\n",
      "iteration: 77440 loss: 0.0033 lr: 0.02\n",
      "iteration: 77450 loss: 0.0054 lr: 0.02\n",
      "iteration: 77460 loss: 0.0046 lr: 0.02\n",
      "iteration: 77470 loss: 0.0027 lr: 0.02\n",
      "iteration: 77480 loss: 0.0037 lr: 0.02\n",
      "iteration: 77490 loss: 0.0039 lr: 0.02\n",
      "iteration: 77500 loss: 0.0040 lr: 0.02\n",
      "iteration: 77510 loss: 0.0038 lr: 0.02\n",
      "iteration: 77520 loss: 0.0040 lr: 0.02\n",
      "iteration: 77530 loss: 0.0028 lr: 0.02\n",
      "iteration: 77540 loss: 0.0034 lr: 0.02\n",
      "iteration: 77550 loss: 0.0041 lr: 0.02\n",
      "iteration: 77560 loss: 0.0028 lr: 0.02\n",
      "iteration: 77570 loss: 0.0035 lr: 0.02\n",
      "iteration: 77580 loss: 0.0035 lr: 0.02\n",
      "iteration: 77590 loss: 0.0038 lr: 0.02\n",
      "iteration: 77600 loss: 0.0025 lr: 0.02\n",
      "iteration: 77610 loss: 0.0036 lr: 0.02\n",
      "iteration: 77620 loss: 0.0044 lr: 0.02\n",
      "iteration: 77630 loss: 0.0045 lr: 0.02\n",
      "iteration: 77640 loss: 0.0037 lr: 0.02\n",
      "iteration: 77650 loss: 0.0035 lr: 0.02\n",
      "iteration: 77660 loss: 0.0048 lr: 0.02\n",
      "iteration: 77670 loss: 0.0033 lr: 0.02\n",
      "iteration: 77680 loss: 0.0042 lr: 0.02\n",
      "iteration: 77690 loss: 0.0044 lr: 0.02\n",
      "iteration: 77700 loss: 0.0035 lr: 0.02\n",
      "iteration: 77710 loss: 0.0040 lr: 0.02\n",
      "iteration: 77720 loss: 0.0032 lr: 0.02\n",
      "iteration: 77730 loss: 0.0047 lr: 0.02\n",
      "iteration: 77740 loss: 0.0032 lr: 0.02\n",
      "iteration: 77750 loss: 0.0029 lr: 0.02\n",
      "iteration: 77760 loss: 0.0045 lr: 0.02\n",
      "iteration: 77770 loss: 0.0051 lr: 0.02\n",
      "iteration: 77780 loss: 0.0033 lr: 0.02\n",
      "iteration: 77790 loss: 0.0051 lr: 0.02\n",
      "iteration: 77800 loss: 0.0039 lr: 0.02\n",
      "iteration: 77810 loss: 0.0037 lr: 0.02\n",
      "iteration: 77820 loss: 0.0031 lr: 0.02\n",
      "iteration: 77830 loss: 0.0030 lr: 0.02\n",
      "iteration: 77840 loss: 0.0053 lr: 0.02\n",
      "iteration: 77850 loss: 0.0032 lr: 0.02\n",
      "iteration: 77860 loss: 0.0046 lr: 0.02\n",
      "iteration: 77870 loss: 0.0032 lr: 0.02\n",
      "iteration: 77880 loss: 0.0040 lr: 0.02\n",
      "iteration: 77890 loss: 0.0031 lr: 0.02\n",
      "iteration: 77900 loss: 0.0027 lr: 0.02\n",
      "iteration: 77910 loss: 0.0036 lr: 0.02\n",
      "iteration: 77920 loss: 0.0041 lr: 0.02\n",
      "iteration: 77930 loss: 0.0032 lr: 0.02\n",
      "iteration: 77940 loss: 0.0043 lr: 0.02\n",
      "iteration: 77950 loss: 0.0038 lr: 0.02\n",
      "iteration: 77960 loss: 0.0031 lr: 0.02\n",
      "iteration: 77970 loss: 0.0034 lr: 0.02\n",
      "iteration: 77980 loss: 0.0034 lr: 0.02\n",
      "iteration: 77990 loss: 0.0027 lr: 0.02\n",
      "iteration: 78000 loss: 0.0040 lr: 0.02\n",
      "iteration: 78010 loss: 0.0042 lr: 0.02\n",
      "iteration: 78020 loss: 0.0033 lr: 0.02\n",
      "iteration: 78030 loss: 0.0033 lr: 0.02\n",
      "iteration: 78040 loss: 0.0031 lr: 0.02\n",
      "iteration: 78050 loss: 0.0041 lr: 0.02\n",
      "iteration: 78060 loss: 0.0035 lr: 0.02\n",
      "iteration: 78070 loss: 0.0034 lr: 0.02\n",
      "iteration: 78080 loss: 0.0034 lr: 0.02\n",
      "iteration: 78090 loss: 0.0049 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 78100 loss: 0.0030 lr: 0.02\n",
      "iteration: 78110 loss: 0.0038 lr: 0.02\n",
      "iteration: 78120 loss: 0.0046 lr: 0.02\n",
      "iteration: 78130 loss: 0.0041 lr: 0.02\n",
      "iteration: 78140 loss: 0.0029 lr: 0.02\n",
      "iteration: 78150 loss: 0.0034 lr: 0.02\n",
      "iteration: 78160 loss: 0.0026 lr: 0.02\n",
      "iteration: 78170 loss: 0.0056 lr: 0.02\n",
      "iteration: 78180 loss: 0.0044 lr: 0.02\n",
      "iteration: 78190 loss: 0.0035 lr: 0.02\n",
      "iteration: 78200 loss: 0.0036 lr: 0.02\n",
      "iteration: 78210 loss: 0.0035 lr: 0.02\n",
      "iteration: 78220 loss: 0.0031 lr: 0.02\n",
      "iteration: 78230 loss: 0.0036 lr: 0.02\n",
      "iteration: 78240 loss: 0.0043 lr: 0.02\n",
      "iteration: 78250 loss: 0.0034 lr: 0.02\n",
      "iteration: 78260 loss: 0.0052 lr: 0.02\n",
      "iteration: 78270 loss: 0.0037 lr: 0.02\n",
      "iteration: 78280 loss: 0.0026 lr: 0.02\n",
      "iteration: 78290 loss: 0.0036 lr: 0.02\n",
      "iteration: 78300 loss: 0.0025 lr: 0.02\n",
      "iteration: 78310 loss: 0.0041 lr: 0.02\n",
      "iteration: 78320 loss: 0.0035 lr: 0.02\n",
      "iteration: 78330 loss: 0.0030 lr: 0.02\n",
      "iteration: 78340 loss: 0.0031 lr: 0.02\n",
      "iteration: 78350 loss: 0.0032 lr: 0.02\n",
      "iteration: 78360 loss: 0.0036 lr: 0.02\n",
      "iteration: 78370 loss: 0.0036 lr: 0.02\n",
      "iteration: 78380 loss: 0.0029 lr: 0.02\n",
      "iteration: 78390 loss: 0.0039 lr: 0.02\n",
      "iteration: 78400 loss: 0.0050 lr: 0.02\n",
      "iteration: 78410 loss: 0.0053 lr: 0.02\n",
      "iteration: 78420 loss: 0.0034 lr: 0.02\n",
      "iteration: 78430 loss: 0.0044 lr: 0.02\n",
      "iteration: 78440 loss: 0.0063 lr: 0.02\n",
      "iteration: 78450 loss: 0.0045 lr: 0.02\n",
      "iteration: 78460 loss: 0.0039 lr: 0.02\n",
      "iteration: 78470 loss: 0.0035 lr: 0.02\n",
      "iteration: 78480 loss: 0.0034 lr: 0.02\n",
      "iteration: 78490 loss: 0.0028 lr: 0.02\n",
      "iteration: 78500 loss: 0.0033 lr: 0.02\n",
      "iteration: 78510 loss: 0.0035 lr: 0.02\n",
      "iteration: 78520 loss: 0.0047 lr: 0.02\n",
      "iteration: 78530 loss: 0.0051 lr: 0.02\n",
      "iteration: 78540 loss: 0.0036 lr: 0.02\n",
      "iteration: 78550 loss: 0.0044 lr: 0.02\n",
      "iteration: 78560 loss: 0.0049 lr: 0.02\n",
      "iteration: 78570 loss: 0.0025 lr: 0.02\n",
      "iteration: 78580 loss: 0.0035 lr: 0.02\n",
      "iteration: 78590 loss: 0.0037 lr: 0.02\n",
      "iteration: 78600 loss: 0.0029 lr: 0.02\n",
      "iteration: 78610 loss: 0.0052 lr: 0.02\n",
      "iteration: 78620 loss: 0.0041 lr: 0.02\n",
      "iteration: 78630 loss: 0.0035 lr: 0.02\n",
      "iteration: 78640 loss: 0.0038 lr: 0.02\n",
      "iteration: 78650 loss: 0.0053 lr: 0.02\n",
      "iteration: 78660 loss: 0.0044 lr: 0.02\n",
      "iteration: 78670 loss: 0.0042 lr: 0.02\n",
      "iteration: 78680 loss: 0.0043 lr: 0.02\n",
      "iteration: 78690 loss: 0.0034 lr: 0.02\n",
      "iteration: 78700 loss: 0.0031 lr: 0.02\n",
      "iteration: 78710 loss: 0.0036 lr: 0.02\n",
      "iteration: 78720 loss: 0.0038 lr: 0.02\n",
      "iteration: 78730 loss: 0.0032 lr: 0.02\n",
      "iteration: 78740 loss: 0.0041 lr: 0.02\n",
      "iteration: 78750 loss: 0.0033 lr: 0.02\n",
      "iteration: 78760 loss: 0.0038 lr: 0.02\n",
      "iteration: 78770 loss: 0.0049 lr: 0.02\n",
      "iteration: 78780 loss: 0.0043 lr: 0.02\n",
      "iteration: 78790 loss: 0.0032 lr: 0.02\n",
      "iteration: 78800 loss: 0.0042 lr: 0.02\n",
      "iteration: 78810 loss: 0.0034 lr: 0.02\n",
      "iteration: 78820 loss: 0.0054 lr: 0.02\n",
      "iteration: 78830 loss: 0.0039 lr: 0.02\n",
      "iteration: 78840 loss: 0.0039 lr: 0.02\n",
      "iteration: 78850 loss: 0.0044 lr: 0.02\n",
      "iteration: 78860 loss: 0.0042 lr: 0.02\n",
      "iteration: 78870 loss: 0.0041 lr: 0.02\n",
      "iteration: 78880 loss: 0.0033 lr: 0.02\n",
      "iteration: 78890 loss: 0.0034 lr: 0.02\n",
      "iteration: 78900 loss: 0.0049 lr: 0.02\n",
      "iteration: 78910 loss: 0.0043 lr: 0.02\n",
      "iteration: 78920 loss: 0.0041 lr: 0.02\n",
      "iteration: 78930 loss: 0.0059 lr: 0.02\n",
      "iteration: 78940 loss: 0.0035 lr: 0.02\n",
      "iteration: 78950 loss: 0.0033 lr: 0.02\n",
      "iteration: 78960 loss: 0.0037 lr: 0.02\n",
      "iteration: 78970 loss: 0.0034 lr: 0.02\n",
      "iteration: 78980 loss: 0.0042 lr: 0.02\n",
      "iteration: 78990 loss: 0.0048 lr: 0.02\n",
      "iteration: 79000 loss: 0.0034 lr: 0.02\n",
      "iteration: 79010 loss: 0.0033 lr: 0.02\n",
      "iteration: 79020 loss: 0.0034 lr: 0.02\n",
      "iteration: 79030 loss: 0.0048 lr: 0.02\n",
      "iteration: 79040 loss: 0.0068 lr: 0.02\n",
      "iteration: 79050 loss: 0.0044 lr: 0.02\n",
      "iteration: 79060 loss: 0.0044 lr: 0.02\n",
      "iteration: 79070 loss: 0.0034 lr: 0.02\n",
      "iteration: 79080 loss: 0.0027 lr: 0.02\n",
      "iteration: 79090 loss: 0.0049 lr: 0.02\n",
      "iteration: 79100 loss: 0.0031 lr: 0.02\n",
      "iteration: 79110 loss: 0.0037 lr: 0.02\n",
      "iteration: 79120 loss: 0.0037 lr: 0.02\n",
      "iteration: 79130 loss: 0.0028 lr: 0.02\n",
      "iteration: 79140 loss: 0.0028 lr: 0.02\n",
      "iteration: 79150 loss: 0.0036 lr: 0.02\n",
      "iteration: 79160 loss: 0.0028 lr: 0.02\n",
      "iteration: 79170 loss: 0.0030 lr: 0.02\n",
      "iteration: 79180 loss: 0.0041 lr: 0.02\n",
      "iteration: 79190 loss: 0.0037 lr: 0.02\n",
      "iteration: 79200 loss: 0.0037 lr: 0.02\n",
      "iteration: 79210 loss: 0.0052 lr: 0.02\n",
      "iteration: 79220 loss: 0.0039 lr: 0.02\n",
      "iteration: 79230 loss: 0.0039 lr: 0.02\n",
      "iteration: 79240 loss: 0.0027 lr: 0.02\n",
      "iteration: 79250 loss: 0.0037 lr: 0.02\n",
      "iteration: 79260 loss: 0.0047 lr: 0.02\n",
      "iteration: 79270 loss: 0.0049 lr: 0.02\n",
      "iteration: 79280 loss: 0.0053 lr: 0.02\n",
      "iteration: 79290 loss: 0.0040 lr: 0.02\n",
      "iteration: 79300 loss: 0.0029 lr: 0.02\n",
      "iteration: 79310 loss: 0.0063 lr: 0.02\n",
      "iteration: 79320 loss: 0.0047 lr: 0.02\n",
      "iteration: 79330 loss: 0.0043 lr: 0.02\n",
      "iteration: 79340 loss: 0.0075 lr: 0.02\n",
      "iteration: 79350 loss: 0.0032 lr: 0.02\n",
      "iteration: 79360 loss: 0.0031 lr: 0.02\n",
      "iteration: 79370 loss: 0.0037 lr: 0.02\n",
      "iteration: 79380 loss: 0.0023 lr: 0.02\n",
      "iteration: 79390 loss: 0.0030 lr: 0.02\n",
      "iteration: 79400 loss: 0.0035 lr: 0.02\n",
      "iteration: 79410 loss: 0.0039 lr: 0.02\n",
      "iteration: 79420 loss: 0.0034 lr: 0.02\n",
      "iteration: 79430 loss: 0.0030 lr: 0.02\n",
      "iteration: 79440 loss: 0.0032 lr: 0.02\n",
      "iteration: 79450 loss: 0.0028 lr: 0.02\n",
      "iteration: 79460 loss: 0.0063 lr: 0.02\n",
      "iteration: 79470 loss: 0.0041 lr: 0.02\n",
      "iteration: 79480 loss: 0.0042 lr: 0.02\n",
      "iteration: 79490 loss: 0.0033 lr: 0.02\n",
      "iteration: 79500 loss: 0.0040 lr: 0.02\n",
      "iteration: 79510 loss: 0.0040 lr: 0.02\n",
      "iteration: 79520 loss: 0.0032 lr: 0.02\n",
      "iteration: 79530 loss: 0.0032 lr: 0.02\n",
      "iteration: 79540 loss: 0.0027 lr: 0.02\n",
      "iteration: 79550 loss: 0.0034 lr: 0.02\n",
      "iteration: 79560 loss: 0.0065 lr: 0.02\n",
      "iteration: 79570 loss: 0.0039 lr: 0.02\n",
      "iteration: 79580 loss: 0.0035 lr: 0.02\n",
      "iteration: 79590 loss: 0.0032 lr: 0.02\n",
      "iteration: 79600 loss: 0.0033 lr: 0.02\n",
      "iteration: 79610 loss: 0.0028 lr: 0.02\n",
      "iteration: 79620 loss: 0.0034 lr: 0.02\n",
      "iteration: 79630 loss: 0.0033 lr: 0.02\n",
      "iteration: 79640 loss: 0.0040 lr: 0.02\n",
      "iteration: 79650 loss: 0.0036 lr: 0.02\n",
      "iteration: 79660 loss: 0.0038 lr: 0.02\n",
      "iteration: 79670 loss: 0.0045 lr: 0.02\n",
      "iteration: 79680 loss: 0.0032 lr: 0.02\n",
      "iteration: 79690 loss: 0.0057 lr: 0.02\n",
      "iteration: 79700 loss: 0.0052 lr: 0.02\n",
      "iteration: 79710 loss: 0.0050 lr: 0.02\n",
      "iteration: 79720 loss: 0.0056 lr: 0.02\n",
      "iteration: 79730 loss: 0.0031 lr: 0.02\n",
      "iteration: 79740 loss: 0.0034 lr: 0.02\n",
      "iteration: 79750 loss: 0.0035 lr: 0.02\n",
      "iteration: 79760 loss: 0.0039 lr: 0.02\n",
      "iteration: 79770 loss: 0.0036 lr: 0.02\n",
      "iteration: 79780 loss: 0.0035 lr: 0.02\n",
      "iteration: 79790 loss: 0.0039 lr: 0.02\n",
      "iteration: 79800 loss: 0.0037 lr: 0.02\n",
      "iteration: 79810 loss: 0.0040 lr: 0.02\n",
      "iteration: 79820 loss: 0.0036 lr: 0.02\n",
      "iteration: 79830 loss: 0.0044 lr: 0.02\n",
      "iteration: 79840 loss: 0.0053 lr: 0.02\n",
      "iteration: 79850 loss: 0.0041 lr: 0.02\n",
      "iteration: 79860 loss: 0.0049 lr: 0.02\n",
      "iteration: 79870 loss: 0.0059 lr: 0.02\n",
      "iteration: 79880 loss: 0.0049 lr: 0.02\n",
      "iteration: 79890 loss: 0.0033 lr: 0.02\n",
      "iteration: 79900 loss: 0.0038 lr: 0.02\n",
      "iteration: 79910 loss: 0.0033 lr: 0.02\n",
      "iteration: 79920 loss: 0.0038 lr: 0.02\n",
      "iteration: 79930 loss: 0.0036 lr: 0.02\n",
      "iteration: 79940 loss: 0.0050 lr: 0.02\n",
      "iteration: 79950 loss: 0.0032 lr: 0.02\n",
      "iteration: 79960 loss: 0.0028 lr: 0.02\n",
      "iteration: 79970 loss: 0.0035 lr: 0.02\n",
      "iteration: 79980 loss: 0.0034 lr: 0.02\n",
      "iteration: 79990 loss: 0.0051 lr: 0.02\n",
      "iteration: 80000 loss: 0.0031 lr: 0.02\n",
      "iteration: 80010 loss: 0.0028 lr: 0.02\n",
      "iteration: 80020 loss: 0.0035 lr: 0.02\n",
      "iteration: 80030 loss: 0.0035 lr: 0.02\n",
      "iteration: 80040 loss: 0.0035 lr: 0.02\n",
      "iteration: 80050 loss: 0.0038 lr: 0.02\n",
      "iteration: 80060 loss: 0.0050 lr: 0.02\n",
      "iteration: 80070 loss: 0.0026 lr: 0.02\n",
      "iteration: 80080 loss: 0.0032 lr: 0.02\n",
      "iteration: 80090 loss: 0.0035 lr: 0.02\n",
      "iteration: 80100 loss: 0.0033 lr: 0.02\n",
      "iteration: 80110 loss: 0.0031 lr: 0.02\n",
      "iteration: 80120 loss: 0.0031 lr: 0.02\n",
      "iteration: 80130 loss: 0.0044 lr: 0.02\n",
      "iteration: 80140 loss: 0.0038 lr: 0.02\n",
      "iteration: 80150 loss: 0.0032 lr: 0.02\n",
      "iteration: 80160 loss: 0.0036 lr: 0.02\n",
      "iteration: 80170 loss: 0.0026 lr: 0.02\n",
      "iteration: 80180 loss: 0.0035 lr: 0.02\n",
      "iteration: 80190 loss: 0.0044 lr: 0.02\n",
      "iteration: 80200 loss: 0.0022 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 80210 loss: 0.0046 lr: 0.02\n",
      "iteration: 80220 loss: 0.0056 lr: 0.02\n",
      "iteration: 80230 loss: 0.0045 lr: 0.02\n",
      "iteration: 80240 loss: 0.0036 lr: 0.02\n",
      "iteration: 80250 loss: 0.0031 lr: 0.02\n",
      "iteration: 80260 loss: 0.0031 lr: 0.02\n",
      "iteration: 80270 loss: 0.0036 lr: 0.02\n",
      "iteration: 80280 loss: 0.0025 lr: 0.02\n",
      "iteration: 80290 loss: 0.0034 lr: 0.02\n",
      "iteration: 80300 loss: 0.0026 lr: 0.02\n",
      "iteration: 80310 loss: 0.0034 lr: 0.02\n",
      "iteration: 80320 loss: 0.0038 lr: 0.02\n",
      "iteration: 80330 loss: 0.0029 lr: 0.02\n",
      "iteration: 80340 loss: 0.0033 lr: 0.02\n",
      "iteration: 80350 loss: 0.0032 lr: 0.02\n",
      "iteration: 80360 loss: 0.0032 lr: 0.02\n",
      "iteration: 80370 loss: 0.0033 lr: 0.02\n",
      "iteration: 80380 loss: 0.0030 lr: 0.02\n",
      "iteration: 80390 loss: 0.0040 lr: 0.02\n",
      "iteration: 80400 loss: 0.0029 lr: 0.02\n",
      "iteration: 80410 loss: 0.0039 lr: 0.02\n",
      "iteration: 80420 loss: 0.0040 lr: 0.02\n",
      "iteration: 80430 loss: 0.0043 lr: 0.02\n",
      "iteration: 80440 loss: 0.0038 lr: 0.02\n",
      "iteration: 80450 loss: 0.0034 lr: 0.02\n",
      "iteration: 80460 loss: 0.0028 lr: 0.02\n",
      "iteration: 80470 loss: 0.0037 lr: 0.02\n",
      "iteration: 80480 loss: 0.0042 lr: 0.02\n",
      "iteration: 80490 loss: 0.0032 lr: 0.02\n",
      "iteration: 80500 loss: 0.0038 lr: 0.02\n",
      "iteration: 80510 loss: 0.0042 lr: 0.02\n",
      "iteration: 80520 loss: 0.0042 lr: 0.02\n",
      "iteration: 80530 loss: 0.0049 lr: 0.02\n",
      "iteration: 80540 loss: 0.0044 lr: 0.02\n",
      "iteration: 80550 loss: 0.0025 lr: 0.02\n",
      "iteration: 80560 loss: 0.0027 lr: 0.02\n",
      "iteration: 80570 loss: 0.0037 lr: 0.02\n",
      "iteration: 80580 loss: 0.0038 lr: 0.02\n",
      "iteration: 80590 loss: 0.0039 lr: 0.02\n",
      "iteration: 80600 loss: 0.0046 lr: 0.02\n",
      "iteration: 80610 loss: 0.0031 lr: 0.02\n",
      "iteration: 80620 loss: 0.0034 lr: 0.02\n",
      "iteration: 80630 loss: 0.0036 lr: 0.02\n",
      "iteration: 80640 loss: 0.0024 lr: 0.02\n",
      "iteration: 80650 loss: 0.0037 lr: 0.02\n",
      "iteration: 80660 loss: 0.0039 lr: 0.02\n",
      "iteration: 80670 loss: 0.0043 lr: 0.02\n",
      "iteration: 80680 loss: 0.0041 lr: 0.02\n",
      "iteration: 80690 loss: 0.0039 lr: 0.02\n",
      "iteration: 80700 loss: 0.0039 lr: 0.02\n",
      "iteration: 80710 loss: 0.0037 lr: 0.02\n",
      "iteration: 80720 loss: 0.0030 lr: 0.02\n",
      "iteration: 80730 loss: 0.0031 lr: 0.02\n",
      "iteration: 80740 loss: 0.0031 lr: 0.02\n",
      "iteration: 80750 loss: 0.0029 lr: 0.02\n",
      "iteration: 80760 loss: 0.0042 lr: 0.02\n",
      "iteration: 80770 loss: 0.0038 lr: 0.02\n",
      "iteration: 80780 loss: 0.0032 lr: 0.02\n",
      "iteration: 80790 loss: 0.0030 lr: 0.02\n",
      "iteration: 80800 loss: 0.0026 lr: 0.02\n",
      "iteration: 80810 loss: 0.0031 lr: 0.02\n",
      "iteration: 80820 loss: 0.0038 lr: 0.02\n",
      "iteration: 80830 loss: 0.0026 lr: 0.02\n",
      "iteration: 80840 loss: 0.0032 lr: 0.02\n",
      "iteration: 80850 loss: 0.0044 lr: 0.02\n",
      "iteration: 80860 loss: 0.0041 lr: 0.02\n",
      "iteration: 80870 loss: 0.0029 lr: 0.02\n",
      "iteration: 80880 loss: 0.0044 lr: 0.02\n",
      "iteration: 80890 loss: 0.0032 lr: 0.02\n",
      "iteration: 80900 loss: 0.0042 lr: 0.02\n",
      "iteration: 80910 loss: 0.0045 lr: 0.02\n",
      "iteration: 80920 loss: 0.0034 lr: 0.02\n",
      "iteration: 80930 loss: 0.0029 lr: 0.02\n",
      "iteration: 80940 loss: 0.0033 lr: 0.02\n",
      "iteration: 80950 loss: 0.0025 lr: 0.02\n",
      "iteration: 80960 loss: 0.0036 lr: 0.02\n",
      "iteration: 80970 loss: 0.0034 lr: 0.02\n",
      "iteration: 80980 loss: 0.0047 lr: 0.02\n",
      "iteration: 80990 loss: 0.0048 lr: 0.02\n",
      "iteration: 81000 loss: 0.0050 lr: 0.02\n",
      "iteration: 81010 loss: 0.0038 lr: 0.02\n",
      "iteration: 81020 loss: 0.0042 lr: 0.02\n",
      "iteration: 81030 loss: 0.0042 lr: 0.02\n",
      "iteration: 81040 loss: 0.0039 lr: 0.02\n",
      "iteration: 81050 loss: 0.0042 lr: 0.02\n",
      "iteration: 81060 loss: 0.0032 lr: 0.02\n",
      "iteration: 81070 loss: 0.0023 lr: 0.02\n",
      "iteration: 81080 loss: 0.0050 lr: 0.02\n",
      "iteration: 81090 loss: 0.0041 lr: 0.02\n",
      "iteration: 81100 loss: 0.0030 lr: 0.02\n",
      "iteration: 81110 loss: 0.0034 lr: 0.02\n",
      "iteration: 81120 loss: 0.0033 lr: 0.02\n",
      "iteration: 81130 loss: 0.0033 lr: 0.02\n",
      "iteration: 81140 loss: 0.0027 lr: 0.02\n",
      "iteration: 81150 loss: 0.0032 lr: 0.02\n",
      "iteration: 81160 loss: 0.0025 lr: 0.02\n",
      "iteration: 81170 loss: 0.0065 lr: 0.02\n",
      "iteration: 81180 loss: 0.0042 lr: 0.02\n",
      "iteration: 81190 loss: 0.0045 lr: 0.02\n",
      "iteration: 81200 loss: 0.0029 lr: 0.02\n",
      "iteration: 81210 loss: 0.0047 lr: 0.02\n",
      "iteration: 81220 loss: 0.0035 lr: 0.02\n",
      "iteration: 81230 loss: 0.0039 lr: 0.02\n",
      "iteration: 81240 loss: 0.0029 lr: 0.02\n",
      "iteration: 81250 loss: 0.0041 lr: 0.02\n",
      "iteration: 81260 loss: 0.0041 lr: 0.02\n",
      "iteration: 81270 loss: 0.0031 lr: 0.02\n",
      "iteration: 81280 loss: 0.0049 lr: 0.02\n",
      "iteration: 81290 loss: 0.0031 lr: 0.02\n",
      "iteration: 81300 loss: 0.0030 lr: 0.02\n",
      "iteration: 81310 loss: 0.0039 lr: 0.02\n",
      "iteration: 81320 loss: 0.0042 lr: 0.02\n",
      "iteration: 81330 loss: 0.0033 lr: 0.02\n",
      "iteration: 81340 loss: 0.0037 lr: 0.02\n",
      "iteration: 81350 loss: 0.0032 lr: 0.02\n",
      "iteration: 81360 loss: 0.0028 lr: 0.02\n",
      "iteration: 81370 loss: 0.0034 lr: 0.02\n",
      "iteration: 81380 loss: 0.0049 lr: 0.02\n",
      "iteration: 81390 loss: 0.0034 lr: 0.02\n",
      "iteration: 81400 loss: 0.0041 lr: 0.02\n",
      "iteration: 81410 loss: 0.0036 lr: 0.02\n",
      "iteration: 81420 loss: 0.0044 lr: 0.02\n",
      "iteration: 81430 loss: 0.0038 lr: 0.02\n",
      "iteration: 81440 loss: 0.0032 lr: 0.02\n",
      "iteration: 81450 loss: 0.0036 lr: 0.02\n",
      "iteration: 81460 loss: 0.0026 lr: 0.02\n",
      "iteration: 81470 loss: 0.0042 lr: 0.02\n",
      "iteration: 81480 loss: 0.0032 lr: 0.02\n",
      "iteration: 81490 loss: 0.0040 lr: 0.02\n",
      "iteration: 81500 loss: 0.0040 lr: 0.02\n",
      "iteration: 81510 loss: 0.0036 lr: 0.02\n",
      "iteration: 81520 loss: 0.0039 lr: 0.02\n",
      "iteration: 81530 loss: 0.0033 lr: 0.02\n",
      "iteration: 81540 loss: 0.0024 lr: 0.02\n",
      "iteration: 81550 loss: 0.0033 lr: 0.02\n",
      "iteration: 81560 loss: 0.0032 lr: 0.02\n",
      "iteration: 81570 loss: 0.0028 lr: 0.02\n",
      "iteration: 81580 loss: 0.0039 lr: 0.02\n",
      "iteration: 81590 loss: 0.0031 lr: 0.02\n",
      "iteration: 81600 loss: 0.0034 lr: 0.02\n",
      "iteration: 81610 loss: 0.0043 lr: 0.02\n",
      "iteration: 81620 loss: 0.0037 lr: 0.02\n",
      "iteration: 81630 loss: 0.0032 lr: 0.02\n",
      "iteration: 81640 loss: 0.0051 lr: 0.02\n",
      "iteration: 81650 loss: 0.0037 lr: 0.02\n",
      "iteration: 81660 loss: 0.0025 lr: 0.02\n",
      "iteration: 81670 loss: 0.0029 lr: 0.02\n",
      "iteration: 81680 loss: 0.0033 lr: 0.02\n",
      "iteration: 81690 loss: 0.0039 lr: 0.02\n",
      "iteration: 81700 loss: 0.0040 lr: 0.02\n",
      "iteration: 81710 loss: 0.0042 lr: 0.02\n",
      "iteration: 81720 loss: 0.0044 lr: 0.02\n",
      "iteration: 81730 loss: 0.0040 lr: 0.02\n",
      "iteration: 81740 loss: 0.0059 lr: 0.02\n",
      "iteration: 81750 loss: 0.0062 lr: 0.02\n",
      "iteration: 81760 loss: 0.0029 lr: 0.02\n",
      "iteration: 81770 loss: 0.0039 lr: 0.02\n",
      "iteration: 81780 loss: 0.0041 lr: 0.02\n",
      "iteration: 81790 loss: 0.0039 lr: 0.02\n",
      "iteration: 81800 loss: 0.0027 lr: 0.02\n",
      "iteration: 81810 loss: 0.0032 lr: 0.02\n",
      "iteration: 81820 loss: 0.0046 lr: 0.02\n",
      "iteration: 81830 loss: 0.0026 lr: 0.02\n",
      "iteration: 81840 loss: 0.0045 lr: 0.02\n",
      "iteration: 81850 loss: 0.0031 lr: 0.02\n",
      "iteration: 81860 loss: 0.0047 lr: 0.02\n",
      "iteration: 81870 loss: 0.0032 lr: 0.02\n",
      "iteration: 81880 loss: 0.0044 lr: 0.02\n",
      "iteration: 81890 loss: 0.0046 lr: 0.02\n",
      "iteration: 81900 loss: 0.0036 lr: 0.02\n",
      "iteration: 81910 loss: 0.0036 lr: 0.02\n",
      "iteration: 81920 loss: 0.0031 lr: 0.02\n",
      "iteration: 81930 loss: 0.0034 lr: 0.02\n",
      "iteration: 81940 loss: 0.0027 lr: 0.02\n",
      "iteration: 81950 loss: 0.0041 lr: 0.02\n",
      "iteration: 81960 loss: 0.0031 lr: 0.02\n",
      "iteration: 81970 loss: 0.0041 lr: 0.02\n",
      "iteration: 81980 loss: 0.0042 lr: 0.02\n",
      "iteration: 81990 loss: 0.0038 lr: 0.02\n",
      "iteration: 82000 loss: 0.0035 lr: 0.02\n",
      "iteration: 82010 loss: 0.0041 lr: 0.02\n",
      "iteration: 82020 loss: 0.0035 lr: 0.02\n",
      "iteration: 82030 loss: 0.0035 lr: 0.02\n",
      "iteration: 82040 loss: 0.0048 lr: 0.02\n",
      "iteration: 82050 loss: 0.0048 lr: 0.02\n",
      "iteration: 82060 loss: 0.0076 lr: 0.02\n",
      "iteration: 82070 loss: 0.0040 lr: 0.02\n",
      "iteration: 82080 loss: 0.0032 lr: 0.02\n",
      "iteration: 82090 loss: 0.0041 lr: 0.02\n",
      "iteration: 82100 loss: 0.0037 lr: 0.02\n",
      "iteration: 82110 loss: 0.0030 lr: 0.02\n",
      "iteration: 82120 loss: 0.0052 lr: 0.02\n",
      "iteration: 82130 loss: 0.0034 lr: 0.02\n",
      "iteration: 82140 loss: 0.0050 lr: 0.02\n",
      "iteration: 82150 loss: 0.0033 lr: 0.02\n",
      "iteration: 82160 loss: 0.0031 lr: 0.02\n",
      "iteration: 82170 loss: 0.0027 lr: 0.02\n",
      "iteration: 82180 loss: 0.0026 lr: 0.02\n",
      "iteration: 82190 loss: 0.0034 lr: 0.02\n",
      "iteration: 82200 loss: 0.0044 lr: 0.02\n",
      "iteration: 82210 loss: 0.0026 lr: 0.02\n",
      "iteration: 82220 loss: 0.0032 lr: 0.02\n",
      "iteration: 82230 loss: 0.0037 lr: 0.02\n",
      "iteration: 82240 loss: 0.0045 lr: 0.02\n",
      "iteration: 82250 loss: 0.0054 lr: 0.02\n",
      "iteration: 82260 loss: 0.0035 lr: 0.02\n",
      "iteration: 82270 loss: 0.0036 lr: 0.02\n",
      "iteration: 82280 loss: 0.0051 lr: 0.02\n",
      "iteration: 82290 loss: 0.0041 lr: 0.02\n",
      "iteration: 82300 loss: 0.0041 lr: 0.02\n",
      "iteration: 82310 loss: 0.0028 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 82320 loss: 0.0032 lr: 0.02\n",
      "iteration: 82330 loss: 0.0050 lr: 0.02\n",
      "iteration: 82340 loss: 0.0029 lr: 0.02\n",
      "iteration: 82350 loss: 0.0026 lr: 0.02\n",
      "iteration: 82360 loss: 0.0046 lr: 0.02\n",
      "iteration: 82370 loss: 0.0033 lr: 0.02\n",
      "iteration: 82380 loss: 0.0029 lr: 0.02\n",
      "iteration: 82390 loss: 0.0033 lr: 0.02\n",
      "iteration: 82400 loss: 0.0031 lr: 0.02\n",
      "iteration: 82410 loss: 0.0030 lr: 0.02\n",
      "iteration: 82420 loss: 0.0047 lr: 0.02\n",
      "iteration: 82430 loss: 0.0036 lr: 0.02\n",
      "iteration: 82440 loss: 0.0040 lr: 0.02\n",
      "iteration: 82450 loss: 0.0027 lr: 0.02\n",
      "iteration: 82460 loss: 0.0040 lr: 0.02\n",
      "iteration: 82470 loss: 0.0024 lr: 0.02\n",
      "iteration: 82480 loss: 0.0036 lr: 0.02\n",
      "iteration: 82490 loss: 0.0031 lr: 0.02\n",
      "iteration: 82500 loss: 0.0021 lr: 0.02\n",
      "iteration: 82510 loss: 0.0058 lr: 0.02\n",
      "iteration: 82520 loss: 0.0040 lr: 0.02\n",
      "iteration: 82530 loss: 0.0029 lr: 0.02\n",
      "iteration: 82540 loss: 0.0030 lr: 0.02\n",
      "iteration: 82550 loss: 0.0044 lr: 0.02\n",
      "iteration: 82560 loss: 0.0039 lr: 0.02\n",
      "iteration: 82570 loss: 0.0039 lr: 0.02\n",
      "iteration: 82580 loss: 0.0034 lr: 0.02\n",
      "iteration: 82590 loss: 0.0037 lr: 0.02\n",
      "iteration: 82600 loss: 0.0034 lr: 0.02\n",
      "iteration: 82610 loss: 0.0026 lr: 0.02\n",
      "iteration: 82620 loss: 0.0033 lr: 0.02\n",
      "iteration: 82630 loss: 0.0044 lr: 0.02\n",
      "iteration: 82640 loss: 0.0036 lr: 0.02\n",
      "iteration: 82650 loss: 0.0030 lr: 0.02\n",
      "iteration: 82660 loss: 0.0045 lr: 0.02\n",
      "iteration: 82670 loss: 0.0030 lr: 0.02\n",
      "iteration: 82680 loss: 0.0035 lr: 0.02\n",
      "iteration: 82690 loss: 0.0027 lr: 0.02\n",
      "iteration: 82700 loss: 0.0037 lr: 0.02\n",
      "iteration: 82710 loss: 0.0033 lr: 0.02\n",
      "iteration: 82720 loss: 0.0030 lr: 0.02\n",
      "iteration: 82730 loss: 0.0044 lr: 0.02\n",
      "iteration: 82740 loss: 0.0040 lr: 0.02\n",
      "iteration: 82750 loss: 0.0034 lr: 0.02\n",
      "iteration: 82760 loss: 0.0035 lr: 0.02\n",
      "iteration: 82770 loss: 0.0034 lr: 0.02\n",
      "iteration: 82780 loss: 0.0035 lr: 0.02\n",
      "iteration: 82790 loss: 0.0042 lr: 0.02\n",
      "iteration: 82800 loss: 0.0030 lr: 0.02\n",
      "iteration: 82810 loss: 0.0037 lr: 0.02\n",
      "iteration: 82820 loss: 0.0037 lr: 0.02\n",
      "iteration: 82830 loss: 0.0032 lr: 0.02\n",
      "iteration: 82840 loss: 0.0032 lr: 0.02\n",
      "iteration: 82850 loss: 0.0029 lr: 0.02\n",
      "iteration: 82860 loss: 0.0033 lr: 0.02\n",
      "iteration: 82870 loss: 0.0037 lr: 0.02\n",
      "iteration: 82880 loss: 0.0028 lr: 0.02\n",
      "iteration: 82890 loss: 0.0031 lr: 0.02\n",
      "iteration: 82900 loss: 0.0049 lr: 0.02\n",
      "iteration: 82910 loss: 0.0044 lr: 0.02\n",
      "iteration: 82920 loss: 0.0042 lr: 0.02\n",
      "iteration: 82930 loss: 0.0037 lr: 0.02\n",
      "iteration: 82940 loss: 0.0033 lr: 0.02\n",
      "iteration: 82950 loss: 0.0024 lr: 0.02\n",
      "iteration: 82960 loss: 0.0028 lr: 0.02\n",
      "iteration: 82970 loss: 0.0040 lr: 0.02\n",
      "iteration: 82980 loss: 0.0039 lr: 0.02\n",
      "iteration: 82990 loss: 0.0035 lr: 0.02\n",
      "iteration: 83000 loss: 0.0062 lr: 0.02\n",
      "iteration: 83010 loss: 0.0038 lr: 0.02\n",
      "iteration: 83020 loss: 0.0034 lr: 0.02\n",
      "iteration: 83030 loss: 0.0030 lr: 0.02\n",
      "iteration: 83040 loss: 0.0036 lr: 0.02\n",
      "iteration: 83050 loss: 0.0031 lr: 0.02\n",
      "iteration: 83060 loss: 0.0036 lr: 0.02\n",
      "iteration: 83070 loss: 0.0046 lr: 0.02\n",
      "iteration: 83080 loss: 0.0040 lr: 0.02\n",
      "iteration: 83090 loss: 0.0041 lr: 0.02\n",
      "iteration: 83100 loss: 0.0051 lr: 0.02\n",
      "iteration: 83110 loss: 0.0028 lr: 0.02\n",
      "iteration: 83120 loss: 0.0038 lr: 0.02\n",
      "iteration: 83130 loss: 0.0040 lr: 0.02\n",
      "iteration: 83140 loss: 0.0032 lr: 0.02\n",
      "iteration: 83150 loss: 0.0047 lr: 0.02\n",
      "iteration: 83160 loss: 0.0041 lr: 0.02\n",
      "iteration: 83170 loss: 0.0031 lr: 0.02\n",
      "iteration: 83180 loss: 0.0035 lr: 0.02\n",
      "iteration: 83190 loss: 0.0049 lr: 0.02\n",
      "iteration: 83200 loss: 0.0045 lr: 0.02\n",
      "iteration: 83210 loss: 0.0032 lr: 0.02\n",
      "iteration: 83220 loss: 0.0048 lr: 0.02\n",
      "iteration: 83230 loss: 0.0043 lr: 0.02\n",
      "iteration: 83240 loss: 0.0047 lr: 0.02\n",
      "iteration: 83250 loss: 0.0035 lr: 0.02\n",
      "iteration: 83260 loss: 0.0038 lr: 0.02\n",
      "iteration: 83270 loss: 0.0031 lr: 0.02\n",
      "iteration: 83280 loss: 0.0032 lr: 0.02\n",
      "iteration: 83290 loss: 0.0052 lr: 0.02\n",
      "iteration: 83300 loss: 0.0040 lr: 0.02\n",
      "iteration: 83310 loss: 0.0038 lr: 0.02\n",
      "iteration: 83320 loss: 0.0043 lr: 0.02\n",
      "iteration: 83330 loss: 0.0028 lr: 0.02\n",
      "iteration: 83340 loss: 0.0041 lr: 0.02\n",
      "iteration: 83350 loss: 0.0040 lr: 0.02\n",
      "iteration: 83360 loss: 0.0035 lr: 0.02\n",
      "iteration: 83370 loss: 0.0029 lr: 0.02\n",
      "iteration: 83380 loss: 0.0031 lr: 0.02\n",
      "iteration: 83390 loss: 0.0026 lr: 0.02\n",
      "iteration: 83400 loss: 0.0044 lr: 0.02\n",
      "iteration: 83410 loss: 0.0034 lr: 0.02\n",
      "iteration: 83420 loss: 0.0031 lr: 0.02\n",
      "iteration: 83430 loss: 0.0028 lr: 0.02\n",
      "iteration: 83440 loss: 0.0041 lr: 0.02\n",
      "iteration: 83450 loss: 0.0031 lr: 0.02\n",
      "iteration: 83460 loss: 0.0026 lr: 0.02\n",
      "iteration: 83470 loss: 0.0033 lr: 0.02\n",
      "iteration: 83480 loss: 0.0036 lr: 0.02\n",
      "iteration: 83490 loss: 0.0030 lr: 0.02\n",
      "iteration: 83500 loss: 0.0039 lr: 0.02\n",
      "iteration: 83510 loss: 0.0030 lr: 0.02\n",
      "iteration: 83520 loss: 0.0041 lr: 0.02\n",
      "iteration: 83530 loss: 0.0043 lr: 0.02\n",
      "iteration: 83540 loss: 0.0038 lr: 0.02\n",
      "iteration: 83550 loss: 0.0044 lr: 0.02\n",
      "iteration: 83560 loss: 0.0045 lr: 0.02\n",
      "iteration: 83570 loss: 0.0044 lr: 0.02\n",
      "iteration: 83580 loss: 0.0042 lr: 0.02\n",
      "iteration: 83590 loss: 0.0023 lr: 0.02\n",
      "iteration: 83600 loss: 0.0057 lr: 0.02\n",
      "iteration: 83610 loss: 0.0035 lr: 0.02\n",
      "iteration: 83620 loss: 0.0032 lr: 0.02\n",
      "iteration: 83630 loss: 0.0042 lr: 0.02\n",
      "iteration: 83640 loss: 0.0043 lr: 0.02\n",
      "iteration: 83650 loss: 0.0032 lr: 0.02\n",
      "iteration: 83660 loss: 0.0049 lr: 0.02\n",
      "iteration: 83670 loss: 0.0033 lr: 0.02\n",
      "iteration: 83680 loss: 0.0032 lr: 0.02\n",
      "iteration: 83690 loss: 0.0052 lr: 0.02\n",
      "iteration: 83700 loss: 0.0024 lr: 0.02\n",
      "iteration: 83710 loss: 0.0037 lr: 0.02\n",
      "iteration: 83720 loss: 0.0024 lr: 0.02\n",
      "iteration: 83730 loss: 0.0043 lr: 0.02\n",
      "iteration: 83740 loss: 0.0043 lr: 0.02\n",
      "iteration: 83750 loss: 0.0055 lr: 0.02\n",
      "iteration: 83760 loss: 0.0053 lr: 0.02\n",
      "iteration: 83770 loss: 0.0042 lr: 0.02\n",
      "iteration: 83780 loss: 0.0047 lr: 0.02\n",
      "iteration: 83790 loss: 0.0038 lr: 0.02\n",
      "iteration: 83800 loss: 0.0039 lr: 0.02\n",
      "iteration: 83810 loss: 0.0041 lr: 0.02\n",
      "iteration: 83820 loss: 0.0049 lr: 0.02\n",
      "iteration: 83830 loss: 0.0034 lr: 0.02\n",
      "iteration: 83840 loss: 0.0031 lr: 0.02\n",
      "iteration: 83850 loss: 0.0038 lr: 0.02\n",
      "iteration: 83860 loss: 0.0028 lr: 0.02\n",
      "iteration: 83870 loss: 0.0031 lr: 0.02\n",
      "iteration: 83880 loss: 0.0034 lr: 0.02\n",
      "iteration: 83890 loss: 0.0042 lr: 0.02\n",
      "iteration: 83900 loss: 0.0037 lr: 0.02\n",
      "iteration: 83910 loss: 0.0041 lr: 0.02\n",
      "iteration: 83920 loss: 0.0038 lr: 0.02\n",
      "iteration: 83930 loss: 0.0040 lr: 0.02\n",
      "iteration: 83940 loss: 0.0045 lr: 0.02\n",
      "iteration: 83950 loss: 0.0050 lr: 0.02\n",
      "iteration: 83960 loss: 0.0039 lr: 0.02\n",
      "iteration: 83970 loss: 0.0038 lr: 0.02\n",
      "iteration: 83980 loss: 0.0030 lr: 0.02\n",
      "iteration: 83990 loss: 0.0020 lr: 0.02\n",
      "iteration: 84000 loss: 0.0034 lr: 0.02\n",
      "iteration: 84010 loss: 0.0039 lr: 0.02\n",
      "iteration: 84020 loss: 0.0029 lr: 0.02\n",
      "iteration: 84030 loss: 0.0031 lr: 0.02\n",
      "iteration: 84040 loss: 0.0038 lr: 0.02\n",
      "iteration: 84050 loss: 0.0054 lr: 0.02\n",
      "iteration: 84060 loss: 0.0031 lr: 0.02\n",
      "iteration: 84070 loss: 0.0037 lr: 0.02\n",
      "iteration: 84080 loss: 0.0042 lr: 0.02\n",
      "iteration: 84090 loss: 0.0044 lr: 0.02\n",
      "iteration: 84100 loss: 0.0035 lr: 0.02\n",
      "iteration: 84110 loss: 0.0038 lr: 0.02\n",
      "iteration: 84120 loss: 0.0049 lr: 0.02\n",
      "iteration: 84130 loss: 0.0043 lr: 0.02\n",
      "iteration: 84140 loss: 0.0037 lr: 0.02\n",
      "iteration: 84150 loss: 0.0024 lr: 0.02\n",
      "iteration: 84160 loss: 0.0028 lr: 0.02\n",
      "iteration: 84170 loss: 0.0037 lr: 0.02\n",
      "iteration: 84180 loss: 0.0034 lr: 0.02\n",
      "iteration: 84190 loss: 0.0024 lr: 0.02\n",
      "iteration: 84200 loss: 0.0028 lr: 0.02\n",
      "iteration: 84210 loss: 0.0025 lr: 0.02\n",
      "iteration: 84220 loss: 0.0037 lr: 0.02\n",
      "iteration: 84230 loss: 0.0026 lr: 0.02\n",
      "iteration: 84240 loss: 0.0053 lr: 0.02\n",
      "iteration: 84250 loss: 0.0035 lr: 0.02\n",
      "iteration: 84260 loss: 0.0049 lr: 0.02\n",
      "iteration: 84270 loss: 0.0045 lr: 0.02\n",
      "iteration: 84280 loss: 0.0050 lr: 0.02\n",
      "iteration: 84290 loss: 0.0033 lr: 0.02\n",
      "iteration: 84300 loss: 0.0035 lr: 0.02\n",
      "iteration: 84310 loss: 0.0035 lr: 0.02\n",
      "iteration: 84320 loss: 0.0044 lr: 0.02\n",
      "iteration: 84330 loss: 0.0025 lr: 0.02\n",
      "iteration: 84340 loss: 0.0032 lr: 0.02\n",
      "iteration: 84350 loss: 0.0031 lr: 0.02\n",
      "iteration: 84360 loss: 0.0037 lr: 0.02\n",
      "iteration: 84370 loss: 0.0025 lr: 0.02\n",
      "iteration: 84380 loss: 0.0045 lr: 0.02\n",
      "iteration: 84390 loss: 0.0042 lr: 0.02\n",
      "iteration: 84400 loss: 0.0033 lr: 0.02\n",
      "iteration: 84410 loss: 0.0025 lr: 0.02\n",
      "iteration: 84420 loss: 0.0026 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 84430 loss: 0.0030 lr: 0.02\n",
      "iteration: 84440 loss: 0.0032 lr: 0.02\n",
      "iteration: 84450 loss: 0.0028 lr: 0.02\n",
      "iteration: 84460 loss: 0.0031 lr: 0.02\n",
      "iteration: 84470 loss: 0.0034 lr: 0.02\n",
      "iteration: 84480 loss: 0.0052 lr: 0.02\n",
      "iteration: 84490 loss: 0.0041 lr: 0.02\n",
      "iteration: 84500 loss: 0.0027 lr: 0.02\n",
      "iteration: 84510 loss: 0.0028 lr: 0.02\n",
      "iteration: 84520 loss: 0.0038 lr: 0.02\n",
      "iteration: 84530 loss: 0.0035 lr: 0.02\n",
      "iteration: 84540 loss: 0.0037 lr: 0.02\n",
      "iteration: 84550 loss: 0.0035 lr: 0.02\n",
      "iteration: 84560 loss: 0.0035 lr: 0.02\n",
      "iteration: 84570 loss: 0.0056 lr: 0.02\n",
      "iteration: 84580 loss: 0.0034 lr: 0.02\n",
      "iteration: 84590 loss: 0.0051 lr: 0.02\n",
      "iteration: 84600 loss: 0.0032 lr: 0.02\n",
      "iteration: 84610 loss: 0.0037 lr: 0.02\n",
      "iteration: 84620 loss: 0.0037 lr: 0.02\n",
      "iteration: 84630 loss: 0.0032 lr: 0.02\n",
      "iteration: 84640 loss: 0.0038 lr: 0.02\n",
      "iteration: 84650 loss: 0.0034 lr: 0.02\n",
      "iteration: 84660 loss: 0.0040 lr: 0.02\n",
      "iteration: 84670 loss: 0.0024 lr: 0.02\n",
      "iteration: 84680 loss: 0.0036 lr: 0.02\n",
      "iteration: 84690 loss: 0.0041 lr: 0.02\n",
      "iteration: 84700 loss: 0.0035 lr: 0.02\n",
      "iteration: 84710 loss: 0.0037 lr: 0.02\n",
      "iteration: 84720 loss: 0.0031 lr: 0.02\n",
      "iteration: 84730 loss: 0.0041 lr: 0.02\n",
      "iteration: 84740 loss: 0.0037 lr: 0.02\n",
      "iteration: 84750 loss: 0.0036 lr: 0.02\n",
      "iteration: 84760 loss: 0.0035 lr: 0.02\n",
      "iteration: 84770 loss: 0.0030 lr: 0.02\n",
      "iteration: 84780 loss: 0.0030 lr: 0.02\n",
      "iteration: 84790 loss: 0.0038 lr: 0.02\n",
      "iteration: 84800 loss: 0.0026 lr: 0.02\n",
      "iteration: 84810 loss: 0.0042 lr: 0.02\n",
      "iteration: 84820 loss: 0.0037 lr: 0.02\n",
      "iteration: 84830 loss: 0.0028 lr: 0.02\n",
      "iteration: 84840 loss: 0.0034 lr: 0.02\n",
      "iteration: 84850 loss: 0.0031 lr: 0.02\n",
      "iteration: 84860 loss: 0.0038 lr: 0.02\n",
      "iteration: 84870 loss: 0.0032 lr: 0.02\n",
      "iteration: 84880 loss: 0.0030 lr: 0.02\n",
      "iteration: 84890 loss: 0.0028 lr: 0.02\n",
      "iteration: 84900 loss: 0.0037 lr: 0.02\n",
      "iteration: 84910 loss: 0.0027 lr: 0.02\n",
      "iteration: 84920 loss: 0.0029 lr: 0.02\n",
      "iteration: 84930 loss: 0.0027 lr: 0.02\n",
      "iteration: 84940 loss: 0.0030 lr: 0.02\n",
      "iteration: 84950 loss: 0.0038 lr: 0.02\n",
      "iteration: 84960 loss: 0.0045 lr: 0.02\n",
      "iteration: 84970 loss: 0.0038 lr: 0.02\n",
      "iteration: 84980 loss: 0.0049 lr: 0.02\n",
      "iteration: 84990 loss: 0.0040 lr: 0.02\n",
      "iteration: 85000 loss: 0.0049 lr: 0.02\n",
      "iteration: 85010 loss: 0.0038 lr: 0.02\n",
      "iteration: 85020 loss: 0.0055 lr: 0.02\n",
      "iteration: 85030 loss: 0.0042 lr: 0.02\n",
      "iteration: 85040 loss: 0.0029 lr: 0.02\n",
      "iteration: 85050 loss: 0.0036 lr: 0.02\n",
      "iteration: 85060 loss: 0.0046 lr: 0.02\n",
      "iteration: 85070 loss: 0.0033 lr: 0.02\n",
      "iteration: 85080 loss: 0.0072 lr: 0.02\n",
      "iteration: 85090 loss: 0.0041 lr: 0.02\n",
      "iteration: 85100 loss: 0.0038 lr: 0.02\n",
      "iteration: 85110 loss: 0.0040 lr: 0.02\n",
      "iteration: 85120 loss: 0.0044 lr: 0.02\n",
      "iteration: 85130 loss: 0.0040 lr: 0.02\n",
      "iteration: 85140 loss: 0.0028 lr: 0.02\n",
      "iteration: 85150 loss: 0.0040 lr: 0.02\n",
      "iteration: 85160 loss: 0.0040 lr: 0.02\n",
      "iteration: 85170 loss: 0.0030 lr: 0.02\n",
      "iteration: 85180 loss: 0.0037 lr: 0.02\n",
      "iteration: 85190 loss: 0.0041 lr: 0.02\n",
      "iteration: 85200 loss: 0.0038 lr: 0.02\n",
      "iteration: 85210 loss: 0.0028 lr: 0.02\n",
      "iteration: 85220 loss: 0.0046 lr: 0.02\n",
      "iteration: 85230 loss: 0.0024 lr: 0.02\n",
      "iteration: 85240 loss: 0.0032 lr: 0.02\n",
      "iteration: 85250 loss: 0.0042 lr: 0.02\n",
      "iteration: 85260 loss: 0.0036 lr: 0.02\n",
      "iteration: 85270 loss: 0.0042 lr: 0.02\n",
      "iteration: 85280 loss: 0.0044 lr: 0.02\n",
      "iteration: 85290 loss: 0.0032 lr: 0.02\n",
      "iteration: 85300 loss: 0.0025 lr: 0.02\n",
      "iteration: 85310 loss: 0.0030 lr: 0.02\n",
      "iteration: 85320 loss: 0.0039 lr: 0.02\n",
      "iteration: 85330 loss: 0.0026 lr: 0.02\n",
      "iteration: 85340 loss: 0.0034 lr: 0.02\n",
      "iteration: 85350 loss: 0.0029 lr: 0.02\n",
      "iteration: 85360 loss: 0.0033 lr: 0.02\n",
      "iteration: 85370 loss: 0.0047 lr: 0.02\n",
      "iteration: 85380 loss: 0.0036 lr: 0.02\n",
      "iteration: 85390 loss: 0.0040 lr: 0.02\n",
      "iteration: 85400 loss: 0.0034 lr: 0.02\n",
      "iteration: 85410 loss: 0.0023 lr: 0.02\n",
      "iteration: 85420 loss: 0.0022 lr: 0.02\n",
      "iteration: 85430 loss: 0.0033 lr: 0.02\n",
      "iteration: 85440 loss: 0.0028 lr: 0.02\n",
      "iteration: 85450 loss: 0.0023 lr: 0.02\n",
      "iteration: 85460 loss: 0.0028 lr: 0.02\n",
      "iteration: 85470 loss: 0.0030 lr: 0.02\n",
      "iteration: 85480 loss: 0.0032 lr: 0.02\n",
      "iteration: 85490 loss: 0.0028 lr: 0.02\n",
      "iteration: 85500 loss: 0.0039 lr: 0.02\n",
      "iteration: 85510 loss: 0.0041 lr: 0.02\n",
      "iteration: 85520 loss: 0.0035 lr: 0.02\n",
      "iteration: 85530 loss: 0.0041 lr: 0.02\n",
      "iteration: 85540 loss: 0.0031 lr: 0.02\n",
      "iteration: 85550 loss: 0.0029 lr: 0.02\n",
      "iteration: 85560 loss: 0.0029 lr: 0.02\n",
      "iteration: 85570 loss: 0.0026 lr: 0.02\n",
      "iteration: 85580 loss: 0.0038 lr: 0.02\n",
      "iteration: 85590 loss: 0.0030 lr: 0.02\n",
      "iteration: 85600 loss: 0.0035 lr: 0.02\n",
      "iteration: 85610 loss: 0.0043 lr: 0.02\n",
      "iteration: 85620 loss: 0.0042 lr: 0.02\n",
      "iteration: 85630 loss: 0.0039 lr: 0.02\n",
      "iteration: 85640 loss: 0.0039 lr: 0.02\n",
      "iteration: 85650 loss: 0.0034 lr: 0.02\n",
      "iteration: 85660 loss: 0.0046 lr: 0.02\n",
      "iteration: 85670 loss: 0.0042 lr: 0.02\n",
      "iteration: 85680 loss: 0.0036 lr: 0.02\n",
      "iteration: 85690 loss: 0.0032 lr: 0.02\n",
      "iteration: 85700 loss: 0.0036 lr: 0.02\n",
      "iteration: 85710 loss: 0.0035 lr: 0.02\n",
      "iteration: 85720 loss: 0.0027 lr: 0.02\n",
      "iteration: 85730 loss: 0.0038 lr: 0.02\n",
      "iteration: 85740 loss: 0.0038 lr: 0.02\n",
      "iteration: 85750 loss: 0.0031 lr: 0.02\n",
      "iteration: 85760 loss: 0.0048 lr: 0.02\n",
      "iteration: 85770 loss: 0.0040 lr: 0.02\n",
      "iteration: 85780 loss: 0.0039 lr: 0.02\n",
      "iteration: 85790 loss: 0.0035 lr: 0.02\n",
      "iteration: 85800 loss: 0.0028 lr: 0.02\n",
      "iteration: 85810 loss: 0.0031 lr: 0.02\n",
      "iteration: 85820 loss: 0.0050 lr: 0.02\n",
      "iteration: 85830 loss: 0.0045 lr: 0.02\n",
      "iteration: 85840 loss: 0.0031 lr: 0.02\n",
      "iteration: 85850 loss: 0.0034 lr: 0.02\n",
      "iteration: 85860 loss: 0.0031 lr: 0.02\n",
      "iteration: 85870 loss: 0.0032 lr: 0.02\n",
      "iteration: 85880 loss: 0.0022 lr: 0.02\n",
      "iteration: 85890 loss: 0.0032 lr: 0.02\n",
      "iteration: 85900 loss: 0.0046 lr: 0.02\n",
      "iteration: 85910 loss: 0.0029 lr: 0.02\n",
      "iteration: 85920 loss: 0.0024 lr: 0.02\n",
      "iteration: 85930 loss: 0.0048 lr: 0.02\n",
      "iteration: 85940 loss: 0.0035 lr: 0.02\n",
      "iteration: 85950 loss: 0.0039 lr: 0.02\n",
      "iteration: 85960 loss: 0.0057 lr: 0.02\n",
      "iteration: 85970 loss: 0.0041 lr: 0.02\n",
      "iteration: 85980 loss: 0.0035 lr: 0.02\n",
      "iteration: 85990 loss: 0.0031 lr: 0.02\n",
      "iteration: 86000 loss: 0.0037 lr: 0.02\n",
      "iteration: 86010 loss: 0.0029 lr: 0.02\n",
      "iteration: 86020 loss: 0.0028 lr: 0.02\n",
      "iteration: 86030 loss: 0.0026 lr: 0.02\n",
      "iteration: 86040 loss: 0.0036 lr: 0.02\n",
      "iteration: 86050 loss: 0.0030 lr: 0.02\n",
      "iteration: 86060 loss: 0.0034 lr: 0.02\n",
      "iteration: 86070 loss: 0.0029 lr: 0.02\n",
      "iteration: 86080 loss: 0.0025 lr: 0.02\n",
      "iteration: 86090 loss: 0.0042 lr: 0.02\n",
      "iteration: 86100 loss: 0.0033 lr: 0.02\n",
      "iteration: 86110 loss: 0.0030 lr: 0.02\n",
      "iteration: 86120 loss: 0.0031 lr: 0.02\n",
      "iteration: 86130 loss: 0.0031 lr: 0.02\n",
      "iteration: 86140 loss: 0.0042 lr: 0.02\n",
      "iteration: 86150 loss: 0.0042 lr: 0.02\n",
      "iteration: 86160 loss: 0.0038 lr: 0.02\n",
      "iteration: 86170 loss: 0.0033 lr: 0.02\n",
      "iteration: 86180 loss: 0.0031 lr: 0.02\n",
      "iteration: 86190 loss: 0.0038 lr: 0.02\n",
      "iteration: 86200 loss: 0.0051 lr: 0.02\n",
      "iteration: 86210 loss: 0.0041 lr: 0.02\n",
      "iteration: 86220 loss: 0.0032 lr: 0.02\n",
      "iteration: 86230 loss: 0.0036 lr: 0.02\n",
      "iteration: 86240 loss: 0.0041 lr: 0.02\n",
      "iteration: 86250 loss: 0.0047 lr: 0.02\n",
      "iteration: 86260 loss: 0.0034 lr: 0.02\n",
      "iteration: 86270 loss: 0.0040 lr: 0.02\n",
      "iteration: 86280 loss: 0.0036 lr: 0.02\n",
      "iteration: 86290 loss: 0.0039 lr: 0.02\n",
      "iteration: 86300 loss: 0.0036 lr: 0.02\n",
      "iteration: 86310 loss: 0.0038 lr: 0.02\n",
      "iteration: 86320 loss: 0.0039 lr: 0.02\n",
      "iteration: 86330 loss: 0.0043 lr: 0.02\n",
      "iteration: 86340 loss: 0.0034 lr: 0.02\n",
      "iteration: 86350 loss: 0.0034 lr: 0.02\n",
      "iteration: 86360 loss: 0.0038 lr: 0.02\n",
      "iteration: 86370 loss: 0.0047 lr: 0.02\n",
      "iteration: 86380 loss: 0.0042 lr: 0.02\n",
      "iteration: 86390 loss: 0.0029 lr: 0.02\n",
      "iteration: 86400 loss: 0.0042 lr: 0.02\n",
      "iteration: 86410 loss: 0.0039 lr: 0.02\n",
      "iteration: 86420 loss: 0.0033 lr: 0.02\n",
      "iteration: 86430 loss: 0.0033 lr: 0.02\n",
      "iteration: 86440 loss: 0.0048 lr: 0.02\n",
      "iteration: 86450 loss: 0.0028 lr: 0.02\n",
      "iteration: 86460 loss: 0.0057 lr: 0.02\n",
      "iteration: 86470 loss: 0.0045 lr: 0.02\n",
      "iteration: 86480 loss: 0.0046 lr: 0.02\n",
      "iteration: 86490 loss: 0.0029 lr: 0.02\n",
      "iteration: 86500 loss: 0.0056 lr: 0.02\n",
      "iteration: 86510 loss: 0.0034 lr: 0.02\n",
      "iteration: 86520 loss: 0.0027 lr: 0.02\n",
      "iteration: 86530 loss: 0.0032 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 86540 loss: 0.0027 lr: 0.02\n",
      "iteration: 86550 loss: 0.0025 lr: 0.02\n",
      "iteration: 86560 loss: 0.0029 lr: 0.02\n",
      "iteration: 86570 loss: 0.0032 lr: 0.02\n",
      "iteration: 86580 loss: 0.0023 lr: 0.02\n",
      "iteration: 86590 loss: 0.0036 lr: 0.02\n",
      "iteration: 86600 loss: 0.0031 lr: 0.02\n",
      "iteration: 86610 loss: 0.0030 lr: 0.02\n",
      "iteration: 86620 loss: 0.0038 lr: 0.02\n",
      "iteration: 86630 loss: 0.0030 lr: 0.02\n",
      "iteration: 86640 loss: 0.0028 lr: 0.02\n",
      "iteration: 86650 loss: 0.0031 lr: 0.02\n",
      "iteration: 86660 loss: 0.0031 lr: 0.02\n",
      "iteration: 86670 loss: 0.0040 lr: 0.02\n",
      "iteration: 86680 loss: 0.0038 lr: 0.02\n",
      "iteration: 86690 loss: 0.0047 lr: 0.02\n",
      "iteration: 86700 loss: 0.0030 lr: 0.02\n",
      "iteration: 86710 loss: 0.0035 lr: 0.02\n",
      "iteration: 86720 loss: 0.0031 lr: 0.02\n",
      "iteration: 86730 loss: 0.0046 lr: 0.02\n",
      "iteration: 86740 loss: 0.0052 lr: 0.02\n",
      "iteration: 86750 loss: 0.0046 lr: 0.02\n",
      "iteration: 86760 loss: 0.0027 lr: 0.02\n",
      "iteration: 86770 loss: 0.0044 lr: 0.02\n",
      "iteration: 86780 loss: 0.0037 lr: 0.02\n",
      "iteration: 86790 loss: 0.0035 lr: 0.02\n",
      "iteration: 86800 loss: 0.0030 lr: 0.02\n",
      "iteration: 86810 loss: 0.0028 lr: 0.02\n",
      "iteration: 86820 loss: 0.0033 lr: 0.02\n",
      "iteration: 86830 loss: 0.0028 lr: 0.02\n",
      "iteration: 86840 loss: 0.0036 lr: 0.02\n",
      "iteration: 86850 loss: 0.0027 lr: 0.02\n",
      "iteration: 86860 loss: 0.0045 lr: 0.02\n",
      "iteration: 86870 loss: 0.0042 lr: 0.02\n",
      "iteration: 86880 loss: 0.0073 lr: 0.02\n",
      "iteration: 86890 loss: 0.0027 lr: 0.02\n",
      "iteration: 86900 loss: 0.0038 lr: 0.02\n",
      "iteration: 86910 loss: 0.0037 lr: 0.02\n",
      "iteration: 86920 loss: 0.0035 lr: 0.02\n",
      "iteration: 86930 loss: 0.0029 lr: 0.02\n",
      "iteration: 86940 loss: 0.0032 lr: 0.02\n",
      "iteration: 86950 loss: 0.0036 lr: 0.02\n",
      "iteration: 86960 loss: 0.0033 lr: 0.02\n",
      "iteration: 86970 loss: 0.0043 lr: 0.02\n",
      "iteration: 86980 loss: 0.0038 lr: 0.02\n",
      "iteration: 86990 loss: 0.0036 lr: 0.02\n",
      "iteration: 87000 loss: 0.0027 lr: 0.02\n",
      "iteration: 87010 loss: 0.0031 lr: 0.02\n",
      "iteration: 87020 loss: 0.0036 lr: 0.02\n",
      "iteration: 87030 loss: 0.0053 lr: 0.02\n",
      "iteration: 87040 loss: 0.0048 lr: 0.02\n",
      "iteration: 87050 loss: 0.0039 lr: 0.02\n",
      "iteration: 87060 loss: 0.0051 lr: 0.02\n",
      "iteration: 87070 loss: 0.0040 lr: 0.02\n",
      "iteration: 87080 loss: 0.0048 lr: 0.02\n",
      "iteration: 87090 loss: 0.0030 lr: 0.02\n",
      "iteration: 87100 loss: 0.0038 lr: 0.02\n",
      "iteration: 87110 loss: 0.0040 lr: 0.02\n",
      "iteration: 87120 loss: 0.0034 lr: 0.02\n",
      "iteration: 87130 loss: 0.0021 lr: 0.02\n",
      "iteration: 87140 loss: 0.0042 lr: 0.02\n",
      "iteration: 87150 loss: 0.0035 lr: 0.02\n",
      "iteration: 87160 loss: 0.0038 lr: 0.02\n",
      "iteration: 87170 loss: 0.0042 lr: 0.02\n",
      "iteration: 87180 loss: 0.0045 lr: 0.02\n",
      "iteration: 87190 loss: 0.0036 lr: 0.02\n",
      "iteration: 87200 loss: 0.0039 lr: 0.02\n",
      "iteration: 87210 loss: 0.0027 lr: 0.02\n",
      "iteration: 87220 loss: 0.0037 lr: 0.02\n",
      "iteration: 87230 loss: 0.0021 lr: 0.02\n",
      "iteration: 87240 loss: 0.0041 lr: 0.02\n",
      "iteration: 87250 loss: 0.0032 lr: 0.02\n",
      "iteration: 87260 loss: 0.0036 lr: 0.02\n",
      "iteration: 87270 loss: 0.0041 lr: 0.02\n",
      "iteration: 87280 loss: 0.0029 lr: 0.02\n",
      "iteration: 87290 loss: 0.0033 lr: 0.02\n",
      "iteration: 87300 loss: 0.0028 lr: 0.02\n",
      "iteration: 87310 loss: 0.0031 lr: 0.02\n",
      "iteration: 87320 loss: 0.0059 lr: 0.02\n",
      "iteration: 87330 loss: 0.0044 lr: 0.02\n",
      "iteration: 87340 loss: 0.0031 lr: 0.02\n",
      "iteration: 87350 loss: 0.0041 lr: 0.02\n",
      "iteration: 87360 loss: 0.0025 lr: 0.02\n",
      "iteration: 87370 loss: 0.0027 lr: 0.02\n",
      "iteration: 87380 loss: 0.0044 lr: 0.02\n",
      "iteration: 87390 loss: 0.0051 lr: 0.02\n",
      "iteration: 87400 loss: 0.0029 lr: 0.02\n",
      "iteration: 87410 loss: 0.0027 lr: 0.02\n",
      "iteration: 87420 loss: 0.0036 lr: 0.02\n",
      "iteration: 87430 loss: 0.0030 lr: 0.02\n",
      "iteration: 87440 loss: 0.0044 lr: 0.02\n",
      "iteration: 87450 loss: 0.0040 lr: 0.02\n",
      "iteration: 87460 loss: 0.0035 lr: 0.02\n",
      "iteration: 87470 loss: 0.0030 lr: 0.02\n",
      "iteration: 87480 loss: 0.0062 lr: 0.02\n",
      "iteration: 87490 loss: 0.0039 lr: 0.02\n",
      "iteration: 87500 loss: 0.0038 lr: 0.02\n",
      "iteration: 87510 loss: 0.0037 lr: 0.02\n",
      "iteration: 87520 loss: 0.0029 lr: 0.02\n",
      "iteration: 87530 loss: 0.0037 lr: 0.02\n",
      "iteration: 87540 loss: 0.0029 lr: 0.02\n",
      "iteration: 87550 loss: 0.0031 lr: 0.02\n",
      "iteration: 87560 loss: 0.0021 lr: 0.02\n",
      "iteration: 87570 loss: 0.0036 lr: 0.02\n",
      "iteration: 87580 loss: 0.0029 lr: 0.02\n",
      "iteration: 87590 loss: 0.0030 lr: 0.02\n",
      "iteration: 87600 loss: 0.0051 lr: 0.02\n",
      "iteration: 87610 loss: 0.0028 lr: 0.02\n",
      "iteration: 87620 loss: 0.0032 lr: 0.02\n",
      "iteration: 87630 loss: 0.0036 lr: 0.02\n",
      "iteration: 87640 loss: 0.0043 lr: 0.02\n",
      "iteration: 87650 loss: 0.0051 lr: 0.02\n",
      "iteration: 87660 loss: 0.0034 lr: 0.02\n",
      "iteration: 87670 loss: 0.0032 lr: 0.02\n",
      "iteration: 87680 loss: 0.0035 lr: 0.02\n",
      "iteration: 87690 loss: 0.0025 lr: 0.02\n",
      "iteration: 87700 loss: 0.0033 lr: 0.02\n",
      "iteration: 87710 loss: 0.0022 lr: 0.02\n",
      "iteration: 87720 loss: 0.0032 lr: 0.02\n",
      "iteration: 87730 loss: 0.0023 lr: 0.02\n",
      "iteration: 87740 loss: 0.0033 lr: 0.02\n",
      "iteration: 87750 loss: 0.0031 lr: 0.02\n",
      "iteration: 87760 loss: 0.0031 lr: 0.02\n",
      "iteration: 87770 loss: 0.0035 lr: 0.02\n",
      "iteration: 87780 loss: 0.0030 lr: 0.02\n",
      "iteration: 87790 loss: 0.0034 lr: 0.02\n",
      "iteration: 87800 loss: 0.0036 lr: 0.02\n",
      "iteration: 87810 loss: 0.0032 lr: 0.02\n",
      "iteration: 87820 loss: 0.0037 lr: 0.02\n",
      "iteration: 87830 loss: 0.0024 lr: 0.02\n",
      "iteration: 87840 loss: 0.0029 lr: 0.02\n",
      "iteration: 87850 loss: 0.0038 lr: 0.02\n",
      "iteration: 87860 loss: 0.0035 lr: 0.02\n",
      "iteration: 87870 loss: 0.0032 lr: 0.02\n",
      "iteration: 87880 loss: 0.0038 lr: 0.02\n",
      "iteration: 87890 loss: 0.0037 lr: 0.02\n",
      "iteration: 87900 loss: 0.0030 lr: 0.02\n",
      "iteration: 87910 loss: 0.0038 lr: 0.02\n",
      "iteration: 87920 loss: 0.0031 lr: 0.02\n",
      "iteration: 87930 loss: 0.0034 lr: 0.02\n",
      "iteration: 87940 loss: 0.0045 lr: 0.02\n",
      "iteration: 87950 loss: 0.0055 lr: 0.02\n",
      "iteration: 87960 loss: 0.0035 lr: 0.02\n",
      "iteration: 87970 loss: 0.0035 lr: 0.02\n",
      "iteration: 87980 loss: 0.0033 lr: 0.02\n",
      "iteration: 87990 loss: 0.0032 lr: 0.02\n",
      "iteration: 88000 loss: 0.0028 lr: 0.02\n",
      "iteration: 88010 loss: 0.0034 lr: 0.02\n",
      "iteration: 88020 loss: 0.0031 lr: 0.02\n",
      "iteration: 88030 loss: 0.0042 lr: 0.02\n",
      "iteration: 88040 loss: 0.0063 lr: 0.02\n",
      "iteration: 88050 loss: 0.0046 lr: 0.02\n",
      "iteration: 88060 loss: 0.0030 lr: 0.02\n",
      "iteration: 88070 loss: 0.0030 lr: 0.02\n",
      "iteration: 88080 loss: 0.0048 lr: 0.02\n",
      "iteration: 88090 loss: 0.0034 lr: 0.02\n",
      "iteration: 88100 loss: 0.0031 lr: 0.02\n",
      "iteration: 88110 loss: 0.0037 lr: 0.02\n",
      "iteration: 88120 loss: 0.0039 lr: 0.02\n",
      "iteration: 88130 loss: 0.0042 lr: 0.02\n",
      "iteration: 88140 loss: 0.0044 lr: 0.02\n",
      "iteration: 88150 loss: 0.0042 lr: 0.02\n",
      "iteration: 88160 loss: 0.0034 lr: 0.02\n",
      "iteration: 88170 loss: 0.0028 lr: 0.02\n",
      "iteration: 88180 loss: 0.0067 lr: 0.02\n",
      "iteration: 88190 loss: 0.0028 lr: 0.02\n",
      "iteration: 88200 loss: 0.0031 lr: 0.02\n",
      "iteration: 88210 loss: 0.0038 lr: 0.02\n",
      "iteration: 88220 loss: 0.0033 lr: 0.02\n",
      "iteration: 88230 loss: 0.0054 lr: 0.02\n",
      "iteration: 88240 loss: 0.0032 lr: 0.02\n",
      "iteration: 88250 loss: 0.0027 lr: 0.02\n",
      "iteration: 88260 loss: 0.0026 lr: 0.02\n",
      "iteration: 88270 loss: 0.0026 lr: 0.02\n",
      "iteration: 88280 loss: 0.0031 lr: 0.02\n",
      "iteration: 88290 loss: 0.0038 lr: 0.02\n",
      "iteration: 88300 loss: 0.0048 lr: 0.02\n",
      "iteration: 88310 loss: 0.0026 lr: 0.02\n",
      "iteration: 88320 loss: 0.0034 lr: 0.02\n",
      "iteration: 88330 loss: 0.0030 lr: 0.02\n",
      "iteration: 88340 loss: 0.0027 lr: 0.02\n",
      "iteration: 88350 loss: 0.0039 lr: 0.02\n",
      "iteration: 88360 loss: 0.0043 lr: 0.02\n",
      "iteration: 88370 loss: 0.0040 lr: 0.02\n",
      "iteration: 88380 loss: 0.0035 lr: 0.02\n",
      "iteration: 88390 loss: 0.0029 lr: 0.02\n",
      "iteration: 88400 loss: 0.0033 lr: 0.02\n",
      "iteration: 88410 loss: 0.0034 lr: 0.02\n",
      "iteration: 88420 loss: 0.0036 lr: 0.02\n",
      "iteration: 88430 loss: 0.0031 lr: 0.02\n",
      "iteration: 88440 loss: 0.0044 lr: 0.02\n",
      "iteration: 88450 loss: 0.0029 lr: 0.02\n",
      "iteration: 88460 loss: 0.0037 lr: 0.02\n",
      "iteration: 88470 loss: 0.0048 lr: 0.02\n",
      "iteration: 88480 loss: 0.0039 lr: 0.02\n",
      "iteration: 88490 loss: 0.0035 lr: 0.02\n",
      "iteration: 88500 loss: 0.0036 lr: 0.02\n",
      "iteration: 88510 loss: 0.0040 lr: 0.02\n",
      "iteration: 88520 loss: 0.0026 lr: 0.02\n",
      "iteration: 88530 loss: 0.0041 lr: 0.02\n",
      "iteration: 88540 loss: 0.0036 lr: 0.02\n",
      "iteration: 88550 loss: 0.0045 lr: 0.02\n",
      "iteration: 88560 loss: 0.0033 lr: 0.02\n",
      "iteration: 88570 loss: 0.0034 lr: 0.02\n",
      "iteration: 88580 loss: 0.0037 lr: 0.02\n",
      "iteration: 88590 loss: 0.0039 lr: 0.02\n",
      "iteration: 88600 loss: 0.0026 lr: 0.02\n",
      "iteration: 88610 loss: 0.0035 lr: 0.02\n",
      "iteration: 88620 loss: 0.0040 lr: 0.02\n",
      "iteration: 88630 loss: 0.0043 lr: 0.02\n",
      "iteration: 88640 loss: 0.0033 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 88650 loss: 0.0025 lr: 0.02\n",
      "iteration: 88660 loss: 0.0039 lr: 0.02\n",
      "iteration: 88670 loss: 0.0032 lr: 0.02\n",
      "iteration: 88680 loss: 0.0044 lr: 0.02\n",
      "iteration: 88690 loss: 0.0036 lr: 0.02\n",
      "iteration: 88700 loss: 0.0027 lr: 0.02\n",
      "iteration: 88710 loss: 0.0032 lr: 0.02\n",
      "iteration: 88720 loss: 0.0043 lr: 0.02\n",
      "iteration: 88730 loss: 0.0036 lr: 0.02\n",
      "iteration: 88740 loss: 0.0032 lr: 0.02\n",
      "iteration: 88750 loss: 0.0034 lr: 0.02\n",
      "iteration: 88760 loss: 0.0030 lr: 0.02\n",
      "iteration: 88770 loss: 0.0026 lr: 0.02\n",
      "iteration: 88780 loss: 0.0039 lr: 0.02\n",
      "iteration: 88790 loss: 0.0031 lr: 0.02\n",
      "iteration: 88800 loss: 0.0033 lr: 0.02\n",
      "iteration: 88810 loss: 0.0025 lr: 0.02\n",
      "iteration: 88820 loss: 0.0029 lr: 0.02\n",
      "iteration: 88830 loss: 0.0029 lr: 0.02\n",
      "iteration: 88840 loss: 0.0028 lr: 0.02\n",
      "iteration: 88850 loss: 0.0024 lr: 0.02\n",
      "iteration: 88860 loss: 0.0018 lr: 0.02\n",
      "iteration: 88870 loss: 0.0032 lr: 0.02\n",
      "iteration: 88880 loss: 0.0025 lr: 0.02\n",
      "iteration: 88890 loss: 0.0034 lr: 0.02\n",
      "iteration: 88900 loss: 0.0024 lr: 0.02\n",
      "iteration: 88910 loss: 0.0039 lr: 0.02\n",
      "iteration: 88920 loss: 0.0023 lr: 0.02\n",
      "iteration: 88930 loss: 0.0030 lr: 0.02\n",
      "iteration: 88940 loss: 0.0031 lr: 0.02\n",
      "iteration: 88950 loss: 0.0035 lr: 0.02\n",
      "iteration: 88960 loss: 0.0031 lr: 0.02\n",
      "iteration: 88970 loss: 0.0026 lr: 0.02\n",
      "iteration: 88980 loss: 0.0032 lr: 0.02\n",
      "iteration: 88990 loss: 0.0025 lr: 0.02\n",
      "iteration: 89000 loss: 0.0025 lr: 0.02\n",
      "iteration: 89010 loss: 0.0025 lr: 0.02\n",
      "iteration: 89020 loss: 0.0024 lr: 0.02\n",
      "iteration: 89030 loss: 0.0048 lr: 0.02\n",
      "iteration: 89040 loss: 0.0032 lr: 0.02\n",
      "iteration: 89050 loss: 0.0053 lr: 0.02\n",
      "iteration: 89060 loss: 0.0034 lr: 0.02\n",
      "iteration: 89070 loss: 0.0036 lr: 0.02\n",
      "iteration: 89080 loss: 0.0024 lr: 0.02\n",
      "iteration: 89090 loss: 0.0039 lr: 0.02\n",
      "iteration: 89100 loss: 0.0044 lr: 0.02\n",
      "iteration: 89110 loss: 0.0020 lr: 0.02\n",
      "iteration: 89120 loss: 0.0037 lr: 0.02\n",
      "iteration: 89130 loss: 0.0043 lr: 0.02\n",
      "iteration: 89140 loss: 0.0040 lr: 0.02\n",
      "iteration: 89150 loss: 0.0020 lr: 0.02\n",
      "iteration: 89160 loss: 0.0029 lr: 0.02\n",
      "iteration: 89170 loss: 0.0034 lr: 0.02\n",
      "iteration: 89180 loss: 0.0034 lr: 0.02\n",
      "iteration: 89190 loss: 0.0037 lr: 0.02\n",
      "iteration: 89200 loss: 0.0032 lr: 0.02\n",
      "iteration: 89210 loss: 0.0036 lr: 0.02\n",
      "iteration: 89220 loss: 0.0029 lr: 0.02\n",
      "iteration: 89230 loss: 0.0040 lr: 0.02\n",
      "iteration: 89240 loss: 0.0043 lr: 0.02\n",
      "iteration: 89250 loss: 0.0032 lr: 0.02\n",
      "iteration: 89260 loss: 0.0038 lr: 0.02\n",
      "iteration: 89270 loss: 0.0026 lr: 0.02\n",
      "iteration: 89280 loss: 0.0032 lr: 0.02\n",
      "iteration: 89290 loss: 0.0028 lr: 0.02\n",
      "iteration: 89300 loss: 0.0047 lr: 0.02\n",
      "iteration: 89310 loss: 0.0039 lr: 0.02\n",
      "iteration: 89320 loss: 0.0045 lr: 0.02\n",
      "iteration: 89330 loss: 0.0039 lr: 0.02\n",
      "iteration: 89340 loss: 0.0044 lr: 0.02\n",
      "iteration: 89350 loss: 0.0030 lr: 0.02\n",
      "iteration: 89360 loss: 0.0031 lr: 0.02\n",
      "iteration: 89370 loss: 0.0032 lr: 0.02\n",
      "iteration: 89380 loss: 0.0048 lr: 0.02\n",
      "iteration: 89390 loss: 0.0045 lr: 0.02\n",
      "iteration: 89400 loss: 0.0035 lr: 0.02\n",
      "iteration: 89410 loss: 0.0033 lr: 0.02\n",
      "iteration: 89420 loss: 0.0035 lr: 0.02\n",
      "iteration: 89430 loss: 0.0027 lr: 0.02\n",
      "iteration: 89440 loss: 0.0032 lr: 0.02\n",
      "iteration: 89450 loss: 0.0029 lr: 0.02\n",
      "iteration: 89460 loss: 0.0029 lr: 0.02\n",
      "iteration: 89470 loss: 0.0030 lr: 0.02\n",
      "iteration: 89480 loss: 0.0032 lr: 0.02\n",
      "iteration: 89490 loss: 0.0037 lr: 0.02\n",
      "iteration: 89500 loss: 0.0029 lr: 0.02\n",
      "iteration: 89510 loss: 0.0036 lr: 0.02\n",
      "iteration: 89520 loss: 0.0024 lr: 0.02\n",
      "iteration: 89530 loss: 0.0045 lr: 0.02\n",
      "iteration: 89540 loss: 0.0047 lr: 0.02\n",
      "iteration: 89550 loss: 0.0053 lr: 0.02\n",
      "iteration: 89560 loss: 0.0060 lr: 0.02\n",
      "iteration: 89570 loss: 0.0041 lr: 0.02\n",
      "iteration: 89580 loss: 0.0034 lr: 0.02\n",
      "iteration: 89590 loss: 0.0043 lr: 0.02\n",
      "iteration: 89600 loss: 0.0037 lr: 0.02\n",
      "iteration: 89610 loss: 0.0041 lr: 0.02\n",
      "iteration: 89620 loss: 0.0021 lr: 0.02\n",
      "iteration: 89630 loss: 0.0036 lr: 0.02\n",
      "iteration: 89640 loss: 0.0032 lr: 0.02\n",
      "iteration: 89650 loss: 0.0038 lr: 0.02\n",
      "iteration: 89660 loss: 0.0035 lr: 0.02\n",
      "iteration: 89670 loss: 0.0034 lr: 0.02\n",
      "iteration: 89680 loss: 0.0060 lr: 0.02\n",
      "iteration: 89690 loss: 0.0033 lr: 0.02\n",
      "iteration: 89700 loss: 0.0027 lr: 0.02\n",
      "iteration: 89710 loss: 0.0037 lr: 0.02\n",
      "iteration: 89720 loss: 0.0031 lr: 0.02\n",
      "iteration: 89730 loss: 0.0037 lr: 0.02\n",
      "iteration: 89740 loss: 0.0030 lr: 0.02\n",
      "iteration: 89750 loss: 0.0029 lr: 0.02\n",
      "iteration: 89760 loss: 0.0042 lr: 0.02\n",
      "iteration: 89770 loss: 0.0029 lr: 0.02\n",
      "iteration: 89780 loss: 0.0024 lr: 0.02\n",
      "iteration: 89790 loss: 0.0040 lr: 0.02\n",
      "iteration: 89800 loss: 0.0043 lr: 0.02\n",
      "iteration: 89810 loss: 0.0032 lr: 0.02\n",
      "iteration: 89820 loss: 0.0034 lr: 0.02\n",
      "iteration: 89830 loss: 0.0035 lr: 0.02\n",
      "iteration: 89840 loss: 0.0032 lr: 0.02\n",
      "iteration: 89850 loss: 0.0028 lr: 0.02\n",
      "iteration: 89860 loss: 0.0042 lr: 0.02\n",
      "iteration: 89870 loss: 0.0039 lr: 0.02\n",
      "iteration: 89880 loss: 0.0034 lr: 0.02\n",
      "iteration: 89890 loss: 0.0051 lr: 0.02\n",
      "iteration: 89900 loss: 0.0036 lr: 0.02\n",
      "iteration: 89910 loss: 0.0034 lr: 0.02\n",
      "iteration: 89920 loss: 0.0030 lr: 0.02\n",
      "iteration: 89930 loss: 0.0030 lr: 0.02\n",
      "iteration: 89940 loss: 0.0026 lr: 0.02\n",
      "iteration: 89950 loss: 0.0042 lr: 0.02\n",
      "iteration: 89960 loss: 0.0034 lr: 0.02\n",
      "iteration: 89970 loss: 0.0040 lr: 0.02\n",
      "iteration: 89980 loss: 0.0043 lr: 0.02\n",
      "iteration: 89990 loss: 0.0034 lr: 0.02\n",
      "iteration: 90000 loss: 0.0038 lr: 0.02\n",
      "iteration: 90010 loss: 0.0031 lr: 0.02\n",
      "iteration: 90020 loss: 0.0043 lr: 0.02\n",
      "iteration: 90030 loss: 0.0026 lr: 0.02\n",
      "iteration: 90040 loss: 0.0040 lr: 0.02\n",
      "iteration: 90050 loss: 0.0029 lr: 0.02\n",
      "iteration: 90060 loss: 0.0030 lr: 0.02\n",
      "iteration: 90070 loss: 0.0048 lr: 0.02\n",
      "iteration: 90080 loss: 0.0042 lr: 0.02\n",
      "iteration: 90090 loss: 0.0046 lr: 0.02\n",
      "iteration: 90100 loss: 0.0029 lr: 0.02\n",
      "iteration: 90110 loss: 0.0040 lr: 0.02\n",
      "iteration: 90120 loss: 0.0040 lr: 0.02\n",
      "iteration: 90130 loss: 0.0039 lr: 0.02\n",
      "iteration: 90140 loss: 0.0027 lr: 0.02\n",
      "iteration: 90150 loss: 0.0039 lr: 0.02\n",
      "iteration: 90160 loss: 0.0046 lr: 0.02\n",
      "iteration: 90170 loss: 0.0029 lr: 0.02\n",
      "iteration: 90180 loss: 0.0029 lr: 0.02\n",
      "iteration: 90190 loss: 0.0031 lr: 0.02\n",
      "iteration: 90200 loss: 0.0026 lr: 0.02\n",
      "iteration: 90210 loss: 0.0025 lr: 0.02\n",
      "iteration: 90220 loss: 0.0031 lr: 0.02\n",
      "iteration: 90230 loss: 0.0035 lr: 0.02\n",
      "iteration: 90240 loss: 0.0041 lr: 0.02\n",
      "iteration: 90250 loss: 0.0048 lr: 0.02\n",
      "iteration: 90260 loss: 0.0032 lr: 0.02\n",
      "iteration: 90270 loss: 0.0046 lr: 0.02\n",
      "iteration: 90280 loss: 0.0050 lr: 0.02\n",
      "iteration: 90290 loss: 0.0031 lr: 0.02\n",
      "iteration: 90300 loss: 0.0042 lr: 0.02\n",
      "iteration: 90310 loss: 0.0028 lr: 0.02\n",
      "iteration: 90320 loss: 0.0056 lr: 0.02\n",
      "iteration: 90330 loss: 0.0036 lr: 0.02\n",
      "iteration: 90340 loss: 0.0058 lr: 0.02\n",
      "iteration: 90350 loss: 0.0031 lr: 0.02\n",
      "iteration: 90360 loss: 0.0037 lr: 0.02\n",
      "iteration: 90370 loss: 0.0035 lr: 0.02\n",
      "iteration: 90380 loss: 0.0038 lr: 0.02\n",
      "iteration: 90390 loss: 0.0035 lr: 0.02\n",
      "iteration: 90400 loss: 0.0039 lr: 0.02\n",
      "iteration: 90410 loss: 0.0029 lr: 0.02\n",
      "iteration: 90420 loss: 0.0050 lr: 0.02\n",
      "iteration: 90430 loss: 0.0053 lr: 0.02\n",
      "iteration: 90440 loss: 0.0034 lr: 0.02\n",
      "iteration: 90450 loss: 0.0036 lr: 0.02\n",
      "iteration: 90460 loss: 0.0030 lr: 0.02\n",
      "iteration: 90470 loss: 0.0024 lr: 0.02\n",
      "iteration: 90480 loss: 0.0039 lr: 0.02\n",
      "iteration: 90490 loss: 0.0027 lr: 0.02\n",
      "iteration: 90500 loss: 0.0031 lr: 0.02\n",
      "iteration: 90510 loss: 0.0038 lr: 0.02\n",
      "iteration: 90520 loss: 0.0031 lr: 0.02\n",
      "iteration: 90530 loss: 0.0036 lr: 0.02\n",
      "iteration: 90540 loss: 0.0049 lr: 0.02\n",
      "iteration: 90550 loss: 0.0029 lr: 0.02\n",
      "iteration: 90560 loss: 0.0036 lr: 0.02\n",
      "iteration: 90570 loss: 0.0036 lr: 0.02\n",
      "iteration: 90580 loss: 0.0038 lr: 0.02\n",
      "iteration: 90590 loss: 0.0035 lr: 0.02\n",
      "iteration: 90600 loss: 0.0036 lr: 0.02\n",
      "iteration: 90610 loss: 0.0034 lr: 0.02\n",
      "iteration: 90620 loss: 0.0033 lr: 0.02\n",
      "iteration: 90630 loss: 0.0056 lr: 0.02\n",
      "iteration: 90640 loss: 0.0032 lr: 0.02\n",
      "iteration: 90650 loss: 0.0037 lr: 0.02\n",
      "iteration: 90660 loss: 0.0032 lr: 0.02\n",
      "iteration: 90670 loss: 0.0056 lr: 0.02\n",
      "iteration: 90680 loss: 0.0033 lr: 0.02\n",
      "iteration: 90690 loss: 0.0035 lr: 0.02\n",
      "iteration: 90700 loss: 0.0035 lr: 0.02\n",
      "iteration: 90710 loss: 0.0032 lr: 0.02\n",
      "iteration: 90720 loss: 0.0031 lr: 0.02\n",
      "iteration: 90730 loss: 0.0045 lr: 0.02\n",
      "iteration: 90740 loss: 0.0038 lr: 0.02\n",
      "iteration: 90750 loss: 0.0038 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 90760 loss: 0.0033 lr: 0.02\n",
      "iteration: 90770 loss: 0.0032 lr: 0.02\n",
      "iteration: 90780 loss: 0.0038 lr: 0.02\n",
      "iteration: 90790 loss: 0.0041 lr: 0.02\n",
      "iteration: 90800 loss: 0.0038 lr: 0.02\n",
      "iteration: 90810 loss: 0.0035 lr: 0.02\n",
      "iteration: 90820 loss: 0.0037 lr: 0.02\n",
      "iteration: 90830 loss: 0.0046 lr: 0.02\n",
      "iteration: 90840 loss: 0.0038 lr: 0.02\n",
      "iteration: 90850 loss: 0.0049 lr: 0.02\n",
      "iteration: 90860 loss: 0.0039 lr: 0.02\n",
      "iteration: 90870 loss: 0.0032 lr: 0.02\n",
      "iteration: 90880 loss: 0.0029 lr: 0.02\n",
      "iteration: 90890 loss: 0.0031 lr: 0.02\n",
      "iteration: 90900 loss: 0.0028 lr: 0.02\n",
      "iteration: 90910 loss: 0.0048 lr: 0.02\n",
      "iteration: 90920 loss: 0.0031 lr: 0.02\n",
      "iteration: 90930 loss: 0.0035 lr: 0.02\n",
      "iteration: 90940 loss: 0.0036 lr: 0.02\n",
      "iteration: 90950 loss: 0.0031 lr: 0.02\n",
      "iteration: 90960 loss: 0.0038 lr: 0.02\n",
      "iteration: 90970 loss: 0.0044 lr: 0.02\n",
      "iteration: 90980 loss: 0.0039 lr: 0.02\n",
      "iteration: 90990 loss: 0.0034 lr: 0.02\n",
      "iteration: 91000 loss: 0.0038 lr: 0.02\n",
      "iteration: 91010 loss: 0.0034 lr: 0.02\n",
      "iteration: 91020 loss: 0.0029 lr: 0.02\n",
      "iteration: 91030 loss: 0.0043 lr: 0.02\n",
      "iteration: 91040 loss: 0.0037 lr: 0.02\n",
      "iteration: 91050 loss: 0.0032 lr: 0.02\n",
      "iteration: 91060 loss: 0.0027 lr: 0.02\n",
      "iteration: 91070 loss: 0.0042 lr: 0.02\n",
      "iteration: 91080 loss: 0.0031 lr: 0.02\n",
      "iteration: 91090 loss: 0.0042 lr: 0.02\n",
      "iteration: 91100 loss: 0.0041 lr: 0.02\n",
      "iteration: 91110 loss: 0.0043 lr: 0.02\n",
      "iteration: 91120 loss: 0.0032 lr: 0.02\n",
      "iteration: 91130 loss: 0.0028 lr: 0.02\n",
      "iteration: 91140 loss: 0.0048 lr: 0.02\n",
      "iteration: 91150 loss: 0.0041 lr: 0.02\n",
      "iteration: 91160 loss: 0.0032 lr: 0.02\n",
      "iteration: 91170 loss: 0.0034 lr: 0.02\n",
      "iteration: 91180 loss: 0.0029 lr: 0.02\n",
      "iteration: 91190 loss: 0.0028 lr: 0.02\n",
      "iteration: 91200 loss: 0.0031 lr: 0.02\n",
      "iteration: 91210 loss: 0.0037 lr: 0.02\n",
      "iteration: 91220 loss: 0.0027 lr: 0.02\n",
      "iteration: 91230 loss: 0.0020 lr: 0.02\n",
      "iteration: 91240 loss: 0.0035 lr: 0.02\n",
      "iteration: 91250 loss: 0.0037 lr: 0.02\n",
      "iteration: 91260 loss: 0.0036 lr: 0.02\n",
      "iteration: 91270 loss: 0.0025 lr: 0.02\n",
      "iteration: 91280 loss: 0.0044 lr: 0.02\n",
      "iteration: 91290 loss: 0.0038 lr: 0.02\n",
      "iteration: 91300 loss: 0.0047 lr: 0.02\n",
      "iteration: 91310 loss: 0.0065 lr: 0.02\n",
      "iteration: 91320 loss: 0.0032 lr: 0.02\n",
      "iteration: 91330 loss: 0.0032 lr: 0.02\n",
      "iteration: 91340 loss: 0.0031 lr: 0.02\n",
      "iteration: 91350 loss: 0.0030 lr: 0.02\n",
      "iteration: 91360 loss: 0.0032 lr: 0.02\n",
      "iteration: 91370 loss: 0.0061 lr: 0.02\n",
      "iteration: 91380 loss: 0.0030 lr: 0.02\n",
      "iteration: 91390 loss: 0.0032 lr: 0.02\n",
      "iteration: 91400 loss: 0.0034 lr: 0.02\n",
      "iteration: 91410 loss: 0.0029 lr: 0.02\n",
      "iteration: 91420 loss: 0.0038 lr: 0.02\n",
      "iteration: 91430 loss: 0.0029 lr: 0.02\n",
      "iteration: 91440 loss: 0.0040 lr: 0.02\n",
      "iteration: 91450 loss: 0.0034 lr: 0.02\n",
      "iteration: 91460 loss: 0.0036 lr: 0.02\n",
      "iteration: 91470 loss: 0.0040 lr: 0.02\n",
      "iteration: 91480 loss: 0.0036 lr: 0.02\n",
      "iteration: 91490 loss: 0.0025 lr: 0.02\n",
      "iteration: 91500 loss: 0.0040 lr: 0.02\n",
      "iteration: 91510 loss: 0.0042 lr: 0.02\n",
      "iteration: 91520 loss: 0.0034 lr: 0.02\n",
      "iteration: 91530 loss: 0.0028 lr: 0.02\n",
      "iteration: 91540 loss: 0.0027 lr: 0.02\n",
      "iteration: 91550 loss: 0.0056 lr: 0.02\n",
      "iteration: 91560 loss: 0.0028 lr: 0.02\n",
      "iteration: 91570 loss: 0.0039 lr: 0.02\n",
      "iteration: 91580 loss: 0.0030 lr: 0.02\n",
      "iteration: 91590 loss: 0.0042 lr: 0.02\n",
      "iteration: 91600 loss: 0.0036 lr: 0.02\n",
      "iteration: 91610 loss: 0.0036 lr: 0.02\n",
      "iteration: 91620 loss: 0.0037 lr: 0.02\n",
      "iteration: 91630 loss: 0.0027 lr: 0.02\n",
      "iteration: 91640 loss: 0.0037 lr: 0.02\n",
      "iteration: 91650 loss: 0.0036 lr: 0.02\n",
      "iteration: 91660 loss: 0.0043 lr: 0.02\n",
      "iteration: 91670 loss: 0.0051 lr: 0.02\n",
      "iteration: 91680 loss: 0.0032 lr: 0.02\n",
      "iteration: 91690 loss: 0.0049 lr: 0.02\n",
      "iteration: 91700 loss: 0.0036 lr: 0.02\n",
      "iteration: 91710 loss: 0.0035 lr: 0.02\n",
      "iteration: 91720 loss: 0.0054 lr: 0.02\n",
      "iteration: 91730 loss: 0.0026 lr: 0.02\n",
      "iteration: 91740 loss: 0.0035 lr: 0.02\n",
      "iteration: 91750 loss: 0.0030 lr: 0.02\n",
      "iteration: 91760 loss: 0.0032 lr: 0.02\n",
      "iteration: 91770 loss: 0.0029 lr: 0.02\n",
      "iteration: 91780 loss: 0.0028 lr: 0.02\n",
      "iteration: 91790 loss: 0.0028 lr: 0.02\n",
      "iteration: 91800 loss: 0.0035 lr: 0.02\n",
      "iteration: 91810 loss: 0.0039 lr: 0.02\n",
      "iteration: 91820 loss: 0.0032 lr: 0.02\n",
      "iteration: 91830 loss: 0.0031 lr: 0.02\n",
      "iteration: 91840 loss: 0.0031 lr: 0.02\n",
      "iteration: 91850 loss: 0.0025 lr: 0.02\n",
      "iteration: 91860 loss: 0.0059 lr: 0.02\n",
      "iteration: 91870 loss: 0.0030 lr: 0.02\n",
      "iteration: 91880 loss: 0.0032 lr: 0.02\n",
      "iteration: 91890 loss: 0.0038 lr: 0.02\n",
      "iteration: 91900 loss: 0.0032 lr: 0.02\n",
      "iteration: 91910 loss: 0.0038 lr: 0.02\n",
      "iteration: 91920 loss: 0.0034 lr: 0.02\n",
      "iteration: 91930 loss: 0.0040 lr: 0.02\n",
      "iteration: 91940 loss: 0.0033 lr: 0.02\n",
      "iteration: 91950 loss: 0.0034 lr: 0.02\n",
      "iteration: 91960 loss: 0.0033 lr: 0.02\n",
      "iteration: 91970 loss: 0.0042 lr: 0.02\n",
      "iteration: 91980 loss: 0.0024 lr: 0.02\n",
      "iteration: 91990 loss: 0.0045 lr: 0.02\n",
      "iteration: 92000 loss: 0.0040 lr: 0.02\n",
      "iteration: 92010 loss: 0.0024 lr: 0.02\n",
      "iteration: 92020 loss: 0.0028 lr: 0.02\n",
      "iteration: 92030 loss: 0.0039 lr: 0.02\n",
      "iteration: 92040 loss: 0.0050 lr: 0.02\n",
      "iteration: 92050 loss: 0.0048 lr: 0.02\n",
      "iteration: 92060 loss: 0.0033 lr: 0.02\n",
      "iteration: 92070 loss: 0.0025 lr: 0.02\n",
      "iteration: 92080 loss: 0.0036 lr: 0.02\n",
      "iteration: 92090 loss: 0.0042 lr: 0.02\n",
      "iteration: 92100 loss: 0.0035 lr: 0.02\n",
      "iteration: 92110 loss: 0.0032 lr: 0.02\n",
      "iteration: 92120 loss: 0.0044 lr: 0.02\n",
      "iteration: 92130 loss: 0.0041 lr: 0.02\n",
      "iteration: 92140 loss: 0.0046 lr: 0.02\n",
      "iteration: 92150 loss: 0.0026 lr: 0.02\n",
      "iteration: 92160 loss: 0.0050 lr: 0.02\n",
      "iteration: 92170 loss: 0.0041 lr: 0.02\n",
      "iteration: 92180 loss: 0.0041 lr: 0.02\n",
      "iteration: 92190 loss: 0.0033 lr: 0.02\n",
      "iteration: 92200 loss: 0.0060 lr: 0.02\n",
      "iteration: 92210 loss: 0.0052 lr: 0.02\n",
      "iteration: 92220 loss: 0.0046 lr: 0.02\n",
      "iteration: 92230 loss: 0.0051 lr: 0.02\n",
      "iteration: 92240 loss: 0.0040 lr: 0.02\n",
      "iteration: 92250 loss: 0.0032 lr: 0.02\n",
      "iteration: 92260 loss: 0.0032 lr: 0.02\n",
      "iteration: 92270 loss: 0.0043 lr: 0.02\n",
      "iteration: 92280 loss: 0.0028 lr: 0.02\n",
      "iteration: 92290 loss: 0.0025 lr: 0.02\n",
      "iteration: 92300 loss: 0.0051 lr: 0.02\n",
      "iteration: 92310 loss: 0.0037 lr: 0.02\n",
      "iteration: 92320 loss: 0.0049 lr: 0.02\n",
      "iteration: 92330 loss: 0.0030 lr: 0.02\n",
      "iteration: 92340 loss: 0.0034 lr: 0.02\n",
      "iteration: 92350 loss: 0.0029 lr: 0.02\n",
      "iteration: 92360 loss: 0.0039 lr: 0.02\n",
      "iteration: 92370 loss: 0.0039 lr: 0.02\n",
      "iteration: 92380 loss: 0.0030 lr: 0.02\n",
      "iteration: 92390 loss: 0.0031 lr: 0.02\n",
      "iteration: 92400 loss: 0.0023 lr: 0.02\n",
      "iteration: 92410 loss: 0.0027 lr: 0.02\n",
      "iteration: 92420 loss: 0.0038 lr: 0.02\n",
      "iteration: 92430 loss: 0.0032 lr: 0.02\n",
      "iteration: 92440 loss: 0.0030 lr: 0.02\n",
      "iteration: 92450 loss: 0.0031 lr: 0.02\n",
      "iteration: 92460 loss: 0.0025 lr: 0.02\n",
      "iteration: 92470 loss: 0.0061 lr: 0.02\n",
      "iteration: 92480 loss: 0.0034 lr: 0.02\n",
      "iteration: 92490 loss: 0.0039 lr: 0.02\n",
      "iteration: 92500 loss: 0.0035 lr: 0.02\n",
      "iteration: 92510 loss: 0.0037 lr: 0.02\n",
      "iteration: 92520 loss: 0.0035 lr: 0.02\n",
      "iteration: 92530 loss: 0.0037 lr: 0.02\n",
      "iteration: 92540 loss: 0.0027 lr: 0.02\n",
      "iteration: 92550 loss: 0.0031 lr: 0.02\n",
      "iteration: 92560 loss: 0.0038 lr: 0.02\n",
      "iteration: 92570 loss: 0.0027 lr: 0.02\n",
      "iteration: 92580 loss: 0.0024 lr: 0.02\n",
      "iteration: 92590 loss: 0.0035 lr: 0.02\n",
      "iteration: 92600 loss: 0.0043 lr: 0.02\n",
      "iteration: 92610 loss: 0.0029 lr: 0.02\n",
      "iteration: 92620 loss: 0.0025 lr: 0.02\n",
      "iteration: 92630 loss: 0.0041 lr: 0.02\n",
      "iteration: 92640 loss: 0.0038 lr: 0.02\n",
      "iteration: 92650 loss: 0.0039 lr: 0.02\n",
      "iteration: 92660 loss: 0.0036 lr: 0.02\n",
      "iteration: 92670 loss: 0.0034 lr: 0.02\n",
      "iteration: 92680 loss: 0.0045 lr: 0.02\n",
      "iteration: 92690 loss: 0.0039 lr: 0.02\n",
      "iteration: 92700 loss: 0.0030 lr: 0.02\n",
      "iteration: 92710 loss: 0.0036 lr: 0.02\n",
      "iteration: 92720 loss: 0.0034 lr: 0.02\n",
      "iteration: 92730 loss: 0.0029 lr: 0.02\n",
      "iteration: 92740 loss: 0.0028 lr: 0.02\n",
      "iteration: 92750 loss: 0.0041 lr: 0.02\n",
      "iteration: 92760 loss: 0.0041 lr: 0.02\n",
      "iteration: 92770 loss: 0.0031 lr: 0.02\n",
      "iteration: 92780 loss: 0.0040 lr: 0.02\n",
      "iteration: 92790 loss: 0.0031 lr: 0.02\n",
      "iteration: 92800 loss: 0.0037 lr: 0.02\n",
      "iteration: 92810 loss: 0.0029 lr: 0.02\n",
      "iteration: 92820 loss: 0.0030 lr: 0.02\n",
      "iteration: 92830 loss: 0.0038 lr: 0.02\n",
      "iteration: 92840 loss: 0.0029 lr: 0.02\n",
      "iteration: 92850 loss: 0.0019 lr: 0.02\n",
      "iteration: 92860 loss: 0.0027 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 92870 loss: 0.0033 lr: 0.02\n",
      "iteration: 92880 loss: 0.0033 lr: 0.02\n",
      "iteration: 92890 loss: 0.0028 lr: 0.02\n",
      "iteration: 92900 loss: 0.0031 lr: 0.02\n",
      "iteration: 92910 loss: 0.0033 lr: 0.02\n",
      "iteration: 92920 loss: 0.0030 lr: 0.02\n",
      "iteration: 92930 loss: 0.0044 lr: 0.02\n",
      "iteration: 92940 loss: 0.0032 lr: 0.02\n",
      "iteration: 92950 loss: 0.0028 lr: 0.02\n",
      "iteration: 92960 loss: 0.0054 lr: 0.02\n",
      "iteration: 92970 loss: 0.0030 lr: 0.02\n",
      "iteration: 92980 loss: 0.0033 lr: 0.02\n",
      "iteration: 92990 loss: 0.0025 lr: 0.02\n",
      "iteration: 93000 loss: 0.0034 lr: 0.02\n",
      "iteration: 93010 loss: 0.0048 lr: 0.02\n",
      "iteration: 93020 loss: 0.0037 lr: 0.02\n",
      "iteration: 93030 loss: 0.0035 lr: 0.02\n",
      "iteration: 93040 loss: 0.0028 lr: 0.02\n",
      "iteration: 93050 loss: 0.0041 lr: 0.02\n",
      "iteration: 93060 loss: 0.0027 lr: 0.02\n",
      "iteration: 93070 loss: 0.0061 lr: 0.02\n",
      "iteration: 93080 loss: 0.0035 lr: 0.02\n",
      "iteration: 93090 loss: 0.0035 lr: 0.02\n",
      "iteration: 93100 loss: 0.0042 lr: 0.02\n",
      "iteration: 93110 loss: 0.0033 lr: 0.02\n",
      "iteration: 93120 loss: 0.0030 lr: 0.02\n",
      "iteration: 93130 loss: 0.0028 lr: 0.02\n",
      "iteration: 93140 loss: 0.0033 lr: 0.02\n",
      "iteration: 93150 loss: 0.0032 lr: 0.02\n",
      "iteration: 93160 loss: 0.0026 lr: 0.02\n",
      "iteration: 93170 loss: 0.0029 lr: 0.02\n",
      "iteration: 93180 loss: 0.0029 lr: 0.02\n",
      "iteration: 93190 loss: 0.0031 lr: 0.02\n",
      "iteration: 93200 loss: 0.0056 lr: 0.02\n",
      "iteration: 93210 loss: 0.0033 lr: 0.02\n",
      "iteration: 93220 loss: 0.0035 lr: 0.02\n",
      "iteration: 93230 loss: 0.0034 lr: 0.02\n",
      "iteration: 93240 loss: 0.0035 lr: 0.02\n",
      "iteration: 93250 loss: 0.0050 lr: 0.02\n",
      "iteration: 93260 loss: 0.0038 lr: 0.02\n",
      "iteration: 93270 loss: 0.0040 lr: 0.02\n",
      "iteration: 93280 loss: 0.0031 lr: 0.02\n",
      "iteration: 93290 loss: 0.0034 lr: 0.02\n",
      "iteration: 93300 loss: 0.0042 lr: 0.02\n",
      "iteration: 93310 loss: 0.0042 lr: 0.02\n",
      "iteration: 93320 loss: 0.0034 lr: 0.02\n",
      "iteration: 93330 loss: 0.0034 lr: 0.02\n",
      "iteration: 93340 loss: 0.0031 lr: 0.02\n",
      "iteration: 93350 loss: 0.0047 lr: 0.02\n",
      "iteration: 93360 loss: 0.0034 lr: 0.02\n",
      "iteration: 93370 loss: 0.0030 lr: 0.02\n",
      "iteration: 93380 loss: 0.0042 lr: 0.02\n",
      "iteration: 93390 loss: 0.0025 lr: 0.02\n",
      "iteration: 93400 loss: 0.0035 lr: 0.02\n",
      "iteration: 93410 loss: 0.0036 lr: 0.02\n",
      "iteration: 93420 loss: 0.0028 lr: 0.02\n",
      "iteration: 93430 loss: 0.0030 lr: 0.02\n",
      "iteration: 93440 loss: 0.0023 lr: 0.02\n",
      "iteration: 93450 loss: 0.0043 lr: 0.02\n",
      "iteration: 93460 loss: 0.0024 lr: 0.02\n",
      "iteration: 93470 loss: 0.0034 lr: 0.02\n",
      "iteration: 93480 loss: 0.0027 lr: 0.02\n",
      "iteration: 93490 loss: 0.0029 lr: 0.02\n",
      "iteration: 93500 loss: 0.0042 lr: 0.02\n",
      "iteration: 93510 loss: 0.0035 lr: 0.02\n",
      "iteration: 93520 loss: 0.0036 lr: 0.02\n",
      "iteration: 93530 loss: 0.0039 lr: 0.02\n",
      "iteration: 93540 loss: 0.0047 lr: 0.02\n",
      "iteration: 93550 loss: 0.0035 lr: 0.02\n",
      "iteration: 93560 loss: 0.0030 lr: 0.02\n",
      "iteration: 93570 loss: 0.0045 lr: 0.02\n",
      "iteration: 93580 loss: 0.0031 lr: 0.02\n",
      "iteration: 93590 loss: 0.0039 lr: 0.02\n",
      "iteration: 93600 loss: 0.0044 lr: 0.02\n",
      "iteration: 93610 loss: 0.0041 lr: 0.02\n",
      "iteration: 93620 loss: 0.0031 lr: 0.02\n",
      "iteration: 93630 loss: 0.0040 lr: 0.02\n",
      "iteration: 93640 loss: 0.0033 lr: 0.02\n",
      "iteration: 93650 loss: 0.0041 lr: 0.02\n",
      "iteration: 93660 loss: 0.0036 lr: 0.02\n",
      "iteration: 93670 loss: 0.0041 lr: 0.02\n",
      "iteration: 93680 loss: 0.0047 lr: 0.02\n",
      "iteration: 93690 loss: 0.0038 lr: 0.02\n",
      "iteration: 93700 loss: 0.0040 lr: 0.02\n",
      "iteration: 93710 loss: 0.0028 lr: 0.02\n",
      "iteration: 93720 loss: 0.0033 lr: 0.02\n",
      "iteration: 93730 loss: 0.0049 lr: 0.02\n",
      "iteration: 93740 loss: 0.0034 lr: 0.02\n",
      "iteration: 93750 loss: 0.0038 lr: 0.02\n",
      "iteration: 93760 loss: 0.0042 lr: 0.02\n",
      "iteration: 93770 loss: 0.0033 lr: 0.02\n",
      "iteration: 93780 loss: 0.0048 lr: 0.02\n",
      "iteration: 93790 loss: 0.0043 lr: 0.02\n",
      "iteration: 93800 loss: 0.0039 lr: 0.02\n",
      "iteration: 93810 loss: 0.0036 lr: 0.02\n",
      "iteration: 93820 loss: 0.0043 lr: 0.02\n",
      "iteration: 93830 loss: 0.0038 lr: 0.02\n",
      "iteration: 93840 loss: 0.0037 lr: 0.02\n",
      "iteration: 93850 loss: 0.0028 lr: 0.02\n",
      "iteration: 93860 loss: 0.0034 lr: 0.02\n",
      "iteration: 93870 loss: 0.0031 lr: 0.02\n",
      "iteration: 93880 loss: 0.0041 lr: 0.02\n",
      "iteration: 93890 loss: 0.0027 lr: 0.02\n",
      "iteration: 93900 loss: 0.0026 lr: 0.02\n",
      "iteration: 93910 loss: 0.0042 lr: 0.02\n",
      "iteration: 93920 loss: 0.0044 lr: 0.02\n",
      "iteration: 93930 loss: 0.0027 lr: 0.02\n",
      "iteration: 93940 loss: 0.0036 lr: 0.02\n",
      "iteration: 93950 loss: 0.0039 lr: 0.02\n",
      "iteration: 93960 loss: 0.0031 lr: 0.02\n",
      "iteration: 93970 loss: 0.0028 lr: 0.02\n",
      "iteration: 93980 loss: 0.0035 lr: 0.02\n",
      "iteration: 93990 loss: 0.0027 lr: 0.02\n",
      "iteration: 94000 loss: 0.0023 lr: 0.02\n",
      "iteration: 94010 loss: 0.0033 lr: 0.02\n",
      "iteration: 94020 loss: 0.0034 lr: 0.02\n",
      "iteration: 94030 loss: 0.0037 lr: 0.02\n",
      "iteration: 94040 loss: 0.0035 lr: 0.02\n",
      "iteration: 94050 loss: 0.0040 lr: 0.02\n",
      "iteration: 94060 loss: 0.0035 lr: 0.02\n",
      "iteration: 94070 loss: 0.0041 lr: 0.02\n",
      "iteration: 94080 loss: 0.0028 lr: 0.02\n",
      "iteration: 94090 loss: 0.0032 lr: 0.02\n",
      "iteration: 94100 loss: 0.0031 lr: 0.02\n",
      "iteration: 94110 loss: 0.0032 lr: 0.02\n",
      "iteration: 94120 loss: 0.0025 lr: 0.02\n",
      "iteration: 94130 loss: 0.0041 lr: 0.02\n",
      "iteration: 94140 loss: 0.0033 lr: 0.02\n",
      "iteration: 94150 loss: 0.0035 lr: 0.02\n",
      "iteration: 94160 loss: 0.0037 lr: 0.02\n",
      "iteration: 94170 loss: 0.0037 lr: 0.02\n",
      "iteration: 94180 loss: 0.0032 lr: 0.02\n",
      "iteration: 94190 loss: 0.0031 lr: 0.02\n",
      "iteration: 94200 loss: 0.0028 lr: 0.02\n",
      "iteration: 94210 loss: 0.0033 lr: 0.02\n",
      "iteration: 94220 loss: 0.0031 lr: 0.02\n",
      "iteration: 94230 loss: 0.0031 lr: 0.02\n",
      "iteration: 94240 loss: 0.0035 lr: 0.02\n",
      "iteration: 94250 loss: 0.0028 lr: 0.02\n",
      "iteration: 94260 loss: 0.0034 lr: 0.02\n",
      "iteration: 94270 loss: 0.0044 lr: 0.02\n",
      "iteration: 94280 loss: 0.0027 lr: 0.02\n",
      "iteration: 94290 loss: 0.0040 lr: 0.02\n",
      "iteration: 94300 loss: 0.0033 lr: 0.02\n",
      "iteration: 94310 loss: 0.0033 lr: 0.02\n",
      "iteration: 94320 loss: 0.0029 lr: 0.02\n",
      "iteration: 94330 loss: 0.0040 lr: 0.02\n",
      "iteration: 94340 loss: 0.0039 lr: 0.02\n",
      "iteration: 94350 loss: 0.0027 lr: 0.02\n",
      "iteration: 94360 loss: 0.0031 lr: 0.02\n",
      "iteration: 94370 loss: 0.0025 lr: 0.02\n",
      "iteration: 94380 loss: 0.0051 lr: 0.02\n",
      "iteration: 94390 loss: 0.0040 lr: 0.02\n",
      "iteration: 94400 loss: 0.0046 lr: 0.02\n",
      "iteration: 94410 loss: 0.0024 lr: 0.02\n",
      "iteration: 94420 loss: 0.0027 lr: 0.02\n",
      "iteration: 94430 loss: 0.0040 lr: 0.02\n",
      "iteration: 94440 loss: 0.0027 lr: 0.02\n",
      "iteration: 94450 loss: 0.0028 lr: 0.02\n",
      "iteration: 94460 loss: 0.0033 lr: 0.02\n",
      "iteration: 94470 loss: 0.0030 lr: 0.02\n",
      "iteration: 94480 loss: 0.0040 lr: 0.02\n",
      "iteration: 94490 loss: 0.0022 lr: 0.02\n",
      "iteration: 94500 loss: 0.0055 lr: 0.02\n",
      "iteration: 94510 loss: 0.0022 lr: 0.02\n",
      "iteration: 94520 loss: 0.0038 lr: 0.02\n",
      "iteration: 94530 loss: 0.0030 lr: 0.02\n",
      "iteration: 94540 loss: 0.0044 lr: 0.02\n",
      "iteration: 94550 loss: 0.0030 lr: 0.02\n",
      "iteration: 94560 loss: 0.0030 lr: 0.02\n",
      "iteration: 94570 loss: 0.0036 lr: 0.02\n",
      "iteration: 94580 loss: 0.0027 lr: 0.02\n",
      "iteration: 94590 loss: 0.0028 lr: 0.02\n",
      "iteration: 94600 loss: 0.0044 lr: 0.02\n",
      "iteration: 94610 loss: 0.0033 lr: 0.02\n",
      "iteration: 94620 loss: 0.0032 lr: 0.02\n",
      "iteration: 94630 loss: 0.0034 lr: 0.02\n",
      "iteration: 94640 loss: 0.0036 lr: 0.02\n",
      "iteration: 94650 loss: 0.0026 lr: 0.02\n",
      "iteration: 94660 loss: 0.0039 lr: 0.02\n",
      "iteration: 94670 loss: 0.0036 lr: 0.02\n",
      "iteration: 94680 loss: 0.0023 lr: 0.02\n",
      "iteration: 94690 loss: 0.0042 lr: 0.02\n",
      "iteration: 94700 loss: 0.0032 lr: 0.02\n",
      "iteration: 94710 loss: 0.0049 lr: 0.02\n",
      "iteration: 94720 loss: 0.0027 lr: 0.02\n",
      "iteration: 94730 loss: 0.0036 lr: 0.02\n",
      "iteration: 94740 loss: 0.0027 lr: 0.02\n",
      "iteration: 94750 loss: 0.0030 lr: 0.02\n",
      "iteration: 94760 loss: 0.0040 lr: 0.02\n",
      "iteration: 94770 loss: 0.0033 lr: 0.02\n",
      "iteration: 94780 loss: 0.0034 lr: 0.02\n",
      "iteration: 94790 loss: 0.0047 lr: 0.02\n",
      "iteration: 94800 loss: 0.0036 lr: 0.02\n",
      "iteration: 94810 loss: 0.0021 lr: 0.02\n",
      "iteration: 94820 loss: 0.0045 lr: 0.02\n",
      "iteration: 94830 loss: 0.0029 lr: 0.02\n",
      "iteration: 94840 loss: 0.0036 lr: 0.02\n",
      "iteration: 94850 loss: 0.0024 lr: 0.02\n",
      "iteration: 94860 loss: 0.0029 lr: 0.02\n",
      "iteration: 94870 loss: 0.0027 lr: 0.02\n",
      "iteration: 94880 loss: 0.0037 lr: 0.02\n",
      "iteration: 94890 loss: 0.0022 lr: 0.02\n",
      "iteration: 94900 loss: 0.0039 lr: 0.02\n",
      "iteration: 94910 loss: 0.0025 lr: 0.02\n",
      "iteration: 94920 loss: 0.0033 lr: 0.02\n",
      "iteration: 94930 loss: 0.0042 lr: 0.02\n",
      "iteration: 94940 loss: 0.0036 lr: 0.02\n",
      "iteration: 94950 loss: 0.0029 lr: 0.02\n",
      "iteration: 94960 loss: 0.0028 lr: 0.02\n",
      "iteration: 94970 loss: 0.0027 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 94980 loss: 0.0048 lr: 0.02\n",
      "iteration: 94990 loss: 0.0045 lr: 0.02\n",
      "iteration: 95000 loss: 0.0058 lr: 0.02\n",
      "iteration: 95010 loss: 0.0052 lr: 0.02\n",
      "iteration: 95020 loss: 0.0046 lr: 0.02\n",
      "iteration: 95030 loss: 0.0025 lr: 0.02\n",
      "iteration: 95040 loss: 0.0031 lr: 0.02\n",
      "iteration: 95050 loss: 0.0029 lr: 0.02\n",
      "iteration: 95060 loss: 0.0033 lr: 0.02\n",
      "iteration: 95070 loss: 0.0030 lr: 0.02\n",
      "iteration: 95080 loss: 0.0031 lr: 0.02\n",
      "iteration: 95090 loss: 0.0048 lr: 0.02\n",
      "iteration: 95100 loss: 0.0029 lr: 0.02\n",
      "iteration: 95110 loss: 0.0044 lr: 0.02\n",
      "iteration: 95120 loss: 0.0032 lr: 0.02\n",
      "iteration: 95130 loss: 0.0031 lr: 0.02\n",
      "iteration: 95140 loss: 0.0032 lr: 0.02\n",
      "iteration: 95150 loss: 0.0033 lr: 0.02\n",
      "iteration: 95160 loss: 0.0027 lr: 0.02\n",
      "iteration: 95170 loss: 0.0049 lr: 0.02\n",
      "iteration: 95180 loss: 0.0031 lr: 0.02\n",
      "iteration: 95190 loss: 0.0037 lr: 0.02\n",
      "iteration: 95200 loss: 0.0025 lr: 0.02\n",
      "iteration: 95210 loss: 0.0040 lr: 0.02\n",
      "iteration: 95220 loss: 0.0034 lr: 0.02\n",
      "iteration: 95230 loss: 0.0039 lr: 0.02\n",
      "iteration: 95240 loss: 0.0031 lr: 0.02\n",
      "iteration: 95250 loss: 0.0041 lr: 0.02\n",
      "iteration: 95260 loss: 0.0038 lr: 0.02\n",
      "iteration: 95270 loss: 0.0028 lr: 0.02\n",
      "iteration: 95280 loss: 0.0045 lr: 0.02\n",
      "iteration: 95290 loss: 0.0033 lr: 0.02\n",
      "iteration: 95300 loss: 0.0036 lr: 0.02\n",
      "iteration: 95310 loss: 0.0037 lr: 0.02\n",
      "iteration: 95320 loss: 0.0036 lr: 0.02\n",
      "iteration: 95330 loss: 0.0029 lr: 0.02\n",
      "iteration: 95340 loss: 0.0030 lr: 0.02\n",
      "iteration: 95350 loss: 0.0035 lr: 0.02\n",
      "iteration: 95360 loss: 0.0030 lr: 0.02\n",
      "iteration: 95370 loss: 0.0036 lr: 0.02\n",
      "iteration: 95380 loss: 0.0038 lr: 0.02\n",
      "iteration: 95390 loss: 0.0045 lr: 0.02\n",
      "iteration: 95400 loss: 0.0031 lr: 0.02\n",
      "iteration: 95410 loss: 0.0028 lr: 0.02\n",
      "iteration: 95420 loss: 0.0037 lr: 0.02\n",
      "iteration: 95430 loss: 0.0026 lr: 0.02\n",
      "iteration: 95440 loss: 0.0025 lr: 0.02\n",
      "iteration: 95450 loss: 0.0036 lr: 0.02\n",
      "iteration: 95460 loss: 0.0039 lr: 0.02\n",
      "iteration: 95470 loss: 0.0034 lr: 0.02\n",
      "iteration: 95480 loss: 0.0028 lr: 0.02\n",
      "iteration: 95490 loss: 0.0040 lr: 0.02\n",
      "iteration: 95500 loss: 0.0034 lr: 0.02\n",
      "iteration: 95510 loss: 0.0032 lr: 0.02\n",
      "iteration: 95520 loss: 0.0040 lr: 0.02\n",
      "iteration: 95530 loss: 0.0059 lr: 0.02\n",
      "iteration: 95540 loss: 0.0043 lr: 0.02\n",
      "iteration: 95550 loss: 0.0036 lr: 0.02\n",
      "iteration: 95560 loss: 0.0040 lr: 0.02\n",
      "iteration: 95570 loss: 0.0030 lr: 0.02\n",
      "iteration: 95580 loss: 0.0023 lr: 0.02\n",
      "iteration: 95590 loss: 0.0026 lr: 0.02\n",
      "iteration: 95600 loss: 0.0048 lr: 0.02\n",
      "iteration: 95610 loss: 0.0073 lr: 0.02\n",
      "iteration: 95620 loss: 0.0031 lr: 0.02\n",
      "iteration: 95630 loss: 0.0048 lr: 0.02\n",
      "iteration: 95640 loss: 0.0039 lr: 0.02\n",
      "iteration: 95650 loss: 0.0037 lr: 0.02\n",
      "iteration: 95660 loss: 0.0023 lr: 0.02\n",
      "iteration: 95670 loss: 0.0033 lr: 0.02\n",
      "iteration: 95680 loss: 0.0026 lr: 0.02\n",
      "iteration: 95690 loss: 0.0036 lr: 0.02\n",
      "iteration: 95700 loss: 0.0026 lr: 0.02\n",
      "iteration: 95710 loss: 0.0045 lr: 0.02\n",
      "iteration: 95720 loss: 0.0030 lr: 0.02\n",
      "iteration: 95730 loss: 0.0035 lr: 0.02\n",
      "iteration: 95740 loss: 0.0023 lr: 0.02\n",
      "iteration: 95750 loss: 0.0041 lr: 0.02\n",
      "iteration: 95760 loss: 0.0042 lr: 0.02\n",
      "iteration: 95770 loss: 0.0024 lr: 0.02\n",
      "iteration: 95780 loss: 0.0026 lr: 0.02\n",
      "iteration: 95790 loss: 0.0026 lr: 0.02\n",
      "iteration: 95800 loss: 0.0028 lr: 0.02\n",
      "iteration: 95810 loss: 0.0038 lr: 0.02\n",
      "iteration: 95820 loss: 0.0049 lr: 0.02\n",
      "iteration: 95830 loss: 0.0034 lr: 0.02\n",
      "iteration: 95840 loss: 0.0039 lr: 0.02\n",
      "iteration: 95850 loss: 0.0033 lr: 0.02\n",
      "iteration: 95860 loss: 0.0025 lr: 0.02\n",
      "iteration: 95870 loss: 0.0044 lr: 0.02\n",
      "iteration: 95880 loss: 0.0037 lr: 0.02\n",
      "iteration: 95890 loss: 0.0037 lr: 0.02\n",
      "iteration: 95900 loss: 0.0027 lr: 0.02\n",
      "iteration: 95910 loss: 0.0025 lr: 0.02\n",
      "iteration: 95920 loss: 0.0035 lr: 0.02\n",
      "iteration: 95930 loss: 0.0040 lr: 0.02\n",
      "iteration: 95940 loss: 0.0027 lr: 0.02\n",
      "iteration: 95950 loss: 0.0034 lr: 0.02\n",
      "iteration: 95960 loss: 0.0029 lr: 0.02\n",
      "iteration: 95970 loss: 0.0042 lr: 0.02\n",
      "iteration: 95980 loss: 0.0032 lr: 0.02\n",
      "iteration: 95990 loss: 0.0028 lr: 0.02\n",
      "iteration: 96000 loss: 0.0024 lr: 0.02\n",
      "iteration: 96010 loss: 0.0027 lr: 0.02\n",
      "iteration: 96020 loss: 0.0034 lr: 0.02\n",
      "iteration: 96030 loss: 0.0030 lr: 0.02\n",
      "iteration: 96040 loss: 0.0023 lr: 0.02\n",
      "iteration: 96050 loss: 0.0023 lr: 0.02\n",
      "iteration: 96060 loss: 0.0036 lr: 0.02\n",
      "iteration: 96070 loss: 0.0036 lr: 0.02\n",
      "iteration: 96080 loss: 0.0035 lr: 0.02\n",
      "iteration: 96090 loss: 0.0030 lr: 0.02\n",
      "iteration: 96100 loss: 0.0029 lr: 0.02\n",
      "iteration: 96110 loss: 0.0038 lr: 0.02\n",
      "iteration: 96120 loss: 0.0033 lr: 0.02\n",
      "iteration: 96130 loss: 0.0039 lr: 0.02\n",
      "iteration: 96140 loss: 0.0032 lr: 0.02\n",
      "iteration: 96150 loss: 0.0046 lr: 0.02\n",
      "iteration: 96160 loss: 0.0041 lr: 0.02\n",
      "iteration: 96170 loss: 0.0047 lr: 0.02\n",
      "iteration: 96180 loss: 0.0025 lr: 0.02\n",
      "iteration: 96190 loss: 0.0057 lr: 0.02\n",
      "iteration: 96200 loss: 0.0025 lr: 0.02\n",
      "iteration: 96210 loss: 0.0034 lr: 0.02\n",
      "iteration: 96220 loss: 0.0028 lr: 0.02\n",
      "iteration: 96230 loss: 0.0031 lr: 0.02\n",
      "iteration: 96240 loss: 0.0036 lr: 0.02\n",
      "iteration: 96250 loss: 0.0030 lr: 0.02\n",
      "iteration: 96260 loss: 0.0031 lr: 0.02\n",
      "iteration: 96270 loss: 0.0035 lr: 0.02\n",
      "iteration: 96280 loss: 0.0032 lr: 0.02\n",
      "iteration: 96290 loss: 0.0029 lr: 0.02\n",
      "iteration: 96300 loss: 0.0038 lr: 0.02\n",
      "iteration: 96310 loss: 0.0023 lr: 0.02\n",
      "iteration: 96320 loss: 0.0039 lr: 0.02\n",
      "iteration: 96330 loss: 0.0030 lr: 0.02\n",
      "iteration: 96340 loss: 0.0043 lr: 0.02\n",
      "iteration: 96350 loss: 0.0044 lr: 0.02\n",
      "iteration: 96360 loss: 0.0034 lr: 0.02\n",
      "iteration: 96370 loss: 0.0048 lr: 0.02\n",
      "iteration: 96380 loss: 0.0035 lr: 0.02\n",
      "iteration: 96390 loss: 0.0033 lr: 0.02\n",
      "iteration: 96400 loss: 0.0032 lr: 0.02\n",
      "iteration: 96410 loss: 0.0032 lr: 0.02\n",
      "iteration: 96420 loss: 0.0057 lr: 0.02\n",
      "iteration: 96430 loss: 0.0035 lr: 0.02\n",
      "iteration: 96440 loss: 0.0036 lr: 0.02\n",
      "iteration: 96450 loss: 0.0046 lr: 0.02\n",
      "iteration: 96460 loss: 0.0041 lr: 0.02\n",
      "iteration: 96470 loss: 0.0044 lr: 0.02\n",
      "iteration: 96480 loss: 0.0038 lr: 0.02\n",
      "iteration: 96490 loss: 0.0035 lr: 0.02\n",
      "iteration: 96500 loss: 0.0035 lr: 0.02\n",
      "iteration: 96510 loss: 0.0045 lr: 0.02\n",
      "iteration: 96520 loss: 0.0027 lr: 0.02\n",
      "iteration: 96530 loss: 0.0038 lr: 0.02\n",
      "iteration: 96540 loss: 0.0031 lr: 0.02\n",
      "iteration: 96550 loss: 0.0023 lr: 0.02\n",
      "iteration: 96560 loss: 0.0025 lr: 0.02\n",
      "iteration: 96570 loss: 0.0036 lr: 0.02\n",
      "iteration: 96580 loss: 0.0031 lr: 0.02\n",
      "iteration: 96590 loss: 0.0021 lr: 0.02\n",
      "iteration: 96600 loss: 0.0029 lr: 0.02\n",
      "iteration: 96610 loss: 0.0043 lr: 0.02\n",
      "iteration: 96620 loss: 0.0033 lr: 0.02\n",
      "iteration: 96630 loss: 0.0024 lr: 0.02\n",
      "iteration: 96640 loss: 0.0033 lr: 0.02\n",
      "iteration: 96650 loss: 0.0043 lr: 0.02\n",
      "iteration: 96660 loss: 0.0049 lr: 0.02\n",
      "iteration: 96670 loss: 0.0034 lr: 0.02\n",
      "iteration: 96680 loss: 0.0032 lr: 0.02\n",
      "iteration: 96690 loss: 0.0025 lr: 0.02\n",
      "iteration: 96700 loss: 0.0027 lr: 0.02\n",
      "iteration: 96710 loss: 0.0025 lr: 0.02\n",
      "iteration: 96720 loss: 0.0024 lr: 0.02\n",
      "iteration: 96730 loss: 0.0035 lr: 0.02\n",
      "iteration: 96740 loss: 0.0026 lr: 0.02\n",
      "iteration: 96750 loss: 0.0020 lr: 0.02\n",
      "iteration: 96760 loss: 0.0036 lr: 0.02\n",
      "iteration: 96770 loss: 0.0029 lr: 0.02\n",
      "iteration: 96780 loss: 0.0039 lr: 0.02\n",
      "iteration: 96790 loss: 0.0031 lr: 0.02\n",
      "iteration: 96800 loss: 0.0048 lr: 0.02\n",
      "iteration: 96810 loss: 0.0037 lr: 0.02\n",
      "iteration: 96820 loss: 0.0032 lr: 0.02\n",
      "iteration: 96830 loss: 0.0044 lr: 0.02\n",
      "iteration: 96840 loss: 0.0027 lr: 0.02\n",
      "iteration: 96850 loss: 0.0033 lr: 0.02\n",
      "iteration: 96860 loss: 0.0038 lr: 0.02\n",
      "iteration: 96870 loss: 0.0031 lr: 0.02\n",
      "iteration: 96880 loss: 0.0029 lr: 0.02\n",
      "iteration: 96890 loss: 0.0030 lr: 0.02\n",
      "iteration: 96900 loss: 0.0028 lr: 0.02\n",
      "iteration: 96910 loss: 0.0039 lr: 0.02\n",
      "iteration: 96920 loss: 0.0030 lr: 0.02\n",
      "iteration: 96930 loss: 0.0029 lr: 0.02\n",
      "iteration: 96940 loss: 0.0046 lr: 0.02\n",
      "iteration: 96950 loss: 0.0044 lr: 0.02\n",
      "iteration: 96960 loss: 0.0028 lr: 0.02\n",
      "iteration: 96970 loss: 0.0037 lr: 0.02\n",
      "iteration: 96980 loss: 0.0032 lr: 0.02\n",
      "iteration: 96990 loss: 0.0035 lr: 0.02\n",
      "iteration: 97000 loss: 0.0031 lr: 0.02\n",
      "iteration: 97010 loss: 0.0040 lr: 0.02\n",
      "iteration: 97020 loss: 0.0032 lr: 0.02\n",
      "iteration: 97030 loss: 0.0021 lr: 0.02\n",
      "iteration: 97040 loss: 0.0023 lr: 0.02\n",
      "iteration: 97050 loss: 0.0034 lr: 0.02\n",
      "iteration: 97060 loss: 0.0030 lr: 0.02\n",
      "iteration: 97070 loss: 0.0036 lr: 0.02\n",
      "iteration: 97080 loss: 0.0024 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 97090 loss: 0.0037 lr: 0.02\n",
      "iteration: 97100 loss: 0.0033 lr: 0.02\n",
      "iteration: 97110 loss: 0.0024 lr: 0.02\n",
      "iteration: 97120 loss: 0.0038 lr: 0.02\n",
      "iteration: 97130 loss: 0.0036 lr: 0.02\n",
      "iteration: 97140 loss: 0.0028 lr: 0.02\n",
      "iteration: 97150 loss: 0.0044 lr: 0.02\n",
      "iteration: 97160 loss: 0.0037 lr: 0.02\n",
      "iteration: 97170 loss: 0.0043 lr: 0.02\n",
      "iteration: 97180 loss: 0.0028 lr: 0.02\n",
      "iteration: 97190 loss: 0.0032 lr: 0.02\n",
      "iteration: 97200 loss: 0.0036 lr: 0.02\n",
      "iteration: 97210 loss: 0.0025 lr: 0.02\n",
      "iteration: 97220 loss: 0.0032 lr: 0.02\n",
      "iteration: 97230 loss: 0.0025 lr: 0.02\n",
      "iteration: 97240 loss: 0.0028 lr: 0.02\n",
      "iteration: 97250 loss: 0.0033 lr: 0.02\n",
      "iteration: 97260 loss: 0.0035 lr: 0.02\n",
      "iteration: 97270 loss: 0.0022 lr: 0.02\n",
      "iteration: 97280 loss: 0.0040 lr: 0.02\n",
      "iteration: 97290 loss: 0.0057 lr: 0.02\n",
      "iteration: 97300 loss: 0.0032 lr: 0.02\n",
      "iteration: 97310 loss: 0.0044 lr: 0.02\n",
      "iteration: 97320 loss: 0.0039 lr: 0.02\n",
      "iteration: 97330 loss: 0.0037 lr: 0.02\n",
      "iteration: 97340 loss: 0.0026 lr: 0.02\n",
      "iteration: 97350 loss: 0.0032 lr: 0.02\n",
      "iteration: 97360 loss: 0.0037 lr: 0.02\n",
      "iteration: 97370 loss: 0.0034 lr: 0.02\n",
      "iteration: 97380 loss: 0.0028 lr: 0.02\n",
      "iteration: 97390 loss: 0.0046 lr: 0.02\n",
      "iteration: 97400 loss: 0.0035 lr: 0.02\n",
      "iteration: 97410 loss: 0.0034 lr: 0.02\n",
      "iteration: 97420 loss: 0.0032 lr: 0.02\n",
      "iteration: 97430 loss: 0.0032 lr: 0.02\n",
      "iteration: 97440 loss: 0.0032 lr: 0.02\n",
      "iteration: 97450 loss: 0.0039 lr: 0.02\n",
      "iteration: 97460 loss: 0.0052 lr: 0.02\n",
      "iteration: 97470 loss: 0.0039 lr: 0.02\n",
      "iteration: 97480 loss: 0.0038 lr: 0.02\n",
      "iteration: 97490 loss: 0.0034 lr: 0.02\n",
      "iteration: 97500 loss: 0.0025 lr: 0.02\n",
      "iteration: 97510 loss: 0.0026 lr: 0.02\n",
      "iteration: 97520 loss: 0.0033 lr: 0.02\n",
      "iteration: 97530 loss: 0.0039 lr: 0.02\n",
      "iteration: 97540 loss: 0.0032 lr: 0.02\n",
      "iteration: 97550 loss: 0.0031 lr: 0.02\n",
      "iteration: 97560 loss: 0.0032 lr: 0.02\n",
      "iteration: 97570 loss: 0.0030 lr: 0.02\n",
      "iteration: 97580 loss: 0.0051 lr: 0.02\n",
      "iteration: 97590 loss: 0.0047 lr: 0.02\n",
      "iteration: 97600 loss: 0.0029 lr: 0.02\n",
      "iteration: 97610 loss: 0.0035 lr: 0.02\n",
      "iteration: 97620 loss: 0.0036 lr: 0.02\n",
      "iteration: 97630 loss: 0.0031 lr: 0.02\n",
      "iteration: 97640 loss: 0.0033 lr: 0.02\n",
      "iteration: 97650 loss: 0.0025 lr: 0.02\n",
      "iteration: 97660 loss: 0.0027 lr: 0.02\n",
      "iteration: 97670 loss: 0.0030 lr: 0.02\n",
      "iteration: 97680 loss: 0.0039 lr: 0.02\n",
      "iteration: 97690 loss: 0.0035 lr: 0.02\n",
      "iteration: 97700 loss: 0.0039 lr: 0.02\n",
      "iteration: 97710 loss: 0.0023 lr: 0.02\n",
      "iteration: 97720 loss: 0.0030 lr: 0.02\n",
      "iteration: 97730 loss: 0.0031 lr: 0.02\n",
      "iteration: 97740 loss: 0.0030 lr: 0.02\n",
      "iteration: 97750 loss: 0.0036 lr: 0.02\n",
      "iteration: 97760 loss: 0.0031 lr: 0.02\n",
      "iteration: 97770 loss: 0.0023 lr: 0.02\n",
      "iteration: 97780 loss: 0.0022 lr: 0.02\n",
      "iteration: 97790 loss: 0.0030 lr: 0.02\n",
      "iteration: 97800 loss: 0.0027 lr: 0.02\n",
      "iteration: 97810 loss: 0.0034 lr: 0.02\n",
      "iteration: 97820 loss: 0.0044 lr: 0.02\n",
      "iteration: 97830 loss: 0.0025 lr: 0.02\n",
      "iteration: 97840 loss: 0.0024 lr: 0.02\n",
      "iteration: 97850 loss: 0.0025 lr: 0.02\n",
      "iteration: 97860 loss: 0.0039 lr: 0.02\n",
      "iteration: 97870 loss: 0.0036 lr: 0.02\n",
      "iteration: 97880 loss: 0.0025 lr: 0.02\n",
      "iteration: 97890 loss: 0.0054 lr: 0.02\n",
      "iteration: 97900 loss: 0.0033 lr: 0.02\n",
      "iteration: 97910 loss: 0.0035 lr: 0.02\n",
      "iteration: 97920 loss: 0.0024 lr: 0.02\n",
      "iteration: 97930 loss: 0.0030 lr: 0.02\n",
      "iteration: 97940 loss: 0.0022 lr: 0.02\n",
      "iteration: 97950 loss: 0.0058 lr: 0.02\n",
      "iteration: 97960 loss: 0.0032 lr: 0.02\n",
      "iteration: 97970 loss: 0.0042 lr: 0.02\n",
      "iteration: 97980 loss: 0.0019 lr: 0.02\n",
      "iteration: 97990 loss: 0.0031 lr: 0.02\n",
      "iteration: 98000 loss: 0.0055 lr: 0.02\n",
      "iteration: 98010 loss: 0.0036 lr: 0.02\n",
      "iteration: 98020 loss: 0.0046 lr: 0.02\n",
      "iteration: 98030 loss: 0.0053 lr: 0.02\n",
      "iteration: 98040 loss: 0.0030 lr: 0.02\n",
      "iteration: 98050 loss: 0.0048 lr: 0.02\n",
      "iteration: 98060 loss: 0.0032 lr: 0.02\n",
      "iteration: 98070 loss: 0.0029 lr: 0.02\n",
      "iteration: 98080 loss: 0.0028 lr: 0.02\n",
      "iteration: 98090 loss: 0.0038 lr: 0.02\n",
      "iteration: 98100 loss: 0.0045 lr: 0.02\n",
      "iteration: 98110 loss: 0.0028 lr: 0.02\n",
      "iteration: 98120 loss: 0.0028 lr: 0.02\n",
      "iteration: 98130 loss: 0.0046 lr: 0.02\n",
      "iteration: 98140 loss: 0.0029 lr: 0.02\n",
      "iteration: 98150 loss: 0.0041 lr: 0.02\n",
      "iteration: 98160 loss: 0.0028 lr: 0.02\n",
      "iteration: 98170 loss: 0.0024 lr: 0.02\n",
      "iteration: 98180 loss: 0.0024 lr: 0.02\n",
      "iteration: 98190 loss: 0.0038 lr: 0.02\n",
      "iteration: 98200 loss: 0.0039 lr: 0.02\n",
      "iteration: 98210 loss: 0.0023 lr: 0.02\n",
      "iteration: 98220 loss: 0.0029 lr: 0.02\n",
      "iteration: 98230 loss: 0.0033 lr: 0.02\n",
      "iteration: 98240 loss: 0.0034 lr: 0.02\n",
      "iteration: 98250 loss: 0.0034 lr: 0.02\n",
      "iteration: 98260 loss: 0.0025 lr: 0.02\n",
      "iteration: 98270 loss: 0.0028 lr: 0.02\n",
      "iteration: 98280 loss: 0.0025 lr: 0.02\n",
      "iteration: 98290 loss: 0.0036 lr: 0.02\n",
      "iteration: 98300 loss: 0.0021 lr: 0.02\n",
      "iteration: 98310 loss: 0.0034 lr: 0.02\n",
      "iteration: 98320 loss: 0.0034 lr: 0.02\n",
      "iteration: 98330 loss: 0.0027 lr: 0.02\n",
      "iteration: 98340 loss: 0.0031 lr: 0.02\n",
      "iteration: 98350 loss: 0.0041 lr: 0.02\n",
      "iteration: 98360 loss: 0.0025 lr: 0.02\n",
      "iteration: 98370 loss: 0.0032 lr: 0.02\n",
      "iteration: 98380 loss: 0.0039 lr: 0.02\n",
      "iteration: 98390 loss: 0.0020 lr: 0.02\n",
      "iteration: 98400 loss: 0.0032 lr: 0.02\n",
      "iteration: 98410 loss: 0.0036 lr: 0.02\n",
      "iteration: 98420 loss: 0.0032 lr: 0.02\n",
      "iteration: 98430 loss: 0.0028 lr: 0.02\n",
      "iteration: 98440 loss: 0.0052 lr: 0.02\n",
      "iteration: 98450 loss: 0.0027 lr: 0.02\n",
      "iteration: 98460 loss: 0.0039 lr: 0.02\n",
      "iteration: 98470 loss: 0.0031 lr: 0.02\n",
      "iteration: 98480 loss: 0.0044 lr: 0.02\n",
      "iteration: 98490 loss: 0.0045 lr: 0.02\n",
      "iteration: 98500 loss: 0.0036 lr: 0.02\n",
      "iteration: 98510 loss: 0.0045 lr: 0.02\n",
      "iteration: 98520 loss: 0.0047 lr: 0.02\n",
      "iteration: 98530 loss: 0.0036 lr: 0.02\n",
      "iteration: 98540 loss: 0.0044 lr: 0.02\n",
      "iteration: 98550 loss: 0.0028 lr: 0.02\n",
      "iteration: 98560 loss: 0.0035 lr: 0.02\n",
      "iteration: 98570 loss: 0.0028 lr: 0.02\n",
      "iteration: 98580 loss: 0.0026 lr: 0.02\n",
      "iteration: 98590 loss: 0.0035 lr: 0.02\n",
      "iteration: 98600 loss: 0.0027 lr: 0.02\n",
      "iteration: 98610 loss: 0.0047 lr: 0.02\n",
      "iteration: 98620 loss: 0.0028 lr: 0.02\n",
      "iteration: 98630 loss: 0.0023 lr: 0.02\n",
      "iteration: 98640 loss: 0.0031 lr: 0.02\n",
      "iteration: 98650 loss: 0.0053 lr: 0.02\n",
      "iteration: 98660 loss: 0.0047 lr: 0.02\n",
      "iteration: 98670 loss: 0.0041 lr: 0.02\n",
      "iteration: 98680 loss: 0.0038 lr: 0.02\n",
      "iteration: 98690 loss: 0.0034 lr: 0.02\n",
      "iteration: 98700 loss: 0.0032 lr: 0.02\n",
      "iteration: 98710 loss: 0.0029 lr: 0.02\n",
      "iteration: 98720 loss: 0.0038 lr: 0.02\n",
      "iteration: 98730 loss: 0.0038 lr: 0.02\n",
      "iteration: 98740 loss: 0.0037 lr: 0.02\n",
      "iteration: 98750 loss: 0.0033 lr: 0.02\n",
      "iteration: 98760 loss: 0.0029 lr: 0.02\n",
      "iteration: 98770 loss: 0.0032 lr: 0.02\n",
      "iteration: 98780 loss: 0.0028 lr: 0.02\n",
      "iteration: 98790 loss: 0.0036 lr: 0.02\n",
      "iteration: 98800 loss: 0.0028 lr: 0.02\n",
      "iteration: 98810 loss: 0.0022 lr: 0.02\n",
      "iteration: 98820 loss: 0.0024 lr: 0.02\n",
      "iteration: 98830 loss: 0.0032 lr: 0.02\n",
      "iteration: 98840 loss: 0.0028 lr: 0.02\n",
      "iteration: 98850 loss: 0.0042 lr: 0.02\n",
      "iteration: 98860 loss: 0.0052 lr: 0.02\n",
      "iteration: 98870 loss: 0.0030 lr: 0.02\n",
      "iteration: 98880 loss: 0.0027 lr: 0.02\n",
      "iteration: 98890 loss: 0.0025 lr: 0.02\n",
      "iteration: 98900 loss: 0.0045 lr: 0.02\n",
      "iteration: 98910 loss: 0.0043 lr: 0.02\n",
      "iteration: 98920 loss: 0.0034 lr: 0.02\n",
      "iteration: 98930 loss: 0.0023 lr: 0.02\n",
      "iteration: 98940 loss: 0.0029 lr: 0.02\n",
      "iteration: 98950 loss: 0.0028 lr: 0.02\n",
      "iteration: 98960 loss: 0.0042 lr: 0.02\n",
      "iteration: 98970 loss: 0.0030 lr: 0.02\n",
      "iteration: 98980 loss: 0.0027 lr: 0.02\n",
      "iteration: 98990 loss: 0.0027 lr: 0.02\n",
      "iteration: 99000 loss: 0.0041 lr: 0.02\n",
      "iteration: 99010 loss: 0.0030 lr: 0.02\n",
      "iteration: 99020 loss: 0.0041 lr: 0.02\n",
      "iteration: 99030 loss: 0.0033 lr: 0.02\n",
      "iteration: 99040 loss: 0.0024 lr: 0.02\n",
      "iteration: 99050 loss: 0.0031 lr: 0.02\n",
      "iteration: 99060 loss: 0.0028 lr: 0.02\n",
      "iteration: 99070 loss: 0.0034 lr: 0.02\n",
      "iteration: 99080 loss: 0.0025 lr: 0.02\n",
      "iteration: 99090 loss: 0.0026 lr: 0.02\n",
      "iteration: 99100 loss: 0.0055 lr: 0.02\n",
      "iteration: 99110 loss: 0.0017 lr: 0.02\n",
      "iteration: 99120 loss: 0.0025 lr: 0.02\n",
      "iteration: 99130 loss: 0.0031 lr: 0.02\n",
      "iteration: 99140 loss: 0.0024 lr: 0.02\n",
      "iteration: 99150 loss: 0.0031 lr: 0.02\n",
      "iteration: 99160 loss: 0.0042 lr: 0.02\n",
      "iteration: 99170 loss: 0.0039 lr: 0.02\n",
      "iteration: 99180 loss: 0.0034 lr: 0.02\n",
      "iteration: 99190 loss: 0.0035 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 99200 loss: 0.0035 lr: 0.02\n",
      "iteration: 99210 loss: 0.0034 lr: 0.02\n",
      "iteration: 99220 loss: 0.0038 lr: 0.02\n",
      "iteration: 99230 loss: 0.0041 lr: 0.02\n",
      "iteration: 99240 loss: 0.0024 lr: 0.02\n",
      "iteration: 99250 loss: 0.0052 lr: 0.02\n",
      "iteration: 99260 loss: 0.0024 lr: 0.02\n",
      "iteration: 99270 loss: 0.0034 lr: 0.02\n",
      "iteration: 99280 loss: 0.0042 lr: 0.02\n",
      "iteration: 99290 loss: 0.0033 lr: 0.02\n",
      "iteration: 99300 loss: 0.0029 lr: 0.02\n",
      "iteration: 99310 loss: 0.0026 lr: 0.02\n",
      "iteration: 99320 loss: 0.0027 lr: 0.02\n",
      "iteration: 99330 loss: 0.0027 lr: 0.02\n",
      "iteration: 99340 loss: 0.0025 lr: 0.02\n",
      "iteration: 99350 loss: 0.0033 lr: 0.02\n",
      "iteration: 99360 loss: 0.0025 lr: 0.02\n",
      "iteration: 99370 loss: 0.0032 lr: 0.02\n",
      "iteration: 99380 loss: 0.0036 lr: 0.02\n",
      "iteration: 99390 loss: 0.0029 lr: 0.02\n",
      "iteration: 99400 loss: 0.0040 lr: 0.02\n",
      "iteration: 99410 loss: 0.0035 lr: 0.02\n",
      "iteration: 99420 loss: 0.0026 lr: 0.02\n",
      "iteration: 99430 loss: 0.0027 lr: 0.02\n",
      "iteration: 99440 loss: 0.0022 lr: 0.02\n",
      "iteration: 99450 loss: 0.0040 lr: 0.02\n",
      "iteration: 99460 loss: 0.0047 lr: 0.02\n",
      "iteration: 99470 loss: 0.0032 lr: 0.02\n",
      "iteration: 99480 loss: 0.0042 lr: 0.02\n",
      "iteration: 99490 loss: 0.0029 lr: 0.02\n",
      "iteration: 99500 loss: 0.0032 lr: 0.02\n",
      "iteration: 99510 loss: 0.0033 lr: 0.02\n",
      "iteration: 99520 loss: 0.0048 lr: 0.02\n",
      "iteration: 99530 loss: 0.0030 lr: 0.02\n",
      "iteration: 99540 loss: 0.0027 lr: 0.02\n",
      "iteration: 99550 loss: 0.0028 lr: 0.02\n",
      "iteration: 99560 loss: 0.0055 lr: 0.02\n",
      "iteration: 99570 loss: 0.0031 lr: 0.02\n",
      "iteration: 99580 loss: 0.0046 lr: 0.02\n",
      "iteration: 99590 loss: 0.0037 lr: 0.02\n",
      "iteration: 99600 loss: 0.0030 lr: 0.02\n",
      "iteration: 99610 loss: 0.0037 lr: 0.02\n",
      "iteration: 99620 loss: 0.0034 lr: 0.02\n",
      "iteration: 99630 loss: 0.0036 lr: 0.02\n",
      "iteration: 99640 loss: 0.0034 lr: 0.02\n",
      "iteration: 99650 loss: 0.0034 lr: 0.02\n",
      "iteration: 99660 loss: 0.0033 lr: 0.02\n",
      "iteration: 99670 loss: 0.0035 lr: 0.02\n",
      "iteration: 99680 loss: 0.0032 lr: 0.02\n",
      "iteration: 99690 loss: 0.0034 lr: 0.02\n",
      "iteration: 99700 loss: 0.0034 lr: 0.02\n",
      "iteration: 99710 loss: 0.0035 lr: 0.02\n",
      "iteration: 99720 loss: 0.0041 lr: 0.02\n",
      "iteration: 99730 loss: 0.0029 lr: 0.02\n",
      "iteration: 99740 loss: 0.0027 lr: 0.02\n",
      "iteration: 99750 loss: 0.0025 lr: 0.02\n",
      "iteration: 99760 loss: 0.0047 lr: 0.02\n",
      "iteration: 99770 loss: 0.0036 lr: 0.02\n",
      "iteration: 99780 loss: 0.0026 lr: 0.02\n",
      "iteration: 99790 loss: 0.0040 lr: 0.02\n",
      "iteration: 99800 loss: 0.0029 lr: 0.02\n",
      "iteration: 99810 loss: 0.0049 lr: 0.02\n",
      "iteration: 99820 loss: 0.0049 lr: 0.02\n",
      "iteration: 99830 loss: 0.0048 lr: 0.02\n",
      "iteration: 99840 loss: 0.0028 lr: 0.02\n",
      "iteration: 99850 loss: 0.0040 lr: 0.02\n",
      "iteration: 99860 loss: 0.0035 lr: 0.02\n",
      "iteration: 99870 loss: 0.0042 lr: 0.02\n",
      "iteration: 99880 loss: 0.0040 lr: 0.02\n",
      "iteration: 99890 loss: 0.0046 lr: 0.02\n",
      "iteration: 99900 loss: 0.0035 lr: 0.02\n",
      "iteration: 99910 loss: 0.0034 lr: 0.02\n",
      "iteration: 99920 loss: 0.0030 lr: 0.02\n",
      "iteration: 99930 loss: 0.0030 lr: 0.02\n",
      "iteration: 99940 loss: 0.0045 lr: 0.02\n",
      "iteration: 99950 loss: 0.0042 lr: 0.02\n",
      "iteration: 99960 loss: 0.0043 lr: 0.02\n",
      "iteration: 99970 loss: 0.0038 lr: 0.02\n",
      "iteration: 99980 loss: 0.0029 lr: 0.02\n",
      "iteration: 99990 loss: 0.0030 lr: 0.02\n",
      "iteration: 100000 loss: 0.0029 lr: 0.02\n",
      "iteration: 100010 loss: 0.0030 lr: 0.02\n",
      "iteration: 100020 loss: 0.0037 lr: 0.02\n",
      "iteration: 100030 loss: 0.0045 lr: 0.02\n",
      "iteration: 100040 loss: 0.0031 lr: 0.02\n",
      "iteration: 100050 loss: 0.0040 lr: 0.02\n",
      "iteration: 100060 loss: 0.0027 lr: 0.02\n",
      "iteration: 100070 loss: 0.0023 lr: 0.02\n",
      "iteration: 100080 loss: 0.0029 lr: 0.02\n",
      "iteration: 100090 loss: 0.0028 lr: 0.02\n",
      "iteration: 100100 loss: 0.0030 lr: 0.02\n",
      "iteration: 100110 loss: 0.0035 lr: 0.02\n",
      "iteration: 100120 loss: 0.0022 lr: 0.02\n",
      "iteration: 100130 loss: 0.0028 lr: 0.02\n",
      "iteration: 100140 loss: 0.0038 lr: 0.02\n",
      "iteration: 100150 loss: 0.0030 lr: 0.02\n",
      "iteration: 100160 loss: 0.0037 lr: 0.02\n",
      "iteration: 100170 loss: 0.0026 lr: 0.02\n",
      "iteration: 100180 loss: 0.0036 lr: 0.02\n",
      "iteration: 100190 loss: 0.0027 lr: 0.02\n",
      "iteration: 100200 loss: 0.0027 lr: 0.02\n",
      "iteration: 100210 loss: 0.0027 lr: 0.02\n",
      "iteration: 100220 loss: 0.0031 lr: 0.02\n",
      "iteration: 100230 loss: 0.0028 lr: 0.02\n",
      "iteration: 100240 loss: 0.0026 lr: 0.02\n",
      "iteration: 100250 loss: 0.0034 lr: 0.02\n",
      "iteration: 100260 loss: 0.0024 lr: 0.02\n",
      "iteration: 100270 loss: 0.0038 lr: 0.02\n",
      "iteration: 100280 loss: 0.0024 lr: 0.02\n",
      "iteration: 100290 loss: 0.0037 lr: 0.02\n",
      "iteration: 100300 loss: 0.0023 lr: 0.02\n",
      "iteration: 100310 loss: 0.0035 lr: 0.02\n",
      "iteration: 100320 loss: 0.0035 lr: 0.02\n",
      "iteration: 100330 loss: 0.0033 lr: 0.02\n",
      "iteration: 100340 loss: 0.0031 lr: 0.02\n",
      "iteration: 100350 loss: 0.0045 lr: 0.02\n",
      "iteration: 100360 loss: 0.0031 lr: 0.02\n",
      "iteration: 100370 loss: 0.0032 lr: 0.02\n",
      "iteration: 100380 loss: 0.0041 lr: 0.02\n",
      "iteration: 100390 loss: 0.0035 lr: 0.02\n",
      "iteration: 100400 loss: 0.0061 lr: 0.02\n",
      "iteration: 100410 loss: 0.0034 lr: 0.02\n",
      "iteration: 100420 loss: 0.0027 lr: 0.02\n",
      "iteration: 100430 loss: 0.0032 lr: 0.02\n",
      "iteration: 100440 loss: 0.0027 lr: 0.02\n",
      "iteration: 100450 loss: 0.0039 lr: 0.02\n",
      "iteration: 100460 loss: 0.0044 lr: 0.02\n",
      "iteration: 100470 loss: 0.0033 lr: 0.02\n",
      "iteration: 100480 loss: 0.0027 lr: 0.02\n",
      "iteration: 100490 loss: 0.0040 lr: 0.02\n",
      "iteration: 100500 loss: 0.0042 lr: 0.02\n",
      "iteration: 100510 loss: 0.0026 lr: 0.02\n",
      "iteration: 100520 loss: 0.0044 lr: 0.02\n",
      "iteration: 100530 loss: 0.0025 lr: 0.02\n",
      "iteration: 100540 loss: 0.0042 lr: 0.02\n",
      "iteration: 100550 loss: 0.0033 lr: 0.02\n",
      "iteration: 100560 loss: 0.0026 lr: 0.02\n",
      "iteration: 100570 loss: 0.0038 lr: 0.02\n",
      "iteration: 100580 loss: 0.0038 lr: 0.02\n",
      "iteration: 100590 loss: 0.0036 lr: 0.02\n",
      "iteration: 100600 loss: 0.0044 lr: 0.02\n",
      "iteration: 100610 loss: 0.0046 lr: 0.02\n",
      "iteration: 100620 loss: 0.0028 lr: 0.02\n",
      "iteration: 100630 loss: 0.0034 lr: 0.02\n",
      "iteration: 100640 loss: 0.0024 lr: 0.02\n",
      "iteration: 100650 loss: 0.0025 lr: 0.02\n",
      "iteration: 100660 loss: 0.0032 lr: 0.02\n",
      "iteration: 100670 loss: 0.0032 lr: 0.02\n",
      "iteration: 100680 loss: 0.0035 lr: 0.02\n",
      "iteration: 100690 loss: 0.0022 lr: 0.02\n",
      "iteration: 100700 loss: 0.0027 lr: 0.02\n",
      "iteration: 100710 loss: 0.0043 lr: 0.02\n",
      "iteration: 100720 loss: 0.0033 lr: 0.02\n",
      "iteration: 100730 loss: 0.0033 lr: 0.02\n",
      "iteration: 100740 loss: 0.0026 lr: 0.02\n",
      "iteration: 100750 loss: 0.0039 lr: 0.02\n",
      "iteration: 100760 loss: 0.0036 lr: 0.02\n",
      "iteration: 100770 loss: 0.0047 lr: 0.02\n",
      "iteration: 100780 loss: 0.0037 lr: 0.02\n",
      "iteration: 100790 loss: 0.0037 lr: 0.02\n",
      "iteration: 100800 loss: 0.0042 lr: 0.02\n",
      "iteration: 100810 loss: 0.0032 lr: 0.02\n",
      "iteration: 100820 loss: 0.0046 lr: 0.02\n",
      "iteration: 100830 loss: 0.0036 lr: 0.02\n",
      "iteration: 100840 loss: 0.0021 lr: 0.02\n",
      "iteration: 100850 loss: 0.0031 lr: 0.02\n",
      "iteration: 100860 loss: 0.0046 lr: 0.02\n",
      "iteration: 100870 loss: 0.0023 lr: 0.02\n",
      "iteration: 100880 loss: 0.0034 lr: 0.02\n",
      "iteration: 100890 loss: 0.0029 lr: 0.02\n",
      "iteration: 100900 loss: 0.0045 lr: 0.02\n",
      "iteration: 100910 loss: 0.0041 lr: 0.02\n",
      "iteration: 100920 loss: 0.0028 lr: 0.02\n",
      "iteration: 100930 loss: 0.0037 lr: 0.02\n",
      "iteration: 100940 loss: 0.0024 lr: 0.02\n",
      "iteration: 100950 loss: 0.0034 lr: 0.02\n",
      "iteration: 100960 loss: 0.0026 lr: 0.02\n",
      "iteration: 100970 loss: 0.0023 lr: 0.02\n",
      "iteration: 100980 loss: 0.0024 lr: 0.02\n",
      "iteration: 100990 loss: 0.0049 lr: 0.02\n",
      "iteration: 101000 loss: 0.0037 lr: 0.02\n",
      "iteration: 101010 loss: 0.0044 lr: 0.02\n",
      "iteration: 101020 loss: 0.0038 lr: 0.02\n",
      "iteration: 101030 loss: 0.0040 lr: 0.02\n",
      "iteration: 101040 loss: 0.0027 lr: 0.02\n",
      "iteration: 101050 loss: 0.0027 lr: 0.02\n",
      "iteration: 101060 loss: 0.0032 lr: 0.02\n",
      "iteration: 101070 loss: 0.0028 lr: 0.02\n",
      "iteration: 101080 loss: 0.0026 lr: 0.02\n",
      "iteration: 101090 loss: 0.0023 lr: 0.02\n",
      "iteration: 101100 loss: 0.0028 lr: 0.02\n",
      "iteration: 101110 loss: 0.0026 lr: 0.02\n",
      "iteration: 101120 loss: 0.0036 lr: 0.02\n",
      "iteration: 101130 loss: 0.0033 lr: 0.02\n",
      "iteration: 101140 loss: 0.0033 lr: 0.02\n",
      "iteration: 101150 loss: 0.0038 lr: 0.02\n",
      "iteration: 101160 loss: 0.0026 lr: 0.02\n",
      "iteration: 101170 loss: 0.0048 lr: 0.02\n",
      "iteration: 101180 loss: 0.0030 lr: 0.02\n",
      "iteration: 101190 loss: 0.0026 lr: 0.02\n",
      "iteration: 101200 loss: 0.0045 lr: 0.02\n",
      "iteration: 101210 loss: 0.0027 lr: 0.02\n",
      "iteration: 101220 loss: 0.0034 lr: 0.02\n",
      "iteration: 101230 loss: 0.0040 lr: 0.02\n",
      "iteration: 101240 loss: 0.0043 lr: 0.02\n",
      "iteration: 101250 loss: 0.0026 lr: 0.02\n",
      "iteration: 101260 loss: 0.0028 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 101270 loss: 0.0032 lr: 0.02\n",
      "iteration: 101280 loss: 0.0035 lr: 0.02\n",
      "iteration: 101290 loss: 0.0034 lr: 0.02\n",
      "iteration: 101300 loss: 0.0039 lr: 0.02\n",
      "iteration: 101310 loss: 0.0036 lr: 0.02\n",
      "iteration: 101320 loss: 0.0031 lr: 0.02\n",
      "iteration: 101330 loss: 0.0031 lr: 0.02\n",
      "iteration: 101340 loss: 0.0039 lr: 0.02\n",
      "iteration: 101350 loss: 0.0034 lr: 0.02\n",
      "iteration: 101360 loss: 0.0029 lr: 0.02\n",
      "iteration: 101370 loss: 0.0036 lr: 0.02\n",
      "iteration: 101380 loss: 0.0037 lr: 0.02\n",
      "iteration: 101390 loss: 0.0030 lr: 0.02\n",
      "iteration: 101400 loss: 0.0040 lr: 0.02\n",
      "iteration: 101410 loss: 0.0041 lr: 0.02\n",
      "iteration: 101420 loss: 0.0030 lr: 0.02\n",
      "iteration: 101430 loss: 0.0026 lr: 0.02\n",
      "iteration: 101440 loss: 0.0041 lr: 0.02\n",
      "iteration: 101450 loss: 0.0025 lr: 0.02\n",
      "iteration: 101460 loss: 0.0048 lr: 0.02\n",
      "iteration: 101470 loss: 0.0023 lr: 0.02\n",
      "iteration: 101480 loss: 0.0029 lr: 0.02\n",
      "iteration: 101490 loss: 0.0030 lr: 0.02\n",
      "iteration: 101500 loss: 0.0023 lr: 0.02\n",
      "iteration: 101510 loss: 0.0023 lr: 0.02\n",
      "iteration: 101520 loss: 0.0031 lr: 0.02\n",
      "iteration: 101530 loss: 0.0032 lr: 0.02\n",
      "iteration: 101540 loss: 0.0045 lr: 0.02\n",
      "iteration: 101550 loss: 0.0041 lr: 0.02\n",
      "iteration: 101560 loss: 0.0026 lr: 0.02\n",
      "iteration: 101570 loss: 0.0060 lr: 0.02\n",
      "iteration: 101580 loss: 0.0033 lr: 0.02\n",
      "iteration: 101590 loss: 0.0029 lr: 0.02\n",
      "iteration: 101600 loss: 0.0035 lr: 0.02\n",
      "iteration: 101610 loss: 0.0043 lr: 0.02\n",
      "iteration: 101620 loss: 0.0023 lr: 0.02\n",
      "iteration: 101630 loss: 0.0025 lr: 0.02\n",
      "iteration: 101640 loss: 0.0028 lr: 0.02\n",
      "iteration: 101650 loss: 0.0032 lr: 0.02\n",
      "iteration: 101660 loss: 0.0058 lr: 0.02\n",
      "iteration: 101670 loss: 0.0039 lr: 0.02\n",
      "iteration: 101680 loss: 0.0027 lr: 0.02\n",
      "iteration: 101690 loss: 0.0031 lr: 0.02\n",
      "iteration: 101700 loss: 0.0043 lr: 0.02\n",
      "iteration: 101710 loss: 0.0037 lr: 0.02\n",
      "iteration: 101720 loss: 0.0032 lr: 0.02\n",
      "iteration: 101730 loss: 0.0031 lr: 0.02\n",
      "iteration: 101740 loss: 0.0044 lr: 0.02\n",
      "iteration: 101750 loss: 0.0029 lr: 0.02\n",
      "iteration: 101760 loss: 0.0042 lr: 0.02\n",
      "iteration: 101770 loss: 0.0038 lr: 0.02\n",
      "iteration: 101780 loss: 0.0033 lr: 0.02\n",
      "iteration: 101790 loss: 0.0044 lr: 0.02\n",
      "iteration: 101800 loss: 0.0038 lr: 0.02\n",
      "iteration: 101810 loss: 0.0031 lr: 0.02\n",
      "iteration: 101820 loss: 0.0037 lr: 0.02\n",
      "iteration: 101830 loss: 0.0054 lr: 0.02\n",
      "iteration: 101840 loss: 0.0060 lr: 0.02\n",
      "iteration: 101850 loss: 0.0044 lr: 0.02\n",
      "iteration: 101860 loss: 0.0042 lr: 0.02\n",
      "iteration: 101870 loss: 0.0027 lr: 0.02\n",
      "iteration: 101880 loss: 0.0035 lr: 0.02\n",
      "iteration: 101890 loss: 0.0042 lr: 0.02\n",
      "iteration: 101900 loss: 0.0031 lr: 0.02\n",
      "iteration: 101910 loss: 0.0030 lr: 0.02\n",
      "iteration: 101920 loss: 0.0047 lr: 0.02\n",
      "iteration: 101930 loss: 0.0039 lr: 0.02\n",
      "iteration: 101940 loss: 0.0054 lr: 0.02\n",
      "iteration: 101950 loss: 0.0035 lr: 0.02\n",
      "iteration: 101960 loss: 0.0053 lr: 0.02\n",
      "iteration: 101970 loss: 0.0032 lr: 0.02\n",
      "iteration: 101980 loss: 0.0026 lr: 0.02\n",
      "iteration: 101990 loss: 0.0026 lr: 0.02\n",
      "iteration: 102000 loss: 0.0022 lr: 0.02\n",
      "iteration: 102010 loss: 0.0023 lr: 0.02\n",
      "iteration: 102020 loss: 0.0035 lr: 0.02\n",
      "iteration: 102030 loss: 0.0041 lr: 0.02\n",
      "iteration: 102040 loss: 0.0027 lr: 0.02\n",
      "iteration: 102050 loss: 0.0026 lr: 0.02\n",
      "iteration: 102060 loss: 0.0054 lr: 0.02\n",
      "iteration: 102070 loss: 0.0028 lr: 0.02\n",
      "iteration: 102080 loss: 0.0032 lr: 0.02\n",
      "iteration: 102090 loss: 0.0025 lr: 0.02\n",
      "iteration: 102100 loss: 0.0029 lr: 0.02\n",
      "iteration: 102110 loss: 0.0050 lr: 0.02\n",
      "iteration: 102120 loss: 0.0039 lr: 0.02\n",
      "iteration: 102130 loss: 0.0044 lr: 0.02\n",
      "iteration: 102140 loss: 0.0028 lr: 0.02\n",
      "iteration: 102150 loss: 0.0032 lr: 0.02\n",
      "iteration: 102160 loss: 0.0042 lr: 0.02\n",
      "iteration: 102170 loss: 0.0033 lr: 0.02\n",
      "iteration: 102180 loss: 0.0028 lr: 0.02\n",
      "iteration: 102190 loss: 0.0023 lr: 0.02\n",
      "iteration: 102200 loss: 0.0024 lr: 0.02\n",
      "iteration: 102210 loss: 0.0032 lr: 0.02\n",
      "iteration: 102220 loss: 0.0027 lr: 0.02\n",
      "iteration: 102230 loss: 0.0044 lr: 0.02\n",
      "iteration: 102240 loss: 0.0036 lr: 0.02\n",
      "iteration: 102250 loss: 0.0034 lr: 0.02\n",
      "iteration: 102260 loss: 0.0032 lr: 0.02\n",
      "iteration: 102270 loss: 0.0029 lr: 0.02\n",
      "iteration: 102280 loss: 0.0032 lr: 0.02\n",
      "iteration: 102290 loss: 0.0031 lr: 0.02\n",
      "iteration: 102300 loss: 0.0035 lr: 0.02\n",
      "iteration: 102310 loss: 0.0028 lr: 0.02\n",
      "iteration: 102320 loss: 0.0030 lr: 0.02\n",
      "iteration: 102330 loss: 0.0030 lr: 0.02\n",
      "iteration: 102340 loss: 0.0027 lr: 0.02\n",
      "iteration: 102350 loss: 0.0025 lr: 0.02\n",
      "iteration: 102360 loss: 0.0027 lr: 0.02\n",
      "iteration: 102370 loss: 0.0027 lr: 0.02\n",
      "iteration: 102380 loss: 0.0041 lr: 0.02\n",
      "iteration: 102390 loss: 0.0035 lr: 0.02\n",
      "iteration: 102400 loss: 0.0031 lr: 0.02\n",
      "iteration: 102410 loss: 0.0027 lr: 0.02\n",
      "iteration: 102420 loss: 0.0033 lr: 0.02\n",
      "iteration: 102430 loss: 0.0029 lr: 0.02\n",
      "iteration: 102440 loss: 0.0051 lr: 0.02\n",
      "iteration: 102450 loss: 0.0029 lr: 0.02\n",
      "iteration: 102460 loss: 0.0036 lr: 0.02\n",
      "iteration: 102470 loss: 0.0025 lr: 0.02\n",
      "iteration: 102480 loss: 0.0042 lr: 0.02\n",
      "iteration: 102490 loss: 0.0031 lr: 0.02\n",
      "iteration: 102500 loss: 0.0020 lr: 0.02\n",
      "iteration: 102510 loss: 0.0030 lr: 0.02\n",
      "iteration: 102520 loss: 0.0027 lr: 0.02\n",
      "iteration: 102530 loss: 0.0033 lr: 0.02\n",
      "iteration: 102540 loss: 0.0032 lr: 0.02\n",
      "iteration: 102550 loss: 0.0030 lr: 0.02\n",
      "iteration: 102560 loss: 0.0041 lr: 0.02\n",
      "iteration: 102570 loss: 0.0028 lr: 0.02\n",
      "iteration: 102580 loss: 0.0027 lr: 0.02\n",
      "iteration: 102590 loss: 0.0028 lr: 0.02\n",
      "iteration: 102600 loss: 0.0040 lr: 0.02\n",
      "iteration: 102610 loss: 0.0060 lr: 0.02\n",
      "iteration: 102620 loss: 0.0032 lr: 0.02\n",
      "iteration: 102630 loss: 0.0032 lr: 0.02\n",
      "iteration: 102640 loss: 0.0028 lr: 0.02\n",
      "iteration: 102650 loss: 0.0054 lr: 0.02\n",
      "iteration: 102660 loss: 0.0040 lr: 0.02\n",
      "iteration: 102670 loss: 0.0027 lr: 0.02\n",
      "iteration: 102680 loss: 0.0033 lr: 0.02\n",
      "iteration: 102690 loss: 0.0030 lr: 0.02\n",
      "iteration: 102700 loss: 0.0026 lr: 0.02\n",
      "iteration: 102710 loss: 0.0023 lr: 0.02\n",
      "iteration: 102720 loss: 0.0031 lr: 0.02\n",
      "iteration: 102730 loss: 0.0038 lr: 0.02\n",
      "iteration: 102740 loss: 0.0030 lr: 0.02\n",
      "iteration: 102750 loss: 0.0033 lr: 0.02\n",
      "iteration: 102760 loss: 0.0032 lr: 0.02\n",
      "iteration: 102770 loss: 0.0034 lr: 0.02\n",
      "iteration: 102780 loss: 0.0033 lr: 0.02\n",
      "iteration: 102790 loss: 0.0026 lr: 0.02\n",
      "iteration: 102800 loss: 0.0037 lr: 0.02\n",
      "iteration: 102810 loss: 0.0036 lr: 0.02\n",
      "iteration: 102820 loss: 0.0024 lr: 0.02\n",
      "iteration: 102830 loss: 0.0025 lr: 0.02\n",
      "iteration: 102840 loss: 0.0029 lr: 0.02\n",
      "iteration: 102850 loss: 0.0028 lr: 0.02\n",
      "iteration: 102860 loss: 0.0025 lr: 0.02\n",
      "iteration: 102870 loss: 0.0029 lr: 0.02\n",
      "iteration: 102880 loss: 0.0044 lr: 0.02\n",
      "iteration: 102890 loss: 0.0032 lr: 0.02\n",
      "iteration: 102900 loss: 0.0036 lr: 0.02\n",
      "iteration: 102910 loss: 0.0041 lr: 0.02\n",
      "iteration: 102920 loss: 0.0028 lr: 0.02\n",
      "iteration: 102930 loss: 0.0050 lr: 0.02\n",
      "iteration: 102940 loss: 0.0035 lr: 0.02\n",
      "iteration: 102950 loss: 0.0023 lr: 0.02\n",
      "iteration: 102960 loss: 0.0028 lr: 0.02\n",
      "iteration: 102970 loss: 0.0024 lr: 0.02\n",
      "iteration: 102980 loss: 0.0026 lr: 0.02\n",
      "iteration: 102990 loss: 0.0030 lr: 0.02\n",
      "iteration: 103000 loss: 0.0027 lr: 0.02\n",
      "iteration: 103010 loss: 0.0027 lr: 0.02\n",
      "iteration: 103020 loss: 0.0025 lr: 0.02\n",
      "iteration: 103030 loss: 0.0045 lr: 0.02\n",
      "iteration: 103040 loss: 0.0033 lr: 0.02\n",
      "iteration: 103050 loss: 0.0045 lr: 0.02\n",
      "iteration: 103060 loss: 0.0029 lr: 0.02\n",
      "iteration: 103070 loss: 0.0030 lr: 0.02\n",
      "iteration: 103080 loss: 0.0029 lr: 0.02\n",
      "iteration: 103090 loss: 0.0033 lr: 0.02\n",
      "iteration: 103100 loss: 0.0029 lr: 0.02\n",
      "iteration: 103110 loss: 0.0035 lr: 0.02\n",
      "iteration: 103120 loss: 0.0031 lr: 0.02\n",
      "iteration: 103130 loss: 0.0036 lr: 0.02\n",
      "iteration: 103140 loss: 0.0027 lr: 0.02\n",
      "iteration: 103150 loss: 0.0033 lr: 0.02\n",
      "iteration: 103160 loss: 0.0041 lr: 0.02\n",
      "iteration: 103170 loss: 0.0035 lr: 0.02\n",
      "iteration: 103180 loss: 0.0032 lr: 0.02\n",
      "iteration: 103190 loss: 0.0032 lr: 0.02\n",
      "iteration: 103200 loss: 0.0024 lr: 0.02\n",
      "iteration: 103210 loss: 0.0025 lr: 0.02\n",
      "iteration: 103220 loss: 0.0028 lr: 0.02\n",
      "iteration: 103230 loss: 0.0029 lr: 0.02\n",
      "iteration: 103240 loss: 0.0039 lr: 0.02\n",
      "iteration: 103250 loss: 0.0045 lr: 0.02\n",
      "iteration: 103260 loss: 0.0034 lr: 0.02\n",
      "iteration: 103270 loss: 0.0030 lr: 0.02\n",
      "iteration: 103280 loss: 0.0048 lr: 0.02\n",
      "iteration: 103290 loss: 0.0037 lr: 0.02\n",
      "iteration: 103300 loss: 0.0032 lr: 0.02\n",
      "iteration: 103310 loss: 0.0033 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 103320 loss: 0.0036 lr: 0.02\n",
      "iteration: 103330 loss: 0.0031 lr: 0.02\n",
      "iteration: 103340 loss: 0.0042 lr: 0.02\n",
      "iteration: 103350 loss: 0.0046 lr: 0.02\n",
      "iteration: 103360 loss: 0.0026 lr: 0.02\n",
      "iteration: 103370 loss: 0.0029 lr: 0.02\n",
      "iteration: 103380 loss: 0.0025 lr: 0.02\n",
      "iteration: 103390 loss: 0.0021 lr: 0.02\n",
      "iteration: 103400 loss: 0.0030 lr: 0.02\n",
      "iteration: 103410 loss: 0.0028 lr: 0.02\n",
      "iteration: 103420 loss: 0.0033 lr: 0.02\n",
      "iteration: 103430 loss: 0.0032 lr: 0.02\n",
      "iteration: 103440 loss: 0.0029 lr: 0.02\n",
      "iteration: 103450 loss: 0.0023 lr: 0.02\n",
      "iteration: 103460 loss: 0.0035 lr: 0.02\n",
      "iteration: 103470 loss: 0.0033 lr: 0.02\n",
      "iteration: 103480 loss: 0.0033 lr: 0.02\n",
      "iteration: 103490 loss: 0.0025 lr: 0.02\n",
      "iteration: 103500 loss: 0.0025 lr: 0.02\n",
      "iteration: 103510 loss: 0.0033 lr: 0.02\n",
      "iteration: 103520 loss: 0.0040 lr: 0.02\n",
      "iteration: 103530 loss: 0.0046 lr: 0.02\n",
      "iteration: 103540 loss: 0.0072 lr: 0.02\n",
      "iteration: 103550 loss: 0.0038 lr: 0.02\n",
      "iteration: 103560 loss: 0.0034 lr: 0.02\n",
      "iteration: 103570 loss: 0.0032 lr: 0.02\n",
      "iteration: 103580 loss: 0.0029 lr: 0.02\n",
      "iteration: 103590 loss: 0.0041 lr: 0.02\n",
      "iteration: 103600 loss: 0.0033 lr: 0.02\n",
      "iteration: 103610 loss: 0.0038 lr: 0.02\n",
      "iteration: 103620 loss: 0.0031 lr: 0.02\n",
      "iteration: 103630 loss: 0.0044 lr: 0.02\n",
      "iteration: 103640 loss: 0.0020 lr: 0.02\n",
      "iteration: 103650 loss: 0.0042 lr: 0.02\n",
      "iteration: 103660 loss: 0.0035 lr: 0.02\n",
      "iteration: 103670 loss: 0.0041 lr: 0.02\n",
      "iteration: 103680 loss: 0.0032 lr: 0.02\n",
      "iteration: 103690 loss: 0.0029 lr: 0.02\n",
      "iteration: 103700 loss: 0.0031 lr: 0.02\n",
      "iteration: 103710 loss: 0.0044 lr: 0.02\n",
      "iteration: 103720 loss: 0.0044 lr: 0.02\n",
      "iteration: 103730 loss: 0.0037 lr: 0.02\n",
      "iteration: 103740 loss: 0.0044 lr: 0.02\n",
      "iteration: 103750 loss: 0.0029 lr: 0.02\n",
      "iteration: 103760 loss: 0.0028 lr: 0.02\n",
      "iteration: 103770 loss: 0.0027 lr: 0.02\n",
      "iteration: 103780 loss: 0.0041 lr: 0.02\n",
      "iteration: 103790 loss: 0.0045 lr: 0.02\n",
      "iteration: 103800 loss: 0.0035 lr: 0.02\n",
      "iteration: 103810 loss: 0.0047 lr: 0.02\n",
      "iteration: 103820 loss: 0.0037 lr: 0.02\n",
      "iteration: 103830 loss: 0.0028 lr: 0.02\n",
      "iteration: 103840 loss: 0.0045 lr: 0.02\n",
      "iteration: 103850 loss: 0.0032 lr: 0.02\n",
      "iteration: 103860 loss: 0.0029 lr: 0.02\n",
      "iteration: 103870 loss: 0.0043 lr: 0.02\n",
      "iteration: 103880 loss: 0.0029 lr: 0.02\n",
      "iteration: 103890 loss: 0.0048 lr: 0.02\n",
      "iteration: 103900 loss: 0.0034 lr: 0.02\n",
      "iteration: 103910 loss: 0.0029 lr: 0.02\n",
      "iteration: 103920 loss: 0.0029 lr: 0.02\n",
      "iteration: 103930 loss: 0.0026 lr: 0.02\n",
      "iteration: 103940 loss: 0.0036 lr: 0.02\n",
      "iteration: 103950 loss: 0.0044 lr: 0.02\n",
      "iteration: 103960 loss: 0.0024 lr: 0.02\n",
      "iteration: 103970 loss: 0.0031 lr: 0.02\n",
      "iteration: 103980 loss: 0.0049 lr: 0.02\n",
      "iteration: 103990 loss: 0.0034 lr: 0.02\n",
      "iteration: 104000 loss: 0.0029 lr: 0.02\n",
      "iteration: 104010 loss: 0.0030 lr: 0.02\n",
      "iteration: 104020 loss: 0.0038 lr: 0.02\n",
      "iteration: 104030 loss: 0.0036 lr: 0.02\n",
      "iteration: 104040 loss: 0.0035 lr: 0.02\n",
      "iteration: 104050 loss: 0.0035 lr: 0.02\n",
      "iteration: 104060 loss: 0.0032 lr: 0.02\n",
      "iteration: 104070 loss: 0.0025 lr: 0.02\n",
      "iteration: 104080 loss: 0.0037 lr: 0.02\n",
      "iteration: 104090 loss: 0.0037 lr: 0.02\n",
      "iteration: 104100 loss: 0.0026 lr: 0.02\n",
      "iteration: 104110 loss: 0.0034 lr: 0.02\n",
      "iteration: 104120 loss: 0.0042 lr: 0.02\n",
      "iteration: 104130 loss: 0.0045 lr: 0.02\n",
      "iteration: 104140 loss: 0.0044 lr: 0.02\n",
      "iteration: 104150 loss: 0.0029 lr: 0.02\n",
      "iteration: 104160 loss: 0.0024 lr: 0.02\n",
      "iteration: 104170 loss: 0.0033 lr: 0.02\n",
      "iteration: 104180 loss: 0.0031 lr: 0.02\n",
      "iteration: 104190 loss: 0.0024 lr: 0.02\n",
      "iteration: 104200 loss: 0.0038 lr: 0.02\n",
      "iteration: 104210 loss: 0.0065 lr: 0.02\n",
      "iteration: 104220 loss: 0.0041 lr: 0.02\n",
      "iteration: 104230 loss: 0.0030 lr: 0.02\n",
      "iteration: 104240 loss: 0.0034 lr: 0.02\n",
      "iteration: 104250 loss: 0.0028 lr: 0.02\n",
      "iteration: 104260 loss: 0.0047 lr: 0.02\n",
      "iteration: 104270 loss: 0.0046 lr: 0.02\n",
      "iteration: 104280 loss: 0.0035 lr: 0.02\n",
      "iteration: 104290 loss: 0.0049 lr: 0.02\n",
      "iteration: 104300 loss: 0.0035 lr: 0.02\n",
      "iteration: 104310 loss: 0.0037 lr: 0.02\n",
      "iteration: 104320 loss: 0.0040 lr: 0.02\n",
      "iteration: 104330 loss: 0.0027 lr: 0.02\n",
      "iteration: 104340 loss: 0.0042 lr: 0.02\n",
      "iteration: 104350 loss: 0.0039 lr: 0.02\n",
      "iteration: 104360 loss: 0.0030 lr: 0.02\n",
      "iteration: 104370 loss: 0.0029 lr: 0.02\n",
      "iteration: 104380 loss: 0.0037 lr: 0.02\n",
      "iteration: 104390 loss: 0.0023 lr: 0.02\n",
      "iteration: 104400 loss: 0.0037 lr: 0.02\n",
      "iteration: 104410 loss: 0.0034 lr: 0.02\n",
      "iteration: 104420 loss: 0.0043 lr: 0.02\n",
      "iteration: 104430 loss: 0.0047 lr: 0.02\n",
      "iteration: 104440 loss: 0.0029 lr: 0.02\n",
      "iteration: 104450 loss: 0.0028 lr: 0.02\n",
      "iteration: 104460 loss: 0.0029 lr: 0.02\n",
      "iteration: 104470 loss: 0.0046 lr: 0.02\n",
      "iteration: 104480 loss: 0.0024 lr: 0.02\n",
      "iteration: 104490 loss: 0.0031 lr: 0.02\n",
      "iteration: 104500 loss: 0.0036 lr: 0.02\n",
      "iteration: 104510 loss: 0.0034 lr: 0.02\n",
      "iteration: 104520 loss: 0.0028 lr: 0.02\n",
      "iteration: 104530 loss: 0.0028 lr: 0.02\n",
      "iteration: 104540 loss: 0.0035 lr: 0.02\n",
      "iteration: 104550 loss: 0.0032 lr: 0.02\n",
      "iteration: 104560 loss: 0.0031 lr: 0.02\n",
      "iteration: 104570 loss: 0.0033 lr: 0.02\n",
      "iteration: 104580 loss: 0.0025 lr: 0.02\n",
      "iteration: 104590 loss: 0.0039 lr: 0.02\n",
      "iteration: 104600 loss: 0.0031 lr: 0.02\n",
      "iteration: 104610 loss: 0.0041 lr: 0.02\n",
      "iteration: 104620 loss: 0.0032 lr: 0.02\n",
      "iteration: 104630 loss: 0.0030 lr: 0.02\n",
      "iteration: 104640 loss: 0.0024 lr: 0.02\n",
      "iteration: 104650 loss: 0.0030 lr: 0.02\n",
      "iteration: 104660 loss: 0.0039 lr: 0.02\n",
      "iteration: 104670 loss: 0.0025 lr: 0.02\n",
      "iteration: 104680 loss: 0.0028 lr: 0.02\n",
      "iteration: 104690 loss: 0.0052 lr: 0.02\n",
      "iteration: 104700 loss: 0.0025 lr: 0.02\n",
      "iteration: 104710 loss: 0.0030 lr: 0.02\n",
      "iteration: 104720 loss: 0.0029 lr: 0.02\n",
      "iteration: 104730 loss: 0.0026 lr: 0.02\n",
      "iteration: 104740 loss: 0.0021 lr: 0.02\n",
      "iteration: 104750 loss: 0.0036 lr: 0.02\n",
      "iteration: 104760 loss: 0.0030 lr: 0.02\n",
      "iteration: 104770 loss: 0.0038 lr: 0.02\n",
      "iteration: 104780 loss: 0.0035 lr: 0.02\n",
      "iteration: 104790 loss: 0.0032 lr: 0.02\n",
      "iteration: 104800 loss: 0.0030 lr: 0.02\n",
      "iteration: 104810 loss: 0.0050 lr: 0.02\n",
      "iteration: 104820 loss: 0.0043 lr: 0.02\n",
      "iteration: 104830 loss: 0.0050 lr: 0.02\n",
      "iteration: 104840 loss: 0.0044 lr: 0.02\n",
      "iteration: 104850 loss: 0.0037 lr: 0.02\n",
      "iteration: 104860 loss: 0.0027 lr: 0.02\n",
      "iteration: 104870 loss: 0.0035 lr: 0.02\n",
      "iteration: 104880 loss: 0.0032 lr: 0.02\n",
      "iteration: 104890 loss: 0.0045 lr: 0.02\n",
      "iteration: 104900 loss: 0.0031 lr: 0.02\n",
      "iteration: 104910 loss: 0.0037 lr: 0.02\n",
      "iteration: 104920 loss: 0.0034 lr: 0.02\n",
      "iteration: 104930 loss: 0.0039 lr: 0.02\n",
      "iteration: 104940 loss: 0.0024 lr: 0.02\n",
      "iteration: 104950 loss: 0.0037 lr: 0.02\n",
      "iteration: 104960 loss: 0.0027 lr: 0.02\n",
      "iteration: 104970 loss: 0.0025 lr: 0.02\n",
      "iteration: 104980 loss: 0.0030 lr: 0.02\n",
      "iteration: 104990 loss: 0.0044 lr: 0.02\n",
      "iteration: 105000 loss: 0.0031 lr: 0.02\n",
      "iteration: 105010 loss: 0.0039 lr: 0.02\n",
      "iteration: 105020 loss: 0.0034 lr: 0.02\n",
      "iteration: 105030 loss: 0.0027 lr: 0.02\n",
      "iteration: 105040 loss: 0.0020 lr: 0.02\n",
      "iteration: 105050 loss: 0.0042 lr: 0.02\n",
      "iteration: 105060 loss: 0.0034 lr: 0.02\n",
      "iteration: 105070 loss: 0.0034 lr: 0.02\n",
      "iteration: 105080 loss: 0.0032 lr: 0.02\n",
      "iteration: 105090 loss: 0.0037 lr: 0.02\n",
      "iteration: 105100 loss: 0.0033 lr: 0.02\n",
      "iteration: 105110 loss: 0.0041 lr: 0.02\n",
      "iteration: 105120 loss: 0.0026 lr: 0.02\n",
      "iteration: 105130 loss: 0.0035 lr: 0.02\n",
      "iteration: 105140 loss: 0.0023 lr: 0.02\n",
      "iteration: 105150 loss: 0.0029 lr: 0.02\n",
      "iteration: 105160 loss: 0.0029 lr: 0.02\n",
      "iteration: 105170 loss: 0.0029 lr: 0.02\n",
      "iteration: 105180 loss: 0.0045 lr: 0.02\n",
      "iteration: 105190 loss: 0.0028 lr: 0.02\n",
      "iteration: 105200 loss: 0.0040 lr: 0.02\n",
      "iteration: 105210 loss: 0.0031 lr: 0.02\n",
      "iteration: 105220 loss: 0.0026 lr: 0.02\n",
      "iteration: 105230 loss: 0.0023 lr: 0.02\n",
      "iteration: 105240 loss: 0.0041 lr: 0.02\n",
      "iteration: 105250 loss: 0.0033 lr: 0.02\n",
      "iteration: 105260 loss: 0.0029 lr: 0.02\n",
      "iteration: 105270 loss: 0.0036 lr: 0.02\n",
      "iteration: 105280 loss: 0.0042 lr: 0.02\n",
      "iteration: 105290 loss: 0.0036 lr: 0.02\n",
      "iteration: 105300 loss: 0.0046 lr: 0.02\n",
      "iteration: 105310 loss: 0.0036 lr: 0.02\n",
      "iteration: 105320 loss: 0.0031 lr: 0.02\n",
      "iteration: 105330 loss: 0.0033 lr: 0.02\n",
      "iteration: 105340 loss: 0.0024 lr: 0.02\n",
      "iteration: 105350 loss: 0.0024 lr: 0.02\n",
      "iteration: 105360 loss: 0.0027 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 105370 loss: 0.0032 lr: 0.02\n",
      "iteration: 105380 loss: 0.0025 lr: 0.02\n",
      "iteration: 105390 loss: 0.0031 lr: 0.02\n",
      "iteration: 105400 loss: 0.0041 lr: 0.02\n",
      "iteration: 105410 loss: 0.0028 lr: 0.02\n",
      "iteration: 105420 loss: 0.0036 lr: 0.02\n",
      "iteration: 105430 loss: 0.0025 lr: 0.02\n",
      "iteration: 105440 loss: 0.0030 lr: 0.02\n",
      "iteration: 105450 loss: 0.0029 lr: 0.02\n",
      "iteration: 105460 loss: 0.0032 lr: 0.02\n",
      "iteration: 105470 loss: 0.0037 lr: 0.02\n",
      "iteration: 105480 loss: 0.0028 lr: 0.02\n",
      "iteration: 105490 loss: 0.0037 lr: 0.02\n",
      "iteration: 105500 loss: 0.0035 lr: 0.02\n",
      "iteration: 105510 loss: 0.0026 lr: 0.02\n",
      "iteration: 105520 loss: 0.0027 lr: 0.02\n",
      "iteration: 105530 loss: 0.0025 lr: 0.02\n",
      "iteration: 105540 loss: 0.0048 lr: 0.02\n",
      "iteration: 105550 loss: 0.0036 lr: 0.02\n",
      "iteration: 105560 loss: 0.0034 lr: 0.02\n",
      "iteration: 105570 loss: 0.0025 lr: 0.02\n",
      "iteration: 105580 loss: 0.0035 lr: 0.02\n",
      "iteration: 105590 loss: 0.0019 lr: 0.02\n",
      "iteration: 105600 loss: 0.0032 lr: 0.02\n",
      "iteration: 105610 loss: 0.0031 lr: 0.02\n",
      "iteration: 105620 loss: 0.0039 lr: 0.02\n",
      "iteration: 105630 loss: 0.0031 lr: 0.02\n",
      "iteration: 105640 loss: 0.0025 lr: 0.02\n",
      "iteration: 105650 loss: 0.0031 lr: 0.02\n",
      "iteration: 105660 loss: 0.0032 lr: 0.02\n",
      "iteration: 105670 loss: 0.0030 lr: 0.02\n",
      "iteration: 105680 loss: 0.0039 lr: 0.02\n",
      "iteration: 105690 loss: 0.0030 lr: 0.02\n",
      "iteration: 105700 loss: 0.0021 lr: 0.02\n",
      "iteration: 105710 loss: 0.0029 lr: 0.02\n",
      "iteration: 105720 loss: 0.0039 lr: 0.02\n",
      "iteration: 105730 loss: 0.0036 lr: 0.02\n",
      "iteration: 105740 loss: 0.0028 lr: 0.02\n",
      "iteration: 105750 loss: 0.0029 lr: 0.02\n",
      "iteration: 105760 loss: 0.0035 lr: 0.02\n",
      "iteration: 105770 loss: 0.0046 lr: 0.02\n",
      "iteration: 105780 loss: 0.0025 lr: 0.02\n",
      "iteration: 105790 loss: 0.0030 lr: 0.02\n",
      "iteration: 105800 loss: 0.0030 lr: 0.02\n",
      "iteration: 105810 loss: 0.0018 lr: 0.02\n",
      "iteration: 105820 loss: 0.0040 lr: 0.02\n",
      "iteration: 105830 loss: 0.0035 lr: 0.02\n",
      "iteration: 105840 loss: 0.0031 lr: 0.02\n",
      "iteration: 105850 loss: 0.0026 lr: 0.02\n",
      "iteration: 105860 loss: 0.0024 lr: 0.02\n",
      "iteration: 105870 loss: 0.0037 lr: 0.02\n",
      "iteration: 105880 loss: 0.0026 lr: 0.02\n",
      "iteration: 105890 loss: 0.0032 lr: 0.02\n",
      "iteration: 105900 loss: 0.0027 lr: 0.02\n",
      "iteration: 105910 loss: 0.0032 lr: 0.02\n",
      "iteration: 105920 loss: 0.0036 lr: 0.02\n",
      "iteration: 105930 loss: 0.0033 lr: 0.02\n",
      "iteration: 105940 loss: 0.0040 lr: 0.02\n",
      "iteration: 105950 loss: 0.0034 lr: 0.02\n",
      "iteration: 105960 loss: 0.0025 lr: 0.02\n",
      "iteration: 105970 loss: 0.0030 lr: 0.02\n",
      "iteration: 105980 loss: 0.0039 lr: 0.02\n",
      "iteration: 105990 loss: 0.0032 lr: 0.02\n",
      "iteration: 106000 loss: 0.0031 lr: 0.02\n",
      "iteration: 106010 loss: 0.0046 lr: 0.02\n",
      "iteration: 106020 loss: 0.0026 lr: 0.02\n",
      "iteration: 106030 loss: 0.0049 lr: 0.02\n",
      "iteration: 106040 loss: 0.0051 lr: 0.02\n",
      "iteration: 106050 loss: 0.0043 lr: 0.02\n",
      "iteration: 106060 loss: 0.0027 lr: 0.02\n",
      "iteration: 106070 loss: 0.0037 lr: 0.02\n",
      "iteration: 106080 loss: 0.0030 lr: 0.02\n",
      "iteration: 106090 loss: 0.0026 lr: 0.02\n",
      "iteration: 106100 loss: 0.0052 lr: 0.02\n",
      "iteration: 106110 loss: 0.0026 lr: 0.02\n",
      "iteration: 106120 loss: 0.0031 lr: 0.02\n",
      "iteration: 106130 loss: 0.0027 lr: 0.02\n",
      "iteration: 106140 loss: 0.0035 lr: 0.02\n",
      "iteration: 106150 loss: 0.0034 lr: 0.02\n",
      "iteration: 106160 loss: 0.0020 lr: 0.02\n",
      "iteration: 106170 loss: 0.0023 lr: 0.02\n",
      "iteration: 106180 loss: 0.0031 lr: 0.02\n",
      "iteration: 106190 loss: 0.0034 lr: 0.02\n",
      "iteration: 106200 loss: 0.0033 lr: 0.02\n",
      "iteration: 106210 loss: 0.0030 lr: 0.02\n",
      "iteration: 106220 loss: 0.0029 lr: 0.02\n",
      "iteration: 106230 loss: 0.0028 lr: 0.02\n",
      "iteration: 106240 loss: 0.0020 lr: 0.02\n",
      "iteration: 106250 loss: 0.0040 lr: 0.02\n",
      "iteration: 106260 loss: 0.0033 lr: 0.02\n",
      "iteration: 106270 loss: 0.0021 lr: 0.02\n",
      "iteration: 106280 loss: 0.0028 lr: 0.02\n",
      "iteration: 106290 loss: 0.0022 lr: 0.02\n",
      "iteration: 106300 loss: 0.0032 lr: 0.02\n",
      "iteration: 106310 loss: 0.0026 lr: 0.02\n",
      "iteration: 106320 loss: 0.0035 lr: 0.02\n",
      "iteration: 106330 loss: 0.0023 lr: 0.02\n",
      "iteration: 106340 loss: 0.0033 lr: 0.02\n",
      "iteration: 106350 loss: 0.0027 lr: 0.02\n",
      "iteration: 106360 loss: 0.0024 lr: 0.02\n",
      "iteration: 106370 loss: 0.0031 lr: 0.02\n",
      "iteration: 106380 loss: 0.0028 lr: 0.02\n",
      "iteration: 106390 loss: 0.0023 lr: 0.02\n",
      "iteration: 106400 loss: 0.0026 lr: 0.02\n",
      "iteration: 106410 loss: 0.0032 lr: 0.02\n",
      "iteration: 106420 loss: 0.0038 lr: 0.02\n",
      "iteration: 106430 loss: 0.0026 lr: 0.02\n",
      "iteration: 106440 loss: 0.0039 lr: 0.02\n",
      "iteration: 106450 loss: 0.0033 lr: 0.02\n",
      "iteration: 106460 loss: 0.0027 lr: 0.02\n",
      "iteration: 106470 loss: 0.0022 lr: 0.02\n",
      "iteration: 106480 loss: 0.0025 lr: 0.02\n",
      "iteration: 106490 loss: 0.0032 lr: 0.02\n",
      "iteration: 106500 loss: 0.0027 lr: 0.02\n",
      "iteration: 106510 loss: 0.0028 lr: 0.02\n",
      "iteration: 106520 loss: 0.0032 lr: 0.02\n",
      "iteration: 106530 loss: 0.0029 lr: 0.02\n",
      "iteration: 106540 loss: 0.0027 lr: 0.02\n",
      "iteration: 106550 loss: 0.0043 lr: 0.02\n",
      "iteration: 106560 loss: 0.0030 lr: 0.02\n",
      "iteration: 106570 loss: 0.0025 lr: 0.02\n",
      "iteration: 106580 loss: 0.0038 lr: 0.02\n",
      "iteration: 106590 loss: 0.0026 lr: 0.02\n",
      "iteration: 106600 loss: 0.0034 lr: 0.02\n",
      "iteration: 106610 loss: 0.0043 lr: 0.02\n",
      "iteration: 106620 loss: 0.0030 lr: 0.02\n",
      "iteration: 106630 loss: 0.0032 lr: 0.02\n",
      "iteration: 106640 loss: 0.0032 lr: 0.02\n",
      "iteration: 106650 loss: 0.0019 lr: 0.02\n",
      "iteration: 106660 loss: 0.0029 lr: 0.02\n",
      "iteration: 106670 loss: 0.0038 lr: 0.02\n",
      "iteration: 106680 loss: 0.0029 lr: 0.02\n",
      "iteration: 106690 loss: 0.0032 lr: 0.02\n",
      "iteration: 106700 loss: 0.0030 lr: 0.02\n",
      "iteration: 106710 loss: 0.0021 lr: 0.02\n",
      "iteration: 106720 loss: 0.0029 lr: 0.02\n",
      "iteration: 106730 loss: 0.0029 lr: 0.02\n",
      "iteration: 106740 loss: 0.0026 lr: 0.02\n",
      "iteration: 106750 loss: 0.0034 lr: 0.02\n",
      "iteration: 106760 loss: 0.0032 lr: 0.02\n",
      "iteration: 106770 loss: 0.0028 lr: 0.02\n",
      "iteration: 106780 loss: 0.0035 lr: 0.02\n",
      "iteration: 106790 loss: 0.0022 lr: 0.02\n",
      "iteration: 106800 loss: 0.0035 lr: 0.02\n",
      "iteration: 106810 loss: 0.0028 lr: 0.02\n",
      "iteration: 106820 loss: 0.0034 lr: 0.02\n",
      "iteration: 106830 loss: 0.0044 lr: 0.02\n",
      "iteration: 106840 loss: 0.0028 lr: 0.02\n",
      "iteration: 106850 loss: 0.0026 lr: 0.02\n",
      "iteration: 106860 loss: 0.0033 lr: 0.02\n",
      "iteration: 106870 loss: 0.0027 lr: 0.02\n",
      "iteration: 106880 loss: 0.0030 lr: 0.02\n",
      "iteration: 106890 loss: 0.0029 lr: 0.02\n",
      "iteration: 106900 loss: 0.0028 lr: 0.02\n",
      "iteration: 106910 loss: 0.0035 lr: 0.02\n",
      "iteration: 106920 loss: 0.0022 lr: 0.02\n",
      "iteration: 106930 loss: 0.0025 lr: 0.02\n",
      "iteration: 106940 loss: 0.0032 lr: 0.02\n",
      "iteration: 106950 loss: 0.0040 lr: 0.02\n",
      "iteration: 106960 loss: 0.0033 lr: 0.02\n",
      "iteration: 106970 loss: 0.0025 lr: 0.02\n",
      "iteration: 106980 loss: 0.0037 lr: 0.02\n",
      "iteration: 106990 loss: 0.0035 lr: 0.02\n",
      "iteration: 107000 loss: 0.0043 lr: 0.02\n",
      "iteration: 107010 loss: 0.0043 lr: 0.02\n",
      "iteration: 107020 loss: 0.0032 lr: 0.02\n",
      "iteration: 107030 loss: 0.0041 lr: 0.02\n",
      "iteration: 107040 loss: 0.0035 lr: 0.02\n",
      "iteration: 107050 loss: 0.0031 lr: 0.02\n",
      "iteration: 107060 loss: 0.0029 lr: 0.02\n",
      "iteration: 107070 loss: 0.0025 lr: 0.02\n",
      "iteration: 107080 loss: 0.0042 lr: 0.02\n",
      "iteration: 107090 loss: 0.0032 lr: 0.02\n",
      "iteration: 107100 loss: 0.0035 lr: 0.02\n",
      "iteration: 107110 loss: 0.0035 lr: 0.02\n",
      "iteration: 107120 loss: 0.0027 lr: 0.02\n",
      "iteration: 107130 loss: 0.0027 lr: 0.02\n",
      "iteration: 107140 loss: 0.0044 lr: 0.02\n",
      "iteration: 107150 loss: 0.0021 lr: 0.02\n",
      "iteration: 107160 loss: 0.0036 lr: 0.02\n",
      "iteration: 107170 loss: 0.0023 lr: 0.02\n",
      "iteration: 107180 loss: 0.0030 lr: 0.02\n",
      "iteration: 107190 loss: 0.0032 lr: 0.02\n",
      "iteration: 107200 loss: 0.0028 lr: 0.02\n",
      "iteration: 107210 loss: 0.0033 lr: 0.02\n",
      "iteration: 107220 loss: 0.0029 lr: 0.02\n",
      "iteration: 107230 loss: 0.0030 lr: 0.02\n",
      "iteration: 107240 loss: 0.0029 lr: 0.02\n",
      "iteration: 107250 loss: 0.0030 lr: 0.02\n",
      "iteration: 107260 loss: 0.0029 lr: 0.02\n",
      "iteration: 107270 loss: 0.0024 lr: 0.02\n",
      "iteration: 107280 loss: 0.0035 lr: 0.02\n",
      "iteration: 107290 loss: 0.0032 lr: 0.02\n",
      "iteration: 107300 loss: 0.0047 lr: 0.02\n",
      "iteration: 107310 loss: 0.0036 lr: 0.02\n",
      "iteration: 107320 loss: 0.0034 lr: 0.02\n",
      "iteration: 107330 loss: 0.0027 lr: 0.02\n",
      "iteration: 107340 loss: 0.0030 lr: 0.02\n",
      "iteration: 107350 loss: 0.0043 lr: 0.02\n",
      "iteration: 107360 loss: 0.0038 lr: 0.02\n",
      "iteration: 107370 loss: 0.0028 lr: 0.02\n",
      "iteration: 107380 loss: 0.0034 lr: 0.02\n",
      "iteration: 107390 loss: 0.0032 lr: 0.02\n",
      "iteration: 107400 loss: 0.0024 lr: 0.02\n",
      "iteration: 107410 loss: 0.0029 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 107420 loss: 0.0023 lr: 0.02\n",
      "iteration: 107430 loss: 0.0034 lr: 0.02\n",
      "iteration: 107440 loss: 0.0033 lr: 0.02\n",
      "iteration: 107450 loss: 0.0053 lr: 0.02\n",
      "iteration: 107460 loss: 0.0032 lr: 0.02\n",
      "iteration: 107470 loss: 0.0035 lr: 0.02\n",
      "iteration: 107480 loss: 0.0041 lr: 0.02\n",
      "iteration: 107490 loss: 0.0055 lr: 0.02\n",
      "iteration: 107500 loss: 0.0028 lr: 0.02\n",
      "iteration: 107510 loss: 0.0046 lr: 0.02\n",
      "iteration: 107520 loss: 0.0025 lr: 0.02\n",
      "iteration: 107530 loss: 0.0028 lr: 0.02\n",
      "iteration: 107540 loss: 0.0032 lr: 0.02\n",
      "iteration: 107550 loss: 0.0025 lr: 0.02\n",
      "iteration: 107560 loss: 0.0029 lr: 0.02\n",
      "iteration: 107570 loss: 0.0043 lr: 0.02\n",
      "iteration: 107580 loss: 0.0030 lr: 0.02\n",
      "iteration: 107590 loss: 0.0038 lr: 0.02\n",
      "iteration: 107600 loss: 0.0025 lr: 0.02\n",
      "iteration: 107610 loss: 0.0053 lr: 0.02\n",
      "iteration: 107620 loss: 0.0034 lr: 0.02\n",
      "iteration: 107630 loss: 0.0034 lr: 0.02\n",
      "iteration: 107640 loss: 0.0037 lr: 0.02\n",
      "iteration: 107650 loss: 0.0035 lr: 0.02\n",
      "iteration: 107660 loss: 0.0041 lr: 0.02\n",
      "iteration: 107670 loss: 0.0034 lr: 0.02\n",
      "iteration: 107680 loss: 0.0030 lr: 0.02\n",
      "iteration: 107690 loss: 0.0036 lr: 0.02\n",
      "iteration: 107700 loss: 0.0030 lr: 0.02\n",
      "iteration: 107710 loss: 0.0033 lr: 0.02\n",
      "iteration: 107720 loss: 0.0019 lr: 0.02\n",
      "iteration: 107730 loss: 0.0032 lr: 0.02\n",
      "iteration: 107740 loss: 0.0029 lr: 0.02\n",
      "iteration: 107750 loss: 0.0032 lr: 0.02\n",
      "iteration: 107760 loss: 0.0032 lr: 0.02\n",
      "iteration: 107770 loss: 0.0045 lr: 0.02\n",
      "iteration: 107780 loss: 0.0034 lr: 0.02\n",
      "iteration: 107790 loss: 0.0028 lr: 0.02\n",
      "iteration: 107800 loss: 0.0026 lr: 0.02\n",
      "iteration: 107810 loss: 0.0022 lr: 0.02\n",
      "iteration: 107820 loss: 0.0024 lr: 0.02\n",
      "iteration: 107830 loss: 0.0026 lr: 0.02\n",
      "iteration: 107840 loss: 0.0028 lr: 0.02\n",
      "iteration: 107850 loss: 0.0045 lr: 0.02\n",
      "iteration: 107860 loss: 0.0029 lr: 0.02\n",
      "iteration: 107870 loss: 0.0028 lr: 0.02\n",
      "iteration: 107880 loss: 0.0039 lr: 0.02\n",
      "iteration: 107890 loss: 0.0030 lr: 0.02\n",
      "iteration: 107900 loss: 0.0028 lr: 0.02\n",
      "iteration: 107910 loss: 0.0038 lr: 0.02\n",
      "iteration: 107920 loss: 0.0034 lr: 0.02\n",
      "iteration: 107930 loss: 0.0026 lr: 0.02\n",
      "iteration: 107940 loss: 0.0030 lr: 0.02\n",
      "iteration: 107950 loss: 0.0031 lr: 0.02\n",
      "iteration: 107960 loss: 0.0029 lr: 0.02\n",
      "iteration: 107970 loss: 0.0023 lr: 0.02\n",
      "iteration: 107980 loss: 0.0025 lr: 0.02\n",
      "iteration: 107990 loss: 0.0018 lr: 0.02\n",
      "iteration: 108000 loss: 0.0032 lr: 0.02\n",
      "iteration: 108010 loss: 0.0036 lr: 0.02\n",
      "iteration: 108020 loss: 0.0037 lr: 0.02\n",
      "iteration: 108030 loss: 0.0025 lr: 0.02\n",
      "iteration: 108040 loss: 0.0039 lr: 0.02\n",
      "iteration: 108050 loss: 0.0039 lr: 0.02\n",
      "iteration: 108060 loss: 0.0026 lr: 0.02\n",
      "iteration: 108070 loss: 0.0054 lr: 0.02\n",
      "iteration: 108080 loss: 0.0024 lr: 0.02\n",
      "iteration: 108090 loss: 0.0023 lr: 0.02\n",
      "iteration: 108100 loss: 0.0026 lr: 0.02\n",
      "iteration: 108110 loss: 0.0023 lr: 0.02\n",
      "iteration: 108120 loss: 0.0036 lr: 0.02\n",
      "iteration: 108130 loss: 0.0028 lr: 0.02\n",
      "iteration: 108140 loss: 0.0023 lr: 0.02\n",
      "iteration: 108150 loss: 0.0024 lr: 0.02\n",
      "iteration: 108160 loss: 0.0046 lr: 0.02\n",
      "iteration: 108170 loss: 0.0044 lr: 0.02\n",
      "iteration: 108180 loss: 0.0026 lr: 0.02\n",
      "iteration: 108190 loss: 0.0035 lr: 0.02\n",
      "iteration: 108200 loss: 0.0027 lr: 0.02\n",
      "iteration: 108210 loss: 0.0028 lr: 0.02\n",
      "iteration: 108220 loss: 0.0043 lr: 0.02\n",
      "iteration: 108230 loss: 0.0028 lr: 0.02\n",
      "iteration: 108240 loss: 0.0032 lr: 0.02\n",
      "iteration: 108250 loss: 0.0031 lr: 0.02\n",
      "iteration: 108260 loss: 0.0031 lr: 0.02\n",
      "iteration: 108270 loss: 0.0033 lr: 0.02\n",
      "iteration: 108280 loss: 0.0051 lr: 0.02\n",
      "iteration: 108290 loss: 0.0028 lr: 0.02\n",
      "iteration: 108300 loss: 0.0028 lr: 0.02\n",
      "iteration: 108310 loss: 0.0032 lr: 0.02\n",
      "iteration: 108320 loss: 0.0043 lr: 0.02\n",
      "iteration: 108330 loss: 0.0036 lr: 0.02\n",
      "iteration: 108340 loss: 0.0026 lr: 0.02\n",
      "iteration: 108350 loss: 0.0025 lr: 0.02\n",
      "iteration: 108360 loss: 0.0030 lr: 0.02\n",
      "iteration: 108370 loss: 0.0029 lr: 0.02\n",
      "iteration: 108380 loss: 0.0034 lr: 0.02\n",
      "iteration: 108390 loss: 0.0035 lr: 0.02\n",
      "iteration: 108400 loss: 0.0032 lr: 0.02\n",
      "iteration: 108410 loss: 0.0026 lr: 0.02\n",
      "iteration: 108420 loss: 0.0044 lr: 0.02\n",
      "iteration: 108430 loss: 0.0030 lr: 0.02\n",
      "iteration: 108440 loss: 0.0031 lr: 0.02\n",
      "iteration: 108450 loss: 0.0027 lr: 0.02\n",
      "iteration: 108460 loss: 0.0025 lr: 0.02\n",
      "iteration: 108470 loss: 0.0031 lr: 0.02\n",
      "iteration: 108480 loss: 0.0025 lr: 0.02\n",
      "iteration: 108490 loss: 0.0033 lr: 0.02\n",
      "iteration: 108500 loss: 0.0042 lr: 0.02\n",
      "iteration: 108510 loss: 0.0054 lr: 0.02\n",
      "iteration: 108520 loss: 0.0027 lr: 0.02\n",
      "iteration: 108530 loss: 0.0052 lr: 0.02\n",
      "iteration: 108540 loss: 0.0036 lr: 0.02\n",
      "iteration: 108550 loss: 0.0040 lr: 0.02\n",
      "iteration: 108560 loss: 0.0023 lr: 0.02\n",
      "iteration: 108570 loss: 0.0035 lr: 0.02\n",
      "iteration: 108580 loss: 0.0026 lr: 0.02\n",
      "iteration: 108590 loss: 0.0032 lr: 0.02\n",
      "iteration: 108600 loss: 0.0032 lr: 0.02\n",
      "iteration: 108610 loss: 0.0030 lr: 0.02\n",
      "iteration: 108620 loss: 0.0018 lr: 0.02\n",
      "iteration: 108630 loss: 0.0026 lr: 0.02\n",
      "iteration: 108640 loss: 0.0025 lr: 0.02\n",
      "iteration: 108650 loss: 0.0023 lr: 0.02\n",
      "iteration: 108660 loss: 0.0038 lr: 0.02\n",
      "iteration: 108670 loss: 0.0034 lr: 0.02\n",
      "iteration: 108680 loss: 0.0035 lr: 0.02\n",
      "iteration: 108690 loss: 0.0031 lr: 0.02\n",
      "iteration: 108700 loss: 0.0030 lr: 0.02\n",
      "iteration: 108710 loss: 0.0031 lr: 0.02\n",
      "iteration: 108720 loss: 0.0037 lr: 0.02\n",
      "iteration: 108730 loss: 0.0055 lr: 0.02\n",
      "iteration: 108740 loss: 0.0031 lr: 0.02\n",
      "iteration: 108750 loss: 0.0046 lr: 0.02\n",
      "iteration: 108760 loss: 0.0035 lr: 0.02\n",
      "iteration: 108770 loss: 0.0031 lr: 0.02\n",
      "iteration: 108780 loss: 0.0046 lr: 0.02\n",
      "iteration: 108790 loss: 0.0027 lr: 0.02\n",
      "iteration: 108800 loss: 0.0049 lr: 0.02\n",
      "iteration: 108810 loss: 0.0028 lr: 0.02\n",
      "iteration: 108820 loss: 0.0023 lr: 0.02\n",
      "iteration: 108830 loss: 0.0040 lr: 0.02\n",
      "iteration: 108840 loss: 0.0040 lr: 0.02\n",
      "iteration: 108850 loss: 0.0027 lr: 0.02\n",
      "iteration: 108860 loss: 0.0027 lr: 0.02\n",
      "iteration: 108870 loss: 0.0023 lr: 0.02\n",
      "iteration: 108880 loss: 0.0034 lr: 0.02\n",
      "iteration: 108890 loss: 0.0036 lr: 0.02\n",
      "iteration: 108900 loss: 0.0031 lr: 0.02\n",
      "iteration: 108910 loss: 0.0032 lr: 0.02\n",
      "iteration: 108920 loss: 0.0024 lr: 0.02\n",
      "iteration: 108930 loss: 0.0033 lr: 0.02\n",
      "iteration: 108940 loss: 0.0024 lr: 0.02\n",
      "iteration: 108950 loss: 0.0026 lr: 0.02\n",
      "iteration: 108960 loss: 0.0030 lr: 0.02\n",
      "iteration: 108970 loss: 0.0029 lr: 0.02\n",
      "iteration: 108980 loss: 0.0037 lr: 0.02\n",
      "iteration: 108990 loss: 0.0024 lr: 0.02\n",
      "iteration: 109000 loss: 0.0030 lr: 0.02\n",
      "iteration: 109010 loss: 0.0029 lr: 0.02\n",
      "iteration: 109020 loss: 0.0028 lr: 0.02\n",
      "iteration: 109030 loss: 0.0026 lr: 0.02\n",
      "iteration: 109040 loss: 0.0025 lr: 0.02\n",
      "iteration: 109050 loss: 0.0033 lr: 0.02\n",
      "iteration: 109060 loss: 0.0031 lr: 0.02\n",
      "iteration: 109070 loss: 0.0032 lr: 0.02\n",
      "iteration: 109080 loss: 0.0021 lr: 0.02\n",
      "iteration: 109090 loss: 0.0025 lr: 0.02\n",
      "iteration: 109100 loss: 0.0030 lr: 0.02\n",
      "iteration: 109110 loss: 0.0032 lr: 0.02\n",
      "iteration: 109120 loss: 0.0027 lr: 0.02\n",
      "iteration: 109130 loss: 0.0025 lr: 0.02\n",
      "iteration: 109140 loss: 0.0029 lr: 0.02\n",
      "iteration: 109150 loss: 0.0036 lr: 0.02\n",
      "iteration: 109160 loss: 0.0020 lr: 0.02\n",
      "iteration: 109170 loss: 0.0034 lr: 0.02\n",
      "iteration: 109180 loss: 0.0027 lr: 0.02\n",
      "iteration: 109190 loss: 0.0039 lr: 0.02\n",
      "iteration: 109200 loss: 0.0030 lr: 0.02\n",
      "iteration: 109210 loss: 0.0035 lr: 0.02\n",
      "iteration: 109220 loss: 0.0024 lr: 0.02\n",
      "iteration: 109230 loss: 0.0034 lr: 0.02\n",
      "iteration: 109240 loss: 0.0028 lr: 0.02\n",
      "iteration: 109250 loss: 0.0028 lr: 0.02\n",
      "iteration: 109260 loss: 0.0033 lr: 0.02\n",
      "iteration: 109270 loss: 0.0023 lr: 0.02\n",
      "iteration: 109280 loss: 0.0030 lr: 0.02\n",
      "iteration: 109290 loss: 0.0034 lr: 0.02\n",
      "iteration: 109300 loss: 0.0036 lr: 0.02\n",
      "iteration: 109310 loss: 0.0038 lr: 0.02\n",
      "iteration: 109320 loss: 0.0024 lr: 0.02\n",
      "iteration: 109330 loss: 0.0043 lr: 0.02\n",
      "iteration: 109340 loss: 0.0030 lr: 0.02\n",
      "iteration: 109350 loss: 0.0040 lr: 0.02\n",
      "iteration: 109360 loss: 0.0038 lr: 0.02\n",
      "iteration: 109370 loss: 0.0029 lr: 0.02\n",
      "iteration: 109380 loss: 0.0042 lr: 0.02\n",
      "iteration: 109390 loss: 0.0042 lr: 0.02\n",
      "iteration: 109400 loss: 0.0035 lr: 0.02\n",
      "iteration: 109410 loss: 0.0023 lr: 0.02\n",
      "iteration: 109420 loss: 0.0038 lr: 0.02\n",
      "iteration: 109430 loss: 0.0028 lr: 0.02\n",
      "iteration: 109440 loss: 0.0035 lr: 0.02\n",
      "iteration: 109450 loss: 0.0030 lr: 0.02\n",
      "iteration: 109460 loss: 0.0030 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 109470 loss: 0.0026 lr: 0.02\n",
      "iteration: 109480 loss: 0.0038 lr: 0.02\n",
      "iteration: 109490 loss: 0.0028 lr: 0.02\n",
      "iteration: 109500 loss: 0.0024 lr: 0.02\n",
      "iteration: 109510 loss: 0.0024 lr: 0.02\n",
      "iteration: 109520 loss: 0.0025 lr: 0.02\n",
      "iteration: 109530 loss: 0.0025 lr: 0.02\n",
      "iteration: 109540 loss: 0.0022 lr: 0.02\n",
      "iteration: 109550 loss: 0.0032 lr: 0.02\n",
      "iteration: 109560 loss: 0.0035 lr: 0.02\n",
      "iteration: 109570 loss: 0.0030 lr: 0.02\n",
      "iteration: 109580 loss: 0.0032 lr: 0.02\n",
      "iteration: 109590 loss: 0.0031 lr: 0.02\n",
      "iteration: 109600 loss: 0.0038 lr: 0.02\n",
      "iteration: 109610 loss: 0.0032 lr: 0.02\n",
      "iteration: 109620 loss: 0.0026 lr: 0.02\n",
      "iteration: 109630 loss: 0.0037 lr: 0.02\n",
      "iteration: 109640 loss: 0.0023 lr: 0.02\n",
      "iteration: 109650 loss: 0.0028 lr: 0.02\n",
      "iteration: 109660 loss: 0.0026 lr: 0.02\n",
      "iteration: 109670 loss: 0.0038 lr: 0.02\n",
      "iteration: 109680 loss: 0.0033 lr: 0.02\n",
      "iteration: 109690 loss: 0.0023 lr: 0.02\n",
      "iteration: 109700 loss: 0.0032 lr: 0.02\n",
      "iteration: 109710 loss: 0.0031 lr: 0.02\n",
      "iteration: 109720 loss: 0.0026 lr: 0.02\n",
      "iteration: 109730 loss: 0.0026 lr: 0.02\n",
      "iteration: 109740 loss: 0.0030 lr: 0.02\n",
      "iteration: 109750 loss: 0.0032 lr: 0.02\n",
      "iteration: 109760 loss: 0.0038 lr: 0.02\n",
      "iteration: 109770 loss: 0.0034 lr: 0.02\n",
      "iteration: 109780 loss: 0.0029 lr: 0.02\n",
      "iteration: 109790 loss: 0.0040 lr: 0.02\n",
      "iteration: 109800 loss: 0.0026 lr: 0.02\n",
      "iteration: 109810 loss: 0.0030 lr: 0.02\n",
      "iteration: 109820 loss: 0.0026 lr: 0.02\n",
      "iteration: 109830 loss: 0.0021 lr: 0.02\n",
      "iteration: 109840 loss: 0.0039 lr: 0.02\n",
      "iteration: 109850 loss: 0.0030 lr: 0.02\n",
      "iteration: 109860 loss: 0.0026 lr: 0.02\n",
      "iteration: 109870 loss: 0.0025 lr: 0.02\n",
      "iteration: 109880 loss: 0.0030 lr: 0.02\n",
      "iteration: 109890 loss: 0.0029 lr: 0.02\n",
      "iteration: 109900 loss: 0.0034 lr: 0.02\n",
      "iteration: 109910 loss: 0.0028 lr: 0.02\n",
      "iteration: 109920 loss: 0.0037 lr: 0.02\n",
      "iteration: 109930 loss: 0.0034 lr: 0.02\n",
      "iteration: 109940 loss: 0.0017 lr: 0.02\n",
      "iteration: 109950 loss: 0.0029 lr: 0.02\n",
      "iteration: 109960 loss: 0.0034 lr: 0.02\n",
      "iteration: 109970 loss: 0.0022 lr: 0.02\n",
      "iteration: 109980 loss: 0.0024 lr: 0.02\n",
      "iteration: 109990 loss: 0.0033 lr: 0.02\n",
      "iteration: 110000 loss: 0.0030 lr: 0.02\n",
      "iteration: 110010 loss: 0.0036 lr: 0.02\n",
      "iteration: 110020 loss: 0.0031 lr: 0.02\n",
      "iteration: 110030 loss: 0.0030 lr: 0.02\n",
      "iteration: 110040 loss: 0.0022 lr: 0.02\n",
      "iteration: 110050 loss: 0.0020 lr: 0.02\n",
      "iteration: 110060 loss: 0.0025 lr: 0.02\n",
      "iteration: 110070 loss: 0.0025 lr: 0.02\n",
      "iteration: 110080 loss: 0.0022 lr: 0.02\n",
      "iteration: 110090 loss: 0.0032 lr: 0.02\n",
      "iteration: 110100 loss: 0.0055 lr: 0.02\n",
      "iteration: 110110 loss: 0.0035 lr: 0.02\n",
      "iteration: 110120 loss: 0.0037 lr: 0.02\n",
      "iteration: 110130 loss: 0.0040 lr: 0.02\n",
      "iteration: 110140 loss: 0.0044 lr: 0.02\n",
      "iteration: 110150 loss: 0.0033 lr: 0.02\n",
      "iteration: 110160 loss: 0.0031 lr: 0.02\n",
      "iteration: 110170 loss: 0.0031 lr: 0.02\n",
      "iteration: 110180 loss: 0.0022 lr: 0.02\n",
      "iteration: 110190 loss: 0.0031 lr: 0.02\n",
      "iteration: 110200 loss: 0.0034 lr: 0.02\n",
      "iteration: 110210 loss: 0.0032 lr: 0.02\n",
      "iteration: 110220 loss: 0.0038 lr: 0.02\n",
      "iteration: 110230 loss: 0.0034 lr: 0.02\n",
      "iteration: 110240 loss: 0.0040 lr: 0.02\n",
      "iteration: 110250 loss: 0.0038 lr: 0.02\n",
      "iteration: 110260 loss: 0.0023 lr: 0.02\n",
      "iteration: 110270 loss: 0.0021 lr: 0.02\n",
      "iteration: 110280 loss: 0.0043 lr: 0.02\n",
      "iteration: 110290 loss: 0.0031 lr: 0.02\n",
      "iteration: 110300 loss: 0.0026 lr: 0.02\n",
      "iteration: 110310 loss: 0.0047 lr: 0.02\n",
      "iteration: 110320 loss: 0.0027 lr: 0.02\n",
      "iteration: 110330 loss: 0.0030 lr: 0.02\n",
      "iteration: 110340 loss: 0.0039 lr: 0.02\n",
      "iteration: 110350 loss: 0.0052 lr: 0.02\n",
      "iteration: 110360 loss: 0.0027 lr: 0.02\n",
      "iteration: 110370 loss: 0.0023 lr: 0.02\n",
      "iteration: 110380 loss: 0.0029 lr: 0.02\n",
      "iteration: 110390 loss: 0.0026 lr: 0.02\n",
      "iteration: 110400 loss: 0.0040 lr: 0.02\n",
      "iteration: 110410 loss: 0.0033 lr: 0.02\n",
      "iteration: 110420 loss: 0.0025 lr: 0.02\n",
      "iteration: 110430 loss: 0.0038 lr: 0.02\n",
      "iteration: 110440 loss: 0.0025 lr: 0.02\n",
      "iteration: 110450 loss: 0.0035 lr: 0.02\n",
      "iteration: 110460 loss: 0.0036 lr: 0.02\n",
      "iteration: 110470 loss: 0.0024 lr: 0.02\n",
      "iteration: 110480 loss: 0.0042 lr: 0.02\n",
      "iteration: 110490 loss: 0.0031 lr: 0.02\n",
      "iteration: 110500 loss: 0.0037 lr: 0.02\n",
      "iteration: 110510 loss: 0.0030 lr: 0.02\n",
      "iteration: 110520 loss: 0.0035 lr: 0.02\n",
      "iteration: 110530 loss: 0.0033 lr: 0.02\n",
      "iteration: 110540 loss: 0.0031 lr: 0.02\n",
      "iteration: 110550 loss: 0.0031 lr: 0.02\n",
      "iteration: 110560 loss: 0.0024 lr: 0.02\n",
      "iteration: 110570 loss: 0.0029 lr: 0.02\n",
      "iteration: 110580 loss: 0.0021 lr: 0.02\n",
      "iteration: 110590 loss: 0.0024 lr: 0.02\n",
      "iteration: 110600 loss: 0.0042 lr: 0.02\n",
      "iteration: 110610 loss: 0.0032 lr: 0.02\n",
      "iteration: 110620 loss: 0.0019 lr: 0.02\n",
      "iteration: 110630 loss: 0.0022 lr: 0.02\n",
      "iteration: 110640 loss: 0.0030 lr: 0.02\n",
      "iteration: 110650 loss: 0.0036 lr: 0.02\n",
      "iteration: 110660 loss: 0.0029 lr: 0.02\n",
      "iteration: 110670 loss: 0.0025 lr: 0.02\n",
      "iteration: 110680 loss: 0.0020 lr: 0.02\n",
      "iteration: 110690 loss: 0.0035 lr: 0.02\n",
      "iteration: 110700 loss: 0.0020 lr: 0.02\n",
      "iteration: 110710 loss: 0.0025 lr: 0.02\n",
      "iteration: 110720 loss: 0.0028 lr: 0.02\n",
      "iteration: 110730 loss: 0.0035 lr: 0.02\n",
      "iteration: 110740 loss: 0.0029 lr: 0.02\n",
      "iteration: 110750 loss: 0.0025 lr: 0.02\n",
      "iteration: 110760 loss: 0.0021 lr: 0.02\n",
      "iteration: 110770 loss: 0.0025 lr: 0.02\n",
      "iteration: 110780 loss: 0.0026 lr: 0.02\n",
      "iteration: 110790 loss: 0.0030 lr: 0.02\n",
      "iteration: 110800 loss: 0.0022 lr: 0.02\n",
      "iteration: 110810 loss: 0.0039 lr: 0.02\n",
      "iteration: 110820 loss: 0.0031 lr: 0.02\n",
      "iteration: 110830 loss: 0.0028 lr: 0.02\n",
      "iteration: 110840 loss: 0.0035 lr: 0.02\n",
      "iteration: 110850 loss: 0.0026 lr: 0.02\n",
      "iteration: 110860 loss: 0.0025 lr: 0.02\n",
      "iteration: 110870 loss: 0.0021 lr: 0.02\n",
      "iteration: 110880 loss: 0.0025 lr: 0.02\n",
      "iteration: 110890 loss: 0.0031 lr: 0.02\n",
      "iteration: 110900 loss: 0.0036 lr: 0.02\n",
      "iteration: 110910 loss: 0.0024 lr: 0.02\n",
      "iteration: 110920 loss: 0.0028 lr: 0.02\n",
      "iteration: 110930 loss: 0.0028 lr: 0.02\n",
      "iteration: 110940 loss: 0.0027 lr: 0.02\n",
      "iteration: 110950 loss: 0.0043 lr: 0.02\n",
      "iteration: 110960 loss: 0.0026 lr: 0.02\n",
      "iteration: 110970 loss: 0.0044 lr: 0.02\n",
      "iteration: 110980 loss: 0.0043 lr: 0.02\n",
      "iteration: 110990 loss: 0.0025 lr: 0.02\n",
      "iteration: 111000 loss: 0.0038 lr: 0.02\n",
      "iteration: 111010 loss: 0.0028 lr: 0.02\n",
      "iteration: 111020 loss: 0.0031 lr: 0.02\n",
      "iteration: 111030 loss: 0.0032 lr: 0.02\n",
      "iteration: 111040 loss: 0.0030 lr: 0.02\n",
      "iteration: 111050 loss: 0.0039 lr: 0.02\n",
      "iteration: 111060 loss: 0.0026 lr: 0.02\n",
      "iteration: 111070 loss: 0.0025 lr: 0.02\n",
      "iteration: 111080 loss: 0.0040 lr: 0.02\n",
      "iteration: 111090 loss: 0.0032 lr: 0.02\n",
      "iteration: 111100 loss: 0.0026 lr: 0.02\n",
      "iteration: 111110 loss: 0.0032 lr: 0.02\n",
      "iteration: 111120 loss: 0.0026 lr: 0.02\n",
      "iteration: 111130 loss: 0.0029 lr: 0.02\n",
      "iteration: 111140 loss: 0.0026 lr: 0.02\n",
      "iteration: 111150 loss: 0.0031 lr: 0.02\n",
      "iteration: 111160 loss: 0.0035 lr: 0.02\n",
      "iteration: 111170 loss: 0.0019 lr: 0.02\n",
      "iteration: 111180 loss: 0.0026 lr: 0.02\n",
      "iteration: 111190 loss: 0.0029 lr: 0.02\n",
      "iteration: 111200 loss: 0.0027 lr: 0.02\n",
      "iteration: 111210 loss: 0.0028 lr: 0.02\n",
      "iteration: 111220 loss: 0.0024 lr: 0.02\n",
      "iteration: 111230 loss: 0.0043 lr: 0.02\n",
      "iteration: 111240 loss: 0.0042 lr: 0.02\n",
      "iteration: 111250 loss: 0.0033 lr: 0.02\n",
      "iteration: 111260 loss: 0.0026 lr: 0.02\n",
      "iteration: 111270 loss: 0.0031 lr: 0.02\n",
      "iteration: 111280 loss: 0.0027 lr: 0.02\n",
      "iteration: 111290 loss: 0.0027 lr: 0.02\n",
      "iteration: 111300 loss: 0.0033 lr: 0.02\n",
      "iteration: 111310 loss: 0.0029 lr: 0.02\n",
      "iteration: 111320 loss: 0.0051 lr: 0.02\n",
      "iteration: 111330 loss: 0.0042 lr: 0.02\n",
      "iteration: 111340 loss: 0.0037 lr: 0.02\n",
      "iteration: 111350 loss: 0.0041 lr: 0.02\n",
      "iteration: 111360 loss: 0.0029 lr: 0.02\n",
      "iteration: 111370 loss: 0.0049 lr: 0.02\n",
      "iteration: 111380 loss: 0.0031 lr: 0.02\n",
      "iteration: 111390 loss: 0.0024 lr: 0.02\n",
      "iteration: 111400 loss: 0.0036 lr: 0.02\n",
      "iteration: 111410 loss: 0.0023 lr: 0.02\n",
      "iteration: 111420 loss: 0.0040 lr: 0.02\n",
      "iteration: 111430 loss: 0.0040 lr: 0.02\n",
      "iteration: 111440 loss: 0.0024 lr: 0.02\n",
      "iteration: 111450 loss: 0.0030 lr: 0.02\n",
      "iteration: 111460 loss: 0.0025 lr: 0.02\n",
      "iteration: 111470 loss: 0.0028 lr: 0.02\n",
      "iteration: 111480 loss: 0.0045 lr: 0.02\n",
      "iteration: 111490 loss: 0.0029 lr: 0.02\n",
      "iteration: 111500 loss: 0.0030 lr: 0.02\n",
      "iteration: 111510 loss: 0.0030 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 111520 loss: 0.0034 lr: 0.02\n",
      "iteration: 111530 loss: 0.0028 lr: 0.02\n",
      "iteration: 111540 loss: 0.0027 lr: 0.02\n",
      "iteration: 111550 loss: 0.0022 lr: 0.02\n",
      "iteration: 111560 loss: 0.0038 lr: 0.02\n",
      "iteration: 111570 loss: 0.0025 lr: 0.02\n",
      "iteration: 111580 loss: 0.0025 lr: 0.02\n",
      "iteration: 111590 loss: 0.0040 lr: 0.02\n",
      "iteration: 111600 loss: 0.0035 lr: 0.02\n",
      "iteration: 111610 loss: 0.0043 lr: 0.02\n",
      "iteration: 111620 loss: 0.0044 lr: 0.02\n",
      "iteration: 111630 loss: 0.0042 lr: 0.02\n",
      "iteration: 111640 loss: 0.0024 lr: 0.02\n",
      "iteration: 111650 loss: 0.0039 lr: 0.02\n",
      "iteration: 111660 loss: 0.0022 lr: 0.02\n",
      "iteration: 111670 loss: 0.0026 lr: 0.02\n",
      "iteration: 111680 loss: 0.0036 lr: 0.02\n",
      "iteration: 111690 loss: 0.0034 lr: 0.02\n",
      "iteration: 111700 loss: 0.0030 lr: 0.02\n",
      "iteration: 111710 loss: 0.0047 lr: 0.02\n",
      "iteration: 111720 loss: 0.0030 lr: 0.02\n",
      "iteration: 111730 loss: 0.0032 lr: 0.02\n",
      "iteration: 111740 loss: 0.0033 lr: 0.02\n",
      "iteration: 111750 loss: 0.0021 lr: 0.02\n",
      "iteration: 111760 loss: 0.0028 lr: 0.02\n",
      "iteration: 111770 loss: 0.0021 lr: 0.02\n",
      "iteration: 111780 loss: 0.0031 lr: 0.02\n",
      "iteration: 111790 loss: 0.0027 lr: 0.02\n",
      "iteration: 111800 loss: 0.0028 lr: 0.02\n",
      "iteration: 111810 loss: 0.0041 lr: 0.02\n",
      "iteration: 111820 loss: 0.0029 lr: 0.02\n",
      "iteration: 111830 loss: 0.0030 lr: 0.02\n",
      "iteration: 111840 loss: 0.0028 lr: 0.02\n",
      "iteration: 111850 loss: 0.0037 lr: 0.02\n",
      "iteration: 111860 loss: 0.0024 lr: 0.02\n",
      "iteration: 111870 loss: 0.0031 lr: 0.02\n",
      "iteration: 111880 loss: 0.0019 lr: 0.02\n",
      "iteration: 111890 loss: 0.0035 lr: 0.02\n",
      "iteration: 111900 loss: 0.0024 lr: 0.02\n",
      "iteration: 111910 loss: 0.0032 lr: 0.02\n",
      "iteration: 111920 loss: 0.0028 lr: 0.02\n",
      "iteration: 111930 loss: 0.0025 lr: 0.02\n",
      "iteration: 111940 loss: 0.0035 lr: 0.02\n",
      "iteration: 111950 loss: 0.0035 lr: 0.02\n",
      "iteration: 111960 loss: 0.0028 lr: 0.02\n",
      "iteration: 111970 loss: 0.0028 lr: 0.02\n",
      "iteration: 111980 loss: 0.0026 lr: 0.02\n",
      "iteration: 111990 loss: 0.0020 lr: 0.02\n",
      "iteration: 112000 loss: 0.0032 lr: 0.02\n",
      "iteration: 112010 loss: 0.0026 lr: 0.02\n",
      "iteration: 112020 loss: 0.0039 lr: 0.02\n",
      "iteration: 112030 loss: 0.0027 lr: 0.02\n",
      "iteration: 112040 loss: 0.0065 lr: 0.02\n",
      "iteration: 112050 loss: 0.0039 lr: 0.02\n",
      "iteration: 112060 loss: 0.0029 lr: 0.02\n",
      "iteration: 112070 loss: 0.0023 lr: 0.02\n",
      "iteration: 112080 loss: 0.0021 lr: 0.02\n",
      "iteration: 112090 loss: 0.0027 lr: 0.02\n",
      "iteration: 112100 loss: 0.0026 lr: 0.02\n",
      "iteration: 112110 loss: 0.0025 lr: 0.02\n",
      "iteration: 112120 loss: 0.0035 lr: 0.02\n",
      "iteration: 112130 loss: 0.0027 lr: 0.02\n",
      "iteration: 112140 loss: 0.0026 lr: 0.02\n",
      "iteration: 112150 loss: 0.0031 lr: 0.02\n",
      "iteration: 112160 loss: 0.0023 lr: 0.02\n",
      "iteration: 112170 loss: 0.0031 lr: 0.02\n",
      "iteration: 112180 loss: 0.0026 lr: 0.02\n",
      "iteration: 112190 loss: 0.0022 lr: 0.02\n",
      "iteration: 112200 loss: 0.0025 lr: 0.02\n",
      "iteration: 112210 loss: 0.0032 lr: 0.02\n",
      "iteration: 112220 loss: 0.0025 lr: 0.02\n",
      "iteration: 112230 loss: 0.0035 lr: 0.02\n",
      "iteration: 112240 loss: 0.0022 lr: 0.02\n",
      "iteration: 112250 loss: 0.0030 lr: 0.02\n",
      "iteration: 112260 loss: 0.0033 lr: 0.02\n",
      "iteration: 112270 loss: 0.0040 lr: 0.02\n",
      "iteration: 112280 loss: 0.0022 lr: 0.02\n",
      "iteration: 112290 loss: 0.0028 lr: 0.02\n",
      "iteration: 112300 loss: 0.0042 lr: 0.02\n",
      "iteration: 112310 loss: 0.0028 lr: 0.02\n",
      "iteration: 112320 loss: 0.0031 lr: 0.02\n",
      "iteration: 112330 loss: 0.0037 lr: 0.02\n",
      "iteration: 112340 loss: 0.0027 lr: 0.02\n",
      "iteration: 112350 loss: 0.0031 lr: 0.02\n",
      "iteration: 112360 loss: 0.0049 lr: 0.02\n",
      "iteration: 112370 loss: 0.0026 lr: 0.02\n",
      "iteration: 112380 loss: 0.0024 lr: 0.02\n",
      "iteration: 112390 loss: 0.0024 lr: 0.02\n",
      "iteration: 112400 loss: 0.0047 lr: 0.02\n",
      "iteration: 112410 loss: 0.0059 lr: 0.02\n",
      "iteration: 112420 loss: 0.0029 lr: 0.02\n",
      "iteration: 112430 loss: 0.0036 lr: 0.02\n",
      "iteration: 112440 loss: 0.0033 lr: 0.02\n",
      "iteration: 112450 loss: 0.0030 lr: 0.02\n",
      "iteration: 112460 loss: 0.0023 lr: 0.02\n",
      "iteration: 112470 loss: 0.0037 lr: 0.02\n",
      "iteration: 112480 loss: 0.0037 lr: 0.02\n",
      "iteration: 112490 loss: 0.0024 lr: 0.02\n",
      "iteration: 112500 loss: 0.0041 lr: 0.02\n",
      "iteration: 112510 loss: 0.0030 lr: 0.02\n",
      "iteration: 112520 loss: 0.0035 lr: 0.02\n",
      "iteration: 112530 loss: 0.0034 lr: 0.02\n",
      "iteration: 112540 loss: 0.0031 lr: 0.02\n",
      "iteration: 112550 loss: 0.0036 lr: 0.02\n",
      "iteration: 112560 loss: 0.0030 lr: 0.02\n",
      "iteration: 112570 loss: 0.0022 lr: 0.02\n",
      "iteration: 112580 loss: 0.0036 lr: 0.02\n",
      "iteration: 112590 loss: 0.0035 lr: 0.02\n",
      "iteration: 112600 loss: 0.0040 lr: 0.02\n",
      "iteration: 112610 loss: 0.0028 lr: 0.02\n",
      "iteration: 112620 loss: 0.0036 lr: 0.02\n",
      "iteration: 112630 loss: 0.0027 lr: 0.02\n",
      "iteration: 112640 loss: 0.0023 lr: 0.02\n",
      "iteration: 112650 loss: 0.0042 lr: 0.02\n",
      "iteration: 112660 loss: 0.0035 lr: 0.02\n",
      "iteration: 112670 loss: 0.0032 lr: 0.02\n",
      "iteration: 112680 loss: 0.0046 lr: 0.02\n",
      "iteration: 112690 loss: 0.0025 lr: 0.02\n",
      "iteration: 112700 loss: 0.0030 lr: 0.02\n",
      "iteration: 112710 loss: 0.0043 lr: 0.02\n",
      "iteration: 112720 loss: 0.0038 lr: 0.02\n",
      "iteration: 112730 loss: 0.0029 lr: 0.02\n",
      "iteration: 112740 loss: 0.0027 lr: 0.02\n",
      "iteration: 112750 loss: 0.0031 lr: 0.02\n",
      "iteration: 112760 loss: 0.0043 lr: 0.02\n",
      "iteration: 112770 loss: 0.0031 lr: 0.02\n",
      "iteration: 112780 loss: 0.0032 lr: 0.02\n",
      "iteration: 112790 loss: 0.0028 lr: 0.02\n",
      "iteration: 112800 loss: 0.0030 lr: 0.02\n",
      "iteration: 112810 loss: 0.0039 lr: 0.02\n",
      "iteration: 112820 loss: 0.0039 lr: 0.02\n",
      "iteration: 112830 loss: 0.0032 lr: 0.02\n",
      "iteration: 112840 loss: 0.0032 lr: 0.02\n",
      "iteration: 112850 loss: 0.0024 lr: 0.02\n",
      "iteration: 112860 loss: 0.0051 lr: 0.02\n",
      "iteration: 112870 loss: 0.0036 lr: 0.02\n",
      "iteration: 112880 loss: 0.0032 lr: 0.02\n",
      "iteration: 112890 loss: 0.0031 lr: 0.02\n",
      "iteration: 112900 loss: 0.0038 lr: 0.02\n",
      "iteration: 112910 loss: 0.0022 lr: 0.02\n",
      "iteration: 112920 loss: 0.0027 lr: 0.02\n",
      "iteration: 112930 loss: 0.0018 lr: 0.02\n",
      "iteration: 112940 loss: 0.0035 lr: 0.02\n",
      "iteration: 112950 loss: 0.0033 lr: 0.02\n",
      "iteration: 112960 loss: 0.0029 lr: 0.02\n",
      "iteration: 112970 loss: 0.0026 lr: 0.02\n",
      "iteration: 112980 loss: 0.0034 lr: 0.02\n",
      "iteration: 112990 loss: 0.0051 lr: 0.02\n",
      "iteration: 113000 loss: 0.0029 lr: 0.02\n",
      "iteration: 113010 loss: 0.0031 lr: 0.02\n",
      "iteration: 113020 loss: 0.0024 lr: 0.02\n",
      "iteration: 113030 loss: 0.0033 lr: 0.02\n",
      "iteration: 113040 loss: 0.0030 lr: 0.02\n",
      "iteration: 113050 loss: 0.0024 lr: 0.02\n",
      "iteration: 113060 loss: 0.0028 lr: 0.02\n",
      "iteration: 113070 loss: 0.0033 lr: 0.02\n",
      "iteration: 113080 loss: 0.0045 lr: 0.02\n",
      "iteration: 113090 loss: 0.0033 lr: 0.02\n",
      "iteration: 113100 loss: 0.0038 lr: 0.02\n",
      "iteration: 113110 loss: 0.0027 lr: 0.02\n",
      "iteration: 113120 loss: 0.0027 lr: 0.02\n",
      "iteration: 113130 loss: 0.0030 lr: 0.02\n",
      "iteration: 113140 loss: 0.0047 lr: 0.02\n",
      "iteration: 113150 loss: 0.0030 lr: 0.02\n",
      "iteration: 113160 loss: 0.0029 lr: 0.02\n",
      "iteration: 113170 loss: 0.0037 lr: 0.02\n",
      "iteration: 113180 loss: 0.0037 lr: 0.02\n",
      "iteration: 113190 loss: 0.0025 lr: 0.02\n",
      "iteration: 113200 loss: 0.0050 lr: 0.02\n",
      "iteration: 113210 loss: 0.0032 lr: 0.02\n",
      "iteration: 113220 loss: 0.0031 lr: 0.02\n",
      "iteration: 113230 loss: 0.0030 lr: 0.02\n",
      "iteration: 113240 loss: 0.0047 lr: 0.02\n",
      "iteration: 113250 loss: 0.0032 lr: 0.02\n",
      "iteration: 113260 loss: 0.0025 lr: 0.02\n",
      "iteration: 113270 loss: 0.0032 lr: 0.02\n",
      "iteration: 113280 loss: 0.0030 lr: 0.02\n",
      "iteration: 113290 loss: 0.0027 lr: 0.02\n",
      "iteration: 113300 loss: 0.0029 lr: 0.02\n",
      "iteration: 113310 loss: 0.0022 lr: 0.02\n",
      "iteration: 113320 loss: 0.0031 lr: 0.02\n",
      "iteration: 113330 loss: 0.0025 lr: 0.02\n",
      "iteration: 113340 loss: 0.0029 lr: 0.02\n",
      "iteration: 113350 loss: 0.0026 lr: 0.02\n",
      "iteration: 113360 loss: 0.0027 lr: 0.02\n",
      "iteration: 113370 loss: 0.0030 lr: 0.02\n",
      "iteration: 113380 loss: 0.0031 lr: 0.02\n",
      "iteration: 113390 loss: 0.0043 lr: 0.02\n",
      "iteration: 113400 loss: 0.0026 lr: 0.02\n",
      "iteration: 113410 loss: 0.0040 lr: 0.02\n",
      "iteration: 113420 loss: 0.0032 lr: 0.02\n",
      "iteration: 113430 loss: 0.0027 lr: 0.02\n",
      "iteration: 113440 loss: 0.0029 lr: 0.02\n",
      "iteration: 113450 loss: 0.0027 lr: 0.02\n",
      "iteration: 113460 loss: 0.0023 lr: 0.02\n",
      "iteration: 113470 loss: 0.0026 lr: 0.02\n",
      "iteration: 113480 loss: 0.0035 lr: 0.02\n",
      "iteration: 113490 loss: 0.0028 lr: 0.02\n",
      "iteration: 113500 loss: 0.0023 lr: 0.02\n",
      "iteration: 113510 loss: 0.0031 lr: 0.02\n",
      "iteration: 113520 loss: 0.0024 lr: 0.02\n",
      "iteration: 113530 loss: 0.0023 lr: 0.02\n",
      "iteration: 113540 loss: 0.0031 lr: 0.02\n",
      "iteration: 113550 loss: 0.0033 lr: 0.02\n",
      "iteration: 113560 loss: 0.0023 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 113570 loss: 0.0023 lr: 0.02\n",
      "iteration: 113580 loss: 0.0035 lr: 0.02\n",
      "iteration: 113590 loss: 0.0023 lr: 0.02\n",
      "iteration: 113600 loss: 0.0036 lr: 0.02\n",
      "iteration: 113610 loss: 0.0044 lr: 0.02\n",
      "iteration: 113620 loss: 0.0046 lr: 0.02\n",
      "iteration: 113630 loss: 0.0034 lr: 0.02\n",
      "iteration: 113640 loss: 0.0036 lr: 0.02\n",
      "iteration: 113650 loss: 0.0038 lr: 0.02\n",
      "iteration: 113660 loss: 0.0027 lr: 0.02\n",
      "iteration: 113670 loss: 0.0032 lr: 0.02\n",
      "iteration: 113680 loss: 0.0026 lr: 0.02\n",
      "iteration: 113690 loss: 0.0036 lr: 0.02\n",
      "iteration: 113700 loss: 0.0038 lr: 0.02\n",
      "iteration: 113710 loss: 0.0022 lr: 0.02\n",
      "iteration: 113720 loss: 0.0022 lr: 0.02\n",
      "iteration: 113730 loss: 0.0026 lr: 0.02\n",
      "iteration: 113740 loss: 0.0036 lr: 0.02\n",
      "iteration: 113750 loss: 0.0037 lr: 0.02\n",
      "iteration: 113760 loss: 0.0025 lr: 0.02\n",
      "iteration: 113770 loss: 0.0052 lr: 0.02\n",
      "iteration: 113780 loss: 0.0032 lr: 0.02\n",
      "iteration: 113790 loss: 0.0039 lr: 0.02\n",
      "iteration: 113800 loss: 0.0026 lr: 0.02\n",
      "iteration: 113810 loss: 0.0027 lr: 0.02\n",
      "iteration: 113820 loss: 0.0029 lr: 0.02\n",
      "iteration: 113830 loss: 0.0026 lr: 0.02\n",
      "iteration: 113840 loss: 0.0029 lr: 0.02\n",
      "iteration: 113850 loss: 0.0034 lr: 0.02\n",
      "iteration: 113860 loss: 0.0023 lr: 0.02\n",
      "iteration: 113870 loss: 0.0028 lr: 0.02\n",
      "iteration: 113880 loss: 0.0025 lr: 0.02\n",
      "iteration: 113890 loss: 0.0029 lr: 0.02\n",
      "iteration: 113900 loss: 0.0023 lr: 0.02\n",
      "iteration: 113910 loss: 0.0037 lr: 0.02\n",
      "iteration: 113920 loss: 0.0026 lr: 0.02\n",
      "iteration: 113930 loss: 0.0026 lr: 0.02\n",
      "iteration: 113940 loss: 0.0029 lr: 0.02\n",
      "iteration: 113950 loss: 0.0022 lr: 0.02\n",
      "iteration: 113960 loss: 0.0035 lr: 0.02\n",
      "iteration: 113970 loss: 0.0029 lr: 0.02\n",
      "iteration: 113980 loss: 0.0039 lr: 0.02\n",
      "iteration: 113990 loss: 0.0033 lr: 0.02\n",
      "iteration: 114000 loss: 0.0030 lr: 0.02\n",
      "iteration: 114010 loss: 0.0045 lr: 0.02\n",
      "iteration: 114020 loss: 0.0041 lr: 0.02\n",
      "iteration: 114030 loss: 0.0031 lr: 0.02\n",
      "iteration: 114040 loss: 0.0026 lr: 0.02\n",
      "iteration: 114050 loss: 0.0020 lr: 0.02\n",
      "iteration: 114060 loss: 0.0033 lr: 0.02\n",
      "iteration: 114070 loss: 0.0018 lr: 0.02\n",
      "iteration: 114080 loss: 0.0024 lr: 0.02\n",
      "iteration: 114090 loss: 0.0032 lr: 0.02\n",
      "iteration: 114100 loss: 0.0026 lr: 0.02\n",
      "iteration: 114110 loss: 0.0034 lr: 0.02\n",
      "iteration: 114120 loss: 0.0046 lr: 0.02\n",
      "iteration: 114130 loss: 0.0024 lr: 0.02\n",
      "iteration: 114140 loss: 0.0044 lr: 0.02\n",
      "iteration: 114150 loss: 0.0024 lr: 0.02\n",
      "iteration: 114160 loss: 0.0024 lr: 0.02\n",
      "iteration: 114170 loss: 0.0027 lr: 0.02\n",
      "iteration: 114180 loss: 0.0057 lr: 0.02\n",
      "iteration: 114190 loss: 0.0032 lr: 0.02\n",
      "iteration: 114200 loss: 0.0077 lr: 0.02\n",
      "iteration: 114210 loss: 0.0033 lr: 0.02\n",
      "iteration: 114220 loss: 0.0029 lr: 0.02\n",
      "iteration: 114230 loss: 0.0025 lr: 0.02\n",
      "iteration: 114240 loss: 0.0021 lr: 0.02\n",
      "iteration: 114250 loss: 0.0026 lr: 0.02\n",
      "iteration: 114260 loss: 0.0024 lr: 0.02\n",
      "iteration: 114270 loss: 0.0025 lr: 0.02\n",
      "iteration: 114280 loss: 0.0033 lr: 0.02\n",
      "iteration: 114290 loss: 0.0033 lr: 0.02\n",
      "iteration: 114300 loss: 0.0028 lr: 0.02\n",
      "iteration: 114310 loss: 0.0030 lr: 0.02\n",
      "iteration: 114320 loss: 0.0041 lr: 0.02\n",
      "iteration: 114330 loss: 0.0028 lr: 0.02\n",
      "iteration: 114340 loss: 0.0025 lr: 0.02\n",
      "iteration: 114350 loss: 0.0036 lr: 0.02\n",
      "iteration: 114360 loss: 0.0038 lr: 0.02\n",
      "iteration: 114370 loss: 0.0026 lr: 0.02\n",
      "iteration: 114380 loss: 0.0032 lr: 0.02\n",
      "iteration: 114390 loss: 0.0040 lr: 0.02\n",
      "iteration: 114400 loss: 0.0039 lr: 0.02\n",
      "iteration: 114410 loss: 0.0029 lr: 0.02\n",
      "iteration: 114420 loss: 0.0030 lr: 0.02\n",
      "iteration: 114430 loss: 0.0036 lr: 0.02\n",
      "iteration: 114440 loss: 0.0028 lr: 0.02\n",
      "iteration: 114450 loss: 0.0025 lr: 0.02\n",
      "iteration: 114460 loss: 0.0035 lr: 0.02\n",
      "iteration: 114470 loss: 0.0034 lr: 0.02\n",
      "iteration: 114480 loss: 0.0036 lr: 0.02\n",
      "iteration: 114490 loss: 0.0027 lr: 0.02\n",
      "iteration: 114500 loss: 0.0023 lr: 0.02\n",
      "iteration: 114510 loss: 0.0022 lr: 0.02\n",
      "iteration: 114520 loss: 0.0038 lr: 0.02\n",
      "iteration: 114530 loss: 0.0034 lr: 0.02\n",
      "iteration: 114540 loss: 0.0029 lr: 0.02\n",
      "iteration: 114550 loss: 0.0032 lr: 0.02\n",
      "iteration: 114560 loss: 0.0041 lr: 0.02\n",
      "iteration: 114570 loss: 0.0039 lr: 0.02\n",
      "iteration: 114580 loss: 0.0035 lr: 0.02\n",
      "iteration: 114590 loss: 0.0045 lr: 0.02\n",
      "iteration: 114600 loss: 0.0022 lr: 0.02\n",
      "iteration: 114610 loss: 0.0032 lr: 0.02\n",
      "iteration: 114620 loss: 0.0030 lr: 0.02\n",
      "iteration: 114630 loss: 0.0044 lr: 0.02\n",
      "iteration: 114640 loss: 0.0027 lr: 0.02\n",
      "iteration: 114650 loss: 0.0026 lr: 0.02\n",
      "iteration: 114660 loss: 0.0026 lr: 0.02\n",
      "iteration: 114670 loss: 0.0032 lr: 0.02\n",
      "iteration: 114680 loss: 0.0034 lr: 0.02\n",
      "iteration: 114690 loss: 0.0040 lr: 0.02\n",
      "iteration: 114700 loss: 0.0026 lr: 0.02\n",
      "iteration: 114710 loss: 0.0037 lr: 0.02\n",
      "iteration: 114720 loss: 0.0024 lr: 0.02\n",
      "iteration: 114730 loss: 0.0031 lr: 0.02\n",
      "iteration: 114740 loss: 0.0031 lr: 0.02\n",
      "iteration: 114750 loss: 0.0033 lr: 0.02\n",
      "iteration: 114760 loss: 0.0035 lr: 0.02\n",
      "iteration: 114770 loss: 0.0026 lr: 0.02\n",
      "iteration: 114780 loss: 0.0037 lr: 0.02\n",
      "iteration: 114790 loss: 0.0023 lr: 0.02\n",
      "iteration: 114800 loss: 0.0033 lr: 0.02\n",
      "iteration: 114810 loss: 0.0031 lr: 0.02\n",
      "iteration: 114820 loss: 0.0052 lr: 0.02\n",
      "iteration: 114830 loss: 0.0021 lr: 0.02\n",
      "iteration: 114840 loss: 0.0021 lr: 0.02\n",
      "iteration: 114850 loss: 0.0033 lr: 0.02\n",
      "iteration: 114860 loss: 0.0030 lr: 0.02\n",
      "iteration: 114870 loss: 0.0032 lr: 0.02\n",
      "iteration: 114880 loss: 0.0029 lr: 0.02\n",
      "iteration: 114890 loss: 0.0023 lr: 0.02\n",
      "iteration: 114900 loss: 0.0027 lr: 0.02\n",
      "iteration: 114910 loss: 0.0029 lr: 0.02\n",
      "iteration: 114920 loss: 0.0037 lr: 0.02\n",
      "iteration: 114930 loss: 0.0026 lr: 0.02\n",
      "iteration: 114940 loss: 0.0041 lr: 0.02\n",
      "iteration: 114950 loss: 0.0040 lr: 0.02\n",
      "iteration: 114960 loss: 0.0025 lr: 0.02\n",
      "iteration: 114970 loss: 0.0031 lr: 0.02\n",
      "iteration: 114980 loss: 0.0028 lr: 0.02\n",
      "iteration: 114990 loss: 0.0046 lr: 0.02\n",
      "iteration: 115000 loss: 0.0027 lr: 0.02\n",
      "iteration: 115010 loss: 0.0039 lr: 0.02\n",
      "iteration: 115020 loss: 0.0028 lr: 0.02\n",
      "iteration: 115030 loss: 0.0032 lr: 0.02\n",
      "iteration: 115040 loss: 0.0033 lr: 0.02\n",
      "iteration: 115050 loss: 0.0047 lr: 0.02\n",
      "iteration: 115060 loss: 0.0032 lr: 0.02\n",
      "iteration: 115070 loss: 0.0026 lr: 0.02\n",
      "iteration: 115080 loss: 0.0027 lr: 0.02\n",
      "iteration: 115090 loss: 0.0025 lr: 0.02\n",
      "iteration: 115100 loss: 0.0027 lr: 0.02\n",
      "iteration: 115110 loss: 0.0026 lr: 0.02\n",
      "iteration: 115120 loss: 0.0023 lr: 0.02\n",
      "iteration: 115130 loss: 0.0032 lr: 0.02\n",
      "iteration: 115140 loss: 0.0025 lr: 0.02\n",
      "iteration: 115150 loss: 0.0027 lr: 0.02\n",
      "iteration: 115160 loss: 0.0031 lr: 0.02\n",
      "iteration: 115170 loss: 0.0027 lr: 0.02\n",
      "iteration: 115180 loss: 0.0035 lr: 0.02\n",
      "iteration: 115190 loss: 0.0029 lr: 0.02\n",
      "iteration: 115200 loss: 0.0029 lr: 0.02\n",
      "iteration: 115210 loss: 0.0029 lr: 0.02\n",
      "iteration: 115220 loss: 0.0035 lr: 0.02\n",
      "iteration: 115230 loss: 0.0030 lr: 0.02\n",
      "iteration: 115240 loss: 0.0027 lr: 0.02\n",
      "iteration: 115250 loss: 0.0025 lr: 0.02\n",
      "iteration: 115260 loss: 0.0028 lr: 0.02\n",
      "iteration: 115270 loss: 0.0026 lr: 0.02\n",
      "iteration: 115280 loss: 0.0041 lr: 0.02\n",
      "iteration: 115290 loss: 0.0026 lr: 0.02\n",
      "iteration: 115300 loss: 0.0032 lr: 0.02\n",
      "iteration: 115310 loss: 0.0031 lr: 0.02\n",
      "iteration: 115320 loss: 0.0044 lr: 0.02\n",
      "iteration: 115330 loss: 0.0039 lr: 0.02\n",
      "iteration: 115340 loss: 0.0028 lr: 0.02\n",
      "iteration: 115350 loss: 0.0029 lr: 0.02\n",
      "iteration: 115360 loss: 0.0022 lr: 0.02\n",
      "iteration: 115370 loss: 0.0026 lr: 0.02\n",
      "iteration: 115380 loss: 0.0025 lr: 0.02\n",
      "iteration: 115390 loss: 0.0029 lr: 0.02\n",
      "iteration: 115400 loss: 0.0028 lr: 0.02\n",
      "iteration: 115410 loss: 0.0038 lr: 0.02\n",
      "iteration: 115420 loss: 0.0019 lr: 0.02\n",
      "iteration: 115430 loss: 0.0028 lr: 0.02\n",
      "iteration: 115440 loss: 0.0023 lr: 0.02\n",
      "iteration: 115450 loss: 0.0024 lr: 0.02\n",
      "iteration: 115460 loss: 0.0023 lr: 0.02\n",
      "iteration: 115470 loss: 0.0031 lr: 0.02\n",
      "iteration: 115480 loss: 0.0037 lr: 0.02\n",
      "iteration: 115490 loss: 0.0035 lr: 0.02\n",
      "iteration: 115500 loss: 0.0023 lr: 0.02\n",
      "iteration: 115510 loss: 0.0035 lr: 0.02\n",
      "iteration: 115520 loss: 0.0029 lr: 0.02\n",
      "iteration: 115530 loss: 0.0029 lr: 0.02\n",
      "iteration: 115540 loss: 0.0032 lr: 0.02\n",
      "iteration: 115550 loss: 0.0033 lr: 0.02\n",
      "iteration: 115560 loss: 0.0037 lr: 0.02\n",
      "iteration: 115570 loss: 0.0025 lr: 0.02\n",
      "iteration: 115580 loss: 0.0029 lr: 0.02\n",
      "iteration: 115590 loss: 0.0039 lr: 0.02\n",
      "iteration: 115600 loss: 0.0024 lr: 0.02\n",
      "iteration: 115610 loss: 0.0023 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 115620 loss: 0.0029 lr: 0.02\n",
      "iteration: 115630 loss: 0.0032 lr: 0.02\n",
      "iteration: 115640 loss: 0.0033 lr: 0.02\n",
      "iteration: 115650 loss: 0.0027 lr: 0.02\n",
      "iteration: 115660 loss: 0.0040 lr: 0.02\n",
      "iteration: 115670 loss: 0.0037 lr: 0.02\n",
      "iteration: 115680 loss: 0.0038 lr: 0.02\n",
      "iteration: 115690 loss: 0.0028 lr: 0.02\n",
      "iteration: 115700 loss: 0.0031 lr: 0.02\n",
      "iteration: 115710 loss: 0.0036 lr: 0.02\n",
      "iteration: 115720 loss: 0.0030 lr: 0.02\n",
      "iteration: 115730 loss: 0.0028 lr: 0.02\n",
      "iteration: 115740 loss: 0.0028 lr: 0.02\n",
      "iteration: 115750 loss: 0.0029 lr: 0.02\n",
      "iteration: 115760 loss: 0.0034 lr: 0.02\n",
      "iteration: 115770 loss: 0.0036 lr: 0.02\n",
      "iteration: 115780 loss: 0.0028 lr: 0.02\n",
      "iteration: 115790 loss: 0.0022 lr: 0.02\n",
      "iteration: 115800 loss: 0.0027 lr: 0.02\n",
      "iteration: 115810 loss: 0.0034 lr: 0.02\n",
      "iteration: 115820 loss: 0.0045 lr: 0.02\n",
      "iteration: 115830 loss: 0.0034 lr: 0.02\n",
      "iteration: 115840 loss: 0.0032 lr: 0.02\n",
      "iteration: 115850 loss: 0.0029 lr: 0.02\n",
      "iteration: 115860 loss: 0.0033 lr: 0.02\n",
      "iteration: 115870 loss: 0.0024 lr: 0.02\n",
      "iteration: 115880 loss: 0.0027 lr: 0.02\n",
      "iteration: 115890 loss: 0.0037 lr: 0.02\n",
      "iteration: 115900 loss: 0.0024 lr: 0.02\n",
      "iteration: 115910 loss: 0.0027 lr: 0.02\n",
      "iteration: 115920 loss: 0.0023 lr: 0.02\n",
      "iteration: 115930 loss: 0.0036 lr: 0.02\n",
      "iteration: 115940 loss: 0.0025 lr: 0.02\n",
      "iteration: 115950 loss: 0.0026 lr: 0.02\n",
      "iteration: 115960 loss: 0.0027 lr: 0.02\n",
      "iteration: 115970 loss: 0.0028 lr: 0.02\n",
      "iteration: 115980 loss: 0.0035 lr: 0.02\n",
      "iteration: 115990 loss: 0.0044 lr: 0.02\n",
      "iteration: 116000 loss: 0.0039 lr: 0.02\n",
      "iteration: 116010 loss: 0.0072 lr: 0.02\n",
      "iteration: 116020 loss: 0.0029 lr: 0.02\n",
      "iteration: 116030 loss: 0.0029 lr: 0.02\n",
      "iteration: 116040 loss: 0.0035 lr: 0.02\n",
      "iteration: 116050 loss: 0.0038 lr: 0.02\n",
      "iteration: 116060 loss: 0.0032 lr: 0.02\n",
      "iteration: 116070 loss: 0.0025 lr: 0.02\n",
      "iteration: 116080 loss: 0.0026 lr: 0.02\n",
      "iteration: 116090 loss: 0.0022 lr: 0.02\n",
      "iteration: 116100 loss: 0.0028 lr: 0.02\n",
      "iteration: 116110 loss: 0.0050 lr: 0.02\n",
      "iteration: 116120 loss: 0.0039 lr: 0.02\n",
      "iteration: 116130 loss: 0.0050 lr: 0.02\n",
      "iteration: 116140 loss: 0.0029 lr: 0.02\n",
      "iteration: 116150 loss: 0.0037 lr: 0.02\n",
      "iteration: 116160 loss: 0.0030 lr: 0.02\n",
      "iteration: 116170 loss: 0.0029 lr: 0.02\n",
      "iteration: 116180 loss: 0.0036 lr: 0.02\n",
      "iteration: 116190 loss: 0.0027 lr: 0.02\n",
      "iteration: 116200 loss: 0.0029 lr: 0.02\n",
      "iteration: 116210 loss: 0.0027 lr: 0.02\n",
      "iteration: 116220 loss: 0.0031 lr: 0.02\n",
      "iteration: 116230 loss: 0.0026 lr: 0.02\n",
      "iteration: 116240 loss: 0.0032 lr: 0.02\n",
      "iteration: 116250 loss: 0.0023 lr: 0.02\n",
      "iteration: 116260 loss: 0.0037 lr: 0.02\n",
      "iteration: 116270 loss: 0.0023 lr: 0.02\n",
      "iteration: 116280 loss: 0.0026 lr: 0.02\n",
      "iteration: 116290 loss: 0.0029 lr: 0.02\n",
      "iteration: 116300 loss: 0.0026 lr: 0.02\n",
      "iteration: 116310 loss: 0.0027 lr: 0.02\n",
      "iteration: 116320 loss: 0.0030 lr: 0.02\n",
      "iteration: 116330 loss: 0.0025 lr: 0.02\n",
      "iteration: 116340 loss: 0.0037 lr: 0.02\n",
      "iteration: 116350 loss: 0.0035 lr: 0.02\n",
      "iteration: 116360 loss: 0.0040 lr: 0.02\n",
      "iteration: 116370 loss: 0.0038 lr: 0.02\n",
      "iteration: 116380 loss: 0.0050 lr: 0.02\n",
      "iteration: 116390 loss: 0.0040 lr: 0.02\n",
      "iteration: 116400 loss: 0.0027 lr: 0.02\n",
      "iteration: 116410 loss: 0.0022 lr: 0.02\n",
      "iteration: 116420 loss: 0.0038 lr: 0.02\n",
      "iteration: 116430 loss: 0.0031 lr: 0.02\n",
      "iteration: 116440 loss: 0.0053 lr: 0.02\n",
      "iteration: 116450 loss: 0.0043 lr: 0.02\n",
      "iteration: 116460 loss: 0.0060 lr: 0.02\n",
      "iteration: 116470 loss: 0.0045 lr: 0.02\n",
      "iteration: 116480 loss: 0.0032 lr: 0.02\n",
      "iteration: 116490 loss: 0.0036 lr: 0.02\n",
      "iteration: 116500 loss: 0.0033 lr: 0.02\n",
      "iteration: 116510 loss: 0.0028 lr: 0.02\n",
      "iteration: 116520 loss: 0.0025 lr: 0.02\n",
      "iteration: 116530 loss: 0.0023 lr: 0.02\n",
      "iteration: 116540 loss: 0.0025 lr: 0.02\n",
      "iteration: 116550 loss: 0.0037 lr: 0.02\n",
      "iteration: 116560 loss: 0.0031 lr: 0.02\n",
      "iteration: 116570 loss: 0.0035 lr: 0.02\n",
      "iteration: 116580 loss: 0.0027 lr: 0.02\n",
      "iteration: 116590 loss: 0.0030 lr: 0.02\n",
      "iteration: 116600 loss: 0.0024 lr: 0.02\n",
      "iteration: 116610 loss: 0.0033 lr: 0.02\n",
      "iteration: 116620 loss: 0.0027 lr: 0.02\n",
      "iteration: 116630 loss: 0.0063 lr: 0.02\n",
      "iteration: 116640 loss: 0.0034 lr: 0.02\n",
      "iteration: 116650 loss: 0.0027 lr: 0.02\n",
      "iteration: 116660 loss: 0.0026 lr: 0.02\n",
      "iteration: 116670 loss: 0.0023 lr: 0.02\n",
      "iteration: 116680 loss: 0.0036 lr: 0.02\n",
      "iteration: 116690 loss: 0.0032 lr: 0.02\n",
      "iteration: 116700 loss: 0.0036 lr: 0.02\n",
      "iteration: 116710 loss: 0.0034 lr: 0.02\n",
      "iteration: 116720 loss: 0.0028 lr: 0.02\n",
      "iteration: 116730 loss: 0.0031 lr: 0.02\n",
      "iteration: 116740 loss: 0.0027 lr: 0.02\n",
      "iteration: 116750 loss: 0.0040 lr: 0.02\n",
      "iteration: 116760 loss: 0.0033 lr: 0.02\n",
      "iteration: 116770 loss: 0.0033 lr: 0.02\n",
      "iteration: 116780 loss: 0.0029 lr: 0.02\n",
      "iteration: 116790 loss: 0.0028 lr: 0.02\n",
      "iteration: 116800 loss: 0.0032 lr: 0.02\n",
      "iteration: 116810 loss: 0.0037 lr: 0.02\n",
      "iteration: 116820 loss: 0.0025 lr: 0.02\n",
      "iteration: 116830 loss: 0.0027 lr: 0.02\n",
      "iteration: 116840 loss: 0.0021 lr: 0.02\n",
      "iteration: 116850 loss: 0.0033 lr: 0.02\n",
      "iteration: 116860 loss: 0.0042 lr: 0.02\n",
      "iteration: 116870 loss: 0.0029 lr: 0.02\n",
      "iteration: 116880 loss: 0.0029 lr: 0.02\n",
      "iteration: 116890 loss: 0.0033 lr: 0.02\n",
      "iteration: 116900 loss: 0.0038 lr: 0.02\n",
      "iteration: 116910 loss: 0.0037 lr: 0.02\n",
      "iteration: 116920 loss: 0.0024 lr: 0.02\n",
      "iteration: 116930 loss: 0.0024 lr: 0.02\n",
      "iteration: 116940 loss: 0.0033 lr: 0.02\n",
      "iteration: 116950 loss: 0.0048 lr: 0.02\n",
      "iteration: 116960 loss: 0.0023 lr: 0.02\n",
      "iteration: 116970 loss: 0.0038 lr: 0.02\n",
      "iteration: 116980 loss: 0.0038 lr: 0.02\n",
      "iteration: 116990 loss: 0.0025 lr: 0.02\n",
      "iteration: 117000 loss: 0.0040 lr: 0.02\n",
      "iteration: 117010 loss: 0.0038 lr: 0.02\n",
      "iteration: 117020 loss: 0.0036 lr: 0.02\n",
      "iteration: 117030 loss: 0.0045 lr: 0.02\n",
      "iteration: 117040 loss: 0.0033 lr: 0.02\n",
      "iteration: 117050 loss: 0.0026 lr: 0.02\n",
      "iteration: 117060 loss: 0.0042 lr: 0.02\n",
      "iteration: 117070 loss: 0.0022 lr: 0.02\n",
      "iteration: 117080 loss: 0.0030 lr: 0.02\n",
      "iteration: 117090 loss: 0.0017 lr: 0.02\n",
      "iteration: 117100 loss: 0.0027 lr: 0.02\n",
      "iteration: 117110 loss: 0.0031 lr: 0.02\n",
      "iteration: 117120 loss: 0.0020 lr: 0.02\n",
      "iteration: 117130 loss: 0.0041 lr: 0.02\n",
      "iteration: 117140 loss: 0.0044 lr: 0.02\n",
      "iteration: 117150 loss: 0.0020 lr: 0.02\n",
      "iteration: 117160 loss: 0.0034 lr: 0.02\n",
      "iteration: 117170 loss: 0.0023 lr: 0.02\n",
      "iteration: 117180 loss: 0.0031 lr: 0.02\n",
      "iteration: 117190 loss: 0.0030 lr: 0.02\n",
      "iteration: 117200 loss: 0.0043 lr: 0.02\n",
      "iteration: 117210 loss: 0.0032 lr: 0.02\n",
      "iteration: 117220 loss: 0.0024 lr: 0.02\n",
      "iteration: 117230 loss: 0.0022 lr: 0.02\n",
      "iteration: 117240 loss: 0.0047 lr: 0.02\n",
      "iteration: 117250 loss: 0.0026 lr: 0.02\n",
      "iteration: 117260 loss: 0.0046 lr: 0.02\n",
      "iteration: 117270 loss: 0.0029 lr: 0.02\n",
      "iteration: 117280 loss: 0.0031 lr: 0.02\n",
      "iteration: 117290 loss: 0.0027 lr: 0.02\n",
      "iteration: 117300 loss: 0.0034 lr: 0.02\n",
      "iteration: 117310 loss: 0.0027 lr: 0.02\n",
      "iteration: 117320 loss: 0.0024 lr: 0.02\n",
      "iteration: 117330 loss: 0.0029 lr: 0.02\n",
      "iteration: 117340 loss: 0.0027 lr: 0.02\n",
      "iteration: 117350 loss: 0.0032 lr: 0.02\n",
      "iteration: 117360 loss: 0.0031 lr: 0.02\n",
      "iteration: 117370 loss: 0.0028 lr: 0.02\n",
      "iteration: 117380 loss: 0.0036 lr: 0.02\n",
      "iteration: 117390 loss: 0.0025 lr: 0.02\n",
      "iteration: 117400 loss: 0.0033 lr: 0.02\n",
      "iteration: 117410 loss: 0.0035 lr: 0.02\n",
      "iteration: 117420 loss: 0.0025 lr: 0.02\n",
      "iteration: 117430 loss: 0.0034 lr: 0.02\n",
      "iteration: 117440 loss: 0.0027 lr: 0.02\n",
      "iteration: 117450 loss: 0.0043 lr: 0.02\n",
      "iteration: 117460 loss: 0.0050 lr: 0.02\n",
      "iteration: 117470 loss: 0.0023 lr: 0.02\n",
      "iteration: 117480 loss: 0.0034 lr: 0.02\n",
      "iteration: 117490 loss: 0.0027 lr: 0.02\n",
      "iteration: 117500 loss: 0.0033 lr: 0.02\n",
      "iteration: 117510 loss: 0.0041 lr: 0.02\n",
      "iteration: 117520 loss: 0.0033 lr: 0.02\n",
      "iteration: 117530 loss: 0.0025 lr: 0.02\n",
      "iteration: 117540 loss: 0.0036 lr: 0.02\n",
      "iteration: 117550 loss: 0.0047 lr: 0.02\n",
      "iteration: 117560 loss: 0.0027 lr: 0.02\n",
      "iteration: 117570 loss: 0.0030 lr: 0.02\n",
      "iteration: 117580 loss: 0.0034 lr: 0.02\n",
      "iteration: 117590 loss: 0.0027 lr: 0.02\n",
      "iteration: 117600 loss: 0.0034 lr: 0.02\n",
      "iteration: 117610 loss: 0.0054 lr: 0.02\n",
      "iteration: 117620 loss: 0.0038 lr: 0.02\n",
      "iteration: 117630 loss: 0.0033 lr: 0.02\n",
      "iteration: 117640 loss: 0.0031 lr: 0.02\n",
      "iteration: 117650 loss: 0.0035 lr: 0.02\n",
      "iteration: 117660 loss: 0.0025 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 117670 loss: 0.0027 lr: 0.02\n",
      "iteration: 117680 loss: 0.0041 lr: 0.02\n",
      "iteration: 117690 loss: 0.0031 lr: 0.02\n",
      "iteration: 117700 loss: 0.0033 lr: 0.02\n",
      "iteration: 117710 loss: 0.0030 lr: 0.02\n",
      "iteration: 117720 loss: 0.0042 lr: 0.02\n",
      "iteration: 117730 loss: 0.0060 lr: 0.02\n",
      "iteration: 117740 loss: 0.0041 lr: 0.02\n",
      "iteration: 117750 loss: 0.0026 lr: 0.02\n",
      "iteration: 117760 loss: 0.0030 lr: 0.02\n",
      "iteration: 117770 loss: 0.0023 lr: 0.02\n",
      "iteration: 117780 loss: 0.0031 lr: 0.02\n",
      "iteration: 117790 loss: 0.0022 lr: 0.02\n",
      "iteration: 117800 loss: 0.0021 lr: 0.02\n",
      "iteration: 117810 loss: 0.0030 lr: 0.02\n",
      "iteration: 117820 loss: 0.0039 lr: 0.02\n",
      "iteration: 117830 loss: 0.0042 lr: 0.02\n",
      "iteration: 117840 loss: 0.0033 lr: 0.02\n",
      "iteration: 117850 loss: 0.0027 lr: 0.02\n",
      "iteration: 117860 loss: 0.0030 lr: 0.02\n",
      "iteration: 117870 loss: 0.0023 lr: 0.02\n",
      "iteration: 117880 loss: 0.0027 lr: 0.02\n",
      "iteration: 117890 loss: 0.0039 lr: 0.02\n",
      "iteration: 117900 loss: 0.0028 lr: 0.02\n",
      "iteration: 117910 loss: 0.0023 lr: 0.02\n",
      "iteration: 117920 loss: 0.0027 lr: 0.02\n",
      "iteration: 117930 loss: 0.0025 lr: 0.02\n",
      "iteration: 117940 loss: 0.0028 lr: 0.02\n",
      "iteration: 117950 loss: 0.0021 lr: 0.02\n",
      "iteration: 117960 loss: 0.0033 lr: 0.02\n",
      "iteration: 117970 loss: 0.0019 lr: 0.02\n",
      "iteration: 117980 loss: 0.0026 lr: 0.02\n",
      "iteration: 117990 loss: 0.0024 lr: 0.02\n",
      "iteration: 118000 loss: 0.0025 lr: 0.02\n",
      "iteration: 118010 loss: 0.0033 lr: 0.02\n",
      "iteration: 118020 loss: 0.0035 lr: 0.02\n",
      "iteration: 118030 loss: 0.0040 lr: 0.02\n",
      "iteration: 118040 loss: 0.0022 lr: 0.02\n",
      "iteration: 118050 loss: 0.0039 lr: 0.02\n",
      "iteration: 118060 loss: 0.0036 lr: 0.02\n",
      "iteration: 118070 loss: 0.0023 lr: 0.02\n",
      "iteration: 118080 loss: 0.0026 lr: 0.02\n",
      "iteration: 118090 loss: 0.0037 lr: 0.02\n",
      "iteration: 118100 loss: 0.0027 lr: 0.02\n",
      "iteration: 118110 loss: 0.0026 lr: 0.02\n",
      "iteration: 118120 loss: 0.0021 lr: 0.02\n",
      "iteration: 118130 loss: 0.0026 lr: 0.02\n",
      "iteration: 118140 loss: 0.0024 lr: 0.02\n",
      "iteration: 118150 loss: 0.0027 lr: 0.02\n",
      "iteration: 118160 loss: 0.0030 lr: 0.02\n",
      "iteration: 118170 loss: 0.0032 lr: 0.02\n",
      "iteration: 118180 loss: 0.0028 lr: 0.02\n",
      "iteration: 118190 loss: 0.0023 lr: 0.02\n",
      "iteration: 118200 loss: 0.0034 lr: 0.02\n",
      "iteration: 118210 loss: 0.0032 lr: 0.02\n",
      "iteration: 118220 loss: 0.0039 lr: 0.02\n",
      "iteration: 118230 loss: 0.0030 lr: 0.02\n",
      "iteration: 118240 loss: 0.0027 lr: 0.02\n",
      "iteration: 118250 loss: 0.0027 lr: 0.02\n",
      "iteration: 118260 loss: 0.0021 lr: 0.02\n",
      "iteration: 118270 loss: 0.0026 lr: 0.02\n",
      "iteration: 118280 loss: 0.0017 lr: 0.02\n",
      "iteration: 118290 loss: 0.0023 lr: 0.02\n",
      "iteration: 118300 loss: 0.0036 lr: 0.02\n",
      "iteration: 118310 loss: 0.0022 lr: 0.02\n",
      "iteration: 118320 loss: 0.0031 lr: 0.02\n",
      "iteration: 118330 loss: 0.0025 lr: 0.02\n",
      "iteration: 118340 loss: 0.0028 lr: 0.02\n",
      "iteration: 118350 loss: 0.0022 lr: 0.02\n",
      "iteration: 118360 loss: 0.0032 lr: 0.02\n",
      "iteration: 118370 loss: 0.0032 lr: 0.02\n",
      "iteration: 118380 loss: 0.0041 lr: 0.02\n",
      "iteration: 118390 loss: 0.0034 lr: 0.02\n",
      "iteration: 118400 loss: 0.0028 lr: 0.02\n",
      "iteration: 118410 loss: 0.0037 lr: 0.02\n",
      "iteration: 118420 loss: 0.0029 lr: 0.02\n",
      "iteration: 118430 loss: 0.0034 lr: 0.02\n",
      "iteration: 118440 loss: 0.0029 lr: 0.02\n",
      "iteration: 118450 loss: 0.0029 lr: 0.02\n",
      "iteration: 118460 loss: 0.0022 lr: 0.02\n",
      "iteration: 118470 loss: 0.0034 lr: 0.02\n",
      "iteration: 118480 loss: 0.0038 lr: 0.02\n",
      "iteration: 118490 loss: 0.0022 lr: 0.02\n",
      "iteration: 118500 loss: 0.0020 lr: 0.02\n",
      "iteration: 118510 loss: 0.0035 lr: 0.02\n",
      "iteration: 118520 loss: 0.0037 lr: 0.02\n",
      "iteration: 118530 loss: 0.0022 lr: 0.02\n",
      "iteration: 118540 loss: 0.0025 lr: 0.02\n",
      "iteration: 118550 loss: 0.0027 lr: 0.02\n",
      "iteration: 118560 loss: 0.0034 lr: 0.02\n",
      "iteration: 118570 loss: 0.0028 lr: 0.02\n",
      "iteration: 118580 loss: 0.0033 lr: 0.02\n",
      "iteration: 118590 loss: 0.0025 lr: 0.02\n",
      "iteration: 118600 loss: 0.0027 lr: 0.02\n",
      "iteration: 118610 loss: 0.0035 lr: 0.02\n",
      "iteration: 118620 loss: 0.0024 lr: 0.02\n",
      "iteration: 118630 loss: 0.0030 lr: 0.02\n",
      "iteration: 118640 loss: 0.0028 lr: 0.02\n",
      "iteration: 118650 loss: 0.0031 lr: 0.02\n",
      "iteration: 118660 loss: 0.0034 lr: 0.02\n",
      "iteration: 118670 loss: 0.0045 lr: 0.02\n",
      "iteration: 118680 loss: 0.0022 lr: 0.02\n",
      "iteration: 118690 loss: 0.0026 lr: 0.02\n",
      "iteration: 118700 loss: 0.0034 lr: 0.02\n",
      "iteration: 118710 loss: 0.0026 lr: 0.02\n",
      "iteration: 118720 loss: 0.0027 lr: 0.02\n",
      "iteration: 118730 loss: 0.0033 lr: 0.02\n",
      "iteration: 118740 loss: 0.0029 lr: 0.02\n",
      "iteration: 118750 loss: 0.0025 lr: 0.02\n",
      "iteration: 118760 loss: 0.0035 lr: 0.02\n",
      "iteration: 118770 loss: 0.0023 lr: 0.02\n",
      "iteration: 118780 loss: 0.0032 lr: 0.02\n",
      "iteration: 118790 loss: 0.0028 lr: 0.02\n",
      "iteration: 118800 loss: 0.0032 lr: 0.02\n",
      "iteration: 118810 loss: 0.0026 lr: 0.02\n",
      "iteration: 118820 loss: 0.0037 lr: 0.02\n",
      "iteration: 118830 loss: 0.0035 lr: 0.02\n",
      "iteration: 118840 loss: 0.0032 lr: 0.02\n",
      "iteration: 118850 loss: 0.0027 lr: 0.02\n",
      "iteration: 118860 loss: 0.0026 lr: 0.02\n",
      "iteration: 118870 loss: 0.0023 lr: 0.02\n",
      "iteration: 118880 loss: 0.0035 lr: 0.02\n",
      "iteration: 118890 loss: 0.0031 lr: 0.02\n",
      "iteration: 118900 loss: 0.0042 lr: 0.02\n",
      "iteration: 118910 loss: 0.0032 lr: 0.02\n",
      "iteration: 118920 loss: 0.0036 lr: 0.02\n",
      "iteration: 118930 loss: 0.0038 lr: 0.02\n",
      "iteration: 118940 loss: 0.0018 lr: 0.02\n",
      "iteration: 118950 loss: 0.0027 lr: 0.02\n",
      "iteration: 118960 loss: 0.0026 lr: 0.02\n",
      "iteration: 118970 loss: 0.0027 lr: 0.02\n",
      "iteration: 118980 loss: 0.0028 lr: 0.02\n",
      "iteration: 118990 loss: 0.0044 lr: 0.02\n",
      "iteration: 119000 loss: 0.0035 lr: 0.02\n",
      "iteration: 119010 loss: 0.0025 lr: 0.02\n",
      "iteration: 119020 loss: 0.0029 lr: 0.02\n",
      "iteration: 119030 loss: 0.0048 lr: 0.02\n",
      "iteration: 119040 loss: 0.0023 lr: 0.02\n",
      "iteration: 119050 loss: 0.0022 lr: 0.02\n",
      "iteration: 119060 loss: 0.0021 lr: 0.02\n",
      "iteration: 119070 loss: 0.0022 lr: 0.02\n",
      "iteration: 119080 loss: 0.0028 lr: 0.02\n",
      "iteration: 119090 loss: 0.0030 lr: 0.02\n",
      "iteration: 119100 loss: 0.0036 lr: 0.02\n",
      "iteration: 119110 loss: 0.0025 lr: 0.02\n",
      "iteration: 119120 loss: 0.0032 lr: 0.02\n",
      "iteration: 119130 loss: 0.0026 lr: 0.02\n",
      "iteration: 119140 loss: 0.0030 lr: 0.02\n",
      "iteration: 119150 loss: 0.0038 lr: 0.02\n",
      "iteration: 119160 loss: 0.0021 lr: 0.02\n",
      "iteration: 119170 loss: 0.0024 lr: 0.02\n",
      "iteration: 119180 loss: 0.0027 lr: 0.02\n",
      "iteration: 119190 loss: 0.0052 lr: 0.02\n",
      "iteration: 119200 loss: 0.0026 lr: 0.02\n",
      "iteration: 119210 loss: 0.0030 lr: 0.02\n",
      "iteration: 119220 loss: 0.0042 lr: 0.02\n",
      "iteration: 119230 loss: 0.0028 lr: 0.02\n",
      "iteration: 119240 loss: 0.0033 lr: 0.02\n",
      "iteration: 119250 loss: 0.0043 lr: 0.02\n",
      "iteration: 119260 loss: 0.0024 lr: 0.02\n",
      "iteration: 119270 loss: 0.0033 lr: 0.02\n",
      "iteration: 119280 loss: 0.0030 lr: 0.02\n",
      "iteration: 119290 loss: 0.0025 lr: 0.02\n",
      "iteration: 119300 loss: 0.0028 lr: 0.02\n",
      "iteration: 119310 loss: 0.0034 lr: 0.02\n",
      "iteration: 119320 loss: 0.0039 lr: 0.02\n",
      "iteration: 119330 loss: 0.0038 lr: 0.02\n",
      "iteration: 119340 loss: 0.0031 lr: 0.02\n",
      "iteration: 119350 loss: 0.0025 lr: 0.02\n",
      "iteration: 119360 loss: 0.0032 lr: 0.02\n",
      "iteration: 119370 loss: 0.0022 lr: 0.02\n",
      "iteration: 119380 loss: 0.0025 lr: 0.02\n",
      "iteration: 119390 loss: 0.0032 lr: 0.02\n",
      "iteration: 119400 loss: 0.0042 lr: 0.02\n",
      "iteration: 119410 loss: 0.0029 lr: 0.02\n",
      "iteration: 119420 loss: 0.0031 lr: 0.02\n",
      "iteration: 119430 loss: 0.0044 lr: 0.02\n",
      "iteration: 119440 loss: 0.0024 lr: 0.02\n",
      "iteration: 119450 loss: 0.0025 lr: 0.02\n",
      "iteration: 119460 loss: 0.0029 lr: 0.02\n",
      "iteration: 119470 loss: 0.0021 lr: 0.02\n",
      "iteration: 119480 loss: 0.0023 lr: 0.02\n",
      "iteration: 119490 loss: 0.0031 lr: 0.02\n",
      "iteration: 119500 loss: 0.0032 lr: 0.02\n",
      "iteration: 119510 loss: 0.0032 lr: 0.02\n",
      "iteration: 119520 loss: 0.0026 lr: 0.02\n",
      "iteration: 119530 loss: 0.0027 lr: 0.02\n",
      "iteration: 119540 loss: 0.0029 lr: 0.02\n",
      "iteration: 119550 loss: 0.0030 lr: 0.02\n",
      "iteration: 119560 loss: 0.0024 lr: 0.02\n",
      "iteration: 119570 loss: 0.0019 lr: 0.02\n",
      "iteration: 119580 loss: 0.0031 lr: 0.02\n",
      "iteration: 119590 loss: 0.0029 lr: 0.02\n",
      "iteration: 119600 loss: 0.0027 lr: 0.02\n",
      "iteration: 119610 loss: 0.0027 lr: 0.02\n",
      "iteration: 119620 loss: 0.0026 lr: 0.02\n",
      "iteration: 119630 loss: 0.0033 lr: 0.02\n",
      "iteration: 119640 loss: 0.0034 lr: 0.02\n",
      "iteration: 119650 loss: 0.0033 lr: 0.02\n",
      "iteration: 119660 loss: 0.0031 lr: 0.02\n",
      "iteration: 119670 loss: 0.0030 lr: 0.02\n",
      "iteration: 119680 loss: 0.0041 lr: 0.02\n",
      "iteration: 119690 loss: 0.0041 lr: 0.02\n",
      "iteration: 119700 loss: 0.0028 lr: 0.02\n",
      "iteration: 119710 loss: 0.0028 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 119720 loss: 0.0026 lr: 0.02\n",
      "iteration: 119730 loss: 0.0051 lr: 0.02\n",
      "iteration: 119740 loss: 0.0027 lr: 0.02\n",
      "iteration: 119750 loss: 0.0027 lr: 0.02\n",
      "iteration: 119760 loss: 0.0025 lr: 0.02\n",
      "iteration: 119770 loss: 0.0025 lr: 0.02\n",
      "iteration: 119780 loss: 0.0027 lr: 0.02\n",
      "iteration: 119790 loss: 0.0025 lr: 0.02\n",
      "iteration: 119800 loss: 0.0033 lr: 0.02\n",
      "iteration: 119810 loss: 0.0027 lr: 0.02\n",
      "iteration: 119820 loss: 0.0029 lr: 0.02\n",
      "iteration: 119830 loss: 0.0032 lr: 0.02\n",
      "iteration: 119840 loss: 0.0025 lr: 0.02\n",
      "iteration: 119850 loss: 0.0036 lr: 0.02\n",
      "iteration: 119860 loss: 0.0032 lr: 0.02\n",
      "iteration: 119870 loss: 0.0024 lr: 0.02\n",
      "iteration: 119880 loss: 0.0026 lr: 0.02\n",
      "iteration: 119890 loss: 0.0023 lr: 0.02\n",
      "iteration: 119900 loss: 0.0021 lr: 0.02\n",
      "iteration: 119910 loss: 0.0047 lr: 0.02\n",
      "iteration: 119920 loss: 0.0030 lr: 0.02\n",
      "iteration: 119930 loss: 0.0033 lr: 0.02\n",
      "iteration: 119940 loss: 0.0030 lr: 0.02\n",
      "iteration: 119950 loss: 0.0026 lr: 0.02\n",
      "iteration: 119960 loss: 0.0037 lr: 0.02\n",
      "iteration: 119970 loss: 0.0029 lr: 0.02\n",
      "iteration: 119980 loss: 0.0032 lr: 0.02\n",
      "iteration: 119990 loss: 0.0023 lr: 0.02\n",
      "iteration: 120000 loss: 0.0047 lr: 0.02\n",
      "iteration: 120010 loss: 0.0029 lr: 0.02\n",
      "iteration: 120020 loss: 0.0023 lr: 0.02\n",
      "iteration: 120030 loss: 0.0025 lr: 0.02\n",
      "iteration: 120040 loss: 0.0029 lr: 0.02\n",
      "iteration: 120050 loss: 0.0032 lr: 0.02\n",
      "iteration: 120060 loss: 0.0031 lr: 0.02\n",
      "iteration: 120070 loss: 0.0037 lr: 0.02\n",
      "iteration: 120080 loss: 0.0038 lr: 0.02\n",
      "iteration: 120090 loss: 0.0035 lr: 0.02\n",
      "iteration: 120100 loss: 0.0024 lr: 0.02\n",
      "iteration: 120110 loss: 0.0026 lr: 0.02\n",
      "iteration: 120120 loss: 0.0032 lr: 0.02\n",
      "iteration: 120130 loss: 0.0023 lr: 0.02\n",
      "iteration: 120140 loss: 0.0028 lr: 0.02\n",
      "iteration: 120150 loss: 0.0027 lr: 0.02\n",
      "iteration: 120160 loss: 0.0032 lr: 0.02\n",
      "iteration: 120170 loss: 0.0034 lr: 0.02\n",
      "iteration: 120180 loss: 0.0018 lr: 0.02\n",
      "iteration: 120190 loss: 0.0029 lr: 0.02\n",
      "iteration: 120200 loss: 0.0032 lr: 0.02\n",
      "iteration: 120210 loss: 0.0025 lr: 0.02\n",
      "iteration: 120220 loss: 0.0031 lr: 0.02\n",
      "iteration: 120230 loss: 0.0025 lr: 0.02\n",
      "iteration: 120240 loss: 0.0023 lr: 0.02\n",
      "iteration: 120250 loss: 0.0029 lr: 0.02\n",
      "iteration: 120260 loss: 0.0028 lr: 0.02\n",
      "iteration: 120270 loss: 0.0022 lr: 0.02\n",
      "iteration: 120280 loss: 0.0023 lr: 0.02\n",
      "iteration: 120290 loss: 0.0029 lr: 0.02\n",
      "iteration: 120300 loss: 0.0032 lr: 0.02\n",
      "iteration: 120310 loss: 0.0024 lr: 0.02\n",
      "iteration: 120320 loss: 0.0030 lr: 0.02\n",
      "iteration: 120330 loss: 0.0024 lr: 0.02\n",
      "iteration: 120340 loss: 0.0026 lr: 0.02\n",
      "iteration: 120350 loss: 0.0030 lr: 0.02\n",
      "iteration: 120360 loss: 0.0032 lr: 0.02\n",
      "iteration: 120370 loss: 0.0042 lr: 0.02\n",
      "iteration: 120380 loss: 0.0021 lr: 0.02\n",
      "iteration: 120390 loss: 0.0030 lr: 0.02\n",
      "iteration: 120400 loss: 0.0034 lr: 0.02\n",
      "iteration: 120410 loss: 0.0037 lr: 0.02\n",
      "iteration: 120420 loss: 0.0026 lr: 0.02\n",
      "iteration: 120430 loss: 0.0029 lr: 0.02\n",
      "iteration: 120440 loss: 0.0039 lr: 0.02\n",
      "iteration: 120450 loss: 0.0035 lr: 0.02\n",
      "iteration: 120460 loss: 0.0030 lr: 0.02\n",
      "iteration: 120470 loss: 0.0025 lr: 0.02\n",
      "iteration: 120480 loss: 0.0025 lr: 0.02\n",
      "iteration: 120490 loss: 0.0024 lr: 0.02\n",
      "iteration: 120500 loss: 0.0022 lr: 0.02\n",
      "iteration: 120510 loss: 0.0019 lr: 0.02\n",
      "iteration: 120520 loss: 0.0023 lr: 0.02\n",
      "iteration: 120530 loss: 0.0019 lr: 0.02\n",
      "iteration: 120540 loss: 0.0025 lr: 0.02\n",
      "iteration: 120550 loss: 0.0019 lr: 0.02\n",
      "iteration: 120560 loss: 0.0042 lr: 0.02\n",
      "iteration: 120570 loss: 0.0033 lr: 0.02\n",
      "iteration: 120580 loss: 0.0028 lr: 0.02\n",
      "iteration: 120590 loss: 0.0037 lr: 0.02\n",
      "iteration: 120600 loss: 0.0030 lr: 0.02\n",
      "iteration: 120610 loss: 0.0041 lr: 0.02\n",
      "iteration: 120620 loss: 0.0025 lr: 0.02\n",
      "iteration: 120630 loss: 0.0045 lr: 0.02\n",
      "iteration: 120640 loss: 0.0026 lr: 0.02\n",
      "iteration: 120650 loss: 0.0027 lr: 0.02\n",
      "iteration: 120660 loss: 0.0039 lr: 0.02\n",
      "iteration: 120670 loss: 0.0024 lr: 0.02\n",
      "iteration: 120680 loss: 0.0027 lr: 0.02\n",
      "iteration: 120690 loss: 0.0025 lr: 0.02\n",
      "iteration: 120700 loss: 0.0030 lr: 0.02\n",
      "iteration: 120710 loss: 0.0034 lr: 0.02\n",
      "iteration: 120720 loss: 0.0031 lr: 0.02\n",
      "iteration: 120730 loss: 0.0023 lr: 0.02\n",
      "iteration: 120740 loss: 0.0025 lr: 0.02\n",
      "iteration: 120750 loss: 0.0033 lr: 0.02\n",
      "iteration: 120760 loss: 0.0036 lr: 0.02\n",
      "iteration: 120770 loss: 0.0027 lr: 0.02\n",
      "iteration: 120780 loss: 0.0036 lr: 0.02\n",
      "iteration: 120790 loss: 0.0027 lr: 0.02\n",
      "iteration: 120800 loss: 0.0028 lr: 0.02\n",
      "iteration: 120810 loss: 0.0028 lr: 0.02\n",
      "iteration: 120820 loss: 0.0053 lr: 0.02\n",
      "iteration: 120830 loss: 0.0027 lr: 0.02\n",
      "iteration: 120840 loss: 0.0024 lr: 0.02\n",
      "iteration: 120850 loss: 0.0021 lr: 0.02\n",
      "iteration: 120860 loss: 0.0026 lr: 0.02\n",
      "iteration: 120870 loss: 0.0026 lr: 0.02\n",
      "iteration: 120880 loss: 0.0027 lr: 0.02\n",
      "iteration: 120890 loss: 0.0026 lr: 0.02\n",
      "iteration: 120900 loss: 0.0027 lr: 0.02\n",
      "iteration: 120910 loss: 0.0028 lr: 0.02\n",
      "iteration: 120920 loss: 0.0023 lr: 0.02\n",
      "iteration: 120930 loss: 0.0042 lr: 0.02\n",
      "iteration: 120940 loss: 0.0028 lr: 0.02\n",
      "iteration: 120950 loss: 0.0030 lr: 0.02\n",
      "iteration: 120960 loss: 0.0035 lr: 0.02\n",
      "iteration: 120970 loss: 0.0027 lr: 0.02\n",
      "iteration: 120980 loss: 0.0030 lr: 0.02\n",
      "iteration: 120990 loss: 0.0030 lr: 0.02\n",
      "iteration: 121000 loss: 0.0021 lr: 0.02\n",
      "iteration: 121010 loss: 0.0038 lr: 0.02\n",
      "iteration: 121020 loss: 0.0026 lr: 0.02\n",
      "iteration: 121030 loss: 0.0030 lr: 0.02\n",
      "iteration: 121040 loss: 0.0039 lr: 0.02\n",
      "iteration: 121050 loss: 0.0037 lr: 0.02\n",
      "iteration: 121060 loss: 0.0031 lr: 0.02\n",
      "iteration: 121070 loss: 0.0031 lr: 0.02\n",
      "iteration: 121080 loss: 0.0038 lr: 0.02\n",
      "iteration: 121090 loss: 0.0033 lr: 0.02\n",
      "iteration: 121100 loss: 0.0023 lr: 0.02\n",
      "iteration: 121110 loss: 0.0030 lr: 0.02\n",
      "iteration: 121120 loss: 0.0032 lr: 0.02\n",
      "iteration: 121130 loss: 0.0024 lr: 0.02\n",
      "iteration: 121140 loss: 0.0022 lr: 0.02\n",
      "iteration: 121150 loss: 0.0030 lr: 0.02\n",
      "iteration: 121160 loss: 0.0025 lr: 0.02\n",
      "iteration: 121170 loss: 0.0023 lr: 0.02\n",
      "iteration: 121180 loss: 0.0028 lr: 0.02\n",
      "iteration: 121190 loss: 0.0017 lr: 0.02\n",
      "iteration: 121200 loss: 0.0025 lr: 0.02\n",
      "iteration: 121210 loss: 0.0029 lr: 0.02\n",
      "iteration: 121220 loss: 0.0033 lr: 0.02\n",
      "iteration: 121230 loss: 0.0031 lr: 0.02\n",
      "iteration: 121240 loss: 0.0028 lr: 0.02\n",
      "iteration: 121250 loss: 0.0021 lr: 0.02\n",
      "iteration: 121260 loss: 0.0038 lr: 0.02\n",
      "iteration: 121270 loss: 0.0018 lr: 0.02\n",
      "iteration: 121280 loss: 0.0030 lr: 0.02\n",
      "iteration: 121290 loss: 0.0039 lr: 0.02\n",
      "iteration: 121300 loss: 0.0030 lr: 0.02\n",
      "iteration: 121310 loss: 0.0035 lr: 0.02\n",
      "iteration: 121320 loss: 0.0033 lr: 0.02\n",
      "iteration: 121330 loss: 0.0024 lr: 0.02\n",
      "iteration: 121340 loss: 0.0027 lr: 0.02\n",
      "iteration: 121350 loss: 0.0028 lr: 0.02\n",
      "iteration: 121360 loss: 0.0023 lr: 0.02\n",
      "iteration: 121370 loss: 0.0023 lr: 0.02\n",
      "iteration: 121380 loss: 0.0026 lr: 0.02\n",
      "iteration: 121390 loss: 0.0029 lr: 0.02\n",
      "iteration: 121400 loss: 0.0027 lr: 0.02\n",
      "iteration: 121410 loss: 0.0029 lr: 0.02\n",
      "iteration: 121420 loss: 0.0025 lr: 0.02\n",
      "iteration: 121430 loss: 0.0019 lr: 0.02\n",
      "iteration: 121440 loss: 0.0023 lr: 0.02\n",
      "iteration: 121450 loss: 0.0025 lr: 0.02\n",
      "iteration: 121460 loss: 0.0036 lr: 0.02\n",
      "iteration: 121470 loss: 0.0044 lr: 0.02\n",
      "iteration: 121480 loss: 0.0031 lr: 0.02\n",
      "iteration: 121490 loss: 0.0031 lr: 0.02\n",
      "iteration: 121500 loss: 0.0020 lr: 0.02\n",
      "iteration: 121510 loss: 0.0029 lr: 0.02\n",
      "iteration: 121520 loss: 0.0025 lr: 0.02\n",
      "iteration: 121530 loss: 0.0030 lr: 0.02\n",
      "iteration: 121540 loss: 0.0028 lr: 0.02\n",
      "iteration: 121550 loss: 0.0024 lr: 0.02\n",
      "iteration: 121560 loss: 0.0034 lr: 0.02\n",
      "iteration: 121570 loss: 0.0024 lr: 0.02\n",
      "iteration: 121580 loss: 0.0021 lr: 0.02\n",
      "iteration: 121590 loss: 0.0025 lr: 0.02\n",
      "iteration: 121600 loss: 0.0029 lr: 0.02\n",
      "iteration: 121610 loss: 0.0021 lr: 0.02\n",
      "iteration: 121620 loss: 0.0023 lr: 0.02\n",
      "iteration: 121630 loss: 0.0033 lr: 0.02\n",
      "iteration: 121640 loss: 0.0019 lr: 0.02\n",
      "iteration: 121650 loss: 0.0048 lr: 0.02\n",
      "iteration: 121660 loss: 0.0025 lr: 0.02\n",
      "iteration: 121670 loss: 0.0038 lr: 0.02\n",
      "iteration: 121680 loss: 0.0041 lr: 0.02\n",
      "iteration: 121690 loss: 0.0052 lr: 0.02\n",
      "iteration: 121700 loss: 0.0037 lr: 0.02\n",
      "iteration: 121710 loss: 0.0033 lr: 0.02\n",
      "iteration: 121720 loss: 0.0020 lr: 0.02\n",
      "iteration: 121730 loss: 0.0028 lr: 0.02\n",
      "iteration: 121740 loss: 0.0032 lr: 0.02\n",
      "iteration: 121750 loss: 0.0034 lr: 0.02\n",
      "iteration: 121760 loss: 0.0031 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 121770 loss: 0.0031 lr: 0.02\n",
      "iteration: 121780 loss: 0.0028 lr: 0.02\n",
      "iteration: 121790 loss: 0.0032 lr: 0.02\n",
      "iteration: 121800 loss: 0.0019 lr: 0.02\n",
      "iteration: 121810 loss: 0.0024 lr: 0.02\n",
      "iteration: 121820 loss: 0.0035 lr: 0.02\n",
      "iteration: 121830 loss: 0.0024 lr: 0.02\n",
      "iteration: 121840 loss: 0.0043 lr: 0.02\n",
      "iteration: 121850 loss: 0.0028 lr: 0.02\n",
      "iteration: 121860 loss: 0.0025 lr: 0.02\n",
      "iteration: 121870 loss: 0.0028 lr: 0.02\n",
      "iteration: 121880 loss: 0.0027 lr: 0.02\n",
      "iteration: 121890 loss: 0.0025 lr: 0.02\n",
      "iteration: 121900 loss: 0.0030 lr: 0.02\n",
      "iteration: 121910 loss: 0.0032 lr: 0.02\n",
      "iteration: 121920 loss: 0.0027 lr: 0.02\n",
      "iteration: 121930 loss: 0.0026 lr: 0.02\n",
      "iteration: 121940 loss: 0.0028 lr: 0.02\n",
      "iteration: 121950 loss: 0.0042 lr: 0.02\n",
      "iteration: 121960 loss: 0.0026 lr: 0.02\n",
      "iteration: 121970 loss: 0.0040 lr: 0.02\n",
      "iteration: 121980 loss: 0.0030 lr: 0.02\n",
      "iteration: 121990 loss: 0.0027 lr: 0.02\n",
      "iteration: 122000 loss: 0.0030 lr: 0.02\n",
      "iteration: 122010 loss: 0.0023 lr: 0.02\n",
      "iteration: 122020 loss: 0.0026 lr: 0.02\n",
      "iteration: 122030 loss: 0.0023 lr: 0.02\n",
      "iteration: 122040 loss: 0.0024 lr: 0.02\n",
      "iteration: 122050 loss: 0.0031 lr: 0.02\n",
      "iteration: 122060 loss: 0.0040 lr: 0.02\n",
      "iteration: 122070 loss: 0.0032 lr: 0.02\n",
      "iteration: 122080 loss: 0.0032 lr: 0.02\n",
      "iteration: 122090 loss: 0.0039 lr: 0.02\n",
      "iteration: 122100 loss: 0.0041 lr: 0.02\n",
      "iteration: 122110 loss: 0.0030 lr: 0.02\n",
      "iteration: 122120 loss: 0.0033 lr: 0.02\n",
      "iteration: 122130 loss: 0.0029 lr: 0.02\n",
      "iteration: 122140 loss: 0.0023 lr: 0.02\n",
      "iteration: 122150 loss: 0.0030 lr: 0.02\n",
      "iteration: 122160 loss: 0.0031 lr: 0.02\n",
      "iteration: 122170 loss: 0.0029 lr: 0.02\n",
      "iteration: 122180 loss: 0.0027 lr: 0.02\n",
      "iteration: 122190 loss: 0.0027 lr: 0.02\n",
      "iteration: 122200 loss: 0.0029 lr: 0.02\n",
      "iteration: 122210 loss: 0.0031 lr: 0.02\n",
      "iteration: 122220 loss: 0.0033 lr: 0.02\n",
      "iteration: 122230 loss: 0.0022 lr: 0.02\n",
      "iteration: 122240 loss: 0.0020 lr: 0.02\n",
      "iteration: 122250 loss: 0.0037 lr: 0.02\n",
      "iteration: 122260 loss: 0.0041 lr: 0.02\n",
      "iteration: 122270 loss: 0.0039 lr: 0.02\n",
      "iteration: 122280 loss: 0.0035 lr: 0.02\n",
      "iteration: 122290 loss: 0.0025 lr: 0.02\n",
      "iteration: 122300 loss: 0.0028 lr: 0.02\n",
      "iteration: 122310 loss: 0.0022 lr: 0.02\n",
      "iteration: 122320 loss: 0.0034 lr: 0.02\n",
      "iteration: 122330 loss: 0.0030 lr: 0.02\n",
      "iteration: 122340 loss: 0.0025 lr: 0.02\n",
      "iteration: 122350 loss: 0.0028 lr: 0.02\n",
      "iteration: 122360 loss: 0.0027 lr: 0.02\n",
      "iteration: 122370 loss: 0.0049 lr: 0.02\n",
      "iteration: 122380 loss: 0.0026 lr: 0.02\n",
      "iteration: 122390 loss: 0.0031 lr: 0.02\n",
      "iteration: 122400 loss: 0.0026 lr: 0.02\n",
      "iteration: 122410 loss: 0.0029 lr: 0.02\n",
      "iteration: 122420 loss: 0.0023 lr: 0.02\n",
      "iteration: 122430 loss: 0.0027 lr: 0.02\n",
      "iteration: 122440 loss: 0.0032 lr: 0.02\n",
      "iteration: 122450 loss: 0.0024 lr: 0.02\n",
      "iteration: 122460 loss: 0.0025 lr: 0.02\n",
      "iteration: 122470 loss: 0.0020 lr: 0.02\n",
      "iteration: 122480 loss: 0.0027 lr: 0.02\n",
      "iteration: 122490 loss: 0.0025 lr: 0.02\n",
      "iteration: 122500 loss: 0.0035 lr: 0.02\n",
      "iteration: 122510 loss: 0.0024 lr: 0.02\n",
      "iteration: 122520 loss: 0.0032 lr: 0.02\n",
      "iteration: 122530 loss: 0.0036 lr: 0.02\n",
      "iteration: 122540 loss: 0.0021 lr: 0.02\n",
      "iteration: 122550 loss: 0.0021 lr: 0.02\n",
      "iteration: 122560 loss: 0.0037 lr: 0.02\n",
      "iteration: 122570 loss: 0.0029 lr: 0.02\n",
      "iteration: 122580 loss: 0.0034 lr: 0.02\n",
      "iteration: 122590 loss: 0.0036 lr: 0.02\n",
      "iteration: 122600 loss: 0.0040 lr: 0.02\n",
      "iteration: 122610 loss: 0.0022 lr: 0.02\n",
      "iteration: 122620 loss: 0.0021 lr: 0.02\n",
      "iteration: 122630 loss: 0.0025 lr: 0.02\n",
      "iteration: 122640 loss: 0.0045 lr: 0.02\n",
      "iteration: 122650 loss: 0.0026 lr: 0.02\n",
      "iteration: 122660 loss: 0.0034 lr: 0.02\n",
      "iteration: 122670 loss: 0.0028 lr: 0.02\n",
      "iteration: 122680 loss: 0.0029 lr: 0.02\n",
      "iteration: 122690 loss: 0.0033 lr: 0.02\n",
      "iteration: 122700 loss: 0.0033 lr: 0.02\n",
      "iteration: 122710 loss: 0.0034 lr: 0.02\n",
      "iteration: 122720 loss: 0.0034 lr: 0.02\n",
      "iteration: 122730 loss: 0.0033 lr: 0.02\n",
      "iteration: 122740 loss: 0.0042 lr: 0.02\n",
      "iteration: 122750 loss: 0.0025 lr: 0.02\n",
      "iteration: 122760 loss: 0.0033 lr: 0.02\n",
      "iteration: 122770 loss: 0.0032 lr: 0.02\n",
      "iteration: 122780 loss: 0.0023 lr: 0.02\n",
      "iteration: 122790 loss: 0.0035 lr: 0.02\n",
      "iteration: 122800 loss: 0.0027 lr: 0.02\n",
      "iteration: 122810 loss: 0.0024 lr: 0.02\n",
      "iteration: 122820 loss: 0.0027 lr: 0.02\n",
      "iteration: 122830 loss: 0.0028 lr: 0.02\n",
      "iteration: 122840 loss: 0.0022 lr: 0.02\n",
      "iteration: 122850 loss: 0.0029 lr: 0.02\n",
      "iteration: 122860 loss: 0.0026 lr: 0.02\n",
      "iteration: 122870 loss: 0.0021 lr: 0.02\n",
      "iteration: 122880 loss: 0.0028 lr: 0.02\n",
      "iteration: 122890 loss: 0.0026 lr: 0.02\n",
      "iteration: 122900 loss: 0.0031 lr: 0.02\n",
      "iteration: 122910 loss: 0.0028 lr: 0.02\n",
      "iteration: 122920 loss: 0.0037 lr: 0.02\n",
      "iteration: 122930 loss: 0.0031 lr: 0.02\n",
      "iteration: 122940 loss: 0.0025 lr: 0.02\n",
      "iteration: 122950 loss: 0.0029 lr: 0.02\n",
      "iteration: 122960 loss: 0.0025 lr: 0.02\n",
      "iteration: 122970 loss: 0.0049 lr: 0.02\n",
      "iteration: 122980 loss: 0.0035 lr: 0.02\n",
      "iteration: 122990 loss: 0.0024 lr: 0.02\n",
      "iteration: 123000 loss: 0.0028 lr: 0.02\n",
      "iteration: 123010 loss: 0.0035 lr: 0.02\n",
      "iteration: 123020 loss: 0.0023 lr: 0.02\n",
      "iteration: 123030 loss: 0.0024 lr: 0.02\n",
      "iteration: 123040 loss: 0.0031 lr: 0.02\n",
      "iteration: 123050 loss: 0.0033 lr: 0.02\n",
      "iteration: 123060 loss: 0.0029 lr: 0.02\n",
      "iteration: 123070 loss: 0.0040 lr: 0.02\n",
      "iteration: 123080 loss: 0.0019 lr: 0.02\n",
      "iteration: 123090 loss: 0.0027 lr: 0.02\n",
      "iteration: 123100 loss: 0.0030 lr: 0.02\n",
      "iteration: 123110 loss: 0.0027 lr: 0.02\n",
      "iteration: 123120 loss: 0.0031 lr: 0.02\n",
      "iteration: 123130 loss: 0.0024 lr: 0.02\n",
      "iteration: 123140 loss: 0.0023 lr: 0.02\n",
      "iteration: 123150 loss: 0.0023 lr: 0.02\n",
      "iteration: 123160 loss: 0.0025 lr: 0.02\n",
      "iteration: 123170 loss: 0.0023 lr: 0.02\n",
      "iteration: 123180 loss: 0.0028 lr: 0.02\n",
      "iteration: 123190 loss: 0.0031 lr: 0.02\n",
      "iteration: 123200 loss: 0.0034 lr: 0.02\n",
      "iteration: 123210 loss: 0.0028 lr: 0.02\n",
      "iteration: 123220 loss: 0.0027 lr: 0.02\n",
      "iteration: 123230 loss: 0.0066 lr: 0.02\n",
      "iteration: 123240 loss: 0.0033 lr: 0.02\n",
      "iteration: 123250 loss: 0.0043 lr: 0.02\n",
      "iteration: 123260 loss: 0.0026 lr: 0.02\n",
      "iteration: 123270 loss: 0.0052 lr: 0.02\n",
      "iteration: 123280 loss: 0.0028 lr: 0.02\n",
      "iteration: 123290 loss: 0.0045 lr: 0.02\n",
      "iteration: 123300 loss: 0.0034 lr: 0.02\n",
      "iteration: 123310 loss: 0.0051 lr: 0.02\n",
      "iteration: 123320 loss: 0.0025 lr: 0.02\n",
      "iteration: 123330 loss: 0.0029 lr: 0.02\n",
      "iteration: 123340 loss: 0.0028 lr: 0.02\n",
      "iteration: 123350 loss: 0.0037 lr: 0.02\n",
      "iteration: 123360 loss: 0.0029 lr: 0.02\n",
      "iteration: 123370 loss: 0.0038 lr: 0.02\n",
      "iteration: 123380 loss: 0.0034 lr: 0.02\n",
      "iteration: 123390 loss: 0.0029 lr: 0.02\n",
      "iteration: 123400 loss: 0.0038 lr: 0.02\n",
      "iteration: 123410 loss: 0.0043 lr: 0.02\n",
      "iteration: 123420 loss: 0.0026 lr: 0.02\n",
      "iteration: 123430 loss: 0.0030 lr: 0.02\n",
      "iteration: 123440 loss: 0.0031 lr: 0.02\n",
      "iteration: 123450 loss: 0.0023 lr: 0.02\n",
      "iteration: 123460 loss: 0.0037 lr: 0.02\n",
      "iteration: 123470 loss: 0.0033 lr: 0.02\n",
      "iteration: 123480 loss: 0.0032 lr: 0.02\n",
      "iteration: 123490 loss: 0.0026 lr: 0.02\n",
      "iteration: 123500 loss: 0.0027 lr: 0.02\n",
      "iteration: 123510 loss: 0.0032 lr: 0.02\n",
      "iteration: 123520 loss: 0.0032 lr: 0.02\n",
      "iteration: 123530 loss: 0.0040 lr: 0.02\n",
      "iteration: 123540 loss: 0.0021 lr: 0.02\n",
      "iteration: 123550 loss: 0.0035 lr: 0.02\n",
      "iteration: 123560 loss: 0.0030 lr: 0.02\n",
      "iteration: 123570 loss: 0.0024 lr: 0.02\n",
      "iteration: 123580 loss: 0.0032 lr: 0.02\n",
      "iteration: 123590 loss: 0.0030 lr: 0.02\n",
      "iteration: 123600 loss: 0.0027 lr: 0.02\n",
      "iteration: 123610 loss: 0.0023 lr: 0.02\n",
      "iteration: 123620 loss: 0.0036 lr: 0.02\n",
      "iteration: 123630 loss: 0.0022 lr: 0.02\n",
      "iteration: 123640 loss: 0.0028 lr: 0.02\n",
      "iteration: 123650 loss: 0.0026 lr: 0.02\n",
      "iteration: 123660 loss: 0.0023 lr: 0.02\n",
      "iteration: 123670 loss: 0.0023 lr: 0.02\n",
      "iteration: 123680 loss: 0.0030 lr: 0.02\n",
      "iteration: 123690 loss: 0.0017 lr: 0.02\n",
      "iteration: 123700 loss: 0.0029 lr: 0.02\n",
      "iteration: 123710 loss: 0.0029 lr: 0.02\n",
      "iteration: 123720 loss: 0.0025 lr: 0.02\n",
      "iteration: 123730 loss: 0.0028 lr: 0.02\n",
      "iteration: 123740 loss: 0.0029 lr: 0.02\n",
      "iteration: 123750 loss: 0.0023 lr: 0.02\n",
      "iteration: 123760 loss: 0.0024 lr: 0.02\n",
      "iteration: 123770 loss: 0.0027 lr: 0.02\n",
      "iteration: 123780 loss: 0.0035 lr: 0.02\n",
      "iteration: 123790 loss: 0.0022 lr: 0.02\n",
      "iteration: 123800 loss: 0.0026 lr: 0.02\n",
      "iteration: 123810 loss: 0.0026 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 123820 loss: 0.0040 lr: 0.02\n",
      "iteration: 123830 loss: 0.0027 lr: 0.02\n",
      "iteration: 123840 loss: 0.0049 lr: 0.02\n",
      "iteration: 123850 loss: 0.0028 lr: 0.02\n",
      "iteration: 123860 loss: 0.0029 lr: 0.02\n",
      "iteration: 123870 loss: 0.0041 lr: 0.02\n",
      "iteration: 123880 loss: 0.0018 lr: 0.02\n",
      "iteration: 123890 loss: 0.0033 lr: 0.02\n",
      "iteration: 123900 loss: 0.0026 lr: 0.02\n",
      "iteration: 123910 loss: 0.0024 lr: 0.02\n",
      "iteration: 123920 loss: 0.0033 lr: 0.02\n",
      "iteration: 123930 loss: 0.0026 lr: 0.02\n",
      "iteration: 123940 loss: 0.0034 lr: 0.02\n",
      "iteration: 123950 loss: 0.0031 lr: 0.02\n",
      "iteration: 123960 loss: 0.0043 lr: 0.02\n",
      "iteration: 123970 loss: 0.0022 lr: 0.02\n",
      "iteration: 123980 loss: 0.0033 lr: 0.02\n",
      "iteration: 123990 loss: 0.0031 lr: 0.02\n",
      "iteration: 124000 loss: 0.0027 lr: 0.02\n",
      "iteration: 124010 loss: 0.0032 lr: 0.02\n",
      "iteration: 124020 loss: 0.0030 lr: 0.02\n",
      "iteration: 124030 loss: 0.0033 lr: 0.02\n",
      "iteration: 124040 loss: 0.0031 lr: 0.02\n",
      "iteration: 124050 loss: 0.0047 lr: 0.02\n",
      "iteration: 124060 loss: 0.0031 lr: 0.02\n",
      "iteration: 124070 loss: 0.0046 lr: 0.02\n",
      "iteration: 124080 loss: 0.0039 lr: 0.02\n",
      "iteration: 124090 loss: 0.0032 lr: 0.02\n",
      "iteration: 124100 loss: 0.0038 lr: 0.02\n",
      "iteration: 124110 loss: 0.0029 lr: 0.02\n",
      "iteration: 124120 loss: 0.0026 lr: 0.02\n",
      "iteration: 124130 loss: 0.0028 lr: 0.02\n",
      "iteration: 124140 loss: 0.0025 lr: 0.02\n",
      "iteration: 124150 loss: 0.0038 lr: 0.02\n",
      "iteration: 124160 loss: 0.0030 lr: 0.02\n",
      "iteration: 124170 loss: 0.0033 lr: 0.02\n",
      "iteration: 124180 loss: 0.0031 lr: 0.02\n",
      "iteration: 124190 loss: 0.0026 lr: 0.02\n",
      "iteration: 124200 loss: 0.0023 lr: 0.02\n",
      "iteration: 124210 loss: 0.0019 lr: 0.02\n",
      "iteration: 124220 loss: 0.0027 lr: 0.02\n",
      "iteration: 124230 loss: 0.0024 lr: 0.02\n",
      "iteration: 124240 loss: 0.0026 lr: 0.02\n",
      "iteration: 124250 loss: 0.0021 lr: 0.02\n",
      "iteration: 124260 loss: 0.0027 lr: 0.02\n",
      "iteration: 124270 loss: 0.0030 lr: 0.02\n",
      "iteration: 124280 loss: 0.0031 lr: 0.02\n",
      "iteration: 124290 loss: 0.0025 lr: 0.02\n",
      "iteration: 124300 loss: 0.0030 lr: 0.02\n",
      "iteration: 124310 loss: 0.0037 lr: 0.02\n",
      "iteration: 124320 loss: 0.0021 lr: 0.02\n",
      "iteration: 124330 loss: 0.0026 lr: 0.02\n",
      "iteration: 124340 loss: 0.0021 lr: 0.02\n",
      "iteration: 124350 loss: 0.0025 lr: 0.02\n",
      "iteration: 124360 loss: 0.0032 lr: 0.02\n",
      "iteration: 124370 loss: 0.0045 lr: 0.02\n",
      "iteration: 124380 loss: 0.0031 lr: 0.02\n",
      "iteration: 124390 loss: 0.0039 lr: 0.02\n",
      "iteration: 124400 loss: 0.0044 lr: 0.02\n",
      "iteration: 124410 loss: 0.0027 lr: 0.02\n",
      "iteration: 124420 loss: 0.0033 lr: 0.02\n",
      "iteration: 124430 loss: 0.0028 lr: 0.02\n",
      "iteration: 124440 loss: 0.0032 lr: 0.02\n",
      "iteration: 124450 loss: 0.0035 lr: 0.02\n",
      "iteration: 124460 loss: 0.0021 lr: 0.02\n",
      "iteration: 124470 loss: 0.0030 lr: 0.02\n",
      "iteration: 124480 loss: 0.0021 lr: 0.02\n",
      "iteration: 124490 loss: 0.0025 lr: 0.02\n",
      "iteration: 124500 loss: 0.0024 lr: 0.02\n",
      "iteration: 124510 loss: 0.0019 lr: 0.02\n",
      "iteration: 124520 loss: 0.0056 lr: 0.02\n",
      "iteration: 124530 loss: 0.0034 lr: 0.02\n",
      "iteration: 124540 loss: 0.0041 lr: 0.02\n",
      "iteration: 124550 loss: 0.0025 lr: 0.02\n",
      "iteration: 124560 loss: 0.0023 lr: 0.02\n",
      "iteration: 124570 loss: 0.0030 lr: 0.02\n",
      "iteration: 124580 loss: 0.0033 lr: 0.02\n",
      "iteration: 124590 loss: 0.0025 lr: 0.02\n",
      "iteration: 124600 loss: 0.0029 lr: 0.02\n",
      "iteration: 124610 loss: 0.0024 lr: 0.02\n",
      "iteration: 124620 loss: 0.0023 lr: 0.02\n",
      "iteration: 124630 loss: 0.0026 lr: 0.02\n",
      "iteration: 124640 loss: 0.0030 lr: 0.02\n",
      "iteration: 124650 loss: 0.0028 lr: 0.02\n",
      "iteration: 124660 loss: 0.0032 lr: 0.02\n",
      "iteration: 124670 loss: 0.0027 lr: 0.02\n",
      "iteration: 124680 loss: 0.0028 lr: 0.02\n",
      "iteration: 124690 loss: 0.0025 lr: 0.02\n",
      "iteration: 124700 loss: 0.0029 lr: 0.02\n",
      "iteration: 124710 loss: 0.0025 lr: 0.02\n",
      "iteration: 124720 loss: 0.0029 lr: 0.02\n",
      "iteration: 124730 loss: 0.0028 lr: 0.02\n",
      "iteration: 124740 loss: 0.0020 lr: 0.02\n",
      "iteration: 124750 loss: 0.0028 lr: 0.02\n",
      "iteration: 124760 loss: 0.0039 lr: 0.02\n",
      "iteration: 124770 loss: 0.0029 lr: 0.02\n",
      "iteration: 124780 loss: 0.0024 lr: 0.02\n",
      "iteration: 124790 loss: 0.0033 lr: 0.02\n",
      "iteration: 124800 loss: 0.0036 lr: 0.02\n",
      "iteration: 124810 loss: 0.0018 lr: 0.02\n",
      "iteration: 124820 loss: 0.0036 lr: 0.02\n",
      "iteration: 124830 loss: 0.0036 lr: 0.02\n",
      "iteration: 124840 loss: 0.0025 lr: 0.02\n",
      "iteration: 124850 loss: 0.0027 lr: 0.02\n",
      "iteration: 124860 loss: 0.0027 lr: 0.02\n",
      "iteration: 124870 loss: 0.0036 lr: 0.02\n",
      "iteration: 124880 loss: 0.0023 lr: 0.02\n",
      "iteration: 124890 loss: 0.0026 lr: 0.02\n",
      "iteration: 124900 loss: 0.0030 lr: 0.02\n",
      "iteration: 124910 loss: 0.0032 lr: 0.02\n",
      "iteration: 124920 loss: 0.0026 lr: 0.02\n",
      "iteration: 124930 loss: 0.0027 lr: 0.02\n",
      "iteration: 124940 loss: 0.0021 lr: 0.02\n",
      "iteration: 124950 loss: 0.0021 lr: 0.02\n",
      "iteration: 124960 loss: 0.0034 lr: 0.02\n",
      "iteration: 124970 loss: 0.0045 lr: 0.02\n",
      "iteration: 124980 loss: 0.0023 lr: 0.02\n",
      "iteration: 124990 loss: 0.0021 lr: 0.02\n",
      "iteration: 125000 loss: 0.0036 lr: 0.02\n",
      "iteration: 125010 loss: 0.0037 lr: 0.02\n",
      "iteration: 125020 loss: 0.0025 lr: 0.02\n",
      "iteration: 125030 loss: 0.0025 lr: 0.02\n",
      "iteration: 125040 loss: 0.0026 lr: 0.02\n",
      "iteration: 125050 loss: 0.0028 lr: 0.02\n",
      "iteration: 125060 loss: 0.0035 lr: 0.02\n",
      "iteration: 125070 loss: 0.0036 lr: 0.02\n",
      "iteration: 125080 loss: 0.0041 lr: 0.02\n",
      "iteration: 125090 loss: 0.0025 lr: 0.02\n",
      "iteration: 125100 loss: 0.0022 lr: 0.02\n",
      "iteration: 125110 loss: 0.0037 lr: 0.02\n",
      "iteration: 125120 loss: 0.0022 lr: 0.02\n",
      "iteration: 125130 loss: 0.0037 lr: 0.02\n",
      "iteration: 125140 loss: 0.0022 lr: 0.02\n",
      "iteration: 125150 loss: 0.0045 lr: 0.02\n",
      "iteration: 125160 loss: 0.0036 lr: 0.02\n",
      "iteration: 125170 loss: 0.0035 lr: 0.02\n",
      "iteration: 125180 loss: 0.0032 lr: 0.02\n",
      "iteration: 125190 loss: 0.0033 lr: 0.02\n",
      "iteration: 125200 loss: 0.0028 lr: 0.02\n",
      "iteration: 125210 loss: 0.0032 lr: 0.02\n",
      "iteration: 125220 loss: 0.0028 lr: 0.02\n",
      "iteration: 125230 loss: 0.0042 lr: 0.02\n",
      "iteration: 125240 loss: 0.0026 lr: 0.02\n",
      "iteration: 125250 loss: 0.0026 lr: 0.02\n",
      "iteration: 125260 loss: 0.0021 lr: 0.02\n",
      "iteration: 125270 loss: 0.0036 lr: 0.02\n",
      "iteration: 125280 loss: 0.0031 lr: 0.02\n",
      "iteration: 125290 loss: 0.0037 lr: 0.02\n",
      "iteration: 125300 loss: 0.0046 lr: 0.02\n",
      "iteration: 125310 loss: 0.0029 lr: 0.02\n",
      "iteration: 125320 loss: 0.0025 lr: 0.02\n",
      "iteration: 125330 loss: 0.0027 lr: 0.02\n",
      "iteration: 125340 loss: 0.0025 lr: 0.02\n",
      "iteration: 125350 loss: 0.0028 lr: 0.02\n",
      "iteration: 125360 loss: 0.0043 lr: 0.02\n",
      "iteration: 125370 loss: 0.0032 lr: 0.02\n",
      "iteration: 125380 loss: 0.0041 lr: 0.02\n",
      "iteration: 125390 loss: 0.0031 lr: 0.02\n",
      "iteration: 125400 loss: 0.0025 lr: 0.02\n",
      "iteration: 125410 loss: 0.0031 lr: 0.02\n",
      "iteration: 125420 loss: 0.0024 lr: 0.02\n",
      "iteration: 125430 loss: 0.0025 lr: 0.02\n",
      "iteration: 125440 loss: 0.0019 lr: 0.02\n",
      "iteration: 125450 loss: 0.0030 lr: 0.02\n",
      "iteration: 125460 loss: 0.0025 lr: 0.02\n",
      "iteration: 125470 loss: 0.0023 lr: 0.02\n",
      "iteration: 125480 loss: 0.0034 lr: 0.02\n",
      "iteration: 125490 loss: 0.0034 lr: 0.02\n",
      "iteration: 125500 loss: 0.0046 lr: 0.02\n",
      "iteration: 125510 loss: 0.0023 lr: 0.02\n",
      "iteration: 125520 loss: 0.0034 lr: 0.02\n",
      "iteration: 125530 loss: 0.0031 lr: 0.02\n",
      "iteration: 125540 loss: 0.0027 lr: 0.02\n",
      "iteration: 125550 loss: 0.0022 lr: 0.02\n",
      "iteration: 125560 loss: 0.0024 lr: 0.02\n",
      "iteration: 125570 loss: 0.0026 lr: 0.02\n",
      "iteration: 125580 loss: 0.0035 lr: 0.02\n",
      "iteration: 125590 loss: 0.0031 lr: 0.02\n",
      "iteration: 125600 loss: 0.0037 lr: 0.02\n",
      "iteration: 125610 loss: 0.0030 lr: 0.02\n",
      "iteration: 125620 loss: 0.0031 lr: 0.02\n",
      "iteration: 125630 loss: 0.0028 lr: 0.02\n",
      "iteration: 125640 loss: 0.0027 lr: 0.02\n",
      "iteration: 125650 loss: 0.0027 lr: 0.02\n",
      "iteration: 125660 loss: 0.0031 lr: 0.02\n",
      "iteration: 125670 loss: 0.0024 lr: 0.02\n",
      "iteration: 125680 loss: 0.0033 lr: 0.02\n",
      "iteration: 125690 loss: 0.0021 lr: 0.02\n",
      "iteration: 125700 loss: 0.0027 lr: 0.02\n",
      "iteration: 125710 loss: 0.0030 lr: 0.02\n",
      "iteration: 125720 loss: 0.0025 lr: 0.02\n",
      "iteration: 125730 loss: 0.0024 lr: 0.02\n",
      "iteration: 125740 loss: 0.0036 lr: 0.02\n",
      "iteration: 125750 loss: 0.0021 lr: 0.02\n",
      "iteration: 125760 loss: 0.0025 lr: 0.02\n",
      "iteration: 125770 loss: 0.0021 lr: 0.02\n",
      "iteration: 125780 loss: 0.0030 lr: 0.02\n",
      "iteration: 125790 loss: 0.0031 lr: 0.02\n",
      "iteration: 125800 loss: 0.0024 lr: 0.02\n",
      "iteration: 125810 loss: 0.0019 lr: 0.02\n",
      "iteration: 125820 loss: 0.0020 lr: 0.02\n",
      "iteration: 125830 loss: 0.0026 lr: 0.02\n",
      "iteration: 125840 loss: 0.0022 lr: 0.02\n",
      "iteration: 125850 loss: 0.0029 lr: 0.02\n",
      "iteration: 125860 loss: 0.0034 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 125870 loss: 0.0024 lr: 0.02\n",
      "iteration: 125880 loss: 0.0027 lr: 0.02\n",
      "iteration: 125890 loss: 0.0029 lr: 0.02\n",
      "iteration: 125900 loss: 0.0030 lr: 0.02\n",
      "iteration: 125910 loss: 0.0023 lr: 0.02\n",
      "iteration: 125920 loss: 0.0028 lr: 0.02\n",
      "iteration: 125930 loss: 0.0025 lr: 0.02\n",
      "iteration: 125940 loss: 0.0033 lr: 0.02\n",
      "iteration: 125950 loss: 0.0024 lr: 0.02\n",
      "iteration: 125960 loss: 0.0029 lr: 0.02\n",
      "iteration: 125970 loss: 0.0027 lr: 0.02\n",
      "iteration: 125980 loss: 0.0029 lr: 0.02\n",
      "iteration: 125990 loss: 0.0024 lr: 0.02\n",
      "iteration: 126000 loss: 0.0022 lr: 0.02\n",
      "iteration: 126010 loss: 0.0030 lr: 0.02\n",
      "iteration: 126020 loss: 0.0027 lr: 0.02\n",
      "iteration: 126030 loss: 0.0030 lr: 0.02\n",
      "iteration: 126040 loss: 0.0026 lr: 0.02\n",
      "iteration: 126050 loss: 0.0030 lr: 0.02\n",
      "iteration: 126060 loss: 0.0045 lr: 0.02\n",
      "iteration: 126070 loss: 0.0026 lr: 0.02\n",
      "iteration: 126080 loss: 0.0028 lr: 0.02\n",
      "iteration: 126090 loss: 0.0030 lr: 0.02\n",
      "iteration: 126100 loss: 0.0041 lr: 0.02\n",
      "iteration: 126110 loss: 0.0025 lr: 0.02\n",
      "iteration: 126120 loss: 0.0030 lr: 0.02\n",
      "iteration: 126130 loss: 0.0027 lr: 0.02\n",
      "iteration: 126140 loss: 0.0025 lr: 0.02\n",
      "iteration: 126150 loss: 0.0034 lr: 0.02\n",
      "iteration: 126160 loss: 0.0019 lr: 0.02\n",
      "iteration: 126170 loss: 0.0022 lr: 0.02\n",
      "iteration: 126180 loss: 0.0026 lr: 0.02\n",
      "iteration: 126190 loss: 0.0018 lr: 0.02\n",
      "iteration: 126200 loss: 0.0022 lr: 0.02\n",
      "iteration: 126210 loss: 0.0026 lr: 0.02\n",
      "iteration: 126220 loss: 0.0027 lr: 0.02\n",
      "iteration: 126230 loss: 0.0030 lr: 0.02\n",
      "iteration: 126240 loss: 0.0038 lr: 0.02\n",
      "iteration: 126250 loss: 0.0022 lr: 0.02\n",
      "iteration: 126260 loss: 0.0033 lr: 0.02\n",
      "iteration: 126270 loss: 0.0029 lr: 0.02\n",
      "iteration: 126280 loss: 0.0030 lr: 0.02\n",
      "iteration: 126290 loss: 0.0024 lr: 0.02\n",
      "iteration: 126300 loss: 0.0018 lr: 0.02\n",
      "iteration: 126310 loss: 0.0027 lr: 0.02\n",
      "iteration: 126320 loss: 0.0030 lr: 0.02\n",
      "iteration: 126330 loss: 0.0030 lr: 0.02\n",
      "iteration: 126340 loss: 0.0023 lr: 0.02\n",
      "iteration: 126350 loss: 0.0022 lr: 0.02\n",
      "iteration: 126360 loss: 0.0024 lr: 0.02\n",
      "iteration: 126370 loss: 0.0033 lr: 0.02\n",
      "iteration: 126380 loss: 0.0029 lr: 0.02\n",
      "iteration: 126390 loss: 0.0032 lr: 0.02\n",
      "iteration: 126400 loss: 0.0032 lr: 0.02\n",
      "iteration: 126410 loss: 0.0020 lr: 0.02\n",
      "iteration: 126420 loss: 0.0021 lr: 0.02\n",
      "iteration: 126430 loss: 0.0040 lr: 0.02\n",
      "iteration: 126440 loss: 0.0022 lr: 0.02\n",
      "iteration: 126450 loss: 0.0036 lr: 0.02\n",
      "iteration: 126460 loss: 0.0027 lr: 0.02\n",
      "iteration: 126470 loss: 0.0026 lr: 0.02\n",
      "iteration: 126480 loss: 0.0039 lr: 0.02\n",
      "iteration: 126490 loss: 0.0024 lr: 0.02\n",
      "iteration: 126500 loss: 0.0028 lr: 0.02\n",
      "iteration: 126510 loss: 0.0033 lr: 0.02\n",
      "iteration: 126520 loss: 0.0022 lr: 0.02\n",
      "iteration: 126530 loss: 0.0027 lr: 0.02\n",
      "iteration: 126540 loss: 0.0026 lr: 0.02\n",
      "iteration: 126550 loss: 0.0021 lr: 0.02\n",
      "iteration: 126560 loss: 0.0027 lr: 0.02\n",
      "iteration: 126570 loss: 0.0025 lr: 0.02\n",
      "iteration: 126580 loss: 0.0030 lr: 0.02\n",
      "iteration: 126590 loss: 0.0028 lr: 0.02\n",
      "iteration: 126600 loss: 0.0040 lr: 0.02\n",
      "iteration: 126610 loss: 0.0038 lr: 0.02\n",
      "iteration: 126620 loss: 0.0026 lr: 0.02\n",
      "iteration: 126630 loss: 0.0025 lr: 0.02\n",
      "iteration: 126640 loss: 0.0028 lr: 0.02\n",
      "iteration: 126650 loss: 0.0039 lr: 0.02\n",
      "iteration: 126660 loss: 0.0025 lr: 0.02\n",
      "iteration: 126670 loss: 0.0039 lr: 0.02\n",
      "iteration: 126680 loss: 0.0025 lr: 0.02\n",
      "iteration: 126690 loss: 0.0040 lr: 0.02\n",
      "iteration: 126700 loss: 0.0030 lr: 0.02\n",
      "iteration: 126710 loss: 0.0023 lr: 0.02\n",
      "iteration: 126720 loss: 0.0031 lr: 0.02\n",
      "iteration: 126730 loss: 0.0022 lr: 0.02\n",
      "iteration: 126740 loss: 0.0034 lr: 0.02\n",
      "iteration: 126750 loss: 0.0026 lr: 0.02\n",
      "iteration: 126760 loss: 0.0025 lr: 0.02\n",
      "iteration: 126770 loss: 0.0020 lr: 0.02\n",
      "iteration: 126780 loss: 0.0024 lr: 0.02\n",
      "iteration: 126790 loss: 0.0021 lr: 0.02\n",
      "iteration: 126800 loss: 0.0019 lr: 0.02\n",
      "iteration: 126810 loss: 0.0019 lr: 0.02\n",
      "iteration: 126820 loss: 0.0025 lr: 0.02\n",
      "iteration: 126830 loss: 0.0031 lr: 0.02\n",
      "iteration: 126840 loss: 0.0026 lr: 0.02\n",
      "iteration: 126850 loss: 0.0025 lr: 0.02\n",
      "iteration: 126860 loss: 0.0025 lr: 0.02\n",
      "iteration: 126870 loss: 0.0040 lr: 0.02\n",
      "iteration: 126880 loss: 0.0032 lr: 0.02\n",
      "iteration: 126890 loss: 0.0019 lr: 0.02\n",
      "iteration: 126900 loss: 0.0036 lr: 0.02\n",
      "iteration: 126910 loss: 0.0035 lr: 0.02\n",
      "iteration: 126920 loss: 0.0036 lr: 0.02\n",
      "iteration: 126930 loss: 0.0027 lr: 0.02\n",
      "iteration: 126940 loss: 0.0033 lr: 0.02\n",
      "iteration: 126950 loss: 0.0035 lr: 0.02\n",
      "iteration: 126960 loss: 0.0023 lr: 0.02\n",
      "iteration: 126970 loss: 0.0032 lr: 0.02\n",
      "iteration: 126980 loss: 0.0025 lr: 0.02\n",
      "iteration: 126990 loss: 0.0030 lr: 0.02\n",
      "iteration: 127000 loss: 0.0032 lr: 0.02\n",
      "iteration: 127010 loss: 0.0028 lr: 0.02\n",
      "iteration: 127020 loss: 0.0024 lr: 0.02\n",
      "iteration: 127030 loss: 0.0025 lr: 0.02\n",
      "iteration: 127040 loss: 0.0035 lr: 0.02\n",
      "iteration: 127050 loss: 0.0024 lr: 0.02\n",
      "iteration: 127060 loss: 0.0021 lr: 0.02\n",
      "iteration: 127070 loss: 0.0035 lr: 0.02\n",
      "iteration: 127080 loss: 0.0037 lr: 0.02\n",
      "iteration: 127090 loss: 0.0039 lr: 0.02\n",
      "iteration: 127100 loss: 0.0025 lr: 0.02\n",
      "iteration: 127110 loss: 0.0031 lr: 0.02\n",
      "iteration: 127120 loss: 0.0028 lr: 0.02\n",
      "iteration: 127130 loss: 0.0036 lr: 0.02\n",
      "iteration: 127140 loss: 0.0026 lr: 0.02\n",
      "iteration: 127150 loss: 0.0024 lr: 0.02\n",
      "iteration: 127160 loss: 0.0032 lr: 0.02\n",
      "iteration: 127170 loss: 0.0022 lr: 0.02\n",
      "iteration: 127180 loss: 0.0023 lr: 0.02\n",
      "iteration: 127190 loss: 0.0019 lr: 0.02\n",
      "iteration: 127200 loss: 0.0031 lr: 0.02\n",
      "iteration: 127210 loss: 0.0030 lr: 0.02\n",
      "iteration: 127220 loss: 0.0023 lr: 0.02\n",
      "iteration: 127230 loss: 0.0033 lr: 0.02\n",
      "iteration: 127240 loss: 0.0028 lr: 0.02\n",
      "iteration: 127250 loss: 0.0021 lr: 0.02\n",
      "iteration: 127260 loss: 0.0049 lr: 0.02\n",
      "iteration: 127270 loss: 0.0024 lr: 0.02\n",
      "iteration: 127280 loss: 0.0046 lr: 0.02\n",
      "iteration: 127290 loss: 0.0023 lr: 0.02\n",
      "iteration: 127300 loss: 0.0045 lr: 0.02\n",
      "iteration: 127310 loss: 0.0030 lr: 0.02\n",
      "iteration: 127320 loss: 0.0022 lr: 0.02\n",
      "iteration: 127330 loss: 0.0023 lr: 0.02\n",
      "iteration: 127340 loss: 0.0032 lr: 0.02\n",
      "iteration: 127350 loss: 0.0029 lr: 0.02\n",
      "iteration: 127360 loss: 0.0034 lr: 0.02\n",
      "iteration: 127370 loss: 0.0033 lr: 0.02\n",
      "iteration: 127380 loss: 0.0036 lr: 0.02\n",
      "iteration: 127390 loss: 0.0025 lr: 0.02\n",
      "iteration: 127400 loss: 0.0021 lr: 0.02\n",
      "iteration: 127410 loss: 0.0034 lr: 0.02\n",
      "iteration: 127420 loss: 0.0032 lr: 0.02\n",
      "iteration: 127430 loss: 0.0025 lr: 0.02\n",
      "iteration: 127440 loss: 0.0028 lr: 0.02\n",
      "iteration: 127450 loss: 0.0026 lr: 0.02\n",
      "iteration: 127460 loss: 0.0026 lr: 0.02\n",
      "iteration: 127470 loss: 0.0027 lr: 0.02\n",
      "iteration: 127480 loss: 0.0023 lr: 0.02\n",
      "iteration: 127490 loss: 0.0030 lr: 0.02\n",
      "iteration: 127500 loss: 0.0025 lr: 0.02\n",
      "iteration: 127510 loss: 0.0037 lr: 0.02\n",
      "iteration: 127520 loss: 0.0039 lr: 0.02\n",
      "iteration: 127530 loss: 0.0049 lr: 0.02\n",
      "iteration: 127540 loss: 0.0033 lr: 0.02\n",
      "iteration: 127550 loss: 0.0032 lr: 0.02\n",
      "iteration: 127560 loss: 0.0034 lr: 0.02\n",
      "iteration: 127570 loss: 0.0027 lr: 0.02\n",
      "iteration: 127580 loss: 0.0033 lr: 0.02\n",
      "iteration: 127590 loss: 0.0035 lr: 0.02\n",
      "iteration: 127600 loss: 0.0022 lr: 0.02\n",
      "iteration: 127610 loss: 0.0022 lr: 0.02\n",
      "iteration: 127620 loss: 0.0027 lr: 0.02\n",
      "iteration: 127630 loss: 0.0043 lr: 0.02\n",
      "iteration: 127640 loss: 0.0025 lr: 0.02\n",
      "iteration: 127650 loss: 0.0028 lr: 0.02\n",
      "iteration: 127660 loss: 0.0029 lr: 0.02\n",
      "iteration: 127670 loss: 0.0024 lr: 0.02\n",
      "iteration: 127680 loss: 0.0026 lr: 0.02\n",
      "iteration: 127690 loss: 0.0025 lr: 0.02\n",
      "iteration: 127700 loss: 0.0022 lr: 0.02\n",
      "iteration: 127710 loss: 0.0030 lr: 0.02\n",
      "iteration: 127720 loss: 0.0028 lr: 0.02\n",
      "iteration: 127730 loss: 0.0042 lr: 0.02\n",
      "iteration: 127740 loss: 0.0037 lr: 0.02\n",
      "iteration: 127750 loss: 0.0036 lr: 0.02\n",
      "iteration: 127760 loss: 0.0025 lr: 0.02\n",
      "iteration: 127770 loss: 0.0021 lr: 0.02\n",
      "iteration: 127780 loss: 0.0023 lr: 0.02\n",
      "iteration: 127790 loss: 0.0035 lr: 0.02\n",
      "iteration: 127800 loss: 0.0024 lr: 0.02\n",
      "iteration: 127810 loss: 0.0022 lr: 0.02\n",
      "iteration: 127820 loss: 0.0032 lr: 0.02\n",
      "iteration: 127830 loss: 0.0035 lr: 0.02\n",
      "iteration: 127840 loss: 0.0031 lr: 0.02\n",
      "iteration: 127850 loss: 0.0020 lr: 0.02\n",
      "iteration: 127860 loss: 0.0030 lr: 0.02\n",
      "iteration: 127870 loss: 0.0032 lr: 0.02\n",
      "iteration: 127880 loss: 0.0026 lr: 0.02\n",
      "iteration: 127890 loss: 0.0033 lr: 0.02\n",
      "iteration: 127900 loss: 0.0040 lr: 0.02\n",
      "iteration: 127910 loss: 0.0023 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 127920 loss: 0.0047 lr: 0.02\n",
      "iteration: 127930 loss: 0.0032 lr: 0.02\n",
      "iteration: 127940 loss: 0.0024 lr: 0.02\n",
      "iteration: 127950 loss: 0.0020 lr: 0.02\n",
      "iteration: 127960 loss: 0.0027 lr: 0.02\n",
      "iteration: 127970 loss: 0.0038 lr: 0.02\n",
      "iteration: 127980 loss: 0.0026 lr: 0.02\n",
      "iteration: 127990 loss: 0.0022 lr: 0.02\n",
      "iteration: 128000 loss: 0.0026 lr: 0.02\n",
      "iteration: 128010 loss: 0.0033 lr: 0.02\n",
      "iteration: 128020 loss: 0.0024 lr: 0.02\n",
      "iteration: 128030 loss: 0.0024 lr: 0.02\n",
      "iteration: 128040 loss: 0.0024 lr: 0.02\n",
      "iteration: 128050 loss: 0.0031 lr: 0.02\n",
      "iteration: 128060 loss: 0.0037 lr: 0.02\n",
      "iteration: 128070 loss: 0.0024 lr: 0.02\n",
      "iteration: 128080 loss: 0.0035 lr: 0.02\n",
      "iteration: 128090 loss: 0.0030 lr: 0.02\n",
      "iteration: 128100 loss: 0.0038 lr: 0.02\n",
      "iteration: 128110 loss: 0.0035 lr: 0.02\n",
      "iteration: 128120 loss: 0.0022 lr: 0.02\n",
      "iteration: 128130 loss: 0.0032 lr: 0.02\n",
      "iteration: 128140 loss: 0.0042 lr: 0.02\n",
      "iteration: 128150 loss: 0.0025 lr: 0.02\n",
      "iteration: 128160 loss: 0.0026 lr: 0.02\n",
      "iteration: 128170 loss: 0.0039 lr: 0.02\n",
      "iteration: 128180 loss: 0.0027 lr: 0.02\n",
      "iteration: 128190 loss: 0.0022 lr: 0.02\n",
      "iteration: 128200 loss: 0.0048 lr: 0.02\n",
      "iteration: 128210 loss: 0.0024 lr: 0.02\n",
      "iteration: 128220 loss: 0.0033 lr: 0.02\n",
      "iteration: 128230 loss: 0.0030 lr: 0.02\n",
      "iteration: 128240 loss: 0.0024 lr: 0.02\n",
      "iteration: 128250 loss: 0.0035 lr: 0.02\n",
      "iteration: 128260 loss: 0.0033 lr: 0.02\n",
      "iteration: 128270 loss: 0.0019 lr: 0.02\n",
      "iteration: 128280 loss: 0.0020 lr: 0.02\n",
      "iteration: 128290 loss: 0.0018 lr: 0.02\n",
      "iteration: 128300 loss: 0.0027 lr: 0.02\n",
      "iteration: 128310 loss: 0.0027 lr: 0.02\n",
      "iteration: 128320 loss: 0.0029 lr: 0.02\n",
      "iteration: 128330 loss: 0.0022 lr: 0.02\n",
      "iteration: 128340 loss: 0.0024 lr: 0.02\n",
      "iteration: 128350 loss: 0.0020 lr: 0.02\n",
      "iteration: 128360 loss: 0.0026 lr: 0.02\n",
      "iteration: 128370 loss: 0.0040 lr: 0.02\n",
      "iteration: 128380 loss: 0.0036 lr: 0.02\n",
      "iteration: 128390 loss: 0.0029 lr: 0.02\n",
      "iteration: 128400 loss: 0.0025 lr: 0.02\n",
      "iteration: 128410 loss: 0.0030 lr: 0.02\n",
      "iteration: 128420 loss: 0.0033 lr: 0.02\n",
      "iteration: 128430 loss: 0.0032 lr: 0.02\n",
      "iteration: 128440 loss: 0.0027 lr: 0.02\n",
      "iteration: 128450 loss: 0.0033 lr: 0.02\n",
      "iteration: 128460 loss: 0.0044 lr: 0.02\n",
      "iteration: 128470 loss: 0.0029 lr: 0.02\n",
      "iteration: 128480 loss: 0.0033 lr: 0.02\n",
      "iteration: 128490 loss: 0.0039 lr: 0.02\n",
      "iteration: 128500 loss: 0.0033 lr: 0.02\n",
      "iteration: 128510 loss: 0.0041 lr: 0.02\n",
      "iteration: 128520 loss: 0.0025 lr: 0.02\n",
      "iteration: 128530 loss: 0.0047 lr: 0.02\n",
      "iteration: 128540 loss: 0.0025 lr: 0.02\n",
      "iteration: 128550 loss: 0.0031 lr: 0.02\n",
      "iteration: 128560 loss: 0.0034 lr: 0.02\n",
      "iteration: 128570 loss: 0.0020 lr: 0.02\n",
      "iteration: 128580 loss: 0.0030 lr: 0.02\n",
      "iteration: 128590 loss: 0.0018 lr: 0.02\n",
      "iteration: 128600 loss: 0.0024 lr: 0.02\n",
      "iteration: 128610 loss: 0.0044 lr: 0.02\n",
      "iteration: 128620 loss: 0.0037 lr: 0.02\n",
      "iteration: 128630 loss: 0.0028 lr: 0.02\n",
      "iteration: 128640 loss: 0.0030 lr: 0.02\n",
      "iteration: 128650 loss: 0.0034 lr: 0.02\n",
      "iteration: 128660 loss: 0.0031 lr: 0.02\n",
      "iteration: 128670 loss: 0.0024 lr: 0.02\n",
      "iteration: 128680 loss: 0.0015 lr: 0.02\n",
      "iteration: 128690 loss: 0.0039 lr: 0.02\n",
      "iteration: 128700 loss: 0.0035 lr: 0.02\n",
      "iteration: 128710 loss: 0.0026 lr: 0.02\n",
      "iteration: 128720 loss: 0.0029 lr: 0.02\n",
      "iteration: 128730 loss: 0.0037 lr: 0.02\n",
      "iteration: 128740 loss: 0.0031 lr: 0.02\n",
      "iteration: 128750 loss: 0.0036 lr: 0.02\n",
      "iteration: 128760 loss: 0.0027 lr: 0.02\n",
      "iteration: 128770 loss: 0.0027 lr: 0.02\n",
      "iteration: 128780 loss: 0.0025 lr: 0.02\n",
      "iteration: 128790 loss: 0.0035 lr: 0.02\n",
      "iteration: 128800 loss: 0.0029 lr: 0.02\n",
      "iteration: 128810 loss: 0.0029 lr: 0.02\n",
      "iteration: 128820 loss: 0.0024 lr: 0.02\n",
      "iteration: 128830 loss: 0.0020 lr: 0.02\n",
      "iteration: 128840 loss: 0.0029 lr: 0.02\n",
      "iteration: 128850 loss: 0.0021 lr: 0.02\n",
      "iteration: 128860 loss: 0.0023 lr: 0.02\n",
      "iteration: 128870 loss: 0.0030 lr: 0.02\n",
      "iteration: 128880 loss: 0.0021 lr: 0.02\n",
      "iteration: 128890 loss: 0.0020 lr: 0.02\n",
      "iteration: 128900 loss: 0.0028 lr: 0.02\n",
      "iteration: 128910 loss: 0.0026 lr: 0.02\n",
      "iteration: 128920 loss: 0.0031 lr: 0.02\n",
      "iteration: 128930 loss: 0.0032 lr: 0.02\n",
      "iteration: 128940 loss: 0.0022 lr: 0.02\n",
      "iteration: 128950 loss: 0.0026 lr: 0.02\n",
      "iteration: 128960 loss: 0.0041 lr: 0.02\n",
      "iteration: 128970 loss: 0.0020 lr: 0.02\n",
      "iteration: 128980 loss: 0.0041 lr: 0.02\n",
      "iteration: 128990 loss: 0.0023 lr: 0.02\n",
      "iteration: 129000 loss: 0.0035 lr: 0.02\n",
      "iteration: 129010 loss: 0.0032 lr: 0.02\n",
      "iteration: 129020 loss: 0.0026 lr: 0.02\n",
      "iteration: 129030 loss: 0.0044 lr: 0.02\n",
      "iteration: 129040 loss: 0.0028 lr: 0.02\n",
      "iteration: 129050 loss: 0.0030 lr: 0.02\n",
      "iteration: 129060 loss: 0.0032 lr: 0.02\n",
      "iteration: 129070 loss: 0.0038 lr: 0.02\n",
      "iteration: 129080 loss: 0.0023 lr: 0.02\n",
      "iteration: 129090 loss: 0.0038 lr: 0.02\n",
      "iteration: 129100 loss: 0.0047 lr: 0.02\n",
      "iteration: 129110 loss: 0.0030 lr: 0.02\n",
      "iteration: 129120 loss: 0.0022 lr: 0.02\n",
      "iteration: 129130 loss: 0.0037 lr: 0.02\n",
      "iteration: 129140 loss: 0.0030 lr: 0.02\n",
      "iteration: 129150 loss: 0.0031 lr: 0.02\n",
      "iteration: 129160 loss: 0.0030 lr: 0.02\n",
      "iteration: 129170 loss: 0.0021 lr: 0.02\n",
      "iteration: 129180 loss: 0.0025 lr: 0.02\n",
      "iteration: 129190 loss: 0.0025 lr: 0.02\n",
      "iteration: 129200 loss: 0.0025 lr: 0.02\n",
      "iteration: 129210 loss: 0.0023 lr: 0.02\n",
      "iteration: 129220 loss: 0.0023 lr: 0.02\n",
      "iteration: 129230 loss: 0.0026 lr: 0.02\n",
      "iteration: 129240 loss: 0.0024 lr: 0.02\n",
      "iteration: 129250 loss: 0.0032 lr: 0.02\n",
      "iteration: 129260 loss: 0.0025 lr: 0.02\n",
      "iteration: 129270 loss: 0.0028 lr: 0.02\n",
      "iteration: 129280 loss: 0.0029 lr: 0.02\n",
      "iteration: 129290 loss: 0.0018 lr: 0.02\n",
      "iteration: 129300 loss: 0.0025 lr: 0.02\n",
      "iteration: 129310 loss: 0.0021 lr: 0.02\n",
      "iteration: 129320 loss: 0.0042 lr: 0.02\n",
      "iteration: 129330 loss: 0.0029 lr: 0.02\n",
      "iteration: 129340 loss: 0.0032 lr: 0.02\n",
      "iteration: 129350 loss: 0.0019 lr: 0.02\n",
      "iteration: 129360 loss: 0.0028 lr: 0.02\n",
      "iteration: 129370 loss: 0.0024 lr: 0.02\n",
      "iteration: 129380 loss: 0.0023 lr: 0.02\n",
      "iteration: 129390 loss: 0.0027 lr: 0.02\n",
      "iteration: 129400 loss: 0.0035 lr: 0.02\n",
      "iteration: 129410 loss: 0.0027 lr: 0.02\n",
      "iteration: 129420 loss: 0.0020 lr: 0.02\n",
      "iteration: 129430 loss: 0.0028 lr: 0.02\n",
      "iteration: 129440 loss: 0.0039 lr: 0.02\n",
      "iteration: 129450 loss: 0.0020 lr: 0.02\n",
      "iteration: 129460 loss: 0.0024 lr: 0.02\n",
      "iteration: 129470 loss: 0.0023 lr: 0.02\n",
      "iteration: 129480 loss: 0.0024 lr: 0.02\n",
      "iteration: 129490 loss: 0.0025 lr: 0.02\n",
      "iteration: 129500 loss: 0.0025 lr: 0.02\n",
      "iteration: 129510 loss: 0.0022 lr: 0.02\n",
      "iteration: 129520 loss: 0.0025 lr: 0.02\n",
      "iteration: 129530 loss: 0.0032 lr: 0.02\n",
      "iteration: 129540 loss: 0.0023 lr: 0.02\n",
      "iteration: 129550 loss: 0.0024 lr: 0.02\n",
      "iteration: 129560 loss: 0.0023 lr: 0.02\n",
      "iteration: 129570 loss: 0.0023 lr: 0.02\n",
      "iteration: 129580 loss: 0.0027 lr: 0.02\n",
      "iteration: 129590 loss: 0.0027 lr: 0.02\n",
      "iteration: 129600 loss: 0.0043 lr: 0.02\n",
      "iteration: 129610 loss: 0.0029 lr: 0.02\n",
      "iteration: 129620 loss: 0.0028 lr: 0.02\n",
      "iteration: 129630 loss: 0.0037 lr: 0.02\n",
      "iteration: 129640 loss: 0.0025 lr: 0.02\n",
      "iteration: 129650 loss: 0.0022 lr: 0.02\n",
      "iteration: 129660 loss: 0.0023 lr: 0.02\n",
      "iteration: 129670 loss: 0.0029 lr: 0.02\n",
      "iteration: 129680 loss: 0.0025 lr: 0.02\n",
      "iteration: 129690 loss: 0.0023 lr: 0.02\n",
      "iteration: 129700 loss: 0.0029 lr: 0.02\n",
      "iteration: 129710 loss: 0.0035 lr: 0.02\n",
      "iteration: 129720 loss: 0.0019 lr: 0.02\n",
      "iteration: 129730 loss: 0.0037 lr: 0.02\n",
      "iteration: 129740 loss: 0.0031 lr: 0.02\n",
      "iteration: 129750 loss: 0.0027 lr: 0.02\n",
      "iteration: 129760 loss: 0.0034 lr: 0.02\n",
      "iteration: 129770 loss: 0.0023 lr: 0.02\n",
      "iteration: 129780 loss: 0.0031 lr: 0.02\n",
      "iteration: 129790 loss: 0.0026 lr: 0.02\n",
      "iteration: 129800 loss: 0.0025 lr: 0.02\n",
      "iteration: 129810 loss: 0.0027 lr: 0.02\n",
      "iteration: 129820 loss: 0.0022 lr: 0.02\n",
      "iteration: 129830 loss: 0.0027 lr: 0.02\n",
      "iteration: 129840 loss: 0.0022 lr: 0.02\n",
      "iteration: 129850 loss: 0.0027 lr: 0.02\n",
      "iteration: 129860 loss: 0.0045 lr: 0.02\n",
      "iteration: 129870 loss: 0.0050 lr: 0.02\n",
      "iteration: 129880 loss: 0.0026 lr: 0.02\n",
      "iteration: 129890 loss: 0.0051 lr: 0.02\n",
      "iteration: 129900 loss: 0.0030 lr: 0.02\n",
      "iteration: 129910 loss: 0.0028 lr: 0.02\n",
      "iteration: 129920 loss: 0.0034 lr: 0.02\n",
      "iteration: 129930 loss: 0.0030 lr: 0.02\n",
      "iteration: 129940 loss: 0.0039 lr: 0.02\n",
      "iteration: 129950 loss: 0.0026 lr: 0.02\n",
      "iteration: 129960 loss: 0.0022 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 129970 loss: 0.0034 lr: 0.02\n",
      "iteration: 129980 loss: 0.0035 lr: 0.02\n",
      "iteration: 129990 loss: 0.0025 lr: 0.02\n",
      "iteration: 130000 loss: 0.0037 lr: 0.02\n",
      "iteration: 130010 loss: 0.0025 lr: 0.02\n",
      "iteration: 130020 loss: 0.0027 lr: 0.02\n",
      "iteration: 130030 loss: 0.0038 lr: 0.02\n",
      "iteration: 130040 loss: 0.0026 lr: 0.02\n",
      "iteration: 130050 loss: 0.0029 lr: 0.02\n",
      "iteration: 130060 loss: 0.0023 lr: 0.02\n",
      "iteration: 130070 loss: 0.0025 lr: 0.02\n",
      "iteration: 130080 loss: 0.0041 lr: 0.02\n",
      "iteration: 130090 loss: 0.0018 lr: 0.02\n",
      "iteration: 130100 loss: 0.0025 lr: 0.02\n",
      "iteration: 130110 loss: 0.0031 lr: 0.02\n",
      "iteration: 130120 loss: 0.0021 lr: 0.02\n",
      "iteration: 130130 loss: 0.0027 lr: 0.02\n",
      "iteration: 130140 loss: 0.0021 lr: 0.02\n",
      "iteration: 130150 loss: 0.0026 lr: 0.02\n",
      "iteration: 130160 loss: 0.0033 lr: 0.02\n",
      "iteration: 130170 loss: 0.0022 lr: 0.02\n",
      "iteration: 130180 loss: 0.0031 lr: 0.02\n",
      "iteration: 130190 loss: 0.0024 lr: 0.02\n",
      "iteration: 130200 loss: 0.0037 lr: 0.02\n",
      "iteration: 130210 loss: 0.0025 lr: 0.02\n",
      "iteration: 130220 loss: 0.0028 lr: 0.02\n",
      "iteration: 130230 loss: 0.0030 lr: 0.02\n",
      "iteration: 130240 loss: 0.0024 lr: 0.02\n",
      "iteration: 130250 loss: 0.0022 lr: 0.02\n",
      "iteration: 130260 loss: 0.0044 lr: 0.02\n",
      "iteration: 130270 loss: 0.0024 lr: 0.02\n",
      "iteration: 130280 loss: 0.0032 lr: 0.02\n",
      "iteration: 130290 loss: 0.0029 lr: 0.02\n",
      "iteration: 130300 loss: 0.0027 lr: 0.02\n",
      "iteration: 130310 loss: 0.0027 lr: 0.02\n",
      "iteration: 130320 loss: 0.0029 lr: 0.02\n",
      "iteration: 130330 loss: 0.0035 lr: 0.02\n",
      "iteration: 130340 loss: 0.0028 lr: 0.02\n",
      "iteration: 130350 loss: 0.0019 lr: 0.02\n",
      "iteration: 130360 loss: 0.0034 lr: 0.02\n",
      "iteration: 130370 loss: 0.0036 lr: 0.02\n",
      "iteration: 130380 loss: 0.0024 lr: 0.02\n",
      "iteration: 130390 loss: 0.0025 lr: 0.02\n",
      "iteration: 130400 loss: 0.0025 lr: 0.02\n",
      "iteration: 130410 loss: 0.0018 lr: 0.02\n",
      "iteration: 130420 loss: 0.0038 lr: 0.02\n",
      "iteration: 130430 loss: 0.0034 lr: 0.02\n",
      "iteration: 130440 loss: 0.0026 lr: 0.02\n",
      "iteration: 130450 loss: 0.0032 lr: 0.02\n",
      "iteration: 130460 loss: 0.0028 lr: 0.02\n",
      "iteration: 130470 loss: 0.0033 lr: 0.02\n",
      "iteration: 130480 loss: 0.0022 lr: 0.02\n",
      "iteration: 130490 loss: 0.0032 lr: 0.02\n",
      "iteration: 130500 loss: 0.0031 lr: 0.02\n",
      "iteration: 130510 loss: 0.0031 lr: 0.02\n",
      "iteration: 130520 loss: 0.0028 lr: 0.02\n",
      "iteration: 130530 loss: 0.0030 lr: 0.02\n",
      "iteration: 130540 loss: 0.0023 lr: 0.02\n",
      "iteration: 130550 loss: 0.0029 lr: 0.02\n",
      "iteration: 130560 loss: 0.0021 lr: 0.02\n",
      "iteration: 130570 loss: 0.0030 lr: 0.02\n",
      "iteration: 130580 loss: 0.0030 lr: 0.02\n",
      "iteration: 130590 loss: 0.0028 lr: 0.02\n",
      "iteration: 130600 loss: 0.0030 lr: 0.02\n",
      "iteration: 130610 loss: 0.0024 lr: 0.02\n",
      "iteration: 130620 loss: 0.0025 lr: 0.02\n",
      "iteration: 130630 loss: 0.0025 lr: 0.02\n",
      "iteration: 130640 loss: 0.0022 lr: 0.02\n",
      "iteration: 130650 loss: 0.0025 lr: 0.02\n",
      "iteration: 130660 loss: 0.0029 lr: 0.02\n",
      "iteration: 130670 loss: 0.0023 lr: 0.02\n",
      "iteration: 130680 loss: 0.0038 lr: 0.02\n",
      "iteration: 130690 loss: 0.0021 lr: 0.02\n",
      "iteration: 130700 loss: 0.0021 lr: 0.02\n",
      "iteration: 130710 loss: 0.0039 lr: 0.02\n",
      "iteration: 130720 loss: 0.0041 lr: 0.02\n",
      "iteration: 130730 loss: 0.0042 lr: 0.02\n",
      "iteration: 130740 loss: 0.0034 lr: 0.02\n",
      "iteration: 130750 loss: 0.0026 lr: 0.02\n",
      "iteration: 130760 loss: 0.0027 lr: 0.02\n",
      "iteration: 130770 loss: 0.0023 lr: 0.02\n",
      "iteration: 130780 loss: 0.0019 lr: 0.02\n",
      "iteration: 130790 loss: 0.0039 lr: 0.02\n",
      "iteration: 130800 loss: 0.0026 lr: 0.02\n",
      "iteration: 130810 loss: 0.0027 lr: 0.02\n",
      "iteration: 130820 loss: 0.0031 lr: 0.02\n",
      "iteration: 130830 loss: 0.0033 lr: 0.02\n",
      "iteration: 130840 loss: 0.0024 lr: 0.02\n",
      "iteration: 130850 loss: 0.0025 lr: 0.02\n",
      "iteration: 130860 loss: 0.0035 lr: 0.02\n",
      "iteration: 130870 loss: 0.0032 lr: 0.02\n",
      "iteration: 130880 loss: 0.0030 lr: 0.02\n",
      "iteration: 130890 loss: 0.0022 lr: 0.02\n",
      "iteration: 130900 loss: 0.0021 lr: 0.02\n",
      "iteration: 130910 loss: 0.0032 lr: 0.02\n",
      "iteration: 130920 loss: 0.0024 lr: 0.02\n",
      "iteration: 130930 loss: 0.0024 lr: 0.02\n",
      "iteration: 130940 loss: 0.0025 lr: 0.02\n",
      "iteration: 130950 loss: 0.0021 lr: 0.02\n",
      "iteration: 130960 loss: 0.0032 lr: 0.02\n",
      "iteration: 130970 loss: 0.0024 lr: 0.02\n",
      "iteration: 130980 loss: 0.0026 lr: 0.02\n",
      "iteration: 130990 loss: 0.0023 lr: 0.02\n",
      "iteration: 131000 loss: 0.0023 lr: 0.02\n",
      "iteration: 131010 loss: 0.0043 lr: 0.02\n",
      "iteration: 131020 loss: 0.0035 lr: 0.02\n",
      "iteration: 131030 loss: 0.0028 lr: 0.02\n",
      "iteration: 131040 loss: 0.0021 lr: 0.02\n",
      "iteration: 131050 loss: 0.0032 lr: 0.02\n",
      "iteration: 131060 loss: 0.0032 lr: 0.02\n",
      "iteration: 131070 loss: 0.0018 lr: 0.02\n",
      "iteration: 131080 loss: 0.0026 lr: 0.02\n",
      "iteration: 131090 loss: 0.0024 lr: 0.02\n",
      "iteration: 131100 loss: 0.0047 lr: 0.02\n",
      "iteration: 131110 loss: 0.0029 lr: 0.02\n",
      "iteration: 131120 loss: 0.0021 lr: 0.02\n",
      "iteration: 131130 loss: 0.0026 lr: 0.02\n",
      "iteration: 131140 loss: 0.0022 lr: 0.02\n",
      "iteration: 131150 loss: 0.0031 lr: 0.02\n",
      "iteration: 131160 loss: 0.0022 lr: 0.02\n",
      "iteration: 131170 loss: 0.0036 lr: 0.02\n",
      "iteration: 131180 loss: 0.0026 lr: 0.02\n",
      "iteration: 131190 loss: 0.0027 lr: 0.02\n",
      "iteration: 131200 loss: 0.0026 lr: 0.02\n",
      "iteration: 131210 loss: 0.0034 lr: 0.02\n",
      "iteration: 131220 loss: 0.0032 lr: 0.02\n",
      "iteration: 131230 loss: 0.0024 lr: 0.02\n",
      "iteration: 131240 loss: 0.0019 lr: 0.02\n",
      "iteration: 131250 loss: 0.0029 lr: 0.02\n",
      "iteration: 131260 loss: 0.0018 lr: 0.02\n",
      "iteration: 131270 loss: 0.0020 lr: 0.02\n",
      "iteration: 131280 loss: 0.0019 lr: 0.02\n",
      "iteration: 131290 loss: 0.0031 lr: 0.02\n",
      "iteration: 131300 loss: 0.0021 lr: 0.02\n",
      "iteration: 131310 loss: 0.0020 lr: 0.02\n",
      "iteration: 131320 loss: 0.0019 lr: 0.02\n",
      "iteration: 131330 loss: 0.0031 lr: 0.02\n",
      "iteration: 131340 loss: 0.0025 lr: 0.02\n",
      "iteration: 131350 loss: 0.0027 lr: 0.02\n",
      "iteration: 131360 loss: 0.0025 lr: 0.02\n",
      "iteration: 131370 loss: 0.0025 lr: 0.02\n",
      "iteration: 131380 loss: 0.0034 lr: 0.02\n",
      "iteration: 131390 loss: 0.0034 lr: 0.02\n",
      "iteration: 131400 loss: 0.0027 lr: 0.02\n",
      "iteration: 131410 loss: 0.0032 lr: 0.02\n",
      "iteration: 131420 loss: 0.0021 lr: 0.02\n",
      "iteration: 131430 loss: 0.0031 lr: 0.02\n",
      "iteration: 131440 loss: 0.0022 lr: 0.02\n",
      "iteration: 131450 loss: 0.0028 lr: 0.02\n",
      "iteration: 131460 loss: 0.0025 lr: 0.02\n",
      "iteration: 131470 loss: 0.0028 lr: 0.02\n",
      "iteration: 131480 loss: 0.0032 lr: 0.02\n",
      "iteration: 131490 loss: 0.0023 lr: 0.02\n",
      "iteration: 131500 loss: 0.0044 lr: 0.02\n",
      "iteration: 131510 loss: 0.0029 lr: 0.02\n",
      "iteration: 131520 loss: 0.0019 lr: 0.02\n",
      "iteration: 131530 loss: 0.0039 lr: 0.02\n",
      "iteration: 131540 loss: 0.0035 lr: 0.02\n",
      "iteration: 131550 loss: 0.0027 lr: 0.02\n",
      "iteration: 131560 loss: 0.0057 lr: 0.02\n",
      "iteration: 131570 loss: 0.0031 lr: 0.02\n",
      "iteration: 131580 loss: 0.0027 lr: 0.02\n",
      "iteration: 131590 loss: 0.0031 lr: 0.02\n",
      "iteration: 131600 loss: 0.0026 lr: 0.02\n",
      "iteration: 131610 loss: 0.0052 lr: 0.02\n",
      "iteration: 131620 loss: 0.0030 lr: 0.02\n",
      "iteration: 131630 loss: 0.0027 lr: 0.02\n",
      "iteration: 131640 loss: 0.0027 lr: 0.02\n",
      "iteration: 131650 loss: 0.0024 lr: 0.02\n",
      "iteration: 131660 loss: 0.0031 lr: 0.02\n",
      "iteration: 131670 loss: 0.0035 lr: 0.02\n",
      "iteration: 131680 loss: 0.0018 lr: 0.02\n",
      "iteration: 131690 loss: 0.0022 lr: 0.02\n",
      "iteration: 131700 loss: 0.0026 lr: 0.02\n",
      "iteration: 131710 loss: 0.0034 lr: 0.02\n",
      "iteration: 131720 loss: 0.0035 lr: 0.02\n",
      "iteration: 131730 loss: 0.0031 lr: 0.02\n",
      "iteration: 131740 loss: 0.0030 lr: 0.02\n",
      "iteration: 131750 loss: 0.0046 lr: 0.02\n",
      "iteration: 131760 loss: 0.0023 lr: 0.02\n",
      "iteration: 131770 loss: 0.0041 lr: 0.02\n",
      "iteration: 131780 loss: 0.0025 lr: 0.02\n",
      "iteration: 131790 loss: 0.0033 lr: 0.02\n",
      "iteration: 131800 loss: 0.0028 lr: 0.02\n",
      "iteration: 131810 loss: 0.0022 lr: 0.02\n",
      "iteration: 131820 loss: 0.0018 lr: 0.02\n",
      "iteration: 131830 loss: 0.0033 lr: 0.02\n",
      "iteration: 131840 loss: 0.0024 lr: 0.02\n",
      "iteration: 131850 loss: 0.0022 lr: 0.02\n",
      "iteration: 131860 loss: 0.0022 lr: 0.02\n",
      "iteration: 131870 loss: 0.0032 lr: 0.02\n",
      "iteration: 131880 loss: 0.0031 lr: 0.02\n",
      "iteration: 131890 loss: 0.0039 lr: 0.02\n",
      "iteration: 131900 loss: 0.0036 lr: 0.02\n",
      "iteration: 131910 loss: 0.0024 lr: 0.02\n",
      "iteration: 131920 loss: 0.0023 lr: 0.02\n",
      "iteration: 131930 loss: 0.0025 lr: 0.02\n",
      "iteration: 131940 loss: 0.0025 lr: 0.02\n",
      "iteration: 131950 loss: 0.0030 lr: 0.02\n",
      "iteration: 131960 loss: 0.0024 lr: 0.02\n",
      "iteration: 131970 loss: 0.0035 lr: 0.02\n",
      "iteration: 131980 loss: 0.0024 lr: 0.02\n",
      "iteration: 131990 loss: 0.0036 lr: 0.02\n",
      "iteration: 132000 loss: 0.0025 lr: 0.02\n",
      "iteration: 132010 loss: 0.0032 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 132020 loss: 0.0031 lr: 0.02\n",
      "iteration: 132030 loss: 0.0023 lr: 0.02\n",
      "iteration: 132040 loss: 0.0034 lr: 0.02\n",
      "iteration: 132050 loss: 0.0026 lr: 0.02\n",
      "iteration: 132060 loss: 0.0035 lr: 0.02\n",
      "iteration: 132070 loss: 0.0028 lr: 0.02\n",
      "iteration: 132080 loss: 0.0040 lr: 0.02\n",
      "iteration: 132090 loss: 0.0042 lr: 0.02\n",
      "iteration: 132100 loss: 0.0036 lr: 0.02\n",
      "iteration: 132110 loss: 0.0021 lr: 0.02\n",
      "iteration: 132120 loss: 0.0030 lr: 0.02\n",
      "iteration: 132130 loss: 0.0028 lr: 0.02\n",
      "iteration: 132140 loss: 0.0025 lr: 0.02\n",
      "iteration: 132150 loss: 0.0024 lr: 0.02\n",
      "iteration: 132160 loss: 0.0031 lr: 0.02\n",
      "iteration: 132170 loss: 0.0041 lr: 0.02\n",
      "iteration: 132180 loss: 0.0044 lr: 0.02\n",
      "iteration: 132190 loss: 0.0037 lr: 0.02\n",
      "iteration: 132200 loss: 0.0025 lr: 0.02\n",
      "iteration: 132210 loss: 0.0045 lr: 0.02\n",
      "iteration: 132220 loss: 0.0033 lr: 0.02\n",
      "iteration: 132230 loss: 0.0021 lr: 0.02\n",
      "iteration: 132240 loss: 0.0039 lr: 0.02\n",
      "iteration: 132250 loss: 0.0037 lr: 0.02\n",
      "iteration: 132260 loss: 0.0039 lr: 0.02\n",
      "iteration: 132270 loss: 0.0046 lr: 0.02\n",
      "iteration: 132280 loss: 0.0023 lr: 0.02\n",
      "iteration: 132290 loss: 0.0033 lr: 0.02\n",
      "iteration: 132300 loss: 0.0027 lr: 0.02\n",
      "iteration: 132310 loss: 0.0030 lr: 0.02\n",
      "iteration: 132320 loss: 0.0025 lr: 0.02\n",
      "iteration: 132330 loss: 0.0026 lr: 0.02\n",
      "iteration: 132340 loss: 0.0024 lr: 0.02\n",
      "iteration: 132350 loss: 0.0026 lr: 0.02\n",
      "iteration: 132360 loss: 0.0030 lr: 0.02\n",
      "iteration: 132370 loss: 0.0039 lr: 0.02\n",
      "iteration: 132380 loss: 0.0026 lr: 0.02\n",
      "iteration: 132390 loss: 0.0033 lr: 0.02\n",
      "iteration: 132400 loss: 0.0036 lr: 0.02\n",
      "iteration: 132410 loss: 0.0021 lr: 0.02\n",
      "iteration: 132420 loss: 0.0028 lr: 0.02\n",
      "iteration: 132430 loss: 0.0037 lr: 0.02\n",
      "iteration: 132440 loss: 0.0025 lr: 0.02\n",
      "iteration: 132450 loss: 0.0030 lr: 0.02\n",
      "iteration: 132460 loss: 0.0031 lr: 0.02\n",
      "iteration: 132470 loss: 0.0023 lr: 0.02\n",
      "iteration: 132480 loss: 0.0025 lr: 0.02\n",
      "iteration: 132490 loss: 0.0030 lr: 0.02\n",
      "iteration: 132500 loss: 0.0029 lr: 0.02\n",
      "iteration: 132510 loss: 0.0030 lr: 0.02\n",
      "iteration: 132520 loss: 0.0026 lr: 0.02\n",
      "iteration: 132530 loss: 0.0022 lr: 0.02\n",
      "iteration: 132540 loss: 0.0030 lr: 0.02\n",
      "iteration: 132550 loss: 0.0035 lr: 0.02\n",
      "iteration: 132560 loss: 0.0036 lr: 0.02\n",
      "iteration: 132570 loss: 0.0031 lr: 0.02\n",
      "iteration: 132580 loss: 0.0048 lr: 0.02\n",
      "iteration: 132590 loss: 0.0027 lr: 0.02\n",
      "iteration: 132600 loss: 0.0029 lr: 0.02\n",
      "iteration: 132610 loss: 0.0022 lr: 0.02\n",
      "iteration: 132620 loss: 0.0028 lr: 0.02\n",
      "iteration: 132630 loss: 0.0041 lr: 0.02\n",
      "iteration: 132640 loss: 0.0021 lr: 0.02\n",
      "iteration: 132650 loss: 0.0023 lr: 0.02\n",
      "iteration: 132660 loss: 0.0032 lr: 0.02\n",
      "iteration: 132670 loss: 0.0024 lr: 0.02\n",
      "iteration: 132680 loss: 0.0027 lr: 0.02\n",
      "iteration: 132690 loss: 0.0029 lr: 0.02\n",
      "iteration: 132700 loss: 0.0025 lr: 0.02\n",
      "iteration: 132710 loss: 0.0026 lr: 0.02\n",
      "iteration: 132720 loss: 0.0030 lr: 0.02\n",
      "iteration: 132730 loss: 0.0026 lr: 0.02\n",
      "iteration: 132740 loss: 0.0028 lr: 0.02\n",
      "iteration: 132750 loss: 0.0030 lr: 0.02\n",
      "iteration: 132760 loss: 0.0030 lr: 0.02\n",
      "iteration: 132770 loss: 0.0026 lr: 0.02\n",
      "iteration: 132780 loss: 0.0025 lr: 0.02\n",
      "iteration: 132790 loss: 0.0022 lr: 0.02\n",
      "iteration: 132800 loss: 0.0027 lr: 0.02\n",
      "iteration: 132810 loss: 0.0035 lr: 0.02\n",
      "iteration: 132820 loss: 0.0030 lr: 0.02\n",
      "iteration: 132830 loss: 0.0024 lr: 0.02\n",
      "iteration: 132840 loss: 0.0028 lr: 0.02\n",
      "iteration: 132850 loss: 0.0037 lr: 0.02\n",
      "iteration: 132860 loss: 0.0023 lr: 0.02\n",
      "iteration: 132870 loss: 0.0033 lr: 0.02\n",
      "iteration: 132880 loss: 0.0023 lr: 0.02\n",
      "iteration: 132890 loss: 0.0021 lr: 0.02\n",
      "iteration: 132900 loss: 0.0042 lr: 0.02\n",
      "iteration: 132910 loss: 0.0031 lr: 0.02\n",
      "iteration: 132920 loss: 0.0027 lr: 0.02\n",
      "iteration: 132930 loss: 0.0031 lr: 0.02\n",
      "iteration: 132940 loss: 0.0035 lr: 0.02\n",
      "iteration: 132950 loss: 0.0033 lr: 0.02\n",
      "iteration: 132960 loss: 0.0027 lr: 0.02\n",
      "iteration: 132970 loss: 0.0028 lr: 0.02\n",
      "iteration: 132980 loss: 0.0029 lr: 0.02\n",
      "iteration: 132990 loss: 0.0031 lr: 0.02\n",
      "iteration: 133000 loss: 0.0028 lr: 0.02\n",
      "iteration: 133010 loss: 0.0022 lr: 0.02\n",
      "iteration: 133020 loss: 0.0021 lr: 0.02\n",
      "iteration: 133030 loss: 0.0028 lr: 0.02\n",
      "iteration: 133040 loss: 0.0027 lr: 0.02\n",
      "iteration: 133050 loss: 0.0026 lr: 0.02\n",
      "iteration: 133060 loss: 0.0028 lr: 0.02\n",
      "iteration: 133070 loss: 0.0035 lr: 0.02\n",
      "iteration: 133080 loss: 0.0027 lr: 0.02\n",
      "iteration: 133090 loss: 0.0028 lr: 0.02\n",
      "iteration: 133100 loss: 0.0031 lr: 0.02\n",
      "iteration: 133110 loss: 0.0023 lr: 0.02\n",
      "iteration: 133120 loss: 0.0023 lr: 0.02\n",
      "iteration: 133130 loss: 0.0021 lr: 0.02\n",
      "iteration: 133140 loss: 0.0028 lr: 0.02\n",
      "iteration: 133150 loss: 0.0023 lr: 0.02\n",
      "iteration: 133160 loss: 0.0030 lr: 0.02\n",
      "iteration: 133170 loss: 0.0024 lr: 0.02\n",
      "iteration: 133180 loss: 0.0023 lr: 0.02\n",
      "iteration: 133190 loss: 0.0032 lr: 0.02\n",
      "iteration: 133200 loss: 0.0021 lr: 0.02\n",
      "iteration: 133210 loss: 0.0029 lr: 0.02\n",
      "iteration: 133220 loss: 0.0024 lr: 0.02\n",
      "iteration: 133230 loss: 0.0016 lr: 0.02\n",
      "iteration: 133240 loss: 0.0029 lr: 0.02\n",
      "iteration: 133250 loss: 0.0024 lr: 0.02\n",
      "iteration: 133260 loss: 0.0027 lr: 0.02\n",
      "iteration: 133270 loss: 0.0041 lr: 0.02\n",
      "iteration: 133280 loss: 0.0024 lr: 0.02\n",
      "iteration: 133290 loss: 0.0025 lr: 0.02\n",
      "iteration: 133300 loss: 0.0021 lr: 0.02\n",
      "iteration: 133310 loss: 0.0024 lr: 0.02\n",
      "iteration: 133320 loss: 0.0031 lr: 0.02\n",
      "iteration: 133330 loss: 0.0027 lr: 0.02\n",
      "iteration: 133340 loss: 0.0030 lr: 0.02\n",
      "iteration: 133350 loss: 0.0035 lr: 0.02\n",
      "iteration: 133360 loss: 0.0032 lr: 0.02\n",
      "iteration: 133370 loss: 0.0020 lr: 0.02\n",
      "iteration: 133380 loss: 0.0027 lr: 0.02\n",
      "iteration: 133390 loss: 0.0021 lr: 0.02\n",
      "iteration: 133400 loss: 0.0026 lr: 0.02\n",
      "iteration: 133410 loss: 0.0021 lr: 0.02\n",
      "iteration: 133420 loss: 0.0035 lr: 0.02\n",
      "iteration: 133430 loss: 0.0028 lr: 0.02\n",
      "iteration: 133440 loss: 0.0020 lr: 0.02\n",
      "iteration: 133450 loss: 0.0022 lr: 0.02\n",
      "iteration: 133460 loss: 0.0022 lr: 0.02\n",
      "iteration: 133470 loss: 0.0033 lr: 0.02\n",
      "iteration: 133480 loss: 0.0021 lr: 0.02\n",
      "iteration: 133490 loss: 0.0037 lr: 0.02\n",
      "iteration: 133500 loss: 0.0023 lr: 0.02\n",
      "iteration: 133510 loss: 0.0023 lr: 0.02\n",
      "iteration: 133520 loss: 0.0040 lr: 0.02\n",
      "iteration: 133530 loss: 0.0032 lr: 0.02\n",
      "iteration: 133540 loss: 0.0026 lr: 0.02\n",
      "iteration: 133550 loss: 0.0029 lr: 0.02\n",
      "iteration: 133560 loss: 0.0032 lr: 0.02\n",
      "iteration: 133570 loss: 0.0020 lr: 0.02\n",
      "iteration: 133580 loss: 0.0025 lr: 0.02\n",
      "iteration: 133590 loss: 0.0017 lr: 0.02\n",
      "iteration: 133600 loss: 0.0019 lr: 0.02\n",
      "iteration: 133610 loss: 0.0027 lr: 0.02\n",
      "iteration: 133620 loss: 0.0027 lr: 0.02\n",
      "iteration: 133630 loss: 0.0023 lr: 0.02\n",
      "iteration: 133640 loss: 0.0035 lr: 0.02\n",
      "iteration: 133650 loss: 0.0026 lr: 0.02\n",
      "iteration: 133660 loss: 0.0025 lr: 0.02\n",
      "iteration: 133670 loss: 0.0019 lr: 0.02\n",
      "iteration: 133680 loss: 0.0018 lr: 0.02\n",
      "iteration: 133690 loss: 0.0019 lr: 0.02\n",
      "iteration: 133700 loss: 0.0016 lr: 0.02\n",
      "iteration: 133710 loss: 0.0019 lr: 0.02\n",
      "iteration: 133720 loss: 0.0028 lr: 0.02\n",
      "iteration: 133730 loss: 0.0021 lr: 0.02\n",
      "iteration: 133740 loss: 0.0024 lr: 0.02\n",
      "iteration: 133750 loss: 0.0023 lr: 0.02\n",
      "iteration: 133760 loss: 0.0026 lr: 0.02\n",
      "iteration: 133770 loss: 0.0043 lr: 0.02\n",
      "iteration: 133780 loss: 0.0029 lr: 0.02\n",
      "iteration: 133790 loss: 0.0029 lr: 0.02\n",
      "iteration: 133800 loss: 0.0026 lr: 0.02\n",
      "iteration: 133810 loss: 0.0026 lr: 0.02\n",
      "iteration: 133820 loss: 0.0024 lr: 0.02\n",
      "iteration: 133830 loss: 0.0033 lr: 0.02\n",
      "iteration: 133840 loss: 0.0032 lr: 0.02\n",
      "iteration: 133850 loss: 0.0032 lr: 0.02\n",
      "iteration: 133860 loss: 0.0032 lr: 0.02\n",
      "iteration: 133870 loss: 0.0036 lr: 0.02\n",
      "iteration: 133880 loss: 0.0037 lr: 0.02\n",
      "iteration: 133890 loss: 0.0035 lr: 0.02\n",
      "iteration: 133900 loss: 0.0022 lr: 0.02\n",
      "iteration: 133910 loss: 0.0029 lr: 0.02\n",
      "iteration: 133920 loss: 0.0021 lr: 0.02\n",
      "iteration: 133930 loss: 0.0022 lr: 0.02\n",
      "iteration: 133940 loss: 0.0026 lr: 0.02\n",
      "iteration: 133950 loss: 0.0030 lr: 0.02\n",
      "iteration: 133960 loss: 0.0034 lr: 0.02\n",
      "iteration: 133970 loss: 0.0027 lr: 0.02\n",
      "iteration: 133980 loss: 0.0032 lr: 0.02\n",
      "iteration: 133990 loss: 0.0028 lr: 0.02\n",
      "iteration: 134000 loss: 0.0022 lr: 0.02\n",
      "iteration: 134010 loss: 0.0023 lr: 0.02\n",
      "iteration: 134020 loss: 0.0028 lr: 0.02\n",
      "iteration: 134030 loss: 0.0023 lr: 0.02\n",
      "iteration: 134040 loss: 0.0026 lr: 0.02\n",
      "iteration: 134050 loss: 0.0033 lr: 0.02\n",
      "iteration: 134060 loss: 0.0025 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 134070 loss: 0.0022 lr: 0.02\n",
      "iteration: 134080 loss: 0.0024 lr: 0.02\n",
      "iteration: 134090 loss: 0.0034 lr: 0.02\n",
      "iteration: 134100 loss: 0.0026 lr: 0.02\n",
      "iteration: 134110 loss: 0.0020 lr: 0.02\n",
      "iteration: 134120 loss: 0.0021 lr: 0.02\n",
      "iteration: 134130 loss: 0.0028 lr: 0.02\n",
      "iteration: 134140 loss: 0.0027 lr: 0.02\n",
      "iteration: 134150 loss: 0.0039 lr: 0.02\n",
      "iteration: 134160 loss: 0.0043 lr: 0.02\n",
      "iteration: 134170 loss: 0.0024 lr: 0.02\n",
      "iteration: 134180 loss: 0.0021 lr: 0.02\n",
      "iteration: 134190 loss: 0.0024 lr: 0.02\n",
      "iteration: 134200 loss: 0.0021 lr: 0.02\n",
      "iteration: 134210 loss: 0.0027 lr: 0.02\n",
      "iteration: 134220 loss: 0.0021 lr: 0.02\n",
      "iteration: 134230 loss: 0.0027 lr: 0.02\n",
      "iteration: 134240 loss: 0.0027 lr: 0.02\n",
      "iteration: 134250 loss: 0.0024 lr: 0.02\n",
      "iteration: 134260 loss: 0.0022 lr: 0.02\n",
      "iteration: 134270 loss: 0.0019 lr: 0.02\n",
      "iteration: 134280 loss: 0.0030 lr: 0.02\n",
      "iteration: 134290 loss: 0.0029 lr: 0.02\n",
      "iteration: 134300 loss: 0.0028 lr: 0.02\n",
      "iteration: 134310 loss: 0.0027 lr: 0.02\n",
      "iteration: 134320 loss: 0.0031 lr: 0.02\n",
      "iteration: 134330 loss: 0.0034 lr: 0.02\n",
      "iteration: 134340 loss: 0.0021 lr: 0.02\n",
      "iteration: 134350 loss: 0.0026 lr: 0.02\n",
      "iteration: 134360 loss: 0.0026 lr: 0.02\n",
      "iteration: 134370 loss: 0.0020 lr: 0.02\n",
      "iteration: 134380 loss: 0.0022 lr: 0.02\n",
      "iteration: 134390 loss: 0.0021 lr: 0.02\n",
      "iteration: 134400 loss: 0.0025 lr: 0.02\n",
      "iteration: 134410 loss: 0.0043 lr: 0.02\n",
      "iteration: 134420 loss: 0.0024 lr: 0.02\n",
      "iteration: 134430 loss: 0.0031 lr: 0.02\n",
      "iteration: 134440 loss: 0.0041 lr: 0.02\n",
      "iteration: 134450 loss: 0.0027 lr: 0.02\n",
      "iteration: 134460 loss: 0.0034 lr: 0.02\n",
      "iteration: 134470 loss: 0.0023 lr: 0.02\n",
      "iteration: 134480 loss: 0.0028 lr: 0.02\n",
      "iteration: 134490 loss: 0.0034 lr: 0.02\n",
      "iteration: 134500 loss: 0.0033 lr: 0.02\n",
      "iteration: 134510 loss: 0.0029 lr: 0.02\n",
      "iteration: 134520 loss: 0.0028 lr: 0.02\n",
      "iteration: 134530 loss: 0.0029 lr: 0.02\n",
      "iteration: 134540 loss: 0.0037 lr: 0.02\n",
      "iteration: 134550 loss: 0.0030 lr: 0.02\n",
      "iteration: 134560 loss: 0.0029 lr: 0.02\n",
      "iteration: 134570 loss: 0.0024 lr: 0.02\n",
      "iteration: 134580 loss: 0.0028 lr: 0.02\n",
      "iteration: 134590 loss: 0.0048 lr: 0.02\n",
      "iteration: 134600 loss: 0.0030 lr: 0.02\n",
      "iteration: 134610 loss: 0.0034 lr: 0.02\n",
      "iteration: 134620 loss: 0.0027 lr: 0.02\n",
      "iteration: 134630 loss: 0.0029 lr: 0.02\n",
      "iteration: 134640 loss: 0.0024 lr: 0.02\n",
      "iteration: 134650 loss: 0.0034 lr: 0.02\n",
      "iteration: 134660 loss: 0.0030 lr: 0.02\n",
      "iteration: 134670 loss: 0.0028 lr: 0.02\n",
      "iteration: 134680 loss: 0.0031 lr: 0.02\n",
      "iteration: 134690 loss: 0.0031 lr: 0.02\n",
      "iteration: 134700 loss: 0.0038 lr: 0.02\n",
      "iteration: 134710 loss: 0.0023 lr: 0.02\n",
      "iteration: 134720 loss: 0.0051 lr: 0.02\n",
      "iteration: 134730 loss: 0.0026 lr: 0.02\n",
      "iteration: 134740 loss: 0.0024 lr: 0.02\n",
      "iteration: 134750 loss: 0.0025 lr: 0.02\n",
      "iteration: 134760 loss: 0.0020 lr: 0.02\n",
      "iteration: 134770 loss: 0.0022 lr: 0.02\n",
      "iteration: 134780 loss: 0.0037 lr: 0.02\n",
      "iteration: 134790 loss: 0.0037 lr: 0.02\n",
      "iteration: 134800 loss: 0.0034 lr: 0.02\n",
      "iteration: 134810 loss: 0.0022 lr: 0.02\n",
      "iteration: 134820 loss: 0.0028 lr: 0.02\n",
      "iteration: 134830 loss: 0.0027 lr: 0.02\n",
      "iteration: 134840 loss: 0.0022 lr: 0.02\n",
      "iteration: 134850 loss: 0.0026 lr: 0.02\n",
      "iteration: 134860 loss: 0.0021 lr: 0.02\n",
      "iteration: 134870 loss: 0.0032 lr: 0.02\n",
      "iteration: 134880 loss: 0.0021 lr: 0.02\n",
      "iteration: 134890 loss: 0.0020 lr: 0.02\n",
      "iteration: 134900 loss: 0.0021 lr: 0.02\n",
      "iteration: 134910 loss: 0.0021 lr: 0.02\n",
      "iteration: 134920 loss: 0.0028 lr: 0.02\n",
      "iteration: 134930 loss: 0.0031 lr: 0.02\n",
      "iteration: 134940 loss: 0.0031 lr: 0.02\n",
      "iteration: 134950 loss: 0.0033 lr: 0.02\n",
      "iteration: 134960 loss: 0.0036 lr: 0.02\n",
      "iteration: 134970 loss: 0.0033 lr: 0.02\n",
      "iteration: 134980 loss: 0.0018 lr: 0.02\n",
      "iteration: 134990 loss: 0.0025 lr: 0.02\n",
      "iteration: 135000 loss: 0.0025 lr: 0.02\n",
      "iteration: 135010 loss: 0.0040 lr: 0.02\n",
      "iteration: 135020 loss: 0.0037 lr: 0.02\n",
      "iteration: 135030 loss: 0.0029 lr: 0.02\n",
      "iteration: 135040 loss: 0.0034 lr: 0.02\n",
      "iteration: 135050 loss: 0.0031 lr: 0.02\n",
      "iteration: 135060 loss: 0.0031 lr: 0.02\n",
      "iteration: 135070 loss: 0.0029 lr: 0.02\n",
      "iteration: 135080 loss: 0.0023 lr: 0.02\n",
      "iteration: 135090 loss: 0.0040 lr: 0.02\n",
      "iteration: 135100 loss: 0.0034 lr: 0.02\n",
      "iteration: 135110 loss: 0.0036 lr: 0.02\n",
      "iteration: 135120 loss: 0.0019 lr: 0.02\n",
      "iteration: 135130 loss: 0.0025 lr: 0.02\n",
      "iteration: 135140 loss: 0.0022 lr: 0.02\n",
      "iteration: 135150 loss: 0.0025 lr: 0.02\n",
      "iteration: 135160 loss: 0.0027 lr: 0.02\n",
      "iteration: 135170 loss: 0.0033 lr: 0.02\n",
      "iteration: 135180 loss: 0.0021 lr: 0.02\n",
      "iteration: 135190 loss: 0.0023 lr: 0.02\n",
      "iteration: 135200 loss: 0.0031 lr: 0.02\n",
      "iteration: 135210 loss: 0.0023 lr: 0.02\n",
      "iteration: 135220 loss: 0.0024 lr: 0.02\n",
      "iteration: 135230 loss: 0.0025 lr: 0.02\n",
      "iteration: 135240 loss: 0.0025 lr: 0.02\n",
      "iteration: 135250 loss: 0.0035 lr: 0.02\n",
      "iteration: 135260 loss: 0.0026 lr: 0.02\n",
      "iteration: 135270 loss: 0.0027 lr: 0.02\n",
      "iteration: 135280 loss: 0.0029 lr: 0.02\n",
      "iteration: 135290 loss: 0.0023 lr: 0.02\n",
      "iteration: 135300 loss: 0.0030 lr: 0.02\n",
      "iteration: 135310 loss: 0.0030 lr: 0.02\n",
      "iteration: 135320 loss: 0.0033 lr: 0.02\n",
      "iteration: 135330 loss: 0.0031 lr: 0.02\n",
      "iteration: 135340 loss: 0.0020 lr: 0.02\n",
      "iteration: 135350 loss: 0.0037 lr: 0.02\n",
      "iteration: 135360 loss: 0.0033 lr: 0.02\n",
      "iteration: 135370 loss: 0.0036 lr: 0.02\n",
      "iteration: 135380 loss: 0.0029 lr: 0.02\n",
      "iteration: 135390 loss: 0.0033 lr: 0.02\n",
      "iteration: 135400 loss: 0.0024 lr: 0.02\n",
      "iteration: 135410 loss: 0.0030 lr: 0.02\n",
      "iteration: 135420 loss: 0.0024 lr: 0.02\n",
      "iteration: 135430 loss: 0.0034 lr: 0.02\n",
      "iteration: 135440 loss: 0.0028 lr: 0.02\n",
      "iteration: 135450 loss: 0.0025 lr: 0.02\n",
      "iteration: 135460 loss: 0.0019 lr: 0.02\n",
      "iteration: 135470 loss: 0.0029 lr: 0.02\n",
      "iteration: 135480 loss: 0.0027 lr: 0.02\n",
      "iteration: 135490 loss: 0.0025 lr: 0.02\n",
      "iteration: 135500 loss: 0.0022 lr: 0.02\n",
      "iteration: 135510 loss: 0.0037 lr: 0.02\n",
      "iteration: 135520 loss: 0.0021 lr: 0.02\n",
      "iteration: 135530 loss: 0.0030 lr: 0.02\n",
      "iteration: 135540 loss: 0.0029 lr: 0.02\n",
      "iteration: 135550 loss: 0.0030 lr: 0.02\n",
      "iteration: 135560 loss: 0.0030 lr: 0.02\n",
      "iteration: 135570 loss: 0.0031 lr: 0.02\n",
      "iteration: 135580 loss: 0.0027 lr: 0.02\n",
      "iteration: 135590 loss: 0.0032 lr: 0.02\n",
      "iteration: 135600 loss: 0.0026 lr: 0.02\n",
      "iteration: 135610 loss: 0.0025 lr: 0.02\n",
      "iteration: 135620 loss: 0.0022 lr: 0.02\n",
      "iteration: 135630 loss: 0.0021 lr: 0.02\n",
      "iteration: 135640 loss: 0.0039 lr: 0.02\n",
      "iteration: 135650 loss: 0.0027 lr: 0.02\n",
      "iteration: 135660 loss: 0.0025 lr: 0.02\n",
      "iteration: 135670 loss: 0.0021 lr: 0.02\n",
      "iteration: 135680 loss: 0.0031 lr: 0.02\n",
      "iteration: 135690 loss: 0.0017 lr: 0.02\n",
      "iteration: 135700 loss: 0.0031 lr: 0.02\n",
      "iteration: 135710 loss: 0.0022 lr: 0.02\n",
      "iteration: 135720 loss: 0.0022 lr: 0.02\n",
      "iteration: 135730 loss: 0.0041 lr: 0.02\n",
      "iteration: 135740 loss: 0.0029 lr: 0.02\n",
      "iteration: 135750 loss: 0.0029 lr: 0.02\n",
      "iteration: 135760 loss: 0.0031 lr: 0.02\n",
      "iteration: 135770 loss: 0.0030 lr: 0.02\n",
      "iteration: 135780 loss: 0.0020 lr: 0.02\n",
      "iteration: 135790 loss: 0.0031 lr: 0.02\n",
      "iteration: 135800 loss: 0.0026 lr: 0.02\n",
      "iteration: 135810 loss: 0.0021 lr: 0.02\n",
      "iteration: 135820 loss: 0.0029 lr: 0.02\n",
      "iteration: 135830 loss: 0.0026 lr: 0.02\n",
      "iteration: 135840 loss: 0.0026 lr: 0.02\n",
      "iteration: 135850 loss: 0.0035 lr: 0.02\n",
      "iteration: 135860 loss: 0.0026 lr: 0.02\n",
      "iteration: 135870 loss: 0.0028 lr: 0.02\n",
      "iteration: 135880 loss: 0.0038 lr: 0.02\n",
      "iteration: 135890 loss: 0.0022 lr: 0.02\n",
      "iteration: 135900 loss: 0.0022 lr: 0.02\n",
      "iteration: 135910 loss: 0.0025 lr: 0.02\n",
      "iteration: 135920 loss: 0.0021 lr: 0.02\n",
      "iteration: 135930 loss: 0.0020 lr: 0.02\n",
      "iteration: 135940 loss: 0.0027 lr: 0.02\n",
      "iteration: 135950 loss: 0.0023 lr: 0.02\n",
      "iteration: 135960 loss: 0.0021 lr: 0.02\n",
      "iteration: 135970 loss: 0.0021 lr: 0.02\n",
      "iteration: 135980 loss: 0.0024 lr: 0.02\n",
      "iteration: 135990 loss: 0.0033 lr: 0.02\n",
      "iteration: 136000 loss: 0.0030 lr: 0.02\n",
      "iteration: 136010 loss: 0.0032 lr: 0.02\n",
      "iteration: 136020 loss: 0.0029 lr: 0.02\n",
      "iteration: 136030 loss: 0.0038 lr: 0.02\n",
      "iteration: 136040 loss: 0.0024 lr: 0.02\n",
      "iteration: 136050 loss: 0.0023 lr: 0.02\n",
      "iteration: 136060 loss: 0.0021 lr: 0.02\n",
      "iteration: 136070 loss: 0.0024 lr: 0.02\n",
      "iteration: 136080 loss: 0.0026 lr: 0.02\n",
      "iteration: 136090 loss: 0.0029 lr: 0.02\n",
      "iteration: 136100 loss: 0.0023 lr: 0.02\n",
      "iteration: 136110 loss: 0.0037 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 136120 loss: 0.0026 lr: 0.02\n",
      "iteration: 136130 loss: 0.0063 lr: 0.02\n",
      "iteration: 136140 loss: 0.0045 lr: 0.02\n",
      "iteration: 136150 loss: 0.0037 lr: 0.02\n",
      "iteration: 136160 loss: 0.0020 lr: 0.02\n",
      "iteration: 136170 loss: 0.0035 lr: 0.02\n",
      "iteration: 136180 loss: 0.0028 lr: 0.02\n",
      "iteration: 136190 loss: 0.0023 lr: 0.02\n",
      "iteration: 136200 loss: 0.0027 lr: 0.02\n",
      "iteration: 136210 loss: 0.0027 lr: 0.02\n",
      "iteration: 136220 loss: 0.0036 lr: 0.02\n",
      "iteration: 136230 loss: 0.0037 lr: 0.02\n",
      "iteration: 136240 loss: 0.0020 lr: 0.02\n",
      "iteration: 136250 loss: 0.0031 lr: 0.02\n",
      "iteration: 136260 loss: 0.0040 lr: 0.02\n",
      "iteration: 136270 loss: 0.0024 lr: 0.02\n",
      "iteration: 136280 loss: 0.0029 lr: 0.02\n",
      "iteration: 136290 loss: 0.0033 lr: 0.02\n",
      "iteration: 136300 loss: 0.0026 lr: 0.02\n",
      "iteration: 136310 loss: 0.0029 lr: 0.02\n",
      "iteration: 136320 loss: 0.0027 lr: 0.02\n",
      "iteration: 136330 loss: 0.0026 lr: 0.02\n",
      "iteration: 136340 loss: 0.0041 lr: 0.02\n",
      "iteration: 136350 loss: 0.0043 lr: 0.02\n",
      "iteration: 136360 loss: 0.0021 lr: 0.02\n",
      "iteration: 136370 loss: 0.0030 lr: 0.02\n",
      "iteration: 136380 loss: 0.0020 lr: 0.02\n",
      "iteration: 136390 loss: 0.0032 lr: 0.02\n",
      "iteration: 136400 loss: 0.0025 lr: 0.02\n",
      "iteration: 136410 loss: 0.0028 lr: 0.02\n",
      "iteration: 136420 loss: 0.0030 lr: 0.02\n",
      "iteration: 136430 loss: 0.0025 lr: 0.02\n",
      "iteration: 136440 loss: 0.0023 lr: 0.02\n",
      "iteration: 136450 loss: 0.0020 lr: 0.02\n",
      "iteration: 136460 loss: 0.0020 lr: 0.02\n",
      "iteration: 136470 loss: 0.0024 lr: 0.02\n",
      "iteration: 136480 loss: 0.0026 lr: 0.02\n",
      "iteration: 136490 loss: 0.0056 lr: 0.02\n",
      "iteration: 136500 loss: 0.0026 lr: 0.02\n",
      "iteration: 136510 loss: 0.0023 lr: 0.02\n",
      "iteration: 136520 loss: 0.0029 lr: 0.02\n",
      "iteration: 136530 loss: 0.0018 lr: 0.02\n",
      "iteration: 136540 loss: 0.0028 lr: 0.02\n",
      "iteration: 136550 loss: 0.0034 lr: 0.02\n",
      "iteration: 136560 loss: 0.0016 lr: 0.02\n",
      "iteration: 136570 loss: 0.0040 lr: 0.02\n",
      "iteration: 136580 loss: 0.0026 lr: 0.02\n",
      "iteration: 136590 loss: 0.0021 lr: 0.02\n",
      "iteration: 136600 loss: 0.0045 lr: 0.02\n",
      "iteration: 136610 loss: 0.0033 lr: 0.02\n",
      "iteration: 136620 loss: 0.0030 lr: 0.02\n",
      "iteration: 136630 loss: 0.0023 lr: 0.02\n",
      "iteration: 136640 loss: 0.0033 lr: 0.02\n",
      "iteration: 136650 loss: 0.0027 lr: 0.02\n",
      "iteration: 136660 loss: 0.0024 lr: 0.02\n",
      "iteration: 136670 loss: 0.0025 lr: 0.02\n",
      "iteration: 136680 loss: 0.0040 lr: 0.02\n",
      "iteration: 136690 loss: 0.0026 lr: 0.02\n",
      "iteration: 136700 loss: 0.0026 lr: 0.02\n",
      "iteration: 136710 loss: 0.0034 lr: 0.02\n",
      "iteration: 136720 loss: 0.0029 lr: 0.02\n",
      "iteration: 136730 loss: 0.0034 lr: 0.02\n",
      "iteration: 136740 loss: 0.0029 lr: 0.02\n",
      "iteration: 136750 loss: 0.0037 lr: 0.02\n",
      "iteration: 136760 loss: 0.0025 lr: 0.02\n",
      "iteration: 136770 loss: 0.0022 lr: 0.02\n",
      "iteration: 136780 loss: 0.0030 lr: 0.02\n",
      "iteration: 136790 loss: 0.0028 lr: 0.02\n",
      "iteration: 136800 loss: 0.0024 lr: 0.02\n",
      "iteration: 136810 loss: 0.0032 lr: 0.02\n",
      "iteration: 136820 loss: 0.0027 lr: 0.02\n",
      "iteration: 136830 loss: 0.0022 lr: 0.02\n",
      "iteration: 136840 loss: 0.0027 lr: 0.02\n",
      "iteration: 136850 loss: 0.0028 lr: 0.02\n",
      "iteration: 136860 loss: 0.0027 lr: 0.02\n",
      "iteration: 136870 loss: 0.0022 lr: 0.02\n",
      "iteration: 136880 loss: 0.0020 lr: 0.02\n",
      "iteration: 136890 loss: 0.0028 lr: 0.02\n",
      "iteration: 136900 loss: 0.0028 lr: 0.02\n",
      "iteration: 136910 loss: 0.0027 lr: 0.02\n",
      "iteration: 136920 loss: 0.0021 lr: 0.02\n",
      "iteration: 136930 loss: 0.0037 lr: 0.02\n",
      "iteration: 136940 loss: 0.0027 lr: 0.02\n",
      "iteration: 136950 loss: 0.0027 lr: 0.02\n",
      "iteration: 136960 loss: 0.0023 lr: 0.02\n",
      "iteration: 136970 loss: 0.0023 lr: 0.02\n",
      "iteration: 136980 loss: 0.0024 lr: 0.02\n",
      "iteration: 136990 loss: 0.0019 lr: 0.02\n",
      "iteration: 137000 loss: 0.0027 lr: 0.02\n",
      "iteration: 137010 loss: 0.0026 lr: 0.02\n",
      "iteration: 137020 loss: 0.0022 lr: 0.02\n",
      "iteration: 137030 loss: 0.0021 lr: 0.02\n",
      "iteration: 137040 loss: 0.0029 lr: 0.02\n",
      "iteration: 137050 loss: 0.0032 lr: 0.02\n",
      "iteration: 137060 loss: 0.0021 lr: 0.02\n",
      "iteration: 137070 loss: 0.0030 lr: 0.02\n",
      "iteration: 137080 loss: 0.0032 lr: 0.02\n",
      "iteration: 137090 loss: 0.0033 lr: 0.02\n",
      "iteration: 137100 loss: 0.0037 lr: 0.02\n",
      "iteration: 137110 loss: 0.0022 lr: 0.02\n",
      "iteration: 137120 loss: 0.0027 lr: 0.02\n",
      "iteration: 137130 loss: 0.0024 lr: 0.02\n",
      "iteration: 137140 loss: 0.0026 lr: 0.02\n",
      "iteration: 137150 loss: 0.0021 lr: 0.02\n",
      "iteration: 137160 loss: 0.0025 lr: 0.02\n",
      "iteration: 137170 loss: 0.0022 lr: 0.02\n",
      "iteration: 137180 loss: 0.0035 lr: 0.02\n",
      "iteration: 137190 loss: 0.0033 lr: 0.02\n",
      "iteration: 137200 loss: 0.0025 lr: 0.02\n",
      "iteration: 137210 loss: 0.0022 lr: 0.02\n",
      "iteration: 137220 loss: 0.0021 lr: 0.02\n",
      "iteration: 137230 loss: 0.0017 lr: 0.02\n",
      "iteration: 137240 loss: 0.0040 lr: 0.02\n",
      "iteration: 137250 loss: 0.0022 lr: 0.02\n",
      "iteration: 137260 loss: 0.0022 lr: 0.02\n",
      "iteration: 137270 loss: 0.0024 lr: 0.02\n",
      "iteration: 137280 loss: 0.0017 lr: 0.02\n",
      "iteration: 137290 loss: 0.0023 lr: 0.02\n",
      "iteration: 137300 loss: 0.0025 lr: 0.02\n",
      "iteration: 137310 loss: 0.0041 lr: 0.02\n",
      "iteration: 137320 loss: 0.0029 lr: 0.02\n",
      "iteration: 137330 loss: 0.0040 lr: 0.02\n",
      "iteration: 137340 loss: 0.0023 lr: 0.02\n",
      "iteration: 137350 loss: 0.0026 lr: 0.02\n",
      "iteration: 137360 loss: 0.0038 lr: 0.02\n",
      "iteration: 137370 loss: 0.0032 lr: 0.02\n",
      "iteration: 137380 loss: 0.0022 lr: 0.02\n",
      "iteration: 137390 loss: 0.0037 lr: 0.02\n",
      "iteration: 137400 loss: 0.0032 lr: 0.02\n",
      "iteration: 137410 loss: 0.0028 lr: 0.02\n",
      "iteration: 137420 loss: 0.0039 lr: 0.02\n",
      "iteration: 137430 loss: 0.0039 lr: 0.02\n",
      "iteration: 137440 loss: 0.0033 lr: 0.02\n",
      "iteration: 137450 loss: 0.0032 lr: 0.02\n",
      "iteration: 137460 loss: 0.0030 lr: 0.02\n",
      "iteration: 137470 loss: 0.0024 lr: 0.02\n",
      "iteration: 137480 loss: 0.0023 lr: 0.02\n",
      "iteration: 137490 loss: 0.0026 lr: 0.02\n",
      "iteration: 137500 loss: 0.0017 lr: 0.02\n",
      "iteration: 137510 loss: 0.0029 lr: 0.02\n",
      "iteration: 137520 loss: 0.0033 lr: 0.02\n",
      "iteration: 137530 loss: 0.0027 lr: 0.02\n",
      "iteration: 137540 loss: 0.0024 lr: 0.02\n",
      "iteration: 137550 loss: 0.0026 lr: 0.02\n",
      "iteration: 137560 loss: 0.0028 lr: 0.02\n",
      "iteration: 137570 loss: 0.0017 lr: 0.02\n",
      "iteration: 137580 loss: 0.0043 lr: 0.02\n",
      "iteration: 137590 loss: 0.0023 lr: 0.02\n",
      "iteration: 137600 loss: 0.0025 lr: 0.02\n",
      "iteration: 137610 loss: 0.0028 lr: 0.02\n",
      "iteration: 137620 loss: 0.0020 lr: 0.02\n",
      "iteration: 137630 loss: 0.0030 lr: 0.02\n",
      "iteration: 137640 loss: 0.0027 lr: 0.02\n",
      "iteration: 137650 loss: 0.0030 lr: 0.02\n",
      "iteration: 137660 loss: 0.0028 lr: 0.02\n",
      "iteration: 137670 loss: 0.0034 lr: 0.02\n",
      "iteration: 137680 loss: 0.0022 lr: 0.02\n",
      "iteration: 137690 loss: 0.0022 lr: 0.02\n",
      "iteration: 137700 loss: 0.0020 lr: 0.02\n",
      "iteration: 137710 loss: 0.0030 lr: 0.02\n",
      "iteration: 137720 loss: 0.0035 lr: 0.02\n",
      "iteration: 137730 loss: 0.0021 lr: 0.02\n",
      "iteration: 137740 loss: 0.0028 lr: 0.02\n",
      "iteration: 137750 loss: 0.0023 lr: 0.02\n",
      "iteration: 137760 loss: 0.0032 lr: 0.02\n",
      "iteration: 137770 loss: 0.0028 lr: 0.02\n",
      "iteration: 137780 loss: 0.0028 lr: 0.02\n",
      "iteration: 137790 loss: 0.0022 lr: 0.02\n",
      "iteration: 137800 loss: 0.0029 lr: 0.02\n",
      "iteration: 137810 loss: 0.0031 lr: 0.02\n",
      "iteration: 137820 loss: 0.0024 lr: 0.02\n",
      "iteration: 137830 loss: 0.0023 lr: 0.02\n",
      "iteration: 137840 loss: 0.0018 lr: 0.02\n",
      "iteration: 137850 loss: 0.0023 lr: 0.02\n",
      "iteration: 137860 loss: 0.0024 lr: 0.02\n",
      "iteration: 137870 loss: 0.0024 lr: 0.02\n",
      "iteration: 137880 loss: 0.0033 lr: 0.02\n",
      "iteration: 137890 loss: 0.0026 lr: 0.02\n",
      "iteration: 137900 loss: 0.0019 lr: 0.02\n",
      "iteration: 137910 loss: 0.0032 lr: 0.02\n",
      "iteration: 137920 loss: 0.0030 lr: 0.02\n",
      "iteration: 137930 loss: 0.0027 lr: 0.02\n",
      "iteration: 137940 loss: 0.0031 lr: 0.02\n",
      "iteration: 137950 loss: 0.0030 lr: 0.02\n",
      "iteration: 137960 loss: 0.0034 lr: 0.02\n",
      "iteration: 137970 loss: 0.0025 lr: 0.02\n",
      "iteration: 137980 loss: 0.0018 lr: 0.02\n",
      "iteration: 137990 loss: 0.0021 lr: 0.02\n",
      "iteration: 138000 loss: 0.0026 lr: 0.02\n",
      "iteration: 138010 loss: 0.0049 lr: 0.02\n",
      "iteration: 138020 loss: 0.0036 lr: 0.02\n",
      "iteration: 138030 loss: 0.0041 lr: 0.02\n",
      "iteration: 138040 loss: 0.0047 lr: 0.02\n",
      "iteration: 138050 loss: 0.0062 lr: 0.02\n",
      "iteration: 138060 loss: 0.0015 lr: 0.02\n",
      "iteration: 138070 loss: 0.0034 lr: 0.02\n",
      "iteration: 138080 loss: 0.0032 lr: 0.02\n",
      "iteration: 138090 loss: 0.0028 lr: 0.02\n",
      "iteration: 138100 loss: 0.0032 lr: 0.02\n",
      "iteration: 138110 loss: 0.0028 lr: 0.02\n",
      "iteration: 138120 loss: 0.0021 lr: 0.02\n",
      "iteration: 138130 loss: 0.0020 lr: 0.02\n",
      "iteration: 138140 loss: 0.0021 lr: 0.02\n",
      "iteration: 138150 loss: 0.0025 lr: 0.02\n",
      "iteration: 138160 loss: 0.0030 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 138170 loss: 0.0028 lr: 0.02\n",
      "iteration: 138180 loss: 0.0027 lr: 0.02\n",
      "iteration: 138190 loss: 0.0026 lr: 0.02\n",
      "iteration: 138200 loss: 0.0030 lr: 0.02\n",
      "iteration: 138210 loss: 0.0020 lr: 0.02\n",
      "iteration: 138220 loss: 0.0025 lr: 0.02\n",
      "iteration: 138230 loss: 0.0026 lr: 0.02\n",
      "iteration: 138240 loss: 0.0022 lr: 0.02\n",
      "iteration: 138250 loss: 0.0028 lr: 0.02\n",
      "iteration: 138260 loss: 0.0020 lr: 0.02\n",
      "iteration: 138270 loss: 0.0028 lr: 0.02\n",
      "iteration: 138280 loss: 0.0028 lr: 0.02\n",
      "iteration: 138290 loss: 0.0023 lr: 0.02\n",
      "iteration: 138300 loss: 0.0018 lr: 0.02\n",
      "iteration: 138310 loss: 0.0022 lr: 0.02\n",
      "iteration: 138320 loss: 0.0029 lr: 0.02\n",
      "iteration: 138330 loss: 0.0029 lr: 0.02\n",
      "iteration: 138340 loss: 0.0023 lr: 0.02\n",
      "iteration: 138350 loss: 0.0033 lr: 0.02\n",
      "iteration: 138360 loss: 0.0026 lr: 0.02\n",
      "iteration: 138370 loss: 0.0033 lr: 0.02\n",
      "iteration: 138380 loss: 0.0027 lr: 0.02\n",
      "iteration: 138390 loss: 0.0028 lr: 0.02\n",
      "iteration: 138400 loss: 0.0039 lr: 0.02\n",
      "iteration: 138410 loss: 0.0031 lr: 0.02\n",
      "iteration: 138420 loss: 0.0032 lr: 0.02\n",
      "iteration: 138430 loss: 0.0029 lr: 0.02\n",
      "iteration: 138440 loss: 0.0021 lr: 0.02\n",
      "iteration: 138450 loss: 0.0028 lr: 0.02\n",
      "iteration: 138460 loss: 0.0031 lr: 0.02\n",
      "iteration: 138470 loss: 0.0023 lr: 0.02\n",
      "iteration: 138480 loss: 0.0034 lr: 0.02\n",
      "iteration: 138490 loss: 0.0024 lr: 0.02\n",
      "iteration: 138500 loss: 0.0027 lr: 0.02\n",
      "iteration: 138510 loss: 0.0021 lr: 0.02\n",
      "iteration: 138520 loss: 0.0022 lr: 0.02\n",
      "iteration: 138530 loss: 0.0029 lr: 0.02\n",
      "iteration: 138540 loss: 0.0027 lr: 0.02\n",
      "iteration: 138550 loss: 0.0025 lr: 0.02\n",
      "iteration: 138560 loss: 0.0022 lr: 0.02\n",
      "iteration: 138570 loss: 0.0026 lr: 0.02\n",
      "iteration: 138580 loss: 0.0026 lr: 0.02\n",
      "iteration: 138590 loss: 0.0031 lr: 0.02\n",
      "iteration: 138600 loss: 0.0031 lr: 0.02\n",
      "iteration: 138610 loss: 0.0034 lr: 0.02\n",
      "iteration: 138620 loss: 0.0021 lr: 0.02\n",
      "iteration: 138630 loss: 0.0020 lr: 0.02\n",
      "iteration: 138640 loss: 0.0024 lr: 0.02\n",
      "iteration: 138650 loss: 0.0023 lr: 0.02\n",
      "iteration: 138660 loss: 0.0019 lr: 0.02\n",
      "iteration: 138670 loss: 0.0035 lr: 0.02\n",
      "iteration: 138680 loss: 0.0017 lr: 0.02\n",
      "iteration: 138690 loss: 0.0026 lr: 0.02\n",
      "iteration: 138700 loss: 0.0023 lr: 0.02\n",
      "iteration: 138710 loss: 0.0024 lr: 0.02\n",
      "iteration: 138720 loss: 0.0026 lr: 0.02\n",
      "iteration: 138730 loss: 0.0025 lr: 0.02\n",
      "iteration: 138740 loss: 0.0051 lr: 0.02\n",
      "iteration: 138750 loss: 0.0033 lr: 0.02\n",
      "iteration: 138760 loss: 0.0027 lr: 0.02\n",
      "iteration: 138770 loss: 0.0021 lr: 0.02\n",
      "iteration: 138780 loss: 0.0021 lr: 0.02\n",
      "iteration: 138790 loss: 0.0017 lr: 0.02\n",
      "iteration: 138800 loss: 0.0026 lr: 0.02\n",
      "iteration: 138810 loss: 0.0024 lr: 0.02\n",
      "iteration: 138820 loss: 0.0025 lr: 0.02\n",
      "iteration: 138830 loss: 0.0021 lr: 0.02\n",
      "iteration: 138840 loss: 0.0023 lr: 0.02\n",
      "iteration: 138850 loss: 0.0026 lr: 0.02\n",
      "iteration: 138860 loss: 0.0019 lr: 0.02\n",
      "iteration: 138870 loss: 0.0021 lr: 0.02\n",
      "iteration: 138880 loss: 0.0031 lr: 0.02\n",
      "iteration: 138890 loss: 0.0030 lr: 0.02\n",
      "iteration: 138900 loss: 0.0023 lr: 0.02\n",
      "iteration: 138910 loss: 0.0024 lr: 0.02\n",
      "iteration: 138920 loss: 0.0019 lr: 0.02\n",
      "iteration: 138930 loss: 0.0024 lr: 0.02\n",
      "iteration: 138940 loss: 0.0023 lr: 0.02\n",
      "iteration: 138950 loss: 0.0024 lr: 0.02\n",
      "iteration: 138960 loss: 0.0024 lr: 0.02\n",
      "iteration: 138970 loss: 0.0025 lr: 0.02\n",
      "iteration: 138980 loss: 0.0036 lr: 0.02\n",
      "iteration: 138990 loss: 0.0033 lr: 0.02\n",
      "iteration: 139000 loss: 0.0028 lr: 0.02\n",
      "iteration: 139010 loss: 0.0033 lr: 0.02\n",
      "iteration: 139020 loss: 0.0028 lr: 0.02\n",
      "iteration: 139030 loss: 0.0027 lr: 0.02\n",
      "iteration: 139040 loss: 0.0028 lr: 0.02\n",
      "iteration: 139050 loss: 0.0020 lr: 0.02\n",
      "iteration: 139060 loss: 0.0023 lr: 0.02\n",
      "iteration: 139070 loss: 0.0032 lr: 0.02\n",
      "iteration: 139080 loss: 0.0027 lr: 0.02\n",
      "iteration: 139090 loss: 0.0020 lr: 0.02\n",
      "iteration: 139100 loss: 0.0038 lr: 0.02\n",
      "iteration: 139110 loss: 0.0023 lr: 0.02\n",
      "iteration: 139120 loss: 0.0031 lr: 0.02\n",
      "iteration: 139130 loss: 0.0032 lr: 0.02\n",
      "iteration: 139140 loss: 0.0033 lr: 0.02\n",
      "iteration: 139150 loss: 0.0028 lr: 0.02\n",
      "iteration: 139160 loss: 0.0035 lr: 0.02\n",
      "iteration: 139170 loss: 0.0033 lr: 0.02\n",
      "iteration: 139180 loss: 0.0034 lr: 0.02\n",
      "iteration: 139190 loss: 0.0019 lr: 0.02\n",
      "iteration: 139200 loss: 0.0033 lr: 0.02\n",
      "iteration: 139210 loss: 0.0025 lr: 0.02\n",
      "iteration: 139220 loss: 0.0019 lr: 0.02\n",
      "iteration: 139230 loss: 0.0020 lr: 0.02\n",
      "iteration: 139240 loss: 0.0023 lr: 0.02\n",
      "iteration: 139250 loss: 0.0024 lr: 0.02\n",
      "iteration: 139260 loss: 0.0035 lr: 0.02\n",
      "iteration: 139270 loss: 0.0020 lr: 0.02\n",
      "iteration: 139280 loss: 0.0020 lr: 0.02\n",
      "iteration: 139290 loss: 0.0029 lr: 0.02\n",
      "iteration: 139300 loss: 0.0018 lr: 0.02\n",
      "iteration: 139310 loss: 0.0039 lr: 0.02\n",
      "iteration: 139320 loss: 0.0024 lr: 0.02\n",
      "iteration: 139330 loss: 0.0030 lr: 0.02\n",
      "iteration: 139340 loss: 0.0023 lr: 0.02\n",
      "iteration: 139350 loss: 0.0028 lr: 0.02\n",
      "iteration: 139360 loss: 0.0024 lr: 0.02\n",
      "iteration: 139370 loss: 0.0022 lr: 0.02\n",
      "iteration: 139380 loss: 0.0024 lr: 0.02\n",
      "iteration: 139390 loss: 0.0019 lr: 0.02\n",
      "iteration: 139400 loss: 0.0043 lr: 0.02\n",
      "iteration: 139410 loss: 0.0031 lr: 0.02\n",
      "iteration: 139420 loss: 0.0022 lr: 0.02\n",
      "iteration: 139430 loss: 0.0027 lr: 0.02\n",
      "iteration: 139440 loss: 0.0025 lr: 0.02\n",
      "iteration: 139450 loss: 0.0040 lr: 0.02\n",
      "iteration: 139460 loss: 0.0032 lr: 0.02\n",
      "iteration: 139470 loss: 0.0032 lr: 0.02\n",
      "iteration: 139480 loss: 0.0038 lr: 0.02\n",
      "iteration: 139490 loss: 0.0030 lr: 0.02\n",
      "iteration: 139500 loss: 0.0023 lr: 0.02\n",
      "iteration: 139510 loss: 0.0039 lr: 0.02\n",
      "iteration: 139520 loss: 0.0029 lr: 0.02\n",
      "iteration: 139530 loss: 0.0025 lr: 0.02\n",
      "iteration: 139540 loss: 0.0022 lr: 0.02\n",
      "iteration: 139550 loss: 0.0032 lr: 0.02\n",
      "iteration: 139560 loss: 0.0034 lr: 0.02\n",
      "iteration: 139570 loss: 0.0021 lr: 0.02\n",
      "iteration: 139580 loss: 0.0036 lr: 0.02\n",
      "iteration: 139590 loss: 0.0024 lr: 0.02\n",
      "iteration: 139600 loss: 0.0029 lr: 0.02\n",
      "iteration: 139610 loss: 0.0024 lr: 0.02\n",
      "iteration: 139620 loss: 0.0028 lr: 0.02\n",
      "iteration: 139630 loss: 0.0028 lr: 0.02\n",
      "iteration: 139640 loss: 0.0036 lr: 0.02\n",
      "iteration: 139650 loss: 0.0032 lr: 0.02\n",
      "iteration: 139660 loss: 0.0021 lr: 0.02\n",
      "iteration: 139670 loss: 0.0026 lr: 0.02\n",
      "iteration: 139680 loss: 0.0024 lr: 0.02\n",
      "iteration: 139690 loss: 0.0034 lr: 0.02\n",
      "iteration: 139700 loss: 0.0023 lr: 0.02\n",
      "iteration: 139710 loss: 0.0024 lr: 0.02\n",
      "iteration: 139720 loss: 0.0035 lr: 0.02\n",
      "iteration: 139730 loss: 0.0061 lr: 0.02\n",
      "iteration: 139740 loss: 0.0033 lr: 0.02\n",
      "iteration: 139750 loss: 0.0020 lr: 0.02\n",
      "iteration: 139760 loss: 0.0040 lr: 0.02\n",
      "iteration: 139770 loss: 0.0029 lr: 0.02\n",
      "iteration: 139780 loss: 0.0021 lr: 0.02\n",
      "iteration: 139790 loss: 0.0042 lr: 0.02\n",
      "iteration: 139800 loss: 0.0037 lr: 0.02\n",
      "iteration: 139810 loss: 0.0037 lr: 0.02\n",
      "iteration: 139820 loss: 0.0046 lr: 0.02\n",
      "iteration: 139830 loss: 0.0027 lr: 0.02\n",
      "iteration: 139840 loss: 0.0019 lr: 0.02\n",
      "iteration: 139850 loss: 0.0025 lr: 0.02\n",
      "iteration: 139860 loss: 0.0028 lr: 0.02\n",
      "iteration: 139870 loss: 0.0017 lr: 0.02\n",
      "iteration: 139880 loss: 0.0022 lr: 0.02\n",
      "iteration: 139890 loss: 0.0018 lr: 0.02\n",
      "iteration: 139900 loss: 0.0028 lr: 0.02\n",
      "iteration: 139910 loss: 0.0024 lr: 0.02\n",
      "iteration: 139920 loss: 0.0023 lr: 0.02\n",
      "iteration: 139930 loss: 0.0030 lr: 0.02\n",
      "iteration: 139940 loss: 0.0025 lr: 0.02\n",
      "iteration: 139950 loss: 0.0021 lr: 0.02\n",
      "iteration: 139960 loss: 0.0028 lr: 0.02\n",
      "iteration: 139970 loss: 0.0022 lr: 0.02\n",
      "iteration: 139980 loss: 0.0024 lr: 0.02\n",
      "iteration: 139990 loss: 0.0022 lr: 0.02\n",
      "iteration: 140000 loss: 0.0021 lr: 0.02\n",
      "iteration: 140010 loss: 0.0034 lr: 0.02\n",
      "iteration: 140020 loss: 0.0035 lr: 0.02\n",
      "iteration: 140030 loss: 0.0034 lr: 0.02\n",
      "iteration: 140040 loss: 0.0028 lr: 0.02\n",
      "iteration: 140050 loss: 0.0031 lr: 0.02\n",
      "iteration: 140060 loss: 0.0025 lr: 0.02\n",
      "iteration: 140070 loss: 0.0032 lr: 0.02\n",
      "iteration: 140080 loss: 0.0030 lr: 0.02\n",
      "iteration: 140090 loss: 0.0025 lr: 0.02\n",
      "iteration: 140100 loss: 0.0022 lr: 0.02\n",
      "iteration: 140110 loss: 0.0023 lr: 0.02\n",
      "iteration: 140120 loss: 0.0022 lr: 0.02\n",
      "iteration: 140130 loss: 0.0026 lr: 0.02\n",
      "iteration: 140140 loss: 0.0038 lr: 0.02\n",
      "iteration: 140150 loss: 0.0025 lr: 0.02\n",
      "iteration: 140160 loss: 0.0018 lr: 0.02\n",
      "iteration: 140170 loss: 0.0025 lr: 0.02\n",
      "iteration: 140180 loss: 0.0030 lr: 0.02\n",
      "iteration: 140190 loss: 0.0042 lr: 0.02\n",
      "iteration: 140200 loss: 0.0023 lr: 0.02\n",
      "iteration: 140210 loss: 0.0037 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 140220 loss: 0.0023 lr: 0.02\n",
      "iteration: 140230 loss: 0.0031 lr: 0.02\n",
      "iteration: 140240 loss: 0.0020 lr: 0.02\n",
      "iteration: 140250 loss: 0.0028 lr: 0.02\n",
      "iteration: 140260 loss: 0.0022 lr: 0.02\n",
      "iteration: 140270 loss: 0.0048 lr: 0.02\n",
      "iteration: 140280 loss: 0.0029 lr: 0.02\n",
      "iteration: 140290 loss: 0.0026 lr: 0.02\n",
      "iteration: 140300 loss: 0.0022 lr: 0.02\n",
      "iteration: 140310 loss: 0.0027 lr: 0.02\n",
      "iteration: 140320 loss: 0.0034 lr: 0.02\n",
      "iteration: 140330 loss: 0.0023 lr: 0.02\n",
      "iteration: 140340 loss: 0.0033 lr: 0.02\n",
      "iteration: 140350 loss: 0.0023 lr: 0.02\n",
      "iteration: 140360 loss: 0.0028 lr: 0.02\n",
      "iteration: 140370 loss: 0.0032 lr: 0.02\n",
      "iteration: 140380 loss: 0.0025 lr: 0.02\n",
      "iteration: 140390 loss: 0.0032 lr: 0.02\n",
      "iteration: 140400 loss: 0.0025 lr: 0.02\n",
      "iteration: 140410 loss: 0.0033 lr: 0.02\n",
      "iteration: 140420 loss: 0.0024 lr: 0.02\n",
      "iteration: 140430 loss: 0.0028 lr: 0.02\n",
      "iteration: 140440 loss: 0.0027 lr: 0.02\n",
      "iteration: 140450 loss: 0.0023 lr: 0.02\n",
      "iteration: 140460 loss: 0.0032 lr: 0.02\n",
      "iteration: 140470 loss: 0.0032 lr: 0.02\n",
      "iteration: 140480 loss: 0.0036 lr: 0.02\n",
      "iteration: 140490 loss: 0.0022 lr: 0.02\n",
      "iteration: 140500 loss: 0.0025 lr: 0.02\n",
      "iteration: 140510 loss: 0.0024 lr: 0.02\n",
      "iteration: 140520 loss: 0.0028 lr: 0.02\n",
      "iteration: 140530 loss: 0.0025 lr: 0.02\n",
      "iteration: 140540 loss: 0.0030 lr: 0.02\n",
      "iteration: 140550 loss: 0.0025 lr: 0.02\n",
      "iteration: 140560 loss: 0.0027 lr: 0.02\n",
      "iteration: 140570 loss: 0.0028 lr: 0.02\n",
      "iteration: 140580 loss: 0.0031 lr: 0.02\n",
      "iteration: 140590 loss: 0.0027 lr: 0.02\n",
      "iteration: 140600 loss: 0.0028 lr: 0.02\n",
      "iteration: 140610 loss: 0.0034 lr: 0.02\n",
      "iteration: 140620 loss: 0.0023 lr: 0.02\n",
      "iteration: 140630 loss: 0.0027 lr: 0.02\n",
      "iteration: 140640 loss: 0.0020 lr: 0.02\n",
      "iteration: 140650 loss: 0.0029 lr: 0.02\n",
      "iteration: 140660 loss: 0.0026 lr: 0.02\n",
      "iteration: 140670 loss: 0.0025 lr: 0.02\n",
      "iteration: 140680 loss: 0.0025 lr: 0.02\n",
      "iteration: 140690 loss: 0.0028 lr: 0.02\n",
      "iteration: 140700 loss: 0.0036 lr: 0.02\n",
      "iteration: 140710 loss: 0.0026 lr: 0.02\n",
      "iteration: 140720 loss: 0.0035 lr: 0.02\n",
      "iteration: 140730 loss: 0.0032 lr: 0.02\n",
      "iteration: 140740 loss: 0.0022 lr: 0.02\n",
      "iteration: 140750 loss: 0.0021 lr: 0.02\n",
      "iteration: 140760 loss: 0.0038 lr: 0.02\n",
      "iteration: 140770 loss: 0.0028 lr: 0.02\n",
      "iteration: 140780 loss: 0.0026 lr: 0.02\n",
      "iteration: 140790 loss: 0.0021 lr: 0.02\n",
      "iteration: 140800 loss: 0.0039 lr: 0.02\n",
      "iteration: 140810 loss: 0.0031 lr: 0.02\n",
      "iteration: 140820 loss: 0.0025 lr: 0.02\n",
      "iteration: 140830 loss: 0.0032 lr: 0.02\n",
      "iteration: 140840 loss: 0.0022 lr: 0.02\n",
      "iteration: 140850 loss: 0.0024 lr: 0.02\n",
      "iteration: 140860 loss: 0.0023 lr: 0.02\n",
      "iteration: 140870 loss: 0.0020 lr: 0.02\n",
      "iteration: 140880 loss: 0.0021 lr: 0.02\n",
      "iteration: 140890 loss: 0.0024 lr: 0.02\n",
      "iteration: 140900 loss: 0.0031 lr: 0.02\n",
      "iteration: 140910 loss: 0.0025 lr: 0.02\n",
      "iteration: 140920 loss: 0.0019 lr: 0.02\n",
      "iteration: 140930 loss: 0.0027 lr: 0.02\n",
      "iteration: 140940 loss: 0.0023 lr: 0.02\n",
      "iteration: 140950 loss: 0.0026 lr: 0.02\n",
      "iteration: 140960 loss: 0.0025 lr: 0.02\n",
      "iteration: 140970 loss: 0.0032 lr: 0.02\n",
      "iteration: 140980 loss: 0.0023 lr: 0.02\n",
      "iteration: 140990 loss: 0.0024 lr: 0.02\n",
      "iteration: 141000 loss: 0.0017 lr: 0.02\n",
      "iteration: 141010 loss: 0.0023 lr: 0.02\n",
      "iteration: 141020 loss: 0.0018 lr: 0.02\n",
      "iteration: 141030 loss: 0.0020 lr: 0.02\n",
      "iteration: 141040 loss: 0.0024 lr: 0.02\n",
      "iteration: 141050 loss: 0.0020 lr: 0.02\n",
      "iteration: 141060 loss: 0.0021 lr: 0.02\n",
      "iteration: 141070 loss: 0.0028 lr: 0.02\n",
      "iteration: 141080 loss: 0.0022 lr: 0.02\n",
      "iteration: 141090 loss: 0.0022 lr: 0.02\n",
      "iteration: 141100 loss: 0.0028 lr: 0.02\n",
      "iteration: 141110 loss: 0.0019 lr: 0.02\n",
      "iteration: 141120 loss: 0.0020 lr: 0.02\n",
      "iteration: 141130 loss: 0.0023 lr: 0.02\n",
      "iteration: 141140 loss: 0.0025 lr: 0.02\n",
      "iteration: 141150 loss: 0.0018 lr: 0.02\n",
      "iteration: 141160 loss: 0.0027 lr: 0.02\n",
      "iteration: 141170 loss: 0.0033 lr: 0.02\n",
      "iteration: 141180 loss: 0.0025 lr: 0.02\n",
      "iteration: 141190 loss: 0.0018 lr: 0.02\n",
      "iteration: 141200 loss: 0.0031 lr: 0.02\n",
      "iteration: 141210 loss: 0.0032 lr: 0.02\n",
      "iteration: 141220 loss: 0.0030 lr: 0.02\n",
      "iteration: 141230 loss: 0.0029 lr: 0.02\n",
      "iteration: 141240 loss: 0.0025 lr: 0.02\n",
      "iteration: 141250 loss: 0.0022 lr: 0.02\n",
      "iteration: 141260 loss: 0.0025 lr: 0.02\n",
      "iteration: 141270 loss: 0.0029 lr: 0.02\n",
      "iteration: 141280 loss: 0.0032 lr: 0.02\n",
      "iteration: 141290 loss: 0.0034 lr: 0.02\n",
      "iteration: 141300 loss: 0.0023 lr: 0.02\n",
      "iteration: 141310 loss: 0.0025 lr: 0.02\n",
      "iteration: 141320 loss: 0.0024 lr: 0.02\n",
      "iteration: 141330 loss: 0.0030 lr: 0.02\n",
      "iteration: 141340 loss: 0.0030 lr: 0.02\n",
      "iteration: 141350 loss: 0.0058 lr: 0.02\n",
      "iteration: 141360 loss: 0.0027 lr: 0.02\n",
      "iteration: 141370 loss: 0.0033 lr: 0.02\n",
      "iteration: 141380 loss: 0.0026 lr: 0.02\n",
      "iteration: 141390 loss: 0.0035 lr: 0.02\n",
      "iteration: 141400 loss: 0.0042 lr: 0.02\n",
      "iteration: 141410 loss: 0.0025 lr: 0.02\n",
      "iteration: 141420 loss: 0.0032 lr: 0.02\n",
      "iteration: 141430 loss: 0.0029 lr: 0.02\n",
      "iteration: 141440 loss: 0.0030 lr: 0.02\n",
      "iteration: 141450 loss: 0.0030 lr: 0.02\n",
      "iteration: 141460 loss: 0.0021 lr: 0.02\n",
      "iteration: 141470 loss: 0.0022 lr: 0.02\n",
      "iteration: 141480 loss: 0.0022 lr: 0.02\n",
      "iteration: 141490 loss: 0.0028 lr: 0.02\n",
      "iteration: 141500 loss: 0.0027 lr: 0.02\n",
      "iteration: 141510 loss: 0.0029 lr: 0.02\n",
      "iteration: 141520 loss: 0.0036 lr: 0.02\n",
      "iteration: 141530 loss: 0.0025 lr: 0.02\n",
      "iteration: 141540 loss: 0.0035 lr: 0.02\n",
      "iteration: 141550 loss: 0.0034 lr: 0.02\n",
      "iteration: 141560 loss: 0.0024 lr: 0.02\n",
      "iteration: 141570 loss: 0.0024 lr: 0.02\n",
      "iteration: 141580 loss: 0.0022 lr: 0.02\n",
      "iteration: 141590 loss: 0.0019 lr: 0.02\n",
      "iteration: 141600 loss: 0.0049 lr: 0.02\n",
      "iteration: 141610 loss: 0.0029 lr: 0.02\n",
      "iteration: 141620 loss: 0.0017 lr: 0.02\n",
      "iteration: 141630 loss: 0.0021 lr: 0.02\n",
      "iteration: 141640 loss: 0.0028 lr: 0.02\n",
      "iteration: 141650 loss: 0.0025 lr: 0.02\n",
      "iteration: 141660 loss: 0.0017 lr: 0.02\n",
      "iteration: 141670 loss: 0.0020 lr: 0.02\n",
      "iteration: 141680 loss: 0.0031 lr: 0.02\n",
      "iteration: 141690 loss: 0.0014 lr: 0.02\n",
      "iteration: 141700 loss: 0.0020 lr: 0.02\n",
      "iteration: 141710 loss: 0.0020 lr: 0.02\n",
      "iteration: 141720 loss: 0.0021 lr: 0.02\n",
      "iteration: 141730 loss: 0.0022 lr: 0.02\n",
      "iteration: 141740 loss: 0.0025 lr: 0.02\n",
      "iteration: 141750 loss: 0.0017 lr: 0.02\n",
      "iteration: 141760 loss: 0.0022 lr: 0.02\n",
      "iteration: 141770 loss: 0.0022 lr: 0.02\n",
      "iteration: 141780 loss: 0.0020 lr: 0.02\n",
      "iteration: 141790 loss: 0.0033 lr: 0.02\n",
      "iteration: 141800 loss: 0.0023 lr: 0.02\n",
      "iteration: 141810 loss: 0.0024 lr: 0.02\n",
      "iteration: 141820 loss: 0.0027 lr: 0.02\n",
      "iteration: 141830 loss: 0.0026 lr: 0.02\n",
      "iteration: 141840 loss: 0.0025 lr: 0.02\n",
      "iteration: 141850 loss: 0.0035 lr: 0.02\n",
      "iteration: 141860 loss: 0.0023 lr: 0.02\n",
      "iteration: 141870 loss: 0.0022 lr: 0.02\n",
      "iteration: 141880 loss: 0.0021 lr: 0.02\n",
      "iteration: 141890 loss: 0.0018 lr: 0.02\n",
      "iteration: 141900 loss: 0.0039 lr: 0.02\n",
      "iteration: 141910 loss: 0.0039 lr: 0.02\n",
      "iteration: 141920 loss: 0.0026 lr: 0.02\n",
      "iteration: 141930 loss: 0.0030 lr: 0.02\n",
      "iteration: 141940 loss: 0.0025 lr: 0.02\n",
      "iteration: 141950 loss: 0.0030 lr: 0.02\n",
      "iteration: 141960 loss: 0.0023 lr: 0.02\n",
      "iteration: 141970 loss: 0.0028 lr: 0.02\n",
      "iteration: 141980 loss: 0.0027 lr: 0.02\n",
      "iteration: 141990 loss: 0.0033 lr: 0.02\n",
      "iteration: 142000 loss: 0.0038 lr: 0.02\n",
      "iteration: 142010 loss: 0.0031 lr: 0.02\n",
      "iteration: 142020 loss: 0.0018 lr: 0.02\n",
      "iteration: 142030 loss: 0.0025 lr: 0.02\n",
      "iteration: 142040 loss: 0.0025 lr: 0.02\n",
      "iteration: 142050 loss: 0.0017 lr: 0.02\n",
      "iteration: 142060 loss: 0.0029 lr: 0.02\n",
      "iteration: 142070 loss: 0.0013 lr: 0.02\n",
      "iteration: 142080 loss: 0.0036 lr: 0.02\n",
      "iteration: 142090 loss: 0.0021 lr: 0.02\n",
      "iteration: 142100 loss: 0.0029 lr: 0.02\n",
      "iteration: 142110 loss: 0.0027 lr: 0.02\n",
      "iteration: 142120 loss: 0.0016 lr: 0.02\n",
      "iteration: 142130 loss: 0.0030 lr: 0.02\n",
      "iteration: 142140 loss: 0.0028 lr: 0.02\n",
      "iteration: 142150 loss: 0.0036 lr: 0.02\n",
      "iteration: 142160 loss: 0.0021 lr: 0.02\n",
      "iteration: 142170 loss: 0.0024 lr: 0.02\n",
      "iteration: 142180 loss: 0.0039 lr: 0.02\n",
      "iteration: 142190 loss: 0.0019 lr: 0.02\n",
      "iteration: 142200 loss: 0.0026 lr: 0.02\n",
      "iteration: 142210 loss: 0.0030 lr: 0.02\n",
      "iteration: 142220 loss: 0.0032 lr: 0.02\n",
      "iteration: 142230 loss: 0.0022 lr: 0.02\n",
      "iteration: 142240 loss: 0.0035 lr: 0.02\n",
      "iteration: 142250 loss: 0.0028 lr: 0.02\n",
      "iteration: 142260 loss: 0.0019 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 142270 loss: 0.0019 lr: 0.02\n",
      "iteration: 142280 loss: 0.0019 lr: 0.02\n",
      "iteration: 142290 loss: 0.0038 lr: 0.02\n",
      "iteration: 142300 loss: 0.0029 lr: 0.02\n",
      "iteration: 142310 loss: 0.0022 lr: 0.02\n",
      "iteration: 142320 loss: 0.0021 lr: 0.02\n",
      "iteration: 142330 loss: 0.0025 lr: 0.02\n",
      "iteration: 142340 loss: 0.0030 lr: 0.02\n",
      "iteration: 142350 loss: 0.0028 lr: 0.02\n",
      "iteration: 142360 loss: 0.0023 lr: 0.02\n",
      "iteration: 142370 loss: 0.0024 lr: 0.02\n",
      "iteration: 142380 loss: 0.0022 lr: 0.02\n",
      "iteration: 142390 loss: 0.0026 lr: 0.02\n",
      "iteration: 142400 loss: 0.0021 lr: 0.02\n",
      "iteration: 142410 loss: 0.0024 lr: 0.02\n",
      "iteration: 142420 loss: 0.0023 lr: 0.02\n",
      "iteration: 142430 loss: 0.0032 lr: 0.02\n",
      "iteration: 142440 loss: 0.0036 lr: 0.02\n",
      "iteration: 142450 loss: 0.0029 lr: 0.02\n",
      "iteration: 142460 loss: 0.0031 lr: 0.02\n",
      "iteration: 142470 loss: 0.0031 lr: 0.02\n",
      "iteration: 142480 loss: 0.0023 lr: 0.02\n",
      "iteration: 142490 loss: 0.0018 lr: 0.02\n",
      "iteration: 142500 loss: 0.0034 lr: 0.02\n",
      "iteration: 142510 loss: 0.0024 lr: 0.02\n",
      "iteration: 142520 loss: 0.0025 lr: 0.02\n",
      "iteration: 142530 loss: 0.0032 lr: 0.02\n",
      "iteration: 142540 loss: 0.0017 lr: 0.02\n",
      "iteration: 142550 loss: 0.0025 lr: 0.02\n",
      "iteration: 142560 loss: 0.0032 lr: 0.02\n",
      "iteration: 142570 loss: 0.0046 lr: 0.02\n",
      "iteration: 142580 loss: 0.0034 lr: 0.02\n",
      "iteration: 142590 loss: 0.0032 lr: 0.02\n",
      "iteration: 142600 loss: 0.0039 lr: 0.02\n",
      "iteration: 142610 loss: 0.0018 lr: 0.02\n",
      "iteration: 142620 loss: 0.0041 lr: 0.02\n",
      "iteration: 142630 loss: 0.0031 lr: 0.02\n",
      "iteration: 142640 loss: 0.0022 lr: 0.02\n",
      "iteration: 142650 loss: 0.0039 lr: 0.02\n",
      "iteration: 142660 loss: 0.0032 lr: 0.02\n",
      "iteration: 142670 loss: 0.0023 lr: 0.02\n",
      "iteration: 142680 loss: 0.0027 lr: 0.02\n",
      "iteration: 142690 loss: 0.0028 lr: 0.02\n",
      "iteration: 142700 loss: 0.0030 lr: 0.02\n",
      "iteration: 142710 loss: 0.0020 lr: 0.02\n",
      "iteration: 142720 loss: 0.0023 lr: 0.02\n",
      "iteration: 142730 loss: 0.0025 lr: 0.02\n",
      "iteration: 142740 loss: 0.0023 lr: 0.02\n",
      "iteration: 142750 loss: 0.0018 lr: 0.02\n",
      "iteration: 142760 loss: 0.0036 lr: 0.02\n",
      "iteration: 142770 loss: 0.0025 lr: 0.02\n",
      "iteration: 142780 loss: 0.0030 lr: 0.02\n",
      "iteration: 142790 loss: 0.0023 lr: 0.02\n",
      "iteration: 142800 loss: 0.0024 lr: 0.02\n",
      "iteration: 142810 loss: 0.0022 lr: 0.02\n",
      "iteration: 142820 loss: 0.0037 lr: 0.02\n",
      "iteration: 142830 loss: 0.0028 lr: 0.02\n",
      "iteration: 142840 loss: 0.0022 lr: 0.02\n",
      "iteration: 142850 loss: 0.0023 lr: 0.02\n",
      "iteration: 142860 loss: 0.0028 lr: 0.02\n",
      "iteration: 142870 loss: 0.0026 lr: 0.02\n",
      "iteration: 142880 loss: 0.0020 lr: 0.02\n",
      "iteration: 142890 loss: 0.0022 lr: 0.02\n",
      "iteration: 142900 loss: 0.0021 lr: 0.02\n",
      "iteration: 142910 loss: 0.0032 lr: 0.02\n",
      "iteration: 142920 loss: 0.0022 lr: 0.02\n",
      "iteration: 142930 loss: 0.0020 lr: 0.02\n",
      "iteration: 142940 loss: 0.0024 lr: 0.02\n",
      "iteration: 142950 loss: 0.0027 lr: 0.02\n",
      "iteration: 142960 loss: 0.0027 lr: 0.02\n",
      "iteration: 142970 loss: 0.0026 lr: 0.02\n",
      "iteration: 142980 loss: 0.0032 lr: 0.02\n",
      "iteration: 142990 loss: 0.0034 lr: 0.02\n",
      "iteration: 143000 loss: 0.0023 lr: 0.02\n",
      "iteration: 143010 loss: 0.0024 lr: 0.02\n",
      "iteration: 143020 loss: 0.0021 lr: 0.02\n",
      "iteration: 143030 loss: 0.0016 lr: 0.02\n",
      "iteration: 143040 loss: 0.0021 lr: 0.02\n",
      "iteration: 143050 loss: 0.0020 lr: 0.02\n",
      "iteration: 143060 loss: 0.0037 lr: 0.02\n",
      "iteration: 143070 loss: 0.0033 lr: 0.02\n",
      "iteration: 143080 loss: 0.0036 lr: 0.02\n",
      "iteration: 143090 loss: 0.0028 lr: 0.02\n",
      "iteration: 143100 loss: 0.0026 lr: 0.02\n",
      "iteration: 143110 loss: 0.0029 lr: 0.02\n",
      "iteration: 143120 loss: 0.0027 lr: 0.02\n",
      "iteration: 143130 loss: 0.0021 lr: 0.02\n",
      "iteration: 143140 loss: 0.0021 lr: 0.02\n",
      "iteration: 143150 loss: 0.0020 lr: 0.02\n",
      "iteration: 143160 loss: 0.0020 lr: 0.02\n",
      "iteration: 143170 loss: 0.0034 lr: 0.02\n",
      "iteration: 143180 loss: 0.0015 lr: 0.02\n",
      "iteration: 143190 loss: 0.0029 lr: 0.02\n",
      "iteration: 143200 loss: 0.0032 lr: 0.02\n",
      "iteration: 143210 loss: 0.0029 lr: 0.02\n",
      "iteration: 143220 loss: 0.0031 lr: 0.02\n",
      "iteration: 143230 loss: 0.0039 lr: 0.02\n",
      "iteration: 143240 loss: 0.0039 lr: 0.02\n",
      "iteration: 143250 loss: 0.0027 lr: 0.02\n",
      "iteration: 143260 loss: 0.0030 lr: 0.02\n",
      "iteration: 143270 loss: 0.0028 lr: 0.02\n",
      "iteration: 143280 loss: 0.0033 lr: 0.02\n",
      "iteration: 143290 loss: 0.0027 lr: 0.02\n",
      "iteration: 143300 loss: 0.0029 lr: 0.02\n",
      "iteration: 143310 loss: 0.0029 lr: 0.02\n",
      "iteration: 143320 loss: 0.0035 lr: 0.02\n",
      "iteration: 143330 loss: 0.0024 lr: 0.02\n",
      "iteration: 143340 loss: 0.0020 lr: 0.02\n",
      "iteration: 143350 loss: 0.0020 lr: 0.02\n",
      "iteration: 143360 loss: 0.0026 lr: 0.02\n",
      "iteration: 143370 loss: 0.0028 lr: 0.02\n",
      "iteration: 143380 loss: 0.0027 lr: 0.02\n",
      "iteration: 143390 loss: 0.0030 lr: 0.02\n",
      "iteration: 143400 loss: 0.0028 lr: 0.02\n",
      "iteration: 143410 loss: 0.0020 lr: 0.02\n",
      "iteration: 143420 loss: 0.0033 lr: 0.02\n",
      "iteration: 143430 loss: 0.0024 lr: 0.02\n",
      "iteration: 143440 loss: 0.0019 lr: 0.02\n",
      "iteration: 143450 loss: 0.0025 lr: 0.02\n",
      "iteration: 143460 loss: 0.0024 lr: 0.02\n",
      "iteration: 143470 loss: 0.0034 lr: 0.02\n",
      "iteration: 143480 loss: 0.0023 lr: 0.02\n",
      "iteration: 143490 loss: 0.0023 lr: 0.02\n",
      "iteration: 143500 loss: 0.0024 lr: 0.02\n",
      "iteration: 143510 loss: 0.0025 lr: 0.02\n",
      "iteration: 143520 loss: 0.0027 lr: 0.02\n",
      "iteration: 143530 loss: 0.0026 lr: 0.02\n",
      "iteration: 143540 loss: 0.0018 lr: 0.02\n",
      "iteration: 143550 loss: 0.0047 lr: 0.02\n",
      "iteration: 143560 loss: 0.0021 lr: 0.02\n",
      "iteration: 143570 loss: 0.0032 lr: 0.02\n",
      "iteration: 143580 loss: 0.0023 lr: 0.02\n",
      "iteration: 143590 loss: 0.0021 lr: 0.02\n",
      "iteration: 143600 loss: 0.0020 lr: 0.02\n",
      "iteration: 143610 loss: 0.0019 lr: 0.02\n",
      "iteration: 143620 loss: 0.0017 lr: 0.02\n",
      "iteration: 143630 loss: 0.0034 lr: 0.02\n",
      "iteration: 143640 loss: 0.0031 lr: 0.02\n",
      "iteration: 143650 loss: 0.0029 lr: 0.02\n",
      "iteration: 143660 loss: 0.0031 lr: 0.02\n",
      "iteration: 143670 loss: 0.0025 lr: 0.02\n",
      "iteration: 143680 loss: 0.0033 lr: 0.02\n",
      "iteration: 143690 loss: 0.0036 lr: 0.02\n",
      "iteration: 143700 loss: 0.0042 lr: 0.02\n",
      "iteration: 143710 loss: 0.0027 lr: 0.02\n",
      "iteration: 143720 loss: 0.0030 lr: 0.02\n",
      "iteration: 143730 loss: 0.0025 lr: 0.02\n",
      "iteration: 143740 loss: 0.0026 lr: 0.02\n",
      "iteration: 143750 loss: 0.0029 lr: 0.02\n",
      "iteration: 143760 loss: 0.0027 lr: 0.02\n",
      "iteration: 143770 loss: 0.0019 lr: 0.02\n",
      "iteration: 143780 loss: 0.0023 lr: 0.02\n",
      "iteration: 143790 loss: 0.0026 lr: 0.02\n",
      "iteration: 143800 loss: 0.0025 lr: 0.02\n",
      "iteration: 143810 loss: 0.0031 lr: 0.02\n",
      "iteration: 143820 loss: 0.0026 lr: 0.02\n",
      "iteration: 143830 loss: 0.0024 lr: 0.02\n",
      "iteration: 143840 loss: 0.0033 lr: 0.02\n",
      "iteration: 143850 loss: 0.0023 lr: 0.02\n",
      "iteration: 143860 loss: 0.0021 lr: 0.02\n",
      "iteration: 143870 loss: 0.0022 lr: 0.02\n",
      "iteration: 143880 loss: 0.0027 lr: 0.02\n",
      "iteration: 143890 loss: 0.0021 lr: 0.02\n",
      "iteration: 143900 loss: 0.0018 lr: 0.02\n",
      "iteration: 143910 loss: 0.0019 lr: 0.02\n",
      "iteration: 143920 loss: 0.0028 lr: 0.02\n",
      "iteration: 143930 loss: 0.0020 lr: 0.02\n",
      "iteration: 143940 loss: 0.0023 lr: 0.02\n",
      "iteration: 143950 loss: 0.0018 lr: 0.02\n",
      "iteration: 143960 loss: 0.0026 lr: 0.02\n",
      "iteration: 143970 loss: 0.0020 lr: 0.02\n",
      "iteration: 143980 loss: 0.0030 lr: 0.02\n",
      "iteration: 143990 loss: 0.0029 lr: 0.02\n",
      "iteration: 144000 loss: 0.0024 lr: 0.02\n",
      "iteration: 144010 loss: 0.0028 lr: 0.02\n",
      "iteration: 144020 loss: 0.0029 lr: 0.02\n",
      "iteration: 144030 loss: 0.0022 lr: 0.02\n",
      "iteration: 144040 loss: 0.0032 lr: 0.02\n",
      "iteration: 144050 loss: 0.0041 lr: 0.02\n",
      "iteration: 144060 loss: 0.0034 lr: 0.02\n",
      "iteration: 144070 loss: 0.0017 lr: 0.02\n",
      "iteration: 144080 loss: 0.0025 lr: 0.02\n",
      "iteration: 144090 loss: 0.0042 lr: 0.02\n",
      "iteration: 144100 loss: 0.0024 lr: 0.02\n",
      "iteration: 144110 loss: 0.0040 lr: 0.02\n",
      "iteration: 144120 loss: 0.0045 lr: 0.02\n",
      "iteration: 144130 loss: 0.0024 lr: 0.02\n",
      "iteration: 144140 loss: 0.0026 lr: 0.02\n",
      "iteration: 144150 loss: 0.0020 lr: 0.02\n",
      "iteration: 144160 loss: 0.0022 lr: 0.02\n",
      "iteration: 144170 loss: 0.0021 lr: 0.02\n",
      "iteration: 144180 loss: 0.0023 lr: 0.02\n",
      "iteration: 144190 loss: 0.0032 lr: 0.02\n",
      "iteration: 144200 loss: 0.0023 lr: 0.02\n",
      "iteration: 144210 loss: 0.0022 lr: 0.02\n",
      "iteration: 144220 loss: 0.0027 lr: 0.02\n",
      "iteration: 144230 loss: 0.0023 lr: 0.02\n",
      "iteration: 144240 loss: 0.0032 lr: 0.02\n",
      "iteration: 144250 loss: 0.0025 lr: 0.02\n",
      "iteration: 144260 loss: 0.0035 lr: 0.02\n",
      "iteration: 144270 loss: 0.0032 lr: 0.02\n",
      "iteration: 144280 loss: 0.0018 lr: 0.02\n",
      "iteration: 144290 loss: 0.0028 lr: 0.02\n",
      "iteration: 144300 loss: 0.0030 lr: 0.02\n",
      "iteration: 144310 loss: 0.0018 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 144320 loss: 0.0024 lr: 0.02\n",
      "iteration: 144330 loss: 0.0025 lr: 0.02\n",
      "iteration: 144340 loss: 0.0033 lr: 0.02\n",
      "iteration: 144350 loss: 0.0023 lr: 0.02\n",
      "iteration: 144360 loss: 0.0031 lr: 0.02\n",
      "iteration: 144370 loss: 0.0031 lr: 0.02\n",
      "iteration: 144380 loss: 0.0020 lr: 0.02\n",
      "iteration: 144390 loss: 0.0037 lr: 0.02\n",
      "iteration: 144400 loss: 0.0036 lr: 0.02\n",
      "iteration: 144410 loss: 0.0032 lr: 0.02\n",
      "iteration: 144420 loss: 0.0022 lr: 0.02\n",
      "iteration: 144430 loss: 0.0032 lr: 0.02\n",
      "iteration: 144440 loss: 0.0023 lr: 0.02\n",
      "iteration: 144450 loss: 0.0023 lr: 0.02\n",
      "iteration: 144460 loss: 0.0020 lr: 0.02\n",
      "iteration: 144470 loss: 0.0024 lr: 0.02\n",
      "iteration: 144480 loss: 0.0029 lr: 0.02\n",
      "iteration: 144490 loss: 0.0028 lr: 0.02\n",
      "iteration: 144500 loss: 0.0028 lr: 0.02\n",
      "iteration: 144510 loss: 0.0024 lr: 0.02\n",
      "iteration: 144520 loss: 0.0039 lr: 0.02\n",
      "iteration: 144530 loss: 0.0025 lr: 0.02\n",
      "iteration: 144540 loss: 0.0025 lr: 0.02\n",
      "iteration: 144550 loss: 0.0028 lr: 0.02\n",
      "iteration: 144560 loss: 0.0032 lr: 0.02\n",
      "iteration: 144570 loss: 0.0027 lr: 0.02\n",
      "iteration: 144580 loss: 0.0031 lr: 0.02\n",
      "iteration: 144590 loss: 0.0029 lr: 0.02\n",
      "iteration: 144600 loss: 0.0028 lr: 0.02\n",
      "iteration: 144610 loss: 0.0021 lr: 0.02\n",
      "iteration: 144620 loss: 0.0030 lr: 0.02\n",
      "iteration: 144630 loss: 0.0039 lr: 0.02\n",
      "iteration: 144640 loss: 0.0033 lr: 0.02\n",
      "iteration: 144650 loss: 0.0024 lr: 0.02\n",
      "iteration: 144660 loss: 0.0029 lr: 0.02\n",
      "iteration: 144670 loss: 0.0018 lr: 0.02\n",
      "iteration: 144680 loss: 0.0033 lr: 0.02\n",
      "iteration: 144690 loss: 0.0039 lr: 0.02\n",
      "iteration: 144700 loss: 0.0022 lr: 0.02\n",
      "iteration: 144710 loss: 0.0017 lr: 0.02\n",
      "iteration: 144720 loss: 0.0022 lr: 0.02\n",
      "iteration: 144730 loss: 0.0028 lr: 0.02\n",
      "iteration: 144740 loss: 0.0032 lr: 0.02\n",
      "iteration: 144750 loss: 0.0043 lr: 0.02\n",
      "iteration: 144760 loss: 0.0028 lr: 0.02\n",
      "iteration: 144770 loss: 0.0035 lr: 0.02\n",
      "iteration: 144780 loss: 0.0021 lr: 0.02\n",
      "iteration: 144790 loss: 0.0022 lr: 0.02\n",
      "iteration: 144800 loss: 0.0022 lr: 0.02\n",
      "iteration: 144810 loss: 0.0028 lr: 0.02\n",
      "iteration: 144820 loss: 0.0028 lr: 0.02\n",
      "iteration: 144830 loss: 0.0031 lr: 0.02\n",
      "iteration: 144840 loss: 0.0019 lr: 0.02\n",
      "iteration: 144850 loss: 0.0021 lr: 0.02\n",
      "iteration: 144860 loss: 0.0026 lr: 0.02\n",
      "iteration: 144870 loss: 0.0028 lr: 0.02\n",
      "iteration: 144880 loss: 0.0024 lr: 0.02\n",
      "iteration: 144890 loss: 0.0024 lr: 0.02\n",
      "iteration: 144900 loss: 0.0034 lr: 0.02\n",
      "iteration: 144910 loss: 0.0028 lr: 0.02\n",
      "iteration: 144920 loss: 0.0027 lr: 0.02\n",
      "iteration: 144930 loss: 0.0028 lr: 0.02\n",
      "iteration: 144940 loss: 0.0026 lr: 0.02\n",
      "iteration: 144950 loss: 0.0019 lr: 0.02\n",
      "iteration: 144960 loss: 0.0017 lr: 0.02\n",
      "iteration: 144970 loss: 0.0019 lr: 0.02\n",
      "iteration: 144980 loss: 0.0023 lr: 0.02\n",
      "iteration: 144990 loss: 0.0026 lr: 0.02\n",
      "iteration: 145000 loss: 0.0025 lr: 0.02\n",
      "iteration: 145010 loss: 0.0021 lr: 0.02\n",
      "iteration: 145020 loss: 0.0020 lr: 0.02\n",
      "iteration: 145030 loss: 0.0030 lr: 0.02\n",
      "iteration: 145040 loss: 0.0022 lr: 0.02\n",
      "iteration: 145050 loss: 0.0016 lr: 0.02\n",
      "iteration: 145060 loss: 0.0034 lr: 0.02\n",
      "iteration: 145070 loss: 0.0023 lr: 0.02\n",
      "iteration: 145080 loss: 0.0036 lr: 0.02\n",
      "iteration: 145090 loss: 0.0026 lr: 0.02\n",
      "iteration: 145100 loss: 0.0022 lr: 0.02\n",
      "iteration: 145110 loss: 0.0034 lr: 0.02\n",
      "iteration: 145120 loss: 0.0032 lr: 0.02\n",
      "iteration: 145130 loss: 0.0022 lr: 0.02\n",
      "iteration: 145140 loss: 0.0024 lr: 0.02\n",
      "iteration: 145150 loss: 0.0024 lr: 0.02\n",
      "iteration: 145160 loss: 0.0030 lr: 0.02\n",
      "iteration: 145170 loss: 0.0025 lr: 0.02\n",
      "iteration: 145180 loss: 0.0025 lr: 0.02\n",
      "iteration: 145190 loss: 0.0026 lr: 0.02\n",
      "iteration: 145200 loss: 0.0027 lr: 0.02\n",
      "iteration: 145210 loss: 0.0019 lr: 0.02\n",
      "iteration: 145220 loss: 0.0022 lr: 0.02\n",
      "iteration: 145230 loss: 0.0024 lr: 0.02\n",
      "iteration: 145240 loss: 0.0033 lr: 0.02\n",
      "iteration: 145250 loss: 0.0041 lr: 0.02\n",
      "iteration: 145260 loss: 0.0030 lr: 0.02\n",
      "iteration: 145270 loss: 0.0039 lr: 0.02\n",
      "iteration: 145280 loss: 0.0021 lr: 0.02\n",
      "iteration: 145290 loss: 0.0027 lr: 0.02\n",
      "iteration: 145300 loss: 0.0022 lr: 0.02\n",
      "iteration: 145310 loss: 0.0020 lr: 0.02\n",
      "iteration: 145320 loss: 0.0020 lr: 0.02\n",
      "iteration: 145330 loss: 0.0023 lr: 0.02\n",
      "iteration: 145340 loss: 0.0034 lr: 0.02\n",
      "iteration: 145350 loss: 0.0028 lr: 0.02\n",
      "iteration: 145360 loss: 0.0028 lr: 0.02\n",
      "iteration: 145370 loss: 0.0022 lr: 0.02\n",
      "iteration: 145380 loss: 0.0024 lr: 0.02\n",
      "iteration: 145390 loss: 0.0025 lr: 0.02\n",
      "iteration: 145400 loss: 0.0030 lr: 0.02\n",
      "iteration: 145410 loss: 0.0018 lr: 0.02\n",
      "iteration: 145420 loss: 0.0027 lr: 0.02\n",
      "iteration: 145430 loss: 0.0018 lr: 0.02\n",
      "iteration: 145440 loss: 0.0023 lr: 0.02\n",
      "iteration: 145450 loss: 0.0029 lr: 0.02\n",
      "iteration: 145460 loss: 0.0032 lr: 0.02\n",
      "iteration: 145470 loss: 0.0026 lr: 0.02\n",
      "iteration: 145480 loss: 0.0022 lr: 0.02\n",
      "iteration: 145490 loss: 0.0035 lr: 0.02\n",
      "iteration: 145500 loss: 0.0023 lr: 0.02\n",
      "iteration: 145510 loss: 0.0026 lr: 0.02\n",
      "iteration: 145520 loss: 0.0021 lr: 0.02\n",
      "iteration: 145530 loss: 0.0024 lr: 0.02\n",
      "iteration: 145540 loss: 0.0029 lr: 0.02\n",
      "iteration: 145550 loss: 0.0029 lr: 0.02\n",
      "iteration: 145560 loss: 0.0033 lr: 0.02\n",
      "iteration: 145570 loss: 0.0026 lr: 0.02\n",
      "iteration: 145580 loss: 0.0020 lr: 0.02\n",
      "iteration: 145590 loss: 0.0042 lr: 0.02\n",
      "iteration: 145600 loss: 0.0034 lr: 0.02\n",
      "iteration: 145610 loss: 0.0025 lr: 0.02\n",
      "iteration: 145620 loss: 0.0024 lr: 0.02\n",
      "iteration: 145630 loss: 0.0029 lr: 0.02\n",
      "iteration: 145640 loss: 0.0029 lr: 0.02\n",
      "iteration: 145650 loss: 0.0034 lr: 0.02\n",
      "iteration: 145660 loss: 0.0025 lr: 0.02\n",
      "iteration: 145670 loss: 0.0028 lr: 0.02\n",
      "iteration: 145680 loss: 0.0023 lr: 0.02\n",
      "iteration: 145690 loss: 0.0019 lr: 0.02\n",
      "iteration: 145700 loss: 0.0026 lr: 0.02\n",
      "iteration: 145710 loss: 0.0031 lr: 0.02\n",
      "iteration: 145720 loss: 0.0024 lr: 0.02\n",
      "iteration: 145730 loss: 0.0035 lr: 0.02\n",
      "iteration: 145740 loss: 0.0022 lr: 0.02\n",
      "iteration: 145750 loss: 0.0018 lr: 0.02\n",
      "iteration: 145760 loss: 0.0026 lr: 0.02\n",
      "iteration: 145770 loss: 0.0022 lr: 0.02\n",
      "iteration: 145780 loss: 0.0021 lr: 0.02\n",
      "iteration: 145790 loss: 0.0022 lr: 0.02\n",
      "iteration: 145800 loss: 0.0033 lr: 0.02\n",
      "iteration: 145810 loss: 0.0022 lr: 0.02\n",
      "iteration: 145820 loss: 0.0024 lr: 0.02\n",
      "iteration: 145830 loss: 0.0027 lr: 0.02\n",
      "iteration: 145840 loss: 0.0023 lr: 0.02\n",
      "iteration: 145850 loss: 0.0021 lr: 0.02\n",
      "iteration: 145860 loss: 0.0027 lr: 0.02\n",
      "iteration: 145870 loss: 0.0030 lr: 0.02\n",
      "iteration: 145880 loss: 0.0026 lr: 0.02\n",
      "iteration: 145890 loss: 0.0018 lr: 0.02\n",
      "iteration: 145900 loss: 0.0018 lr: 0.02\n",
      "iteration: 145910 loss: 0.0021 lr: 0.02\n",
      "iteration: 145920 loss: 0.0018 lr: 0.02\n",
      "iteration: 145930 loss: 0.0023 lr: 0.02\n",
      "iteration: 145940 loss: 0.0020 lr: 0.02\n",
      "iteration: 145950 loss: 0.0034 lr: 0.02\n",
      "iteration: 145960 loss: 0.0021 lr: 0.02\n",
      "iteration: 145970 loss: 0.0025 lr: 0.02\n",
      "iteration: 145980 loss: 0.0027 lr: 0.02\n",
      "iteration: 145990 loss: 0.0020 lr: 0.02\n",
      "iteration: 146000 loss: 0.0032 lr: 0.02\n",
      "iteration: 146010 loss: 0.0025 lr: 0.02\n",
      "iteration: 146020 loss: 0.0037 lr: 0.02\n",
      "iteration: 146030 loss: 0.0030 lr: 0.02\n",
      "iteration: 146040 loss: 0.0023 lr: 0.02\n",
      "iteration: 146050 loss: 0.0026 lr: 0.02\n",
      "iteration: 146060 loss: 0.0022 lr: 0.02\n",
      "iteration: 146070 loss: 0.0021 lr: 0.02\n",
      "iteration: 146080 loss: 0.0020 lr: 0.02\n",
      "iteration: 146090 loss: 0.0019 lr: 0.02\n",
      "iteration: 146100 loss: 0.0026 lr: 0.02\n",
      "iteration: 146110 loss: 0.0027 lr: 0.02\n",
      "iteration: 146120 loss: 0.0016 lr: 0.02\n",
      "iteration: 146130 loss: 0.0023 lr: 0.02\n",
      "iteration: 146140 loss: 0.0025 lr: 0.02\n",
      "iteration: 146150 loss: 0.0032 lr: 0.02\n",
      "iteration: 146160 loss: 0.0023 lr: 0.02\n",
      "iteration: 146170 loss: 0.0021 lr: 0.02\n",
      "iteration: 146180 loss: 0.0027 lr: 0.02\n",
      "iteration: 146190 loss: 0.0024 lr: 0.02\n",
      "iteration: 146200 loss: 0.0020 lr: 0.02\n",
      "iteration: 146210 loss: 0.0020 lr: 0.02\n",
      "iteration: 146220 loss: 0.0035 lr: 0.02\n",
      "iteration: 146230 loss: 0.0034 lr: 0.02\n",
      "iteration: 146240 loss: 0.0024 lr: 0.02\n",
      "iteration: 146250 loss: 0.0034 lr: 0.02\n",
      "iteration: 146260 loss: 0.0036 lr: 0.02\n",
      "iteration: 146270 loss: 0.0020 lr: 0.02\n",
      "iteration: 146280 loss: 0.0035 lr: 0.02\n",
      "iteration: 146290 loss: 0.0026 lr: 0.02\n",
      "iteration: 146300 loss: 0.0030 lr: 0.02\n",
      "iteration: 146310 loss: 0.0023 lr: 0.02\n",
      "iteration: 146320 loss: 0.0023 lr: 0.02\n",
      "iteration: 146330 loss: 0.0026 lr: 0.02\n",
      "iteration: 146340 loss: 0.0035 lr: 0.02\n",
      "iteration: 146350 loss: 0.0032 lr: 0.02\n",
      "iteration: 146360 loss: 0.0030 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 146370 loss: 0.0023 lr: 0.02\n",
      "iteration: 146380 loss: 0.0019 lr: 0.02\n",
      "iteration: 146390 loss: 0.0033 lr: 0.02\n",
      "iteration: 146400 loss: 0.0028 lr: 0.02\n",
      "iteration: 146410 loss: 0.0022 lr: 0.02\n",
      "iteration: 146420 loss: 0.0026 lr: 0.02\n",
      "iteration: 146430 loss: 0.0024 lr: 0.02\n",
      "iteration: 146440 loss: 0.0022 lr: 0.02\n",
      "iteration: 146450 loss: 0.0030 lr: 0.02\n",
      "iteration: 146460 loss: 0.0032 lr: 0.02\n",
      "iteration: 146470 loss: 0.0017 lr: 0.02\n",
      "iteration: 146480 loss: 0.0030 lr: 0.02\n",
      "iteration: 146490 loss: 0.0021 lr: 0.02\n",
      "iteration: 146500 loss: 0.0027 lr: 0.02\n",
      "iteration: 146510 loss: 0.0025 lr: 0.02\n",
      "iteration: 146520 loss: 0.0022 lr: 0.02\n",
      "iteration: 146530 loss: 0.0031 lr: 0.02\n",
      "iteration: 146540 loss: 0.0022 lr: 0.02\n",
      "iteration: 146550 loss: 0.0020 lr: 0.02\n",
      "iteration: 146560 loss: 0.0025 lr: 0.02\n",
      "iteration: 146570 loss: 0.0031 lr: 0.02\n",
      "iteration: 146580 loss: 0.0037 lr: 0.02\n",
      "iteration: 146590 loss: 0.0021 lr: 0.02\n",
      "iteration: 146600 loss: 0.0026 lr: 0.02\n",
      "iteration: 146610 loss: 0.0023 lr: 0.02\n",
      "iteration: 146620 loss: 0.0028 lr: 0.02\n",
      "iteration: 146630 loss: 0.0025 lr: 0.02\n",
      "iteration: 146640 loss: 0.0020 lr: 0.02\n",
      "iteration: 146650 loss: 0.0023 lr: 0.02\n",
      "iteration: 146660 loss: 0.0022 lr: 0.02\n",
      "iteration: 146670 loss: 0.0019 lr: 0.02\n",
      "iteration: 146680 loss: 0.0016 lr: 0.02\n",
      "iteration: 146690 loss: 0.0022 lr: 0.02\n",
      "iteration: 146700 loss: 0.0027 lr: 0.02\n",
      "iteration: 146710 loss: 0.0017 lr: 0.02\n",
      "iteration: 146720 loss: 0.0027 lr: 0.02\n",
      "iteration: 146730 loss: 0.0018 lr: 0.02\n",
      "iteration: 146740 loss: 0.0023 lr: 0.02\n",
      "iteration: 146750 loss: 0.0029 lr: 0.02\n",
      "iteration: 146760 loss: 0.0020 lr: 0.02\n",
      "iteration: 146770 loss: 0.0023 lr: 0.02\n",
      "iteration: 146780 loss: 0.0024 lr: 0.02\n",
      "iteration: 146790 loss: 0.0025 lr: 0.02\n",
      "iteration: 146800 loss: 0.0032 lr: 0.02\n",
      "iteration: 146810 loss: 0.0024 lr: 0.02\n",
      "iteration: 146820 loss: 0.0025 lr: 0.02\n",
      "iteration: 146830 loss: 0.0045 lr: 0.02\n",
      "iteration: 146840 loss: 0.0034 lr: 0.02\n",
      "iteration: 146850 loss: 0.0040 lr: 0.02\n",
      "iteration: 146860 loss: 0.0025 lr: 0.02\n",
      "iteration: 146870 loss: 0.0026 lr: 0.02\n",
      "iteration: 146880 loss: 0.0033 lr: 0.02\n",
      "iteration: 146890 loss: 0.0039 lr: 0.02\n",
      "iteration: 146900 loss: 0.0028 lr: 0.02\n",
      "iteration: 146910 loss: 0.0029 lr: 0.02\n",
      "iteration: 146920 loss: 0.0028 lr: 0.02\n",
      "iteration: 146930 loss: 0.0027 lr: 0.02\n",
      "iteration: 146940 loss: 0.0027 lr: 0.02\n",
      "iteration: 146950 loss: 0.0021 lr: 0.02\n",
      "iteration: 146960 loss: 0.0028 lr: 0.02\n",
      "iteration: 146970 loss: 0.0022 lr: 0.02\n",
      "iteration: 146980 loss: 0.0019 lr: 0.02\n",
      "iteration: 146990 loss: 0.0029 lr: 0.02\n",
      "iteration: 147000 loss: 0.0022 lr: 0.02\n",
      "iteration: 147010 loss: 0.0024 lr: 0.02\n",
      "iteration: 147020 loss: 0.0021 lr: 0.02\n",
      "iteration: 147030 loss: 0.0026 lr: 0.02\n",
      "iteration: 147040 loss: 0.0032 lr: 0.02\n",
      "iteration: 147050 loss: 0.0021 lr: 0.02\n",
      "iteration: 147060 loss: 0.0024 lr: 0.02\n",
      "iteration: 147070 loss: 0.0022 lr: 0.02\n",
      "iteration: 147080 loss: 0.0025 lr: 0.02\n",
      "iteration: 147090 loss: 0.0036 lr: 0.02\n",
      "iteration: 147100 loss: 0.0038 lr: 0.02\n",
      "iteration: 147110 loss: 0.0032 lr: 0.02\n",
      "iteration: 147120 loss: 0.0023 lr: 0.02\n",
      "iteration: 147130 loss: 0.0026 lr: 0.02\n",
      "iteration: 147140 loss: 0.0030 lr: 0.02\n",
      "iteration: 147150 loss: 0.0024 lr: 0.02\n",
      "iteration: 147160 loss: 0.0025 lr: 0.02\n",
      "iteration: 147170 loss: 0.0036 lr: 0.02\n",
      "iteration: 147180 loss: 0.0028 lr: 0.02\n",
      "iteration: 147190 loss: 0.0033 lr: 0.02\n",
      "iteration: 147200 loss: 0.0027 lr: 0.02\n",
      "iteration: 147210 loss: 0.0029 lr: 0.02\n",
      "iteration: 147220 loss: 0.0025 lr: 0.02\n",
      "iteration: 147230 loss: 0.0032 lr: 0.02\n",
      "iteration: 147240 loss: 0.0023 lr: 0.02\n",
      "iteration: 147250 loss: 0.0027 lr: 0.02\n",
      "iteration: 147260 loss: 0.0024 lr: 0.02\n",
      "iteration: 147270 loss: 0.0023 lr: 0.02\n",
      "iteration: 147280 loss: 0.0022 lr: 0.02\n",
      "iteration: 147290 loss: 0.0023 lr: 0.02\n",
      "iteration: 147300 loss: 0.0019 lr: 0.02\n",
      "iteration: 147310 loss: 0.0028 lr: 0.02\n",
      "iteration: 147320 loss: 0.0019 lr: 0.02\n",
      "iteration: 147330 loss: 0.0026 lr: 0.02\n",
      "iteration: 147340 loss: 0.0028 lr: 0.02\n",
      "iteration: 147350 loss: 0.0027 lr: 0.02\n",
      "iteration: 147360 loss: 0.0026 lr: 0.02\n",
      "iteration: 147370 loss: 0.0029 lr: 0.02\n",
      "iteration: 147380 loss: 0.0024 lr: 0.02\n",
      "iteration: 147390 loss: 0.0029 lr: 0.02\n",
      "iteration: 147400 loss: 0.0028 lr: 0.02\n",
      "iteration: 147410 loss: 0.0025 lr: 0.02\n",
      "iteration: 147420 loss: 0.0024 lr: 0.02\n",
      "iteration: 147430 loss: 0.0019 lr: 0.02\n",
      "iteration: 147440 loss: 0.0021 lr: 0.02\n",
      "iteration: 147450 loss: 0.0030 lr: 0.02\n",
      "iteration: 147460 loss: 0.0026 lr: 0.02\n",
      "iteration: 147470 loss: 0.0023 lr: 0.02\n",
      "iteration: 147480 loss: 0.0029 lr: 0.02\n",
      "iteration: 147490 loss: 0.0019 lr: 0.02\n",
      "iteration: 147500 loss: 0.0025 lr: 0.02\n",
      "iteration: 147510 loss: 0.0021 lr: 0.02\n",
      "iteration: 147520 loss: 0.0022 lr: 0.02\n",
      "iteration: 147530 loss: 0.0037 lr: 0.02\n",
      "iteration: 147540 loss: 0.0030 lr: 0.02\n",
      "iteration: 147550 loss: 0.0022 lr: 0.02\n",
      "iteration: 147560 loss: 0.0024 lr: 0.02\n",
      "iteration: 147570 loss: 0.0028 lr: 0.02\n",
      "iteration: 147580 loss: 0.0027 lr: 0.02\n",
      "iteration: 147590 loss: 0.0022 lr: 0.02\n",
      "iteration: 147600 loss: 0.0026 lr: 0.02\n",
      "iteration: 147610 loss: 0.0024 lr: 0.02\n",
      "iteration: 147620 loss: 0.0029 lr: 0.02\n",
      "iteration: 147630 loss: 0.0044 lr: 0.02\n",
      "iteration: 147640 loss: 0.0020 lr: 0.02\n",
      "iteration: 147650 loss: 0.0024 lr: 0.02\n",
      "iteration: 147660 loss: 0.0026 lr: 0.02\n",
      "iteration: 147670 loss: 0.0026 lr: 0.02\n",
      "iteration: 147680 loss: 0.0031 lr: 0.02\n",
      "iteration: 147690 loss: 0.0033 lr: 0.02\n",
      "iteration: 147700 loss: 0.0029 lr: 0.02\n",
      "iteration: 147710 loss: 0.0032 lr: 0.02\n",
      "iteration: 147720 loss: 0.0041 lr: 0.02\n",
      "iteration: 147730 loss: 0.0021 lr: 0.02\n",
      "iteration: 147740 loss: 0.0029 lr: 0.02\n",
      "iteration: 147750 loss: 0.0019 lr: 0.02\n",
      "iteration: 147760 loss: 0.0017 lr: 0.02\n",
      "iteration: 147770 loss: 0.0030 lr: 0.02\n",
      "iteration: 147780 loss: 0.0019 lr: 0.02\n",
      "iteration: 147790 loss: 0.0027 lr: 0.02\n",
      "iteration: 147800 loss: 0.0020 lr: 0.02\n",
      "iteration: 147810 loss: 0.0043 lr: 0.02\n",
      "iteration: 147820 loss: 0.0024 lr: 0.02\n",
      "iteration: 147830 loss: 0.0030 lr: 0.02\n",
      "iteration: 147840 loss: 0.0028 lr: 0.02\n",
      "iteration: 147850 loss: 0.0024 lr: 0.02\n",
      "iteration: 147860 loss: 0.0022 lr: 0.02\n",
      "iteration: 147870 loss: 0.0032 lr: 0.02\n",
      "iteration: 147880 loss: 0.0031 lr: 0.02\n",
      "iteration: 147890 loss: 0.0026 lr: 0.02\n",
      "iteration: 147900 loss: 0.0023 lr: 0.02\n",
      "iteration: 147910 loss: 0.0025 lr: 0.02\n",
      "iteration: 147920 loss: 0.0028 lr: 0.02\n",
      "iteration: 147930 loss: 0.0032 lr: 0.02\n",
      "iteration: 147940 loss: 0.0032 lr: 0.02\n",
      "iteration: 147950 loss: 0.0026 lr: 0.02\n",
      "iteration: 147960 loss: 0.0023 lr: 0.02\n",
      "iteration: 147970 loss: 0.0020 lr: 0.02\n",
      "iteration: 147980 loss: 0.0029 lr: 0.02\n",
      "iteration: 147990 loss: 0.0028 lr: 0.02\n",
      "iteration: 148000 loss: 0.0035 lr: 0.02\n",
      "iteration: 148010 loss: 0.0024 lr: 0.02\n",
      "iteration: 148020 loss: 0.0027 lr: 0.02\n",
      "iteration: 148030 loss: 0.0021 lr: 0.02\n",
      "iteration: 148040 loss: 0.0031 lr: 0.02\n",
      "iteration: 148050 loss: 0.0020 lr: 0.02\n",
      "iteration: 148060 loss: 0.0022 lr: 0.02\n",
      "iteration: 148070 loss: 0.0018 lr: 0.02\n",
      "iteration: 148080 loss: 0.0035 lr: 0.02\n",
      "iteration: 148090 loss: 0.0022 lr: 0.02\n",
      "iteration: 148100 loss: 0.0017 lr: 0.02\n",
      "iteration: 148110 loss: 0.0017 lr: 0.02\n",
      "iteration: 148120 loss: 0.0030 lr: 0.02\n",
      "iteration: 148130 loss: 0.0035 lr: 0.02\n",
      "iteration: 148140 loss: 0.0033 lr: 0.02\n",
      "iteration: 148150 loss: 0.0025 lr: 0.02\n",
      "iteration: 148160 loss: 0.0028 lr: 0.02\n",
      "iteration: 148170 loss: 0.0040 lr: 0.02\n",
      "iteration: 148180 loss: 0.0018 lr: 0.02\n",
      "iteration: 148190 loss: 0.0018 lr: 0.02\n",
      "iteration: 148200 loss: 0.0020 lr: 0.02\n",
      "iteration: 148210 loss: 0.0033 lr: 0.02\n",
      "iteration: 148220 loss: 0.0023 lr: 0.02\n",
      "iteration: 148230 loss: 0.0030 lr: 0.02\n",
      "iteration: 148240 loss: 0.0024 lr: 0.02\n",
      "iteration: 148250 loss: 0.0026 lr: 0.02\n",
      "iteration: 148260 loss: 0.0019 lr: 0.02\n",
      "iteration: 148270 loss: 0.0020 lr: 0.02\n",
      "iteration: 148280 loss: 0.0030 lr: 0.02\n",
      "iteration: 148290 loss: 0.0023 lr: 0.02\n",
      "iteration: 148300 loss: 0.0020 lr: 0.02\n",
      "iteration: 148310 loss: 0.0029 lr: 0.02\n",
      "iteration: 148320 loss: 0.0035 lr: 0.02\n",
      "iteration: 148330 loss: 0.0026 lr: 0.02\n",
      "iteration: 148340 loss: 0.0047 lr: 0.02\n",
      "iteration: 148350 loss: 0.0029 lr: 0.02\n",
      "iteration: 148360 loss: 0.0027 lr: 0.02\n",
      "iteration: 148370 loss: 0.0024 lr: 0.02\n",
      "iteration: 148380 loss: 0.0031 lr: 0.02\n",
      "iteration: 148390 loss: 0.0020 lr: 0.02\n",
      "iteration: 148400 loss: 0.0030 lr: 0.02\n",
      "iteration: 148410 loss: 0.0024 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 148420 loss: 0.0022 lr: 0.02\n",
      "iteration: 148430 loss: 0.0019 lr: 0.02\n",
      "iteration: 148440 loss: 0.0049 lr: 0.02\n",
      "iteration: 148450 loss: 0.0032 lr: 0.02\n",
      "iteration: 148460 loss: 0.0039 lr: 0.02\n",
      "iteration: 148470 loss: 0.0030 lr: 0.02\n",
      "iteration: 148480 loss: 0.0020 lr: 0.02\n",
      "iteration: 148490 loss: 0.0029 lr: 0.02\n",
      "iteration: 148500 loss: 0.0028 lr: 0.02\n",
      "iteration: 148510 loss: 0.0029 lr: 0.02\n",
      "iteration: 148520 loss: 0.0027 lr: 0.02\n",
      "iteration: 148530 loss: 0.0017 lr: 0.02\n",
      "iteration: 148540 loss: 0.0016 lr: 0.02\n",
      "iteration: 148550 loss: 0.0029 lr: 0.02\n",
      "iteration: 148560 loss: 0.0031 lr: 0.02\n",
      "iteration: 148570 loss: 0.0022 lr: 0.02\n",
      "iteration: 148580 loss: 0.0026 lr: 0.02\n",
      "iteration: 148590 loss: 0.0021 lr: 0.02\n",
      "iteration: 148600 loss: 0.0026 lr: 0.02\n",
      "iteration: 148610 loss: 0.0027 lr: 0.02\n",
      "iteration: 148620 loss: 0.0019 lr: 0.02\n",
      "iteration: 148630 loss: 0.0019 lr: 0.02\n",
      "iteration: 148640 loss: 0.0039 lr: 0.02\n",
      "iteration: 148650 loss: 0.0035 lr: 0.02\n",
      "iteration: 148660 loss: 0.0027 lr: 0.02\n",
      "iteration: 148670 loss: 0.0026 lr: 0.02\n",
      "iteration: 148680 loss: 0.0029 lr: 0.02\n",
      "iteration: 148690 loss: 0.0017 lr: 0.02\n",
      "iteration: 148700 loss: 0.0032 lr: 0.02\n",
      "iteration: 148710 loss: 0.0024 lr: 0.02\n",
      "iteration: 148720 loss: 0.0016 lr: 0.02\n",
      "iteration: 148730 loss: 0.0019 lr: 0.02\n",
      "iteration: 148740 loss: 0.0021 lr: 0.02\n",
      "iteration: 148750 loss: 0.0025 lr: 0.02\n",
      "iteration: 148760 loss: 0.0025 lr: 0.02\n",
      "iteration: 148770 loss: 0.0028 lr: 0.02\n",
      "iteration: 148780 loss: 0.0028 lr: 0.02\n",
      "iteration: 148790 loss: 0.0027 lr: 0.02\n",
      "iteration: 148800 loss: 0.0021 lr: 0.02\n",
      "iteration: 148810 loss: 0.0035 lr: 0.02\n",
      "iteration: 148820 loss: 0.0026 lr: 0.02\n",
      "iteration: 148830 loss: 0.0027 lr: 0.02\n",
      "iteration: 148840 loss: 0.0024 lr: 0.02\n",
      "iteration: 148850 loss: 0.0024 lr: 0.02\n",
      "iteration: 148860 loss: 0.0023 lr: 0.02\n",
      "iteration: 148870 loss: 0.0034 lr: 0.02\n",
      "iteration: 148880 loss: 0.0018 lr: 0.02\n",
      "iteration: 148890 loss: 0.0028 lr: 0.02\n",
      "iteration: 148900 loss: 0.0032 lr: 0.02\n",
      "iteration: 148910 loss: 0.0019 lr: 0.02\n",
      "iteration: 148920 loss: 0.0020 lr: 0.02\n",
      "iteration: 148930 loss: 0.0023 lr: 0.02\n",
      "iteration: 148940 loss: 0.0029 lr: 0.02\n",
      "iteration: 148950 loss: 0.0030 lr: 0.02\n",
      "iteration: 148960 loss: 0.0029 lr: 0.02\n",
      "iteration: 148970 loss: 0.0028 lr: 0.02\n",
      "iteration: 148980 loss: 0.0048 lr: 0.02\n",
      "iteration: 148990 loss: 0.0018 lr: 0.02\n",
      "iteration: 149000 loss: 0.0028 lr: 0.02\n",
      "iteration: 149010 loss: 0.0023 lr: 0.02\n",
      "iteration: 149020 loss: 0.0022 lr: 0.02\n",
      "iteration: 149030 loss: 0.0027 lr: 0.02\n",
      "iteration: 149040 loss: 0.0018 lr: 0.02\n",
      "iteration: 149050 loss: 0.0025 lr: 0.02\n",
      "iteration: 149060 loss: 0.0030 lr: 0.02\n",
      "iteration: 149070 loss: 0.0029 lr: 0.02\n",
      "iteration: 149080 loss: 0.0024 lr: 0.02\n",
      "iteration: 149090 loss: 0.0025 lr: 0.02\n",
      "iteration: 149100 loss: 0.0021 lr: 0.02\n",
      "iteration: 149110 loss: 0.0033 lr: 0.02\n",
      "iteration: 149120 loss: 0.0017 lr: 0.02\n",
      "iteration: 149130 loss: 0.0017 lr: 0.02\n",
      "iteration: 149140 loss: 0.0034 lr: 0.02\n",
      "iteration: 149150 loss: 0.0029 lr: 0.02\n",
      "iteration: 149160 loss: 0.0024 lr: 0.02\n",
      "iteration: 149170 loss: 0.0026 lr: 0.02\n",
      "iteration: 149180 loss: 0.0027 lr: 0.02\n",
      "iteration: 149190 loss: 0.0020 lr: 0.02\n",
      "iteration: 149200 loss: 0.0026 lr: 0.02\n",
      "iteration: 149210 loss: 0.0016 lr: 0.02\n",
      "iteration: 149220 loss: 0.0048 lr: 0.02\n",
      "iteration: 149230 loss: 0.0027 lr: 0.02\n",
      "iteration: 149240 loss: 0.0030 lr: 0.02\n",
      "iteration: 149250 loss: 0.0029 lr: 0.02\n",
      "iteration: 149260 loss: 0.0032 lr: 0.02\n",
      "iteration: 149270 loss: 0.0026 lr: 0.02\n",
      "iteration: 149280 loss: 0.0021 lr: 0.02\n",
      "iteration: 149290 loss: 0.0027 lr: 0.02\n",
      "iteration: 149300 loss: 0.0020 lr: 0.02\n",
      "iteration: 149310 loss: 0.0018 lr: 0.02\n",
      "iteration: 149320 loss: 0.0026 lr: 0.02\n",
      "iteration: 149330 loss: 0.0027 lr: 0.02\n",
      "iteration: 149340 loss: 0.0021 lr: 0.02\n",
      "iteration: 149350 loss: 0.0030 lr: 0.02\n",
      "iteration: 149360 loss: 0.0019 lr: 0.02\n",
      "iteration: 149370 loss: 0.0026 lr: 0.02\n",
      "iteration: 149380 loss: 0.0022 lr: 0.02\n",
      "iteration: 149390 loss: 0.0028 lr: 0.02\n",
      "iteration: 149400 loss: 0.0021 lr: 0.02\n",
      "iteration: 149410 loss: 0.0020 lr: 0.02\n",
      "iteration: 149420 loss: 0.0022 lr: 0.02\n",
      "iteration: 149430 loss: 0.0032 lr: 0.02\n",
      "iteration: 149440 loss: 0.0033 lr: 0.02\n",
      "iteration: 149450 loss: 0.0029 lr: 0.02\n",
      "iteration: 149460 loss: 0.0020 lr: 0.02\n",
      "iteration: 149470 loss: 0.0023 lr: 0.02\n",
      "iteration: 149480 loss: 0.0021 lr: 0.02\n",
      "iteration: 149490 loss: 0.0025 lr: 0.02\n",
      "iteration: 149500 loss: 0.0030 lr: 0.02\n",
      "iteration: 149510 loss: 0.0021 lr: 0.02\n",
      "iteration: 149520 loss: 0.0022 lr: 0.02\n",
      "iteration: 149530 loss: 0.0024 lr: 0.02\n",
      "iteration: 149540 loss: 0.0027 lr: 0.02\n",
      "iteration: 149550 loss: 0.0018 lr: 0.02\n",
      "iteration: 149560 loss: 0.0017 lr: 0.02\n",
      "iteration: 149570 loss: 0.0025 lr: 0.02\n",
      "iteration: 149580 loss: 0.0016 lr: 0.02\n",
      "iteration: 149590 loss: 0.0034 lr: 0.02\n",
      "iteration: 149600 loss: 0.0018 lr: 0.02\n",
      "iteration: 149610 loss: 0.0028 lr: 0.02\n",
      "iteration: 149620 loss: 0.0028 lr: 0.02\n",
      "iteration: 149630 loss: 0.0055 lr: 0.02\n",
      "iteration: 149640 loss: 0.0026 lr: 0.02\n",
      "iteration: 149650 loss: 0.0023 lr: 0.02\n",
      "iteration: 149660 loss: 0.0016 lr: 0.02\n",
      "iteration: 149670 loss: 0.0028 lr: 0.02\n",
      "iteration: 149680 loss: 0.0039 lr: 0.02\n",
      "iteration: 149690 loss: 0.0021 lr: 0.02\n",
      "iteration: 149700 loss: 0.0031 lr: 0.02\n",
      "iteration: 149710 loss: 0.0031 lr: 0.02\n",
      "iteration: 149720 loss: 0.0027 lr: 0.02\n",
      "iteration: 149730 loss: 0.0022 lr: 0.02\n",
      "iteration: 149740 loss: 0.0035 lr: 0.02\n",
      "iteration: 149750 loss: 0.0023 lr: 0.02\n",
      "iteration: 149760 loss: 0.0020 lr: 0.02\n",
      "iteration: 149770 loss: 0.0026 lr: 0.02\n",
      "iteration: 149780 loss: 0.0023 lr: 0.02\n",
      "iteration: 149790 loss: 0.0019 lr: 0.02\n",
      "iteration: 149800 loss: 0.0030 lr: 0.02\n",
      "iteration: 149810 loss: 0.0025 lr: 0.02\n",
      "iteration: 149820 loss: 0.0013 lr: 0.02\n",
      "iteration: 149830 loss: 0.0024 lr: 0.02\n",
      "iteration: 149840 loss: 0.0024 lr: 0.02\n",
      "iteration: 149850 loss: 0.0027 lr: 0.02\n",
      "iteration: 149860 loss: 0.0023 lr: 0.02\n",
      "iteration: 149870 loss: 0.0021 lr: 0.02\n",
      "iteration: 149880 loss: 0.0024 lr: 0.02\n",
      "iteration: 149890 loss: 0.0029 lr: 0.02\n",
      "iteration: 149900 loss: 0.0028 lr: 0.02\n",
      "iteration: 149910 loss: 0.0022 lr: 0.02\n",
      "iteration: 149920 loss: 0.0023 lr: 0.02\n",
      "iteration: 149930 loss: 0.0028 lr: 0.02\n",
      "iteration: 149940 loss: 0.0026 lr: 0.02\n",
      "iteration: 149950 loss: 0.0019 lr: 0.02\n",
      "iteration: 149960 loss: 0.0029 lr: 0.02\n",
      "iteration: 149970 loss: 0.0021 lr: 0.02\n",
      "iteration: 149980 loss: 0.0021 lr: 0.02\n",
      "iteration: 149990 loss: 0.0019 lr: 0.02\n",
      "iteration: 150000 loss: 0.0024 lr: 0.02\n",
      "iteration: 150010 loss: 0.0032 lr: 0.02\n",
      "iteration: 150020 loss: 0.0025 lr: 0.02\n",
      "iteration: 150030 loss: 0.0028 lr: 0.02\n",
      "iteration: 150040 loss: 0.0030 lr: 0.02\n",
      "iteration: 150050 loss: 0.0019 lr: 0.02\n",
      "iteration: 150060 loss: 0.0039 lr: 0.02\n",
      "iteration: 150070 loss: 0.0020 lr: 0.02\n",
      "iteration: 150080 loss: 0.0021 lr: 0.02\n",
      "iteration: 150090 loss: 0.0017 lr: 0.02\n",
      "iteration: 150100 loss: 0.0038 lr: 0.02\n",
      "iteration: 150110 loss: 0.0016 lr: 0.02\n",
      "iteration: 150120 loss: 0.0019 lr: 0.02\n",
      "iteration: 150130 loss: 0.0030 lr: 0.02\n",
      "iteration: 150140 loss: 0.0034 lr: 0.02\n",
      "iteration: 150150 loss: 0.0022 lr: 0.02\n",
      "iteration: 150160 loss: 0.0024 lr: 0.02\n",
      "iteration: 150170 loss: 0.0029 lr: 0.02\n",
      "iteration: 150180 loss: 0.0027 lr: 0.02\n",
      "iteration: 150190 loss: 0.0018 lr: 0.02\n",
      "iteration: 150200 loss: 0.0025 lr: 0.02\n",
      "iteration: 150210 loss: 0.0028 lr: 0.02\n",
      "iteration: 150220 loss: 0.0020 lr: 0.02\n",
      "iteration: 150230 loss: 0.0022 lr: 0.02\n",
      "iteration: 150240 loss: 0.0020 lr: 0.02\n",
      "iteration: 150250 loss: 0.0025 lr: 0.02\n",
      "iteration: 150260 loss: 0.0026 lr: 0.02\n",
      "iteration: 150270 loss: 0.0020 lr: 0.02\n",
      "iteration: 150280 loss: 0.0014 lr: 0.02\n",
      "iteration: 150290 loss: 0.0024 lr: 0.02\n",
      "iteration: 150300 loss: 0.0029 lr: 0.02\n",
      "iteration: 150310 loss: 0.0029 lr: 0.02\n",
      "iteration: 150320 loss: 0.0032 lr: 0.02\n",
      "iteration: 150330 loss: 0.0028 lr: 0.02\n",
      "iteration: 150340 loss: 0.0029 lr: 0.02\n",
      "iteration: 150350 loss: 0.0017 lr: 0.02\n",
      "iteration: 150360 loss: 0.0030 lr: 0.02\n",
      "iteration: 150370 loss: 0.0040 lr: 0.02\n",
      "iteration: 150380 loss: 0.0024 lr: 0.02\n",
      "iteration: 150390 loss: 0.0037 lr: 0.02\n",
      "iteration: 150400 loss: 0.0025 lr: 0.02\n",
      "iteration: 150410 loss: 0.0034 lr: 0.02\n",
      "iteration: 150420 loss: 0.0043 lr: 0.02\n",
      "iteration: 150430 loss: 0.0028 lr: 0.02\n",
      "iteration: 150440 loss: 0.0029 lr: 0.02\n",
      "iteration: 150450 loss: 0.0030 lr: 0.02\n",
      "iteration: 150460 loss: 0.0024 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 150470 loss: 0.0025 lr: 0.02\n",
      "iteration: 150480 loss: 0.0025 lr: 0.02\n",
      "iteration: 150490 loss: 0.0019 lr: 0.02\n",
      "iteration: 150500 loss: 0.0019 lr: 0.02\n",
      "iteration: 150510 loss: 0.0036 lr: 0.02\n",
      "iteration: 150520 loss: 0.0031 lr: 0.02\n",
      "iteration: 150530 loss: 0.0027 lr: 0.02\n",
      "iteration: 150540 loss: 0.0024 lr: 0.02\n",
      "iteration: 150550 loss: 0.0025 lr: 0.02\n",
      "iteration: 150560 loss: 0.0031 lr: 0.02\n",
      "iteration: 150570 loss: 0.0024 lr: 0.02\n",
      "iteration: 150580 loss: 0.0023 lr: 0.02\n",
      "iteration: 150590 loss: 0.0032 lr: 0.02\n",
      "iteration: 150600 loss: 0.0022 lr: 0.02\n",
      "iteration: 150610 loss: 0.0023 lr: 0.02\n",
      "iteration: 150620 loss: 0.0022 lr: 0.02\n",
      "iteration: 150630 loss: 0.0035 lr: 0.02\n",
      "iteration: 150640 loss: 0.0032 lr: 0.02\n",
      "iteration: 150650 loss: 0.0029 lr: 0.02\n",
      "iteration: 150660 loss: 0.0029 lr: 0.02\n",
      "iteration: 150670 loss: 0.0025 lr: 0.02\n",
      "iteration: 150680 loss: 0.0026 lr: 0.02\n",
      "iteration: 150690 loss: 0.0020 lr: 0.02\n",
      "iteration: 150700 loss: 0.0025 lr: 0.02\n",
      "iteration: 150710 loss: 0.0019 lr: 0.02\n",
      "iteration: 150720 loss: 0.0035 lr: 0.02\n",
      "iteration: 150730 loss: 0.0030 lr: 0.02\n",
      "iteration: 150740 loss: 0.0023 lr: 0.02\n",
      "iteration: 150750 loss: 0.0028 lr: 0.02\n",
      "iteration: 150760 loss: 0.0022 lr: 0.02\n",
      "iteration: 150770 loss: 0.0021 lr: 0.02\n",
      "iteration: 150780 loss: 0.0023 lr: 0.02\n",
      "iteration: 150790 loss: 0.0025 lr: 0.02\n",
      "iteration: 150800 loss: 0.0021 lr: 0.02\n",
      "iteration: 150810 loss: 0.0025 lr: 0.02\n",
      "iteration: 150820 loss: 0.0024 lr: 0.02\n",
      "iteration: 150830 loss: 0.0019 lr: 0.02\n",
      "iteration: 150840 loss: 0.0025 lr: 0.02\n",
      "iteration: 150850 loss: 0.0041 lr: 0.02\n",
      "iteration: 150860 loss: 0.0031 lr: 0.02\n",
      "iteration: 150870 loss: 0.0021 lr: 0.02\n",
      "iteration: 150880 loss: 0.0022 lr: 0.02\n",
      "iteration: 150890 loss: 0.0028 lr: 0.02\n",
      "iteration: 150900 loss: 0.0017 lr: 0.02\n",
      "iteration: 150910 loss: 0.0032 lr: 0.02\n",
      "iteration: 150920 loss: 0.0018 lr: 0.02\n",
      "iteration: 150930 loss: 0.0043 lr: 0.02\n",
      "iteration: 150940 loss: 0.0025 lr: 0.02\n",
      "iteration: 150950 loss: 0.0025 lr: 0.02\n",
      "iteration: 150960 loss: 0.0020 lr: 0.02\n",
      "iteration: 150970 loss: 0.0020 lr: 0.02\n",
      "iteration: 150980 loss: 0.0038 lr: 0.02\n",
      "iteration: 150990 loss: 0.0021 lr: 0.02\n",
      "iteration: 151000 loss: 0.0026 lr: 0.02\n",
      "iteration: 151010 loss: 0.0023 lr: 0.02\n",
      "iteration: 151020 loss: 0.0033 lr: 0.02\n",
      "iteration: 151030 loss: 0.0031 lr: 0.02\n",
      "iteration: 151040 loss: 0.0020 lr: 0.02\n",
      "iteration: 151050 loss: 0.0018 lr: 0.02\n",
      "iteration: 151060 loss: 0.0030 lr: 0.02\n",
      "iteration: 151070 loss: 0.0031 lr: 0.02\n",
      "iteration: 151080 loss: 0.0022 lr: 0.02\n",
      "iteration: 151090 loss: 0.0015 lr: 0.02\n",
      "iteration: 151100 loss: 0.0028 lr: 0.02\n",
      "iteration: 151110 loss: 0.0023 lr: 0.02\n",
      "iteration: 151120 loss: 0.0029 lr: 0.02\n",
      "iteration: 151130 loss: 0.0020 lr: 0.02\n",
      "iteration: 151140 loss: 0.0021 lr: 0.02\n",
      "iteration: 151150 loss: 0.0024 lr: 0.02\n",
      "iteration: 151160 loss: 0.0040 lr: 0.02\n",
      "iteration: 151170 loss: 0.0022 lr: 0.02\n",
      "iteration: 151180 loss: 0.0020 lr: 0.02\n",
      "iteration: 151190 loss: 0.0023 lr: 0.02\n",
      "iteration: 151200 loss: 0.0019 lr: 0.02\n",
      "iteration: 151210 loss: 0.0020 lr: 0.02\n",
      "iteration: 151220 loss: 0.0040 lr: 0.02\n",
      "iteration: 151230 loss: 0.0024 lr: 0.02\n",
      "iteration: 151240 loss: 0.0028 lr: 0.02\n",
      "iteration: 151250 loss: 0.0025 lr: 0.02\n",
      "iteration: 151260 loss: 0.0021 lr: 0.02\n",
      "iteration: 151270 loss: 0.0029 lr: 0.02\n",
      "iteration: 151280 loss: 0.0029 lr: 0.02\n",
      "iteration: 151290 loss: 0.0026 lr: 0.02\n",
      "iteration: 151300 loss: 0.0022 lr: 0.02\n",
      "iteration: 151310 loss: 0.0030 lr: 0.02\n",
      "iteration: 151320 loss: 0.0021 lr: 0.02\n",
      "iteration: 151330 loss: 0.0033 lr: 0.02\n",
      "iteration: 151340 loss: 0.0027 lr: 0.02\n",
      "iteration: 151350 loss: 0.0032 lr: 0.02\n",
      "iteration: 151360 loss: 0.0033 lr: 0.02\n",
      "iteration: 151370 loss: 0.0018 lr: 0.02\n",
      "iteration: 151380 loss: 0.0021 lr: 0.02\n",
      "iteration: 151390 loss: 0.0023 lr: 0.02\n",
      "iteration: 151400 loss: 0.0020 lr: 0.02\n",
      "iteration: 151410 loss: 0.0029 lr: 0.02\n",
      "iteration: 151420 loss: 0.0024 lr: 0.02\n",
      "iteration: 151430 loss: 0.0032 lr: 0.02\n",
      "iteration: 151440 loss: 0.0028 lr: 0.02\n",
      "iteration: 151450 loss: 0.0023 lr: 0.02\n",
      "iteration: 151460 loss: 0.0020 lr: 0.02\n",
      "iteration: 151470 loss: 0.0028 lr: 0.02\n",
      "iteration: 151480 loss: 0.0027 lr: 0.02\n",
      "iteration: 151490 loss: 0.0034 lr: 0.02\n",
      "iteration: 151500 loss: 0.0028 lr: 0.02\n",
      "iteration: 151510 loss: 0.0035 lr: 0.02\n",
      "iteration: 151520 loss: 0.0021 lr: 0.02\n",
      "iteration: 151530 loss: 0.0032 lr: 0.02\n",
      "iteration: 151540 loss: 0.0026 lr: 0.02\n",
      "iteration: 151550 loss: 0.0027 lr: 0.02\n",
      "iteration: 151560 loss: 0.0019 lr: 0.02\n",
      "iteration: 151570 loss: 0.0024 lr: 0.02\n",
      "iteration: 151580 loss: 0.0024 lr: 0.02\n",
      "iteration: 151590 loss: 0.0019 lr: 0.02\n",
      "iteration: 151600 loss: 0.0032 lr: 0.02\n",
      "iteration: 151610 loss: 0.0034 lr: 0.02\n",
      "iteration: 151620 loss: 0.0032 lr: 0.02\n",
      "iteration: 151630 loss: 0.0024 lr: 0.02\n",
      "iteration: 151640 loss: 0.0042 lr: 0.02\n",
      "iteration: 151650 loss: 0.0025 lr: 0.02\n",
      "iteration: 151660 loss: 0.0018 lr: 0.02\n",
      "iteration: 151670 loss: 0.0022 lr: 0.02\n",
      "iteration: 151680 loss: 0.0021 lr: 0.02\n",
      "iteration: 151690 loss: 0.0028 lr: 0.02\n",
      "iteration: 151700 loss: 0.0032 lr: 0.02\n",
      "iteration: 151710 loss: 0.0022 lr: 0.02\n",
      "iteration: 151720 loss: 0.0025 lr: 0.02\n",
      "iteration: 151730 loss: 0.0020 lr: 0.02\n",
      "iteration: 151740 loss: 0.0017 lr: 0.02\n",
      "iteration: 151750 loss: 0.0025 lr: 0.02\n",
      "iteration: 151760 loss: 0.0023 lr: 0.02\n",
      "iteration: 151770 loss: 0.0028 lr: 0.02\n",
      "iteration: 151780 loss: 0.0017 lr: 0.02\n",
      "iteration: 151790 loss: 0.0020 lr: 0.02\n",
      "iteration: 151800 loss: 0.0034 lr: 0.02\n",
      "iteration: 151810 loss: 0.0040 lr: 0.02\n",
      "iteration: 151820 loss: 0.0027 lr: 0.02\n",
      "iteration: 151830 loss: 0.0027 lr: 0.02\n",
      "iteration: 151840 loss: 0.0029 lr: 0.02\n",
      "iteration: 151850 loss: 0.0020 lr: 0.02\n",
      "iteration: 151860 loss: 0.0033 lr: 0.02\n",
      "iteration: 151870 loss: 0.0026 lr: 0.02\n",
      "iteration: 151880 loss: 0.0022 lr: 0.02\n",
      "iteration: 151890 loss: 0.0020 lr: 0.02\n",
      "iteration: 151900 loss: 0.0027 lr: 0.02\n",
      "iteration: 151910 loss: 0.0023 lr: 0.02\n",
      "iteration: 151920 loss: 0.0026 lr: 0.02\n",
      "iteration: 151930 loss: 0.0028 lr: 0.02\n",
      "iteration: 151940 loss: 0.0038 lr: 0.02\n",
      "iteration: 151950 loss: 0.0022 lr: 0.02\n",
      "iteration: 151960 loss: 0.0027 lr: 0.02\n",
      "iteration: 151970 loss: 0.0025 lr: 0.02\n",
      "iteration: 151980 loss: 0.0021 lr: 0.02\n",
      "iteration: 151990 loss: 0.0039 lr: 0.02\n",
      "iteration: 152000 loss: 0.0032 lr: 0.02\n",
      "iteration: 152010 loss: 0.0024 lr: 0.02\n",
      "iteration: 152020 loss: 0.0029 lr: 0.02\n",
      "iteration: 152030 loss: 0.0019 lr: 0.02\n",
      "iteration: 152040 loss: 0.0037 lr: 0.02\n",
      "iteration: 152050 loss: 0.0028 lr: 0.02\n",
      "iteration: 152060 loss: 0.0026 lr: 0.02\n",
      "iteration: 152070 loss: 0.0033 lr: 0.02\n",
      "iteration: 152080 loss: 0.0026 lr: 0.02\n",
      "iteration: 152090 loss: 0.0030 lr: 0.02\n",
      "iteration: 152100 loss: 0.0028 lr: 0.02\n",
      "iteration: 152110 loss: 0.0016 lr: 0.02\n",
      "iteration: 152120 loss: 0.0021 lr: 0.02\n",
      "iteration: 152130 loss: 0.0026 lr: 0.02\n",
      "iteration: 152140 loss: 0.0021 lr: 0.02\n",
      "iteration: 152150 loss: 0.0020 lr: 0.02\n",
      "iteration: 152160 loss: 0.0030 lr: 0.02\n",
      "iteration: 152170 loss: 0.0024 lr: 0.02\n",
      "iteration: 152180 loss: 0.0017 lr: 0.02\n",
      "iteration: 152190 loss: 0.0025 lr: 0.02\n",
      "iteration: 152200 loss: 0.0021 lr: 0.02\n",
      "iteration: 152210 loss: 0.0036 lr: 0.02\n",
      "iteration: 152220 loss: 0.0024 lr: 0.02\n",
      "iteration: 152230 loss: 0.0021 lr: 0.02\n",
      "iteration: 152240 loss: 0.0031 lr: 0.02\n",
      "iteration: 152250 loss: 0.0023 lr: 0.02\n",
      "iteration: 152260 loss: 0.0023 lr: 0.02\n",
      "iteration: 152270 loss: 0.0029 lr: 0.02\n",
      "iteration: 152280 loss: 0.0020 lr: 0.02\n",
      "iteration: 152290 loss: 0.0034 lr: 0.02\n",
      "iteration: 152300 loss: 0.0022 lr: 0.02\n",
      "iteration: 152310 loss: 0.0020 lr: 0.02\n",
      "iteration: 152320 loss: 0.0030 lr: 0.02\n",
      "iteration: 152330 loss: 0.0030 lr: 0.02\n",
      "iteration: 152340 loss: 0.0025 lr: 0.02\n",
      "iteration: 152350 loss: 0.0025 lr: 0.02\n",
      "iteration: 152360 loss: 0.0022 lr: 0.02\n",
      "iteration: 152370 loss: 0.0027 lr: 0.02\n",
      "iteration: 152380 loss: 0.0023 lr: 0.02\n",
      "iteration: 152390 loss: 0.0030 lr: 0.02\n",
      "iteration: 152400 loss: 0.0016 lr: 0.02\n",
      "iteration: 152410 loss: 0.0017 lr: 0.02\n",
      "iteration: 152420 loss: 0.0026 lr: 0.02\n",
      "iteration: 152430 loss: 0.0024 lr: 0.02\n",
      "iteration: 152440 loss: 0.0029 lr: 0.02\n",
      "iteration: 152450 loss: 0.0024 lr: 0.02\n",
      "iteration: 152460 loss: 0.0027 lr: 0.02\n",
      "iteration: 152470 loss: 0.0027 lr: 0.02\n",
      "iteration: 152480 loss: 0.0031 lr: 0.02\n",
      "iteration: 152490 loss: 0.0022 lr: 0.02\n",
      "iteration: 152500 loss: 0.0025 lr: 0.02\n",
      "iteration: 152510 loss: 0.0023 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 152520 loss: 0.0033 lr: 0.02\n",
      "iteration: 152530 loss: 0.0036 lr: 0.02\n",
      "iteration: 152540 loss: 0.0025 lr: 0.02\n",
      "iteration: 152550 loss: 0.0031 lr: 0.02\n",
      "iteration: 152560 loss: 0.0017 lr: 0.02\n",
      "iteration: 152570 loss: 0.0032 lr: 0.02\n",
      "iteration: 152580 loss: 0.0031 lr: 0.02\n",
      "iteration: 152590 loss: 0.0025 lr: 0.02\n",
      "iteration: 152600 loss: 0.0022 lr: 0.02\n",
      "iteration: 152610 loss: 0.0041 lr: 0.02\n",
      "iteration: 152620 loss: 0.0015 lr: 0.02\n",
      "iteration: 152630 loss: 0.0025 lr: 0.02\n",
      "iteration: 152640 loss: 0.0023 lr: 0.02\n",
      "iteration: 152650 loss: 0.0023 lr: 0.02\n",
      "iteration: 152660 loss: 0.0026 lr: 0.02\n",
      "iteration: 152670 loss: 0.0028 lr: 0.02\n",
      "iteration: 152680 loss: 0.0032 lr: 0.02\n",
      "iteration: 152690 loss: 0.0029 lr: 0.02\n",
      "iteration: 152700 loss: 0.0032 lr: 0.02\n",
      "iteration: 152710 loss: 0.0025 lr: 0.02\n",
      "iteration: 152720 loss: 0.0027 lr: 0.02\n",
      "iteration: 152730 loss: 0.0020 lr: 0.02\n",
      "iteration: 152740 loss: 0.0017 lr: 0.02\n",
      "iteration: 152750 loss: 0.0031 lr: 0.02\n",
      "iteration: 152760 loss: 0.0020 lr: 0.02\n",
      "iteration: 152770 loss: 0.0022 lr: 0.02\n",
      "iteration: 152780 loss: 0.0024 lr: 0.02\n",
      "iteration: 152790 loss: 0.0023 lr: 0.02\n",
      "iteration: 152800 loss: 0.0035 lr: 0.02\n",
      "iteration: 152810 loss: 0.0032 lr: 0.02\n",
      "iteration: 152820 loss: 0.0015 lr: 0.02\n",
      "iteration: 152830 loss: 0.0028 lr: 0.02\n",
      "iteration: 152840 loss: 0.0017 lr: 0.02\n",
      "iteration: 152850 loss: 0.0029 lr: 0.02\n",
      "iteration: 152860 loss: 0.0024 lr: 0.02\n",
      "iteration: 152870 loss: 0.0024 lr: 0.02\n",
      "iteration: 152880 loss: 0.0022 lr: 0.02\n",
      "iteration: 152890 loss: 0.0031 lr: 0.02\n",
      "iteration: 152900 loss: 0.0024 lr: 0.02\n",
      "iteration: 152910 loss: 0.0022 lr: 0.02\n",
      "iteration: 152920 loss: 0.0023 lr: 0.02\n",
      "iteration: 152930 loss: 0.0022 lr: 0.02\n",
      "iteration: 152940 loss: 0.0026 lr: 0.02\n",
      "iteration: 152950 loss: 0.0043 lr: 0.02\n",
      "iteration: 152960 loss: 0.0031 lr: 0.02\n",
      "iteration: 152970 loss: 0.0030 lr: 0.02\n",
      "iteration: 152980 loss: 0.0030 lr: 0.02\n",
      "iteration: 152990 loss: 0.0027 lr: 0.02\n",
      "iteration: 153000 loss: 0.0028 lr: 0.02\n",
      "iteration: 153010 loss: 0.0021 lr: 0.02\n",
      "iteration: 153020 loss: 0.0032 lr: 0.02\n",
      "iteration: 153030 loss: 0.0020 lr: 0.02\n",
      "iteration: 153040 loss: 0.0019 lr: 0.02\n",
      "iteration: 153050 loss: 0.0017 lr: 0.02\n",
      "iteration: 153060 loss: 0.0021 lr: 0.02\n",
      "iteration: 153070 loss: 0.0021 lr: 0.02\n",
      "iteration: 153080 loss: 0.0045 lr: 0.02\n",
      "iteration: 153090 loss: 0.0032 lr: 0.02\n",
      "iteration: 153100 loss: 0.0025 lr: 0.02\n",
      "iteration: 153110 loss: 0.0023 lr: 0.02\n",
      "iteration: 153120 loss: 0.0017 lr: 0.02\n",
      "iteration: 153130 loss: 0.0027 lr: 0.02\n",
      "iteration: 153140 loss: 0.0023 lr: 0.02\n",
      "iteration: 153150 loss: 0.0023 lr: 0.02\n",
      "iteration: 153160 loss: 0.0018 lr: 0.02\n",
      "iteration: 153170 loss: 0.0025 lr: 0.02\n",
      "iteration: 153180 loss: 0.0022 lr: 0.02\n",
      "iteration: 153190 loss: 0.0031 lr: 0.02\n",
      "iteration: 153200 loss: 0.0030 lr: 0.02\n",
      "iteration: 153210 loss: 0.0033 lr: 0.02\n",
      "iteration: 153220 loss: 0.0028 lr: 0.02\n",
      "iteration: 153230 loss: 0.0031 lr: 0.02\n",
      "iteration: 153240 loss: 0.0020 lr: 0.02\n",
      "iteration: 153250 loss: 0.0022 lr: 0.02\n",
      "iteration: 153260 loss: 0.0022 lr: 0.02\n",
      "iteration: 153270 loss: 0.0021 lr: 0.02\n",
      "iteration: 153280 loss: 0.0022 lr: 0.02\n",
      "iteration: 153290 loss: 0.0018 lr: 0.02\n",
      "iteration: 153300 loss: 0.0038 lr: 0.02\n",
      "iteration: 153310 loss: 0.0032 lr: 0.02\n",
      "iteration: 153320 loss: 0.0026 lr: 0.02\n",
      "iteration: 153330 loss: 0.0023 lr: 0.02\n",
      "iteration: 153340 loss: 0.0030 lr: 0.02\n",
      "iteration: 153350 loss: 0.0027 lr: 0.02\n",
      "iteration: 153360 loss: 0.0021 lr: 0.02\n",
      "iteration: 153370 loss: 0.0022 lr: 0.02\n",
      "iteration: 153380 loss: 0.0032 lr: 0.02\n",
      "iteration: 153390 loss: 0.0018 lr: 0.02\n",
      "iteration: 153400 loss: 0.0019 lr: 0.02\n",
      "iteration: 153410 loss: 0.0031 lr: 0.02\n",
      "iteration: 153420 loss: 0.0024 lr: 0.02\n",
      "iteration: 153430 loss: 0.0029 lr: 0.02\n",
      "iteration: 153440 loss: 0.0029 lr: 0.02\n",
      "iteration: 153450 loss: 0.0028 lr: 0.02\n",
      "iteration: 153460 loss: 0.0026 lr: 0.02\n",
      "iteration: 153470 loss: 0.0025 lr: 0.02\n",
      "iteration: 153480 loss: 0.0017 lr: 0.02\n",
      "iteration: 153490 loss: 0.0019 lr: 0.02\n",
      "iteration: 153500 loss: 0.0026 lr: 0.02\n",
      "iteration: 153510 loss: 0.0022 lr: 0.02\n",
      "iteration: 153520 loss: 0.0023 lr: 0.02\n",
      "iteration: 153530 loss: 0.0032 lr: 0.02\n",
      "iteration: 153540 loss: 0.0030 lr: 0.02\n",
      "iteration: 153550 loss: 0.0030 lr: 0.02\n",
      "iteration: 153560 loss: 0.0028 lr: 0.02\n",
      "iteration: 153570 loss: 0.0019 lr: 0.02\n",
      "iteration: 153580 loss: 0.0029 lr: 0.02\n",
      "iteration: 153590 loss: 0.0022 lr: 0.02\n",
      "iteration: 153600 loss: 0.0068 lr: 0.02\n",
      "iteration: 153610 loss: 0.0026 lr: 0.02\n",
      "iteration: 153620 loss: 0.0023 lr: 0.02\n",
      "iteration: 153630 loss: 0.0029 lr: 0.02\n",
      "iteration: 153640 loss: 0.0020 lr: 0.02\n",
      "iteration: 153650 loss: 0.0038 lr: 0.02\n",
      "iteration: 153660 loss: 0.0043 lr: 0.02\n",
      "iteration: 153670 loss: 0.0023 lr: 0.02\n",
      "iteration: 153680 loss: 0.0021 lr: 0.02\n",
      "iteration: 153690 loss: 0.0024 lr: 0.02\n",
      "iteration: 153700 loss: 0.0024 lr: 0.02\n",
      "iteration: 153710 loss: 0.0023 lr: 0.02\n",
      "iteration: 153720 loss: 0.0019 lr: 0.02\n",
      "iteration: 153730 loss: 0.0029 lr: 0.02\n",
      "iteration: 153740 loss: 0.0028 lr: 0.02\n",
      "iteration: 153750 loss: 0.0020 lr: 0.02\n",
      "iteration: 153760 loss: 0.0021 lr: 0.02\n",
      "iteration: 153770 loss: 0.0027 lr: 0.02\n",
      "iteration: 153780 loss: 0.0027 lr: 0.02\n",
      "iteration: 153790 loss: 0.0031 lr: 0.02\n",
      "iteration: 153800 loss: 0.0028 lr: 0.02\n",
      "iteration: 153810 loss: 0.0022 lr: 0.02\n",
      "iteration: 153820 loss: 0.0023 lr: 0.02\n",
      "iteration: 153830 loss: 0.0027 lr: 0.02\n",
      "iteration: 153840 loss: 0.0023 lr: 0.02\n",
      "iteration: 153850 loss: 0.0028 lr: 0.02\n",
      "iteration: 153860 loss: 0.0022 lr: 0.02\n",
      "iteration: 153870 loss: 0.0027 lr: 0.02\n",
      "iteration: 153880 loss: 0.0019 lr: 0.02\n",
      "iteration: 153890 loss: 0.0030 lr: 0.02\n",
      "iteration: 153900 loss: 0.0033 lr: 0.02\n",
      "iteration: 153910 loss: 0.0019 lr: 0.02\n",
      "iteration: 153920 loss: 0.0034 lr: 0.02\n",
      "iteration: 153930 loss: 0.0030 lr: 0.02\n",
      "iteration: 153940 loss: 0.0029 lr: 0.02\n",
      "iteration: 153950 loss: 0.0029 lr: 0.02\n",
      "iteration: 153960 loss: 0.0033 lr: 0.02\n",
      "iteration: 153970 loss: 0.0034 lr: 0.02\n",
      "iteration: 153980 loss: 0.0031 lr: 0.02\n",
      "iteration: 153990 loss: 0.0029 lr: 0.02\n",
      "iteration: 154000 loss: 0.0031 lr: 0.02\n",
      "iteration: 154010 loss: 0.0026 lr: 0.02\n",
      "iteration: 154020 loss: 0.0033 lr: 0.02\n",
      "iteration: 154030 loss: 0.0033 lr: 0.02\n",
      "iteration: 154040 loss: 0.0028 lr: 0.02\n",
      "iteration: 154050 loss: 0.0019 lr: 0.02\n",
      "iteration: 154060 loss: 0.0022 lr: 0.02\n",
      "iteration: 154070 loss: 0.0027 lr: 0.02\n",
      "iteration: 154080 loss: 0.0030 lr: 0.02\n",
      "iteration: 154090 loss: 0.0026 lr: 0.02\n",
      "iteration: 154100 loss: 0.0026 lr: 0.02\n",
      "iteration: 154110 loss: 0.0020 lr: 0.02\n",
      "iteration: 154120 loss: 0.0027 lr: 0.02\n",
      "iteration: 154130 loss: 0.0021 lr: 0.02\n",
      "iteration: 154140 loss: 0.0017 lr: 0.02\n",
      "iteration: 154150 loss: 0.0031 lr: 0.02\n",
      "iteration: 154160 loss: 0.0029 lr: 0.02\n",
      "iteration: 154170 loss: 0.0033 lr: 0.02\n",
      "iteration: 154180 loss: 0.0026 lr: 0.02\n",
      "iteration: 154190 loss: 0.0032 lr: 0.02\n",
      "iteration: 154200 loss: 0.0021 lr: 0.02\n",
      "iteration: 154210 loss: 0.0023 lr: 0.02\n",
      "iteration: 154220 loss: 0.0027 lr: 0.02\n",
      "iteration: 154230 loss: 0.0023 lr: 0.02\n",
      "iteration: 154240 loss: 0.0028 lr: 0.02\n",
      "iteration: 154250 loss: 0.0026 lr: 0.02\n",
      "iteration: 154260 loss: 0.0020 lr: 0.02\n",
      "iteration: 154270 loss: 0.0021 lr: 0.02\n",
      "iteration: 154280 loss: 0.0019 lr: 0.02\n",
      "iteration: 154290 loss: 0.0021 lr: 0.02\n",
      "iteration: 154300 loss: 0.0017 lr: 0.02\n",
      "iteration: 154310 loss: 0.0021 lr: 0.02\n",
      "iteration: 154320 loss: 0.0025 lr: 0.02\n",
      "iteration: 154330 loss: 0.0018 lr: 0.02\n",
      "iteration: 154340 loss: 0.0022 lr: 0.02\n",
      "iteration: 154350 loss: 0.0018 lr: 0.02\n",
      "iteration: 154360 loss: 0.0019 lr: 0.02\n",
      "iteration: 154370 loss: 0.0014 lr: 0.02\n",
      "iteration: 154380 loss: 0.0016 lr: 0.02\n",
      "iteration: 154390 loss: 0.0017 lr: 0.02\n",
      "iteration: 154400 loss: 0.0018 lr: 0.02\n",
      "iteration: 154410 loss: 0.0026 lr: 0.02\n",
      "iteration: 154420 loss: 0.0024 lr: 0.02\n",
      "iteration: 154430 loss: 0.0024 lr: 0.02\n",
      "iteration: 154440 loss: 0.0025 lr: 0.02\n",
      "iteration: 154450 loss: 0.0030 lr: 0.02\n",
      "iteration: 154460 loss: 0.0028 lr: 0.02\n",
      "iteration: 154470 loss: 0.0028 lr: 0.02\n",
      "iteration: 154480 loss: 0.0025 lr: 0.02\n",
      "iteration: 154490 loss: 0.0030 lr: 0.02\n",
      "iteration: 154500 loss: 0.0026 lr: 0.02\n",
      "iteration: 154510 loss: 0.0019 lr: 0.02\n",
      "iteration: 154520 loss: 0.0021 lr: 0.02\n",
      "iteration: 154530 loss: 0.0019 lr: 0.02\n",
      "iteration: 154540 loss: 0.0028 lr: 0.02\n",
      "iteration: 154550 loss: 0.0023 lr: 0.02\n",
      "iteration: 154560 loss: 0.0025 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 154570 loss: 0.0021 lr: 0.02\n",
      "iteration: 154580 loss: 0.0019 lr: 0.02\n",
      "iteration: 154590 loss: 0.0023 lr: 0.02\n",
      "iteration: 154600 loss: 0.0025 lr: 0.02\n",
      "iteration: 154610 loss: 0.0042 lr: 0.02\n",
      "iteration: 154620 loss: 0.0032 lr: 0.02\n",
      "iteration: 154630 loss: 0.0039 lr: 0.02\n",
      "iteration: 154640 loss: 0.0016 lr: 0.02\n",
      "iteration: 154650 loss: 0.0020 lr: 0.02\n",
      "iteration: 154660 loss: 0.0030 lr: 0.02\n",
      "iteration: 154670 loss: 0.0022 lr: 0.02\n",
      "iteration: 154680 loss: 0.0021 lr: 0.02\n",
      "iteration: 154690 loss: 0.0016 lr: 0.02\n",
      "iteration: 154700 loss: 0.0025 lr: 0.02\n",
      "iteration: 154710 loss: 0.0018 lr: 0.02\n",
      "iteration: 154720 loss: 0.0022 lr: 0.02\n",
      "iteration: 154730 loss: 0.0036 lr: 0.02\n",
      "iteration: 154740 loss: 0.0026 lr: 0.02\n",
      "iteration: 154750 loss: 0.0043 lr: 0.02\n",
      "iteration: 154760 loss: 0.0023 lr: 0.02\n",
      "iteration: 154770 loss: 0.0035 lr: 0.02\n",
      "iteration: 154780 loss: 0.0024 lr: 0.02\n",
      "iteration: 154790 loss: 0.0016 lr: 0.02\n",
      "iteration: 154800 loss: 0.0023 lr: 0.02\n",
      "iteration: 154810 loss: 0.0023 lr: 0.02\n",
      "iteration: 154820 loss: 0.0028 lr: 0.02\n",
      "iteration: 154830 loss: 0.0024 lr: 0.02\n",
      "iteration: 154840 loss: 0.0022 lr: 0.02\n",
      "iteration: 154850 loss: 0.0017 lr: 0.02\n",
      "iteration: 154860 loss: 0.0027 lr: 0.02\n",
      "iteration: 154870 loss: 0.0030 lr: 0.02\n",
      "iteration: 154880 loss: 0.0023 lr: 0.02\n",
      "iteration: 154890 loss: 0.0031 lr: 0.02\n",
      "iteration: 154900 loss: 0.0018 lr: 0.02\n",
      "iteration: 154910 loss: 0.0031 lr: 0.02\n",
      "iteration: 154920 loss: 0.0035 lr: 0.02\n",
      "iteration: 154930 loss: 0.0037 lr: 0.02\n",
      "iteration: 154940 loss: 0.0029 lr: 0.02\n",
      "iteration: 154950 loss: 0.0034 lr: 0.02\n",
      "iteration: 154960 loss: 0.0033 lr: 0.02\n",
      "iteration: 154970 loss: 0.0034 lr: 0.02\n",
      "iteration: 154980 loss: 0.0027 lr: 0.02\n",
      "iteration: 154990 loss: 0.0039 lr: 0.02\n",
      "iteration: 155000 loss: 0.0036 lr: 0.02\n",
      "iteration: 155010 loss: 0.0030 lr: 0.02\n",
      "iteration: 155020 loss: 0.0021 lr: 0.02\n",
      "iteration: 155030 loss: 0.0030 lr: 0.02\n",
      "iteration: 155040 loss: 0.0030 lr: 0.02\n",
      "iteration: 155050 loss: 0.0028 lr: 0.02\n",
      "iteration: 155060 loss: 0.0029 lr: 0.02\n",
      "iteration: 155070 loss: 0.0022 lr: 0.02\n",
      "iteration: 155080 loss: 0.0019 lr: 0.02\n",
      "iteration: 155090 loss: 0.0027 lr: 0.02\n",
      "iteration: 155100 loss: 0.0026 lr: 0.02\n",
      "iteration: 155110 loss: 0.0028 lr: 0.02\n",
      "iteration: 155120 loss: 0.0026 lr: 0.02\n",
      "iteration: 155130 loss: 0.0022 lr: 0.02\n",
      "iteration: 155140 loss: 0.0023 lr: 0.02\n",
      "iteration: 155150 loss: 0.0020 lr: 0.02\n",
      "iteration: 155160 loss: 0.0026 lr: 0.02\n",
      "iteration: 155170 loss: 0.0018 lr: 0.02\n",
      "iteration: 155180 loss: 0.0022 lr: 0.02\n",
      "iteration: 155190 loss: 0.0022 lr: 0.02\n",
      "iteration: 155200 loss: 0.0033 lr: 0.02\n",
      "iteration: 155210 loss: 0.0022 lr: 0.02\n",
      "iteration: 155220 loss: 0.0021 lr: 0.02\n",
      "iteration: 155230 loss: 0.0016 lr: 0.02\n",
      "iteration: 155240 loss: 0.0021 lr: 0.02\n",
      "iteration: 155250 loss: 0.0027 lr: 0.02\n",
      "iteration: 155260 loss: 0.0023 lr: 0.02\n",
      "iteration: 155270 loss: 0.0024 lr: 0.02\n",
      "iteration: 155280 loss: 0.0026 lr: 0.02\n",
      "iteration: 155290 loss: 0.0022 lr: 0.02\n",
      "iteration: 155300 loss: 0.0017 lr: 0.02\n",
      "iteration: 155310 loss: 0.0027 lr: 0.02\n",
      "iteration: 155320 loss: 0.0028 lr: 0.02\n",
      "iteration: 155330 loss: 0.0024 lr: 0.02\n",
      "iteration: 155340 loss: 0.0019 lr: 0.02\n",
      "iteration: 155350 loss: 0.0021 lr: 0.02\n",
      "iteration: 155360 loss: 0.0030 lr: 0.02\n",
      "iteration: 155370 loss: 0.0021 lr: 0.02\n",
      "iteration: 155380 loss: 0.0028 lr: 0.02\n",
      "iteration: 155390 loss: 0.0035 lr: 0.02\n",
      "iteration: 155400 loss: 0.0022 lr: 0.02\n",
      "iteration: 155410 loss: 0.0018 lr: 0.02\n",
      "iteration: 155420 loss: 0.0036 lr: 0.02\n",
      "iteration: 155430 loss: 0.0027 lr: 0.02\n",
      "iteration: 155440 loss: 0.0042 lr: 0.02\n",
      "iteration: 155450 loss: 0.0028 lr: 0.02\n",
      "iteration: 155460 loss: 0.0022 lr: 0.02\n",
      "iteration: 155470 loss: 0.0033 lr: 0.02\n",
      "iteration: 155480 loss: 0.0019 lr: 0.02\n",
      "iteration: 155490 loss: 0.0021 lr: 0.02\n",
      "iteration: 155500 loss: 0.0025 lr: 0.02\n",
      "iteration: 155510 loss: 0.0032 lr: 0.02\n",
      "iteration: 155520 loss: 0.0026 lr: 0.02\n",
      "iteration: 155530 loss: 0.0026 lr: 0.02\n",
      "iteration: 155540 loss: 0.0035 lr: 0.02\n",
      "iteration: 155550 loss: 0.0027 lr: 0.02\n",
      "iteration: 155560 loss: 0.0019 lr: 0.02\n",
      "iteration: 155570 loss: 0.0030 lr: 0.02\n",
      "iteration: 155580 loss: 0.0030 lr: 0.02\n",
      "iteration: 155590 loss: 0.0043 lr: 0.02\n",
      "iteration: 155600 loss: 0.0023 lr: 0.02\n",
      "iteration: 155610 loss: 0.0025 lr: 0.02\n",
      "iteration: 155620 loss: 0.0021 lr: 0.02\n",
      "iteration: 155630 loss: 0.0027 lr: 0.02\n",
      "iteration: 155640 loss: 0.0023 lr: 0.02\n",
      "iteration: 155650 loss: 0.0018 lr: 0.02\n",
      "iteration: 155660 loss: 0.0021 lr: 0.02\n",
      "iteration: 155670 loss: 0.0019 lr: 0.02\n",
      "iteration: 155680 loss: 0.0025 lr: 0.02\n",
      "iteration: 155690 loss: 0.0043 lr: 0.02\n",
      "iteration: 155700 loss: 0.0029 lr: 0.02\n",
      "iteration: 155710 loss: 0.0025 lr: 0.02\n",
      "iteration: 155720 loss: 0.0025 lr: 0.02\n",
      "iteration: 155730 loss: 0.0029 lr: 0.02\n",
      "iteration: 155740 loss: 0.0027 lr: 0.02\n",
      "iteration: 155750 loss: 0.0022 lr: 0.02\n",
      "iteration: 155760 loss: 0.0024 lr: 0.02\n",
      "iteration: 155770 loss: 0.0023 lr: 0.02\n",
      "iteration: 155780 loss: 0.0024 lr: 0.02\n",
      "iteration: 155790 loss: 0.0041 lr: 0.02\n",
      "iteration: 155800 loss: 0.0019 lr: 0.02\n",
      "iteration: 155810 loss: 0.0026 lr: 0.02\n",
      "iteration: 155820 loss: 0.0029 lr: 0.02\n",
      "iteration: 155830 loss: 0.0024 lr: 0.02\n",
      "iteration: 155840 loss: 0.0048 lr: 0.02\n",
      "iteration: 155850 loss: 0.0020 lr: 0.02\n",
      "iteration: 155860 loss: 0.0025 lr: 0.02\n",
      "iteration: 155870 loss: 0.0025 lr: 0.02\n",
      "iteration: 155880 loss: 0.0023 lr: 0.02\n",
      "iteration: 155890 loss: 0.0018 lr: 0.02\n",
      "iteration: 155900 loss: 0.0020 lr: 0.02\n",
      "iteration: 155910 loss: 0.0033 lr: 0.02\n",
      "iteration: 155920 loss: 0.0026 lr: 0.02\n",
      "iteration: 155930 loss: 0.0030 lr: 0.02\n",
      "iteration: 155940 loss: 0.0018 lr: 0.02\n",
      "iteration: 155950 loss: 0.0021 lr: 0.02\n",
      "iteration: 155960 loss: 0.0033 lr: 0.02\n",
      "iteration: 155970 loss: 0.0019 lr: 0.02\n",
      "iteration: 155980 loss: 0.0023 lr: 0.02\n",
      "iteration: 155990 loss: 0.0025 lr: 0.02\n",
      "iteration: 156000 loss: 0.0027 lr: 0.02\n",
      "iteration: 156010 loss: 0.0025 lr: 0.02\n",
      "iteration: 156020 loss: 0.0022 lr: 0.02\n",
      "iteration: 156030 loss: 0.0025 lr: 0.02\n",
      "iteration: 156040 loss: 0.0023 lr: 0.02\n",
      "iteration: 156050 loss: 0.0024 lr: 0.02\n",
      "iteration: 156060 loss: 0.0024 lr: 0.02\n",
      "iteration: 156070 loss: 0.0018 lr: 0.02\n",
      "iteration: 156080 loss: 0.0026 lr: 0.02\n",
      "iteration: 156090 loss: 0.0025 lr: 0.02\n",
      "iteration: 156100 loss: 0.0025 lr: 0.02\n",
      "iteration: 156110 loss: 0.0021 lr: 0.02\n",
      "iteration: 156120 loss: 0.0025 lr: 0.02\n",
      "iteration: 156130 loss: 0.0018 lr: 0.02\n",
      "iteration: 156140 loss: 0.0019 lr: 0.02\n",
      "iteration: 156150 loss: 0.0033 lr: 0.02\n",
      "iteration: 156160 loss: 0.0018 lr: 0.02\n",
      "iteration: 156170 loss: 0.0032 lr: 0.02\n",
      "iteration: 156180 loss: 0.0030 lr: 0.02\n",
      "iteration: 156190 loss: 0.0029 lr: 0.02\n",
      "iteration: 156200 loss: 0.0028 lr: 0.02\n",
      "iteration: 156210 loss: 0.0034 lr: 0.02\n",
      "iteration: 156220 loss: 0.0031 lr: 0.02\n",
      "iteration: 156230 loss: 0.0053 lr: 0.02\n",
      "iteration: 156240 loss: 0.0025 lr: 0.02\n",
      "iteration: 156250 loss: 0.0027 lr: 0.02\n",
      "iteration: 156260 loss: 0.0023 lr: 0.02\n",
      "iteration: 156270 loss: 0.0024 lr: 0.02\n",
      "iteration: 156280 loss: 0.0023 lr: 0.02\n",
      "iteration: 156290 loss: 0.0020 lr: 0.02\n",
      "iteration: 156300 loss: 0.0037 lr: 0.02\n",
      "iteration: 156310 loss: 0.0019 lr: 0.02\n",
      "iteration: 156320 loss: 0.0020 lr: 0.02\n",
      "iteration: 156330 loss: 0.0022 lr: 0.02\n",
      "iteration: 156340 loss: 0.0027 lr: 0.02\n",
      "iteration: 156350 loss: 0.0017 lr: 0.02\n",
      "iteration: 156360 loss: 0.0023 lr: 0.02\n",
      "iteration: 156370 loss: 0.0030 lr: 0.02\n",
      "iteration: 156380 loss: 0.0021 lr: 0.02\n",
      "iteration: 156390 loss: 0.0027 lr: 0.02\n",
      "iteration: 156400 loss: 0.0037 lr: 0.02\n",
      "iteration: 156410 loss: 0.0032 lr: 0.02\n",
      "iteration: 156420 loss: 0.0048 lr: 0.02\n",
      "iteration: 156430 loss: 0.0022 lr: 0.02\n",
      "iteration: 156440 loss: 0.0062 lr: 0.02\n",
      "iteration: 156450 loss: 0.0036 lr: 0.02\n",
      "iteration: 156460 loss: 0.0033 lr: 0.02\n",
      "iteration: 156470 loss: 0.0023 lr: 0.02\n",
      "iteration: 156480 loss: 0.0026 lr: 0.02\n",
      "iteration: 156490 loss: 0.0019 lr: 0.02\n",
      "iteration: 156500 loss: 0.0027 lr: 0.02\n",
      "iteration: 156510 loss: 0.0035 lr: 0.02\n",
      "iteration: 156520 loss: 0.0030 lr: 0.02\n",
      "iteration: 156530 loss: 0.0029 lr: 0.02\n",
      "iteration: 156540 loss: 0.0024 lr: 0.02\n",
      "iteration: 156550 loss: 0.0021 lr: 0.02\n",
      "iteration: 156560 loss: 0.0024 lr: 0.02\n",
      "iteration: 156570 loss: 0.0035 lr: 0.02\n",
      "iteration: 156580 loss: 0.0019 lr: 0.02\n",
      "iteration: 156590 loss: 0.0024 lr: 0.02\n",
      "iteration: 156600 loss: 0.0023 lr: 0.02\n",
      "iteration: 156610 loss: 0.0029 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 156620 loss: 0.0026 lr: 0.02\n",
      "iteration: 156630 loss: 0.0027 lr: 0.02\n",
      "iteration: 156640 loss: 0.0017 lr: 0.02\n",
      "iteration: 156650 loss: 0.0032 lr: 0.02\n",
      "iteration: 156660 loss: 0.0024 lr: 0.02\n",
      "iteration: 156670 loss: 0.0025 lr: 0.02\n",
      "iteration: 156680 loss: 0.0022 lr: 0.02\n",
      "iteration: 156690 loss: 0.0037 lr: 0.02\n",
      "iteration: 156700 loss: 0.0016 lr: 0.02\n",
      "iteration: 156710 loss: 0.0033 lr: 0.02\n",
      "iteration: 156720 loss: 0.0030 lr: 0.02\n",
      "iteration: 156730 loss: 0.0027 lr: 0.02\n",
      "iteration: 156740 loss: 0.0016 lr: 0.02\n",
      "iteration: 156750 loss: 0.0026 lr: 0.02\n",
      "iteration: 156760 loss: 0.0021 lr: 0.02\n",
      "iteration: 156770 loss: 0.0025 lr: 0.02\n",
      "iteration: 156780 loss: 0.0025 lr: 0.02\n",
      "iteration: 156790 loss: 0.0021 lr: 0.02\n",
      "iteration: 156800 loss: 0.0028 lr: 0.02\n",
      "iteration: 156810 loss: 0.0026 lr: 0.02\n",
      "iteration: 156820 loss: 0.0023 lr: 0.02\n",
      "iteration: 156830 loss: 0.0017 lr: 0.02\n",
      "iteration: 156840 loss: 0.0027 lr: 0.02\n",
      "iteration: 156850 loss: 0.0027 lr: 0.02\n",
      "iteration: 156860 loss: 0.0027 lr: 0.02\n",
      "iteration: 156870 loss: 0.0023 lr: 0.02\n",
      "iteration: 156880 loss: 0.0024 lr: 0.02\n",
      "iteration: 156890 loss: 0.0037 lr: 0.02\n",
      "iteration: 156900 loss: 0.0020 lr: 0.02\n",
      "iteration: 156910 loss: 0.0027 lr: 0.02\n",
      "iteration: 156920 loss: 0.0033 lr: 0.02\n",
      "iteration: 156930 loss: 0.0051 lr: 0.02\n",
      "iteration: 156940 loss: 0.0026 lr: 0.02\n",
      "iteration: 156950 loss: 0.0019 lr: 0.02\n",
      "iteration: 156960 loss: 0.0024 lr: 0.02\n",
      "iteration: 156970 loss: 0.0047 lr: 0.02\n",
      "iteration: 156980 loss: 0.0031 lr: 0.02\n",
      "iteration: 156990 loss: 0.0024 lr: 0.02\n",
      "iteration: 157000 loss: 0.0019 lr: 0.02\n",
      "iteration: 157010 loss: 0.0022 lr: 0.02\n",
      "iteration: 157020 loss: 0.0022 lr: 0.02\n",
      "iteration: 157030 loss: 0.0016 lr: 0.02\n",
      "iteration: 157040 loss: 0.0027 lr: 0.02\n",
      "iteration: 157050 loss: 0.0031 lr: 0.02\n",
      "iteration: 157060 loss: 0.0023 lr: 0.02\n",
      "iteration: 157070 loss: 0.0053 lr: 0.02\n",
      "iteration: 157080 loss: 0.0029 lr: 0.02\n",
      "iteration: 157090 loss: 0.0027 lr: 0.02\n",
      "iteration: 157100 loss: 0.0024 lr: 0.02\n",
      "iteration: 157110 loss: 0.0018 lr: 0.02\n",
      "iteration: 157120 loss: 0.0028 lr: 0.02\n",
      "iteration: 157130 loss: 0.0023 lr: 0.02\n",
      "iteration: 157140 loss: 0.0022 lr: 0.02\n",
      "iteration: 157150 loss: 0.0021 lr: 0.02\n",
      "iteration: 157160 loss: 0.0029 lr: 0.02\n",
      "iteration: 157170 loss: 0.0024 lr: 0.02\n",
      "iteration: 157180 loss: 0.0023 lr: 0.02\n",
      "iteration: 157190 loss: 0.0034 lr: 0.02\n",
      "iteration: 157200 loss: 0.0020 lr: 0.02\n",
      "iteration: 157210 loss: 0.0025 lr: 0.02\n",
      "iteration: 157220 loss: 0.0024 lr: 0.02\n",
      "iteration: 157230 loss: 0.0022 lr: 0.02\n",
      "iteration: 157240 loss: 0.0034 lr: 0.02\n",
      "iteration: 157250 loss: 0.0024 lr: 0.02\n",
      "iteration: 157260 loss: 0.0033 lr: 0.02\n",
      "iteration: 157270 loss: 0.0018 lr: 0.02\n",
      "iteration: 157280 loss: 0.0025 lr: 0.02\n",
      "iteration: 157290 loss: 0.0019 lr: 0.02\n",
      "iteration: 157300 loss: 0.0024 lr: 0.02\n",
      "iteration: 157310 loss: 0.0022 lr: 0.02\n",
      "iteration: 157320 loss: 0.0025 lr: 0.02\n",
      "iteration: 157330 loss: 0.0039 lr: 0.02\n",
      "iteration: 157340 loss: 0.0030 lr: 0.02\n",
      "iteration: 157350 loss: 0.0026 lr: 0.02\n",
      "iteration: 157360 loss: 0.0032 lr: 0.02\n",
      "iteration: 157370 loss: 0.0019 lr: 0.02\n",
      "iteration: 157380 loss: 0.0022 lr: 0.02\n",
      "iteration: 157390 loss: 0.0023 lr: 0.02\n",
      "iteration: 157400 loss: 0.0027 lr: 0.02\n",
      "iteration: 157410 loss: 0.0026 lr: 0.02\n",
      "iteration: 157420 loss: 0.0029 lr: 0.02\n",
      "iteration: 157430 loss: 0.0027 lr: 0.02\n",
      "iteration: 157440 loss: 0.0021 lr: 0.02\n",
      "iteration: 157450 loss: 0.0020 lr: 0.02\n",
      "iteration: 157460 loss: 0.0042 lr: 0.02\n",
      "iteration: 157470 loss: 0.0024 lr: 0.02\n",
      "iteration: 157480 loss: 0.0022 lr: 0.02\n",
      "iteration: 157490 loss: 0.0022 lr: 0.02\n",
      "iteration: 157500 loss: 0.0018 lr: 0.02\n",
      "iteration: 157510 loss: 0.0025 lr: 0.02\n",
      "iteration: 157520 loss: 0.0021 lr: 0.02\n",
      "iteration: 157530 loss: 0.0018 lr: 0.02\n",
      "iteration: 157540 loss: 0.0029 lr: 0.02\n",
      "iteration: 157550 loss: 0.0023 lr: 0.02\n",
      "iteration: 157560 loss: 0.0020 lr: 0.02\n",
      "iteration: 157570 loss: 0.0016 lr: 0.02\n",
      "iteration: 157580 loss: 0.0023 lr: 0.02\n",
      "iteration: 157590 loss: 0.0018 lr: 0.02\n",
      "iteration: 157600 loss: 0.0054 lr: 0.02\n",
      "iteration: 157610 loss: 0.0016 lr: 0.02\n",
      "iteration: 157620 loss: 0.0025 lr: 0.02\n",
      "iteration: 157630 loss: 0.0023 lr: 0.02\n",
      "iteration: 157640 loss: 0.0026 lr: 0.02\n",
      "iteration: 157650 loss: 0.0021 lr: 0.02\n",
      "iteration: 157660 loss: 0.0019 lr: 0.02\n",
      "iteration: 157670 loss: 0.0021 lr: 0.02\n",
      "iteration: 157680 loss: 0.0019 lr: 0.02\n",
      "iteration: 157690 loss: 0.0028 lr: 0.02\n",
      "iteration: 157700 loss: 0.0030 lr: 0.02\n",
      "iteration: 157710 loss: 0.0026 lr: 0.02\n",
      "iteration: 157720 loss: 0.0045 lr: 0.02\n",
      "iteration: 157730 loss: 0.0031 lr: 0.02\n",
      "iteration: 157740 loss: 0.0026 lr: 0.02\n",
      "iteration: 157750 loss: 0.0030 lr: 0.02\n",
      "iteration: 157760 loss: 0.0032 lr: 0.02\n",
      "iteration: 157770 loss: 0.0018 lr: 0.02\n",
      "iteration: 157780 loss: 0.0047 lr: 0.02\n",
      "iteration: 157790 loss: 0.0025 lr: 0.02\n",
      "iteration: 157800 loss: 0.0023 lr: 0.02\n",
      "iteration: 157810 loss: 0.0029 lr: 0.02\n",
      "iteration: 157820 loss: 0.0030 lr: 0.02\n",
      "iteration: 157830 loss: 0.0021 lr: 0.02\n",
      "iteration: 157840 loss: 0.0015 lr: 0.02\n",
      "iteration: 157850 loss: 0.0024 lr: 0.02\n",
      "iteration: 157860 loss: 0.0025 lr: 0.02\n",
      "iteration: 157870 loss: 0.0022 lr: 0.02\n",
      "iteration: 157880 loss: 0.0026 lr: 0.02\n",
      "iteration: 157890 loss: 0.0033 lr: 0.02\n",
      "iteration: 157900 loss: 0.0019 lr: 0.02\n",
      "iteration: 157910 loss: 0.0029 lr: 0.02\n",
      "iteration: 157920 loss: 0.0020 lr: 0.02\n",
      "iteration: 157930 loss: 0.0023 lr: 0.02\n",
      "iteration: 157940 loss: 0.0021 lr: 0.02\n",
      "iteration: 157950 loss: 0.0021 lr: 0.02\n",
      "iteration: 157960 loss: 0.0025 lr: 0.02\n",
      "iteration: 157970 loss: 0.0029 lr: 0.02\n",
      "iteration: 157980 loss: 0.0019 lr: 0.02\n",
      "iteration: 157990 loss: 0.0018 lr: 0.02\n",
      "iteration: 158000 loss: 0.0043 lr: 0.02\n",
      "iteration: 158010 loss: 0.0023 lr: 0.02\n",
      "iteration: 158020 loss: 0.0026 lr: 0.02\n",
      "iteration: 158030 loss: 0.0024 lr: 0.02\n",
      "iteration: 158040 loss: 0.0025 lr: 0.02\n",
      "iteration: 158050 loss: 0.0025 lr: 0.02\n",
      "iteration: 158060 loss: 0.0020 lr: 0.02\n",
      "iteration: 158070 loss: 0.0026 lr: 0.02\n",
      "iteration: 158080 loss: 0.0028 lr: 0.02\n",
      "iteration: 158090 loss: 0.0023 lr: 0.02\n",
      "iteration: 158100 loss: 0.0024 lr: 0.02\n",
      "iteration: 158110 loss: 0.0030 lr: 0.02\n",
      "iteration: 158120 loss: 0.0025 lr: 0.02\n",
      "iteration: 158130 loss: 0.0032 lr: 0.02\n",
      "iteration: 158140 loss: 0.0023 lr: 0.02\n",
      "iteration: 158150 loss: 0.0026 lr: 0.02\n",
      "iteration: 158160 loss: 0.0027 lr: 0.02\n",
      "iteration: 158170 loss: 0.0029 lr: 0.02\n",
      "iteration: 158180 loss: 0.0018 lr: 0.02\n",
      "iteration: 158190 loss: 0.0018 lr: 0.02\n",
      "iteration: 158200 loss: 0.0031 lr: 0.02\n",
      "iteration: 158210 loss: 0.0024 lr: 0.02\n",
      "iteration: 158220 loss: 0.0025 lr: 0.02\n",
      "iteration: 158230 loss: 0.0034 lr: 0.02\n",
      "iteration: 158240 loss: 0.0021 lr: 0.02\n",
      "iteration: 158250 loss: 0.0027 lr: 0.02\n",
      "iteration: 158260 loss: 0.0027 lr: 0.02\n",
      "iteration: 158270 loss: 0.0027 lr: 0.02\n",
      "iteration: 158280 loss: 0.0049 lr: 0.02\n",
      "iteration: 158290 loss: 0.0036 lr: 0.02\n",
      "iteration: 158300 loss: 0.0023 lr: 0.02\n",
      "iteration: 158310 loss: 0.0033 lr: 0.02\n",
      "iteration: 158320 loss: 0.0026 lr: 0.02\n",
      "iteration: 158330 loss: 0.0032 lr: 0.02\n",
      "iteration: 158340 loss: 0.0022 lr: 0.02\n",
      "iteration: 158350 loss: 0.0019 lr: 0.02\n",
      "iteration: 158360 loss: 0.0018 lr: 0.02\n",
      "iteration: 158370 loss: 0.0026 lr: 0.02\n",
      "iteration: 158380 loss: 0.0024 lr: 0.02\n",
      "iteration: 158390 loss: 0.0026 lr: 0.02\n",
      "iteration: 158400 loss: 0.0022 lr: 0.02\n",
      "iteration: 158410 loss: 0.0027 lr: 0.02\n",
      "iteration: 158420 loss: 0.0024 lr: 0.02\n",
      "iteration: 158430 loss: 0.0024 lr: 0.02\n",
      "iteration: 158440 loss: 0.0021 lr: 0.02\n",
      "iteration: 158450 loss: 0.0020 lr: 0.02\n",
      "iteration: 158460 loss: 0.0015 lr: 0.02\n",
      "iteration: 158470 loss: 0.0028 lr: 0.02\n",
      "iteration: 158480 loss: 0.0024 lr: 0.02\n",
      "iteration: 158490 loss: 0.0023 lr: 0.02\n",
      "iteration: 158500 loss: 0.0019 lr: 0.02\n",
      "iteration: 158510 loss: 0.0025 lr: 0.02\n",
      "iteration: 158520 loss: 0.0022 lr: 0.02\n",
      "iteration: 158530 loss: 0.0016 lr: 0.02\n",
      "iteration: 158540 loss: 0.0025 lr: 0.02\n",
      "iteration: 158550 loss: 0.0024 lr: 0.02\n",
      "iteration: 158560 loss: 0.0037 lr: 0.02\n",
      "iteration: 158570 loss: 0.0017 lr: 0.02\n",
      "iteration: 158580 loss: 0.0029 lr: 0.02\n",
      "iteration: 158590 loss: 0.0033 lr: 0.02\n",
      "iteration: 158600 loss: 0.0034 lr: 0.02\n",
      "iteration: 158610 loss: 0.0023 lr: 0.02\n",
      "iteration: 158620 loss: 0.0029 lr: 0.02\n",
      "iteration: 158630 loss: 0.0043 lr: 0.02\n",
      "iteration: 158640 loss: 0.0017 lr: 0.02\n",
      "iteration: 158650 loss: 0.0027 lr: 0.02\n",
      "iteration: 158660 loss: 0.0029 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 158670 loss: 0.0033 lr: 0.02\n",
      "iteration: 158680 loss: 0.0022 lr: 0.02\n",
      "iteration: 158690 loss: 0.0021 lr: 0.02\n",
      "iteration: 158700 loss: 0.0024 lr: 0.02\n",
      "iteration: 158710 loss: 0.0023 lr: 0.02\n",
      "iteration: 158720 loss: 0.0033 lr: 0.02\n",
      "iteration: 158730 loss: 0.0022 lr: 0.02\n",
      "iteration: 158740 loss: 0.0021 lr: 0.02\n",
      "iteration: 158750 loss: 0.0027 lr: 0.02\n",
      "iteration: 158760 loss: 0.0030 lr: 0.02\n",
      "iteration: 158770 loss: 0.0020 lr: 0.02\n",
      "iteration: 158780 loss: 0.0021 lr: 0.02\n",
      "iteration: 158790 loss: 0.0023 lr: 0.02\n",
      "iteration: 158800 loss: 0.0031 lr: 0.02\n",
      "iteration: 158810 loss: 0.0024 lr: 0.02\n",
      "iteration: 158820 loss: 0.0025 lr: 0.02\n",
      "iteration: 158830 loss: 0.0030 lr: 0.02\n",
      "iteration: 158840 loss: 0.0016 lr: 0.02\n",
      "iteration: 158850 loss: 0.0035 lr: 0.02\n",
      "iteration: 158860 loss: 0.0024 lr: 0.02\n",
      "iteration: 158870 loss: 0.0023 lr: 0.02\n",
      "iteration: 158880 loss: 0.0020 lr: 0.02\n",
      "iteration: 158890 loss: 0.0035 lr: 0.02\n",
      "iteration: 158900 loss: 0.0022 lr: 0.02\n",
      "iteration: 158910 loss: 0.0025 lr: 0.02\n",
      "iteration: 158920 loss: 0.0025 lr: 0.02\n",
      "iteration: 158930 loss: 0.0039 lr: 0.02\n",
      "iteration: 158940 loss: 0.0019 lr: 0.02\n",
      "iteration: 158950 loss: 0.0019 lr: 0.02\n",
      "iteration: 158960 loss: 0.0021 lr: 0.02\n",
      "iteration: 158970 loss: 0.0028 lr: 0.02\n",
      "iteration: 158980 loss: 0.0026 lr: 0.02\n",
      "iteration: 158990 loss: 0.0020 lr: 0.02\n",
      "iteration: 159000 loss: 0.0020 lr: 0.02\n",
      "iteration: 159010 loss: 0.0018 lr: 0.02\n",
      "iteration: 159020 loss: 0.0015 lr: 0.02\n",
      "iteration: 159030 loss: 0.0022 lr: 0.02\n",
      "iteration: 159040 loss: 0.0027 lr: 0.02\n",
      "iteration: 159050 loss: 0.0026 lr: 0.02\n",
      "iteration: 159060 loss: 0.0037 lr: 0.02\n",
      "iteration: 159070 loss: 0.0023 lr: 0.02\n",
      "iteration: 159080 loss: 0.0026 lr: 0.02\n",
      "iteration: 159090 loss: 0.0018 lr: 0.02\n",
      "iteration: 159100 loss: 0.0019 lr: 0.02\n",
      "iteration: 159110 loss: 0.0019 lr: 0.02\n",
      "iteration: 159120 loss: 0.0027 lr: 0.02\n",
      "iteration: 159130 loss: 0.0028 lr: 0.02\n",
      "iteration: 159140 loss: 0.0029 lr: 0.02\n",
      "iteration: 159150 loss: 0.0028 lr: 0.02\n",
      "iteration: 159160 loss: 0.0023 lr: 0.02\n",
      "iteration: 159170 loss: 0.0035 lr: 0.02\n",
      "iteration: 159180 loss: 0.0022 lr: 0.02\n",
      "iteration: 159190 loss: 0.0023 lr: 0.02\n",
      "iteration: 159200 loss: 0.0027 lr: 0.02\n",
      "iteration: 159210 loss: 0.0022 lr: 0.02\n",
      "iteration: 159220 loss: 0.0026 lr: 0.02\n",
      "iteration: 159230 loss: 0.0049 lr: 0.02\n",
      "iteration: 159240 loss: 0.0024 lr: 0.02\n",
      "iteration: 159250 loss: 0.0030 lr: 0.02\n",
      "iteration: 159260 loss: 0.0027 lr: 0.02\n",
      "iteration: 159270 loss: 0.0022 lr: 0.02\n",
      "iteration: 159280 loss: 0.0021 lr: 0.02\n",
      "iteration: 159290 loss: 0.0031 lr: 0.02\n",
      "iteration: 159300 loss: 0.0027 lr: 0.02\n",
      "iteration: 159310 loss: 0.0035 lr: 0.02\n",
      "iteration: 159320 loss: 0.0027 lr: 0.02\n",
      "iteration: 159330 loss: 0.0020 lr: 0.02\n",
      "iteration: 159340 loss: 0.0021 lr: 0.02\n",
      "iteration: 159350 loss: 0.0022 lr: 0.02\n",
      "iteration: 159360 loss: 0.0028 lr: 0.02\n",
      "iteration: 159370 loss: 0.0026 lr: 0.02\n",
      "iteration: 159380 loss: 0.0020 lr: 0.02\n",
      "iteration: 159390 loss: 0.0018 lr: 0.02\n",
      "iteration: 159400 loss: 0.0022 lr: 0.02\n",
      "iteration: 159410 loss: 0.0017 lr: 0.02\n",
      "iteration: 159420 loss: 0.0032 lr: 0.02\n",
      "iteration: 159430 loss: 0.0021 lr: 0.02\n",
      "iteration: 159440 loss: 0.0021 lr: 0.02\n",
      "iteration: 159450 loss: 0.0035 lr: 0.02\n",
      "iteration: 159460 loss: 0.0027 lr: 0.02\n",
      "iteration: 159470 loss: 0.0017 lr: 0.02\n",
      "iteration: 159480 loss: 0.0037 lr: 0.02\n",
      "iteration: 159490 loss: 0.0018 lr: 0.02\n",
      "iteration: 159500 loss: 0.0021 lr: 0.02\n",
      "iteration: 159510 loss: 0.0023 lr: 0.02\n",
      "iteration: 159520 loss: 0.0025 lr: 0.02\n",
      "iteration: 159530 loss: 0.0041 lr: 0.02\n",
      "iteration: 159540 loss: 0.0038 lr: 0.02\n",
      "iteration: 159550 loss: 0.0013 lr: 0.02\n",
      "iteration: 159560 loss: 0.0026 lr: 0.02\n",
      "iteration: 159570 loss: 0.0026 lr: 0.02\n",
      "iteration: 159580 loss: 0.0022 lr: 0.02\n",
      "iteration: 159590 loss: 0.0027 lr: 0.02\n",
      "iteration: 159600 loss: 0.0034 lr: 0.02\n",
      "iteration: 159610 loss: 0.0020 lr: 0.02\n",
      "iteration: 159620 loss: 0.0035 lr: 0.02\n",
      "iteration: 159630 loss: 0.0034 lr: 0.02\n",
      "iteration: 159640 loss: 0.0023 lr: 0.02\n",
      "iteration: 159650 loss: 0.0030 lr: 0.02\n",
      "iteration: 159660 loss: 0.0022 lr: 0.02\n",
      "iteration: 159670 loss: 0.0019 lr: 0.02\n",
      "iteration: 159680 loss: 0.0028 lr: 0.02\n",
      "iteration: 159690 loss: 0.0018 lr: 0.02\n",
      "iteration: 159700 loss: 0.0024 lr: 0.02\n",
      "iteration: 159710 loss: 0.0026 lr: 0.02\n",
      "iteration: 159720 loss: 0.0026 lr: 0.02\n",
      "iteration: 159730 loss: 0.0041 lr: 0.02\n",
      "iteration: 159740 loss: 0.0026 lr: 0.02\n",
      "iteration: 159750 loss: 0.0024 lr: 0.02\n",
      "iteration: 159760 loss: 0.0016 lr: 0.02\n",
      "iteration: 159770 loss: 0.0025 lr: 0.02\n",
      "iteration: 159780 loss: 0.0028 lr: 0.02\n",
      "iteration: 159790 loss: 0.0022 lr: 0.02\n",
      "iteration: 159800 loss: 0.0023 lr: 0.02\n",
      "iteration: 159810 loss: 0.0020 lr: 0.02\n",
      "iteration: 159820 loss: 0.0027 lr: 0.02\n",
      "iteration: 159830 loss: 0.0036 lr: 0.02\n",
      "iteration: 159840 loss: 0.0024 lr: 0.02\n",
      "iteration: 159850 loss: 0.0027 lr: 0.02\n",
      "iteration: 159860 loss: 0.0026 lr: 0.02\n",
      "iteration: 159870 loss: 0.0026 lr: 0.02\n",
      "iteration: 159880 loss: 0.0025 lr: 0.02\n",
      "iteration: 159890 loss: 0.0029 lr: 0.02\n",
      "iteration: 159900 loss: 0.0024 lr: 0.02\n",
      "iteration: 159910 loss: 0.0023 lr: 0.02\n",
      "iteration: 159920 loss: 0.0030 lr: 0.02\n",
      "iteration: 159930 loss: 0.0024 lr: 0.02\n",
      "iteration: 159940 loss: 0.0021 lr: 0.02\n",
      "iteration: 159950 loss: 0.0036 lr: 0.02\n",
      "iteration: 159960 loss: 0.0022 lr: 0.02\n",
      "iteration: 159970 loss: 0.0030 lr: 0.02\n",
      "iteration: 159980 loss: 0.0038 lr: 0.02\n",
      "iteration: 159990 loss: 0.0029 lr: 0.02\n",
      "iteration: 160000 loss: 0.0028 lr: 0.02\n",
      "iteration: 160010 loss: 0.0021 lr: 0.02\n",
      "iteration: 160020 loss: 0.0028 lr: 0.02\n",
      "iteration: 160030 loss: 0.0025 lr: 0.02\n",
      "iteration: 160040 loss: 0.0017 lr: 0.02\n",
      "iteration: 160050 loss: 0.0020 lr: 0.02\n",
      "iteration: 160060 loss: 0.0023 lr: 0.02\n",
      "iteration: 160070 loss: 0.0024 lr: 0.02\n",
      "iteration: 160080 loss: 0.0018 lr: 0.02\n",
      "iteration: 160090 loss: 0.0021 lr: 0.02\n",
      "iteration: 160100 loss: 0.0029 lr: 0.02\n",
      "iteration: 160110 loss: 0.0022 lr: 0.02\n",
      "iteration: 160120 loss: 0.0033 lr: 0.02\n",
      "iteration: 160130 loss: 0.0023 lr: 0.02\n",
      "iteration: 160140 loss: 0.0044 lr: 0.02\n",
      "iteration: 160150 loss: 0.0028 lr: 0.02\n",
      "iteration: 160160 loss: 0.0031 lr: 0.02\n",
      "iteration: 160170 loss: 0.0025 lr: 0.02\n",
      "iteration: 160180 loss: 0.0016 lr: 0.02\n",
      "iteration: 160190 loss: 0.0030 lr: 0.02\n",
      "iteration: 160200 loss: 0.0021 lr: 0.02\n",
      "iteration: 160210 loss: 0.0020 lr: 0.02\n",
      "iteration: 160220 loss: 0.0022 lr: 0.02\n",
      "iteration: 160230 loss: 0.0023 lr: 0.02\n",
      "iteration: 160240 loss: 0.0034 lr: 0.02\n",
      "iteration: 160250 loss: 0.0027 lr: 0.02\n",
      "iteration: 160260 loss: 0.0024 lr: 0.02\n",
      "iteration: 160270 loss: 0.0020 lr: 0.02\n",
      "iteration: 160280 loss: 0.0024 lr: 0.02\n",
      "iteration: 160290 loss: 0.0021 lr: 0.02\n",
      "iteration: 160300 loss: 0.0032 lr: 0.02\n",
      "iteration: 160310 loss: 0.0025 lr: 0.02\n",
      "iteration: 160320 loss: 0.0025 lr: 0.02\n",
      "iteration: 160330 loss: 0.0028 lr: 0.02\n",
      "iteration: 160340 loss: 0.0015 lr: 0.02\n",
      "iteration: 160350 loss: 0.0025 lr: 0.02\n",
      "iteration: 160360 loss: 0.0026 lr: 0.02\n",
      "iteration: 160370 loss: 0.0014 lr: 0.02\n",
      "iteration: 160380 loss: 0.0024 lr: 0.02\n",
      "iteration: 160390 loss: 0.0025 lr: 0.02\n",
      "iteration: 160400 loss: 0.0031 lr: 0.02\n",
      "iteration: 160410 loss: 0.0024 lr: 0.02\n",
      "iteration: 160420 loss: 0.0028 lr: 0.02\n",
      "iteration: 160430 loss: 0.0025 lr: 0.02\n",
      "iteration: 160440 loss: 0.0024 lr: 0.02\n",
      "iteration: 160450 loss: 0.0031 lr: 0.02\n",
      "iteration: 160460 loss: 0.0036 lr: 0.02\n",
      "iteration: 160470 loss: 0.0019 lr: 0.02\n",
      "iteration: 160480 loss: 0.0021 lr: 0.02\n",
      "iteration: 160490 loss: 0.0027 lr: 0.02\n",
      "iteration: 160500 loss: 0.0023 lr: 0.02\n",
      "iteration: 160510 loss: 0.0029 lr: 0.02\n",
      "iteration: 160520 loss: 0.0032 lr: 0.02\n",
      "iteration: 160530 loss: 0.0031 lr: 0.02\n",
      "iteration: 160540 loss: 0.0029 lr: 0.02\n",
      "iteration: 160550 loss: 0.0030 lr: 0.02\n",
      "iteration: 160560 loss: 0.0024 lr: 0.02\n",
      "iteration: 160570 loss: 0.0025 lr: 0.02\n",
      "iteration: 160580 loss: 0.0034 lr: 0.02\n",
      "iteration: 160590 loss: 0.0024 lr: 0.02\n",
      "iteration: 160600 loss: 0.0030 lr: 0.02\n",
      "iteration: 160610 loss: 0.0024 lr: 0.02\n",
      "iteration: 160620 loss: 0.0023 lr: 0.02\n",
      "iteration: 160630 loss: 0.0026 lr: 0.02\n",
      "iteration: 160640 loss: 0.0028 lr: 0.02\n",
      "iteration: 160650 loss: 0.0021 lr: 0.02\n",
      "iteration: 160660 loss: 0.0021 lr: 0.02\n",
      "iteration: 160670 loss: 0.0020 lr: 0.02\n",
      "iteration: 160680 loss: 0.0024 lr: 0.02\n",
      "iteration: 160690 loss: 0.0017 lr: 0.02\n",
      "iteration: 160700 loss: 0.0026 lr: 0.02\n",
      "iteration: 160710 loss: 0.0029 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 160720 loss: 0.0020 lr: 0.02\n",
      "iteration: 160730 loss: 0.0024 lr: 0.02\n",
      "iteration: 160740 loss: 0.0034 lr: 0.02\n",
      "iteration: 160750 loss: 0.0025 lr: 0.02\n",
      "iteration: 160760 loss: 0.0027 lr: 0.02\n",
      "iteration: 160770 loss: 0.0033 lr: 0.02\n",
      "iteration: 160780 loss: 0.0026 lr: 0.02\n",
      "iteration: 160790 loss: 0.0028 lr: 0.02\n",
      "iteration: 160800 loss: 0.0028 lr: 0.02\n",
      "iteration: 160810 loss: 0.0021 lr: 0.02\n",
      "iteration: 160820 loss: 0.0027 lr: 0.02\n",
      "iteration: 160830 loss: 0.0035 lr: 0.02\n",
      "iteration: 160840 loss: 0.0026 lr: 0.02\n",
      "iteration: 160850 loss: 0.0021 lr: 0.02\n",
      "iteration: 160860 loss: 0.0025 lr: 0.02\n",
      "iteration: 160870 loss: 0.0019 lr: 0.02\n",
      "iteration: 160880 loss: 0.0018 lr: 0.02\n",
      "iteration: 160890 loss: 0.0027 lr: 0.02\n",
      "iteration: 160900 loss: 0.0022 lr: 0.02\n",
      "iteration: 160910 loss: 0.0022 lr: 0.02\n",
      "iteration: 160920 loss: 0.0022 lr: 0.02\n",
      "iteration: 160930 loss: 0.0034 lr: 0.02\n",
      "iteration: 160940 loss: 0.0025 lr: 0.02\n",
      "iteration: 160950 loss: 0.0029 lr: 0.02\n",
      "iteration: 160960 loss: 0.0034 lr: 0.02\n",
      "iteration: 160970 loss: 0.0024 lr: 0.02\n",
      "iteration: 160980 loss: 0.0028 lr: 0.02\n",
      "iteration: 160990 loss: 0.0022 lr: 0.02\n",
      "iteration: 161000 loss: 0.0027 lr: 0.02\n",
      "iteration: 161010 loss: 0.0018 lr: 0.02\n",
      "iteration: 161020 loss: 0.0022 lr: 0.02\n",
      "iteration: 161030 loss: 0.0024 lr: 0.02\n",
      "iteration: 161040 loss: 0.0022 lr: 0.02\n",
      "iteration: 161050 loss: 0.0028 lr: 0.02\n",
      "iteration: 161060 loss: 0.0023 lr: 0.02\n",
      "iteration: 161070 loss: 0.0034 lr: 0.02\n",
      "iteration: 161080 loss: 0.0024 lr: 0.02\n",
      "iteration: 161090 loss: 0.0018 lr: 0.02\n",
      "iteration: 161100 loss: 0.0022 lr: 0.02\n",
      "iteration: 161110 loss: 0.0024 lr: 0.02\n",
      "iteration: 161120 loss: 0.0019 lr: 0.02\n",
      "iteration: 161130 loss: 0.0020 lr: 0.02\n",
      "iteration: 161140 loss: 0.0024 lr: 0.02\n",
      "iteration: 161150 loss: 0.0017 lr: 0.02\n",
      "iteration: 161160 loss: 0.0021 lr: 0.02\n",
      "iteration: 161170 loss: 0.0023 lr: 0.02\n",
      "iteration: 161180 loss: 0.0033 lr: 0.02\n",
      "iteration: 161190 loss: 0.0031 lr: 0.02\n",
      "iteration: 161200 loss: 0.0023 lr: 0.02\n",
      "iteration: 161210 loss: 0.0027 lr: 0.02\n",
      "iteration: 161220 loss: 0.0017 lr: 0.02\n",
      "iteration: 161230 loss: 0.0027 lr: 0.02\n",
      "iteration: 161240 loss: 0.0019 lr: 0.02\n",
      "iteration: 161250 loss: 0.0025 lr: 0.02\n",
      "iteration: 161260 loss: 0.0028 lr: 0.02\n",
      "iteration: 161270 loss: 0.0024 lr: 0.02\n",
      "iteration: 161280 loss: 0.0036 lr: 0.02\n",
      "iteration: 161290 loss: 0.0049 lr: 0.02\n",
      "iteration: 161300 loss: 0.0033 lr: 0.02\n",
      "iteration: 161310 loss: 0.0035 lr: 0.02\n",
      "iteration: 161320 loss: 0.0024 lr: 0.02\n",
      "iteration: 161330 loss: 0.0030 lr: 0.02\n",
      "iteration: 161340 loss: 0.0022 lr: 0.02\n",
      "iteration: 161350 loss: 0.0025 lr: 0.02\n",
      "iteration: 161360 loss: 0.0027 lr: 0.02\n",
      "iteration: 161370 loss: 0.0024 lr: 0.02\n",
      "iteration: 161380 loss: 0.0019 lr: 0.02\n",
      "iteration: 161390 loss: 0.0030 lr: 0.02\n",
      "iteration: 161400 loss: 0.0034 lr: 0.02\n",
      "iteration: 161410 loss: 0.0031 lr: 0.02\n",
      "iteration: 161420 loss: 0.0027 lr: 0.02\n",
      "iteration: 161430 loss: 0.0017 lr: 0.02\n",
      "iteration: 161440 loss: 0.0025 lr: 0.02\n",
      "iteration: 161450 loss: 0.0018 lr: 0.02\n",
      "iteration: 161460 loss: 0.0023 lr: 0.02\n",
      "iteration: 161470 loss: 0.0020 lr: 0.02\n",
      "iteration: 161480 loss: 0.0024 lr: 0.02\n",
      "iteration: 161490 loss: 0.0020 lr: 0.02\n",
      "iteration: 161500 loss: 0.0024 lr: 0.02\n",
      "iteration: 161510 loss: 0.0022 lr: 0.02\n",
      "iteration: 161520 loss: 0.0026 lr: 0.02\n",
      "iteration: 161530 loss: 0.0036 lr: 0.02\n",
      "iteration: 161540 loss: 0.0019 lr: 0.02\n",
      "iteration: 161550 loss: 0.0031 lr: 0.02\n",
      "iteration: 161560 loss: 0.0026 lr: 0.02\n",
      "iteration: 161570 loss: 0.0028 lr: 0.02\n",
      "iteration: 161580 loss: 0.0016 lr: 0.02\n",
      "iteration: 161590 loss: 0.0030 lr: 0.02\n",
      "iteration: 161600 loss: 0.0021 lr: 0.02\n",
      "iteration: 161610 loss: 0.0032 lr: 0.02\n",
      "iteration: 161620 loss: 0.0020 lr: 0.02\n",
      "iteration: 161630 loss: 0.0021 lr: 0.02\n",
      "iteration: 161640 loss: 0.0024 lr: 0.02\n",
      "iteration: 161650 loss: 0.0024 lr: 0.02\n",
      "iteration: 161660 loss: 0.0027 lr: 0.02\n",
      "iteration: 161670 loss: 0.0033 lr: 0.02\n",
      "iteration: 161680 loss: 0.0027 lr: 0.02\n",
      "iteration: 161690 loss: 0.0033 lr: 0.02\n",
      "iteration: 161700 loss: 0.0022 lr: 0.02\n",
      "iteration: 161710 loss: 0.0019 lr: 0.02\n",
      "iteration: 161720 loss: 0.0019 lr: 0.02\n",
      "iteration: 161730 loss: 0.0024 lr: 0.02\n",
      "iteration: 161740 loss: 0.0024 lr: 0.02\n",
      "iteration: 161750 loss: 0.0016 lr: 0.02\n",
      "iteration: 161760 loss: 0.0030 lr: 0.02\n",
      "iteration: 161770 loss: 0.0039 lr: 0.02\n",
      "iteration: 161780 loss: 0.0028 lr: 0.02\n",
      "iteration: 161790 loss: 0.0025 lr: 0.02\n",
      "iteration: 161800 loss: 0.0020 lr: 0.02\n",
      "iteration: 161810 loss: 0.0029 lr: 0.02\n",
      "iteration: 161820 loss: 0.0028 lr: 0.02\n",
      "iteration: 161830 loss: 0.0025 lr: 0.02\n",
      "iteration: 161840 loss: 0.0020 lr: 0.02\n",
      "iteration: 161850 loss: 0.0017 lr: 0.02\n",
      "iteration: 161860 loss: 0.0034 lr: 0.02\n",
      "iteration: 161870 loss: 0.0053 lr: 0.02\n",
      "iteration: 161880 loss: 0.0032 lr: 0.02\n",
      "iteration: 161890 loss: 0.0023 lr: 0.02\n",
      "iteration: 161900 loss: 0.0021 lr: 0.02\n",
      "iteration: 161910 loss: 0.0038 lr: 0.02\n",
      "iteration: 161920 loss: 0.0022 lr: 0.02\n",
      "iteration: 161930 loss: 0.0027 lr: 0.02\n",
      "iteration: 161940 loss: 0.0019 lr: 0.02\n",
      "iteration: 161950 loss: 0.0028 lr: 0.02\n",
      "iteration: 161960 loss: 0.0025 lr: 0.02\n",
      "iteration: 161970 loss: 0.0022 lr: 0.02\n",
      "iteration: 161980 loss: 0.0023 lr: 0.02\n",
      "iteration: 161990 loss: 0.0023 lr: 0.02\n",
      "iteration: 162000 loss: 0.0025 lr: 0.02\n",
      "iteration: 162010 loss: 0.0021 lr: 0.02\n",
      "iteration: 162020 loss: 0.0026 lr: 0.02\n",
      "iteration: 162030 loss: 0.0025 lr: 0.02\n",
      "iteration: 162040 loss: 0.0026 lr: 0.02\n",
      "iteration: 162050 loss: 0.0042 lr: 0.02\n",
      "iteration: 162060 loss: 0.0035 lr: 0.02\n",
      "iteration: 162070 loss: 0.0023 lr: 0.02\n",
      "iteration: 162080 loss: 0.0024 lr: 0.02\n",
      "iteration: 162090 loss: 0.0024 lr: 0.02\n",
      "iteration: 162100 loss: 0.0028 lr: 0.02\n",
      "iteration: 162110 loss: 0.0035 lr: 0.02\n",
      "iteration: 162120 loss: 0.0021 lr: 0.02\n",
      "iteration: 162130 loss: 0.0020 lr: 0.02\n",
      "iteration: 162140 loss: 0.0022 lr: 0.02\n",
      "iteration: 162150 loss: 0.0024 lr: 0.02\n",
      "iteration: 162160 loss: 0.0027 lr: 0.02\n",
      "iteration: 162170 loss: 0.0019 lr: 0.02\n",
      "iteration: 162180 loss: 0.0024 lr: 0.02\n",
      "iteration: 162190 loss: 0.0022 lr: 0.02\n",
      "iteration: 162200 loss: 0.0030 lr: 0.02\n",
      "iteration: 162210 loss: 0.0019 lr: 0.02\n",
      "iteration: 162220 loss: 0.0022 lr: 0.02\n",
      "iteration: 162230 loss: 0.0016 lr: 0.02\n",
      "iteration: 162240 loss: 0.0024 lr: 0.02\n",
      "iteration: 162250 loss: 0.0021 lr: 0.02\n",
      "iteration: 162260 loss: 0.0041 lr: 0.02\n",
      "iteration: 162270 loss: 0.0041 lr: 0.02\n",
      "iteration: 162280 loss: 0.0032 lr: 0.02\n",
      "iteration: 162290 loss: 0.0024 lr: 0.02\n",
      "iteration: 162300 loss: 0.0021 lr: 0.02\n",
      "iteration: 162310 loss: 0.0022 lr: 0.02\n",
      "iteration: 162320 loss: 0.0034 lr: 0.02\n",
      "iteration: 162330 loss: 0.0032 lr: 0.02\n",
      "iteration: 162340 loss: 0.0025 lr: 0.02\n",
      "iteration: 162350 loss: 0.0029 lr: 0.02\n",
      "iteration: 162360 loss: 0.0027 lr: 0.02\n",
      "iteration: 162370 loss: 0.0044 lr: 0.02\n",
      "iteration: 162380 loss: 0.0021 lr: 0.02\n",
      "iteration: 162390 loss: 0.0034 lr: 0.02\n",
      "iteration: 162400 loss: 0.0021 lr: 0.02\n",
      "iteration: 162410 loss: 0.0018 lr: 0.02\n",
      "iteration: 162420 loss: 0.0016 lr: 0.02\n",
      "iteration: 162430 loss: 0.0019 lr: 0.02\n",
      "iteration: 162440 loss: 0.0018 lr: 0.02\n",
      "iteration: 162450 loss: 0.0021 lr: 0.02\n",
      "iteration: 162460 loss: 0.0021 lr: 0.02\n",
      "iteration: 162470 loss: 0.0028 lr: 0.02\n",
      "iteration: 162480 loss: 0.0015 lr: 0.02\n",
      "iteration: 162490 loss: 0.0013 lr: 0.02\n",
      "iteration: 162500 loss: 0.0027 lr: 0.02\n",
      "iteration: 162510 loss: 0.0040 lr: 0.02\n",
      "iteration: 162520 loss: 0.0023 lr: 0.02\n",
      "iteration: 162530 loss: 0.0017 lr: 0.02\n",
      "iteration: 162540 loss: 0.0021 lr: 0.02\n",
      "iteration: 162550 loss: 0.0025 lr: 0.02\n",
      "iteration: 162560 loss: 0.0019 lr: 0.02\n",
      "iteration: 162570 loss: 0.0022 lr: 0.02\n",
      "iteration: 162580 loss: 0.0025 lr: 0.02\n",
      "iteration: 162590 loss: 0.0031 lr: 0.02\n",
      "iteration: 162600 loss: 0.0025 lr: 0.02\n",
      "iteration: 162610 loss: 0.0023 lr: 0.02\n",
      "iteration: 162620 loss: 0.0018 lr: 0.02\n",
      "iteration: 162630 loss: 0.0014 lr: 0.02\n",
      "iteration: 162640 loss: 0.0016 lr: 0.02\n",
      "iteration: 162650 loss: 0.0019 lr: 0.02\n",
      "iteration: 162660 loss: 0.0023 lr: 0.02\n",
      "iteration: 162670 loss: 0.0024 lr: 0.02\n",
      "iteration: 162680 loss: 0.0022 lr: 0.02\n",
      "iteration: 162690 loss: 0.0026 lr: 0.02\n",
      "iteration: 162700 loss: 0.0023 lr: 0.02\n",
      "iteration: 162710 loss: 0.0028 lr: 0.02\n",
      "iteration: 162720 loss: 0.0028 lr: 0.02\n",
      "iteration: 162730 loss: 0.0018 lr: 0.02\n",
      "iteration: 162740 loss: 0.0031 lr: 0.02\n",
      "iteration: 162750 loss: 0.0019 lr: 0.02\n",
      "iteration: 162760 loss: 0.0029 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 162770 loss: 0.0012 lr: 0.02\n",
      "iteration: 162780 loss: 0.0021 lr: 0.02\n",
      "iteration: 162790 loss: 0.0023 lr: 0.02\n",
      "iteration: 162800 loss: 0.0020 lr: 0.02\n",
      "iteration: 162810 loss: 0.0031 lr: 0.02\n",
      "iteration: 162820 loss: 0.0017 lr: 0.02\n",
      "iteration: 162830 loss: 0.0022 lr: 0.02\n",
      "iteration: 162840 loss: 0.0011 lr: 0.02\n",
      "iteration: 162850 loss: 0.0021 lr: 0.02\n",
      "iteration: 162860 loss: 0.0017 lr: 0.02\n",
      "iteration: 162870 loss: 0.0022 lr: 0.02\n",
      "iteration: 162880 loss: 0.0020 lr: 0.02\n",
      "iteration: 162890 loss: 0.0017 lr: 0.02\n",
      "iteration: 162900 loss: 0.0025 lr: 0.02\n",
      "iteration: 162910 loss: 0.0025 lr: 0.02\n",
      "iteration: 162920 loss: 0.0023 lr: 0.02\n",
      "iteration: 162930 loss: 0.0025 lr: 0.02\n",
      "iteration: 162940 loss: 0.0022 lr: 0.02\n",
      "iteration: 162950 loss: 0.0019 lr: 0.02\n",
      "iteration: 162960 loss: 0.0023 lr: 0.02\n",
      "iteration: 162970 loss: 0.0023 lr: 0.02\n",
      "iteration: 162980 loss: 0.0020 lr: 0.02\n",
      "iteration: 162990 loss: 0.0015 lr: 0.02\n",
      "iteration: 163000 loss: 0.0019 lr: 0.02\n",
      "iteration: 163010 loss: 0.0023 lr: 0.02\n",
      "iteration: 163020 loss: 0.0023 lr: 0.02\n",
      "iteration: 163030 loss: 0.0021 lr: 0.02\n",
      "iteration: 163040 loss: 0.0026 lr: 0.02\n",
      "iteration: 163050 loss: 0.0030 lr: 0.02\n",
      "iteration: 163060 loss: 0.0021 lr: 0.02\n",
      "iteration: 163070 loss: 0.0026 lr: 0.02\n",
      "iteration: 163080 loss: 0.0018 lr: 0.02\n",
      "iteration: 163090 loss: 0.0028 lr: 0.02\n",
      "iteration: 163100 loss: 0.0027 lr: 0.02\n",
      "iteration: 163110 loss: 0.0022 lr: 0.02\n",
      "iteration: 163120 loss: 0.0022 lr: 0.02\n",
      "iteration: 163130 loss: 0.0028 lr: 0.02\n",
      "iteration: 163140 loss: 0.0023 lr: 0.02\n",
      "iteration: 163150 loss: 0.0027 lr: 0.02\n",
      "iteration: 163160 loss: 0.0024 lr: 0.02\n",
      "iteration: 163170 loss: 0.0023 lr: 0.02\n",
      "iteration: 163180 loss: 0.0027 lr: 0.02\n",
      "iteration: 163190 loss: 0.0024 lr: 0.02\n",
      "iteration: 163200 loss: 0.0024 lr: 0.02\n",
      "iteration: 163210 loss: 0.0022 lr: 0.02\n",
      "iteration: 163220 loss: 0.0028 lr: 0.02\n",
      "iteration: 163230 loss: 0.0022 lr: 0.02\n",
      "iteration: 163240 loss: 0.0020 lr: 0.02\n",
      "iteration: 163250 loss: 0.0026 lr: 0.02\n",
      "iteration: 163260 loss: 0.0026 lr: 0.02\n",
      "iteration: 163270 loss: 0.0030 lr: 0.02\n",
      "iteration: 163280 loss: 0.0026 lr: 0.02\n",
      "iteration: 163290 loss: 0.0042 lr: 0.02\n",
      "iteration: 163300 loss: 0.0023 lr: 0.02\n",
      "iteration: 163310 loss: 0.0025 lr: 0.02\n",
      "iteration: 163320 loss: 0.0030 lr: 0.02\n",
      "iteration: 163330 loss: 0.0021 lr: 0.02\n",
      "iteration: 163340 loss: 0.0023 lr: 0.02\n",
      "iteration: 163350 loss: 0.0025 lr: 0.02\n",
      "iteration: 163360 loss: 0.0020 lr: 0.02\n",
      "iteration: 163370 loss: 0.0020 lr: 0.02\n",
      "iteration: 163380 loss: 0.0021 lr: 0.02\n",
      "iteration: 163390 loss: 0.0028 lr: 0.02\n",
      "iteration: 163400 loss: 0.0030 lr: 0.02\n",
      "iteration: 163410 loss: 0.0028 lr: 0.02\n",
      "iteration: 163420 loss: 0.0024 lr: 0.02\n",
      "iteration: 163430 loss: 0.0034 lr: 0.02\n",
      "iteration: 163440 loss: 0.0033 lr: 0.02\n",
      "iteration: 163450 loss: 0.0025 lr: 0.02\n",
      "iteration: 163460 loss: 0.0024 lr: 0.02\n",
      "iteration: 163470 loss: 0.0029 lr: 0.02\n",
      "iteration: 163480 loss: 0.0027 lr: 0.02\n",
      "iteration: 163490 loss: 0.0030 lr: 0.02\n",
      "iteration: 163500 loss: 0.0030 lr: 0.02\n",
      "iteration: 163510 loss: 0.0025 lr: 0.02\n",
      "iteration: 163520 loss: 0.0024 lr: 0.02\n",
      "iteration: 163530 loss: 0.0041 lr: 0.02\n",
      "iteration: 163540 loss: 0.0025 lr: 0.02\n",
      "iteration: 163550 loss: 0.0020 lr: 0.02\n",
      "iteration: 163560 loss: 0.0025 lr: 0.02\n",
      "iteration: 163570 loss: 0.0022 lr: 0.02\n",
      "iteration: 163580 loss: 0.0019 lr: 0.02\n",
      "iteration: 163590 loss: 0.0029 lr: 0.02\n",
      "iteration: 163600 loss: 0.0018 lr: 0.02\n",
      "iteration: 163610 loss: 0.0023 lr: 0.02\n",
      "iteration: 163620 loss: 0.0034 lr: 0.02\n",
      "iteration: 163630 loss: 0.0023 lr: 0.02\n",
      "iteration: 163640 loss: 0.0017 lr: 0.02\n",
      "iteration: 163650 loss: 0.0018 lr: 0.02\n",
      "iteration: 163660 loss: 0.0022 lr: 0.02\n",
      "iteration: 163670 loss: 0.0023 lr: 0.02\n",
      "iteration: 163680 loss: 0.0022 lr: 0.02\n",
      "iteration: 163690 loss: 0.0020 lr: 0.02\n",
      "iteration: 163700 loss: 0.0019 lr: 0.02\n",
      "iteration: 163710 loss: 0.0018 lr: 0.02\n",
      "iteration: 163720 loss: 0.0023 lr: 0.02\n",
      "iteration: 163730 loss: 0.0020 lr: 0.02\n",
      "iteration: 163740 loss: 0.0016 lr: 0.02\n",
      "iteration: 163750 loss: 0.0016 lr: 0.02\n",
      "iteration: 163760 loss: 0.0030 lr: 0.02\n",
      "iteration: 163770 loss: 0.0022 lr: 0.02\n",
      "iteration: 163780 loss: 0.0026 lr: 0.02\n",
      "iteration: 163790 loss: 0.0020 lr: 0.02\n",
      "iteration: 163800 loss: 0.0020 lr: 0.02\n",
      "iteration: 163810 loss: 0.0021 lr: 0.02\n",
      "iteration: 163820 loss: 0.0028 lr: 0.02\n",
      "iteration: 163830 loss: 0.0020 lr: 0.02\n",
      "iteration: 163840 loss: 0.0016 lr: 0.02\n",
      "iteration: 163850 loss: 0.0019 lr: 0.02\n",
      "iteration: 163860 loss: 0.0024 lr: 0.02\n",
      "iteration: 163870 loss: 0.0019 lr: 0.02\n",
      "iteration: 163880 loss: 0.0021 lr: 0.02\n",
      "iteration: 163890 loss: 0.0022 lr: 0.02\n",
      "iteration: 163900 loss: 0.0031 lr: 0.02\n",
      "iteration: 163910 loss: 0.0021 lr: 0.02\n",
      "iteration: 163920 loss: 0.0021 lr: 0.02\n",
      "iteration: 163930 loss: 0.0027 lr: 0.02\n",
      "iteration: 163940 loss: 0.0022 lr: 0.02\n",
      "iteration: 163950 loss: 0.0015 lr: 0.02\n",
      "iteration: 163960 loss: 0.0025 lr: 0.02\n",
      "iteration: 163970 loss: 0.0019 lr: 0.02\n",
      "iteration: 163980 loss: 0.0022 lr: 0.02\n",
      "iteration: 163990 loss: 0.0031 lr: 0.02\n",
      "iteration: 164000 loss: 0.0027 lr: 0.02\n",
      "iteration: 164010 loss: 0.0021 lr: 0.02\n",
      "iteration: 164020 loss: 0.0026 lr: 0.02\n",
      "iteration: 164030 loss: 0.0019 lr: 0.02\n",
      "iteration: 164040 loss: 0.0032 lr: 0.02\n",
      "iteration: 164050 loss: 0.0025 lr: 0.02\n",
      "iteration: 164060 loss: 0.0026 lr: 0.02\n",
      "iteration: 164070 loss: 0.0025 lr: 0.02\n",
      "iteration: 164080 loss: 0.0021 lr: 0.02\n",
      "iteration: 164090 loss: 0.0018 lr: 0.02\n",
      "iteration: 164100 loss: 0.0026 lr: 0.02\n",
      "iteration: 164110 loss: 0.0016 lr: 0.02\n",
      "iteration: 164120 loss: 0.0023 lr: 0.02\n",
      "iteration: 164130 loss: 0.0022 lr: 0.02\n",
      "iteration: 164140 loss: 0.0031 lr: 0.02\n",
      "iteration: 164150 loss: 0.0035 lr: 0.02\n",
      "iteration: 164160 loss: 0.0029 lr: 0.02\n",
      "iteration: 164170 loss: 0.0047 lr: 0.02\n",
      "iteration: 164180 loss: 0.0020 lr: 0.02\n",
      "iteration: 164190 loss: 0.0033 lr: 0.02\n",
      "iteration: 164200 loss: 0.0029 lr: 0.02\n",
      "iteration: 164210 loss: 0.0023 lr: 0.02\n",
      "iteration: 164220 loss: 0.0026 lr: 0.02\n",
      "iteration: 164230 loss: 0.0024 lr: 0.02\n",
      "iteration: 164240 loss: 0.0021 lr: 0.02\n",
      "iteration: 164250 loss: 0.0021 lr: 0.02\n",
      "iteration: 164260 loss: 0.0025 lr: 0.02\n",
      "iteration: 164270 loss: 0.0025 lr: 0.02\n",
      "iteration: 164280 loss: 0.0022 lr: 0.02\n",
      "iteration: 164290 loss: 0.0020 lr: 0.02\n",
      "iteration: 164300 loss: 0.0024 lr: 0.02\n",
      "iteration: 164310 loss: 0.0023 lr: 0.02\n",
      "iteration: 164320 loss: 0.0026 lr: 0.02\n",
      "iteration: 164330 loss: 0.0016 lr: 0.02\n",
      "iteration: 164340 loss: 0.0017 lr: 0.02\n",
      "iteration: 164350 loss: 0.0025 lr: 0.02\n",
      "iteration: 164360 loss: 0.0028 lr: 0.02\n",
      "iteration: 164370 loss: 0.0028 lr: 0.02\n",
      "iteration: 164380 loss: 0.0029 lr: 0.02\n",
      "iteration: 164390 loss: 0.0020 lr: 0.02\n",
      "iteration: 164400 loss: 0.0025 lr: 0.02\n",
      "iteration: 164410 loss: 0.0018 lr: 0.02\n",
      "iteration: 164420 loss: 0.0017 lr: 0.02\n",
      "iteration: 164430 loss: 0.0027 lr: 0.02\n",
      "iteration: 164440 loss: 0.0025 lr: 0.02\n",
      "iteration: 164450 loss: 0.0022 lr: 0.02\n",
      "iteration: 164460 loss: 0.0021 lr: 0.02\n",
      "iteration: 164470 loss: 0.0021 lr: 0.02\n",
      "iteration: 164480 loss: 0.0033 lr: 0.02\n",
      "iteration: 164490 loss: 0.0022 lr: 0.02\n",
      "iteration: 164500 loss: 0.0021 lr: 0.02\n",
      "iteration: 164510 loss: 0.0031 lr: 0.02\n",
      "iteration: 164520 loss: 0.0029 lr: 0.02\n",
      "iteration: 164530 loss: 0.0045 lr: 0.02\n",
      "iteration: 164540 loss: 0.0021 lr: 0.02\n",
      "iteration: 164550 loss: 0.0028 lr: 0.02\n",
      "iteration: 164560 loss: 0.0024 lr: 0.02\n",
      "iteration: 164570 loss: 0.0040 lr: 0.02\n",
      "iteration: 164580 loss: 0.0029 lr: 0.02\n",
      "iteration: 164590 loss: 0.0023 lr: 0.02\n",
      "iteration: 164600 loss: 0.0024 lr: 0.02\n",
      "iteration: 164610 loss: 0.0021 lr: 0.02\n",
      "iteration: 164620 loss: 0.0029 lr: 0.02\n",
      "iteration: 164630 loss: 0.0031 lr: 0.02\n",
      "iteration: 164640 loss: 0.0047 lr: 0.02\n",
      "iteration: 164650 loss: 0.0024 lr: 0.02\n",
      "iteration: 164660 loss: 0.0026 lr: 0.02\n",
      "iteration: 164670 loss: 0.0023 lr: 0.02\n",
      "iteration: 164680 loss: 0.0027 lr: 0.02\n",
      "iteration: 164690 loss: 0.0061 lr: 0.02\n",
      "iteration: 164700 loss: 0.0027 lr: 0.02\n",
      "iteration: 164710 loss: 0.0029 lr: 0.02\n",
      "iteration: 164720 loss: 0.0025 lr: 0.02\n",
      "iteration: 164730 loss: 0.0023 lr: 0.02\n",
      "iteration: 164740 loss: 0.0025 lr: 0.02\n",
      "iteration: 164750 loss: 0.0032 lr: 0.02\n",
      "iteration: 164760 loss: 0.0044 lr: 0.02\n",
      "iteration: 164770 loss: 0.0022 lr: 0.02\n",
      "iteration: 164780 loss: 0.0027 lr: 0.02\n",
      "iteration: 164790 loss: 0.0023 lr: 0.02\n",
      "iteration: 164800 loss: 0.0021 lr: 0.02\n",
      "iteration: 164810 loss: 0.0023 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 164820 loss: 0.0025 lr: 0.02\n",
      "iteration: 164830 loss: 0.0018 lr: 0.02\n",
      "iteration: 164840 loss: 0.0024 lr: 0.02\n",
      "iteration: 164850 loss: 0.0029 lr: 0.02\n",
      "iteration: 164860 loss: 0.0022 lr: 0.02\n",
      "iteration: 164870 loss: 0.0026 lr: 0.02\n",
      "iteration: 164880 loss: 0.0023 lr: 0.02\n",
      "iteration: 164890 loss: 0.0020 lr: 0.02\n",
      "iteration: 164900 loss: 0.0032 lr: 0.02\n",
      "iteration: 164910 loss: 0.0024 lr: 0.02\n",
      "iteration: 164920 loss: 0.0017 lr: 0.02\n",
      "iteration: 164930 loss: 0.0016 lr: 0.02\n",
      "iteration: 164940 loss: 0.0023 lr: 0.02\n",
      "iteration: 164950 loss: 0.0033 lr: 0.02\n",
      "iteration: 164960 loss: 0.0018 lr: 0.02\n",
      "iteration: 164970 loss: 0.0027 lr: 0.02\n",
      "iteration: 164980 loss: 0.0023 lr: 0.02\n",
      "iteration: 164990 loss: 0.0022 lr: 0.02\n",
      "iteration: 165000 loss: 0.0025 lr: 0.02\n",
      "iteration: 165010 loss: 0.0031 lr: 0.02\n",
      "iteration: 165020 loss: 0.0027 lr: 0.02\n",
      "iteration: 165030 loss: 0.0029 lr: 0.02\n",
      "iteration: 165040 loss: 0.0020 lr: 0.02\n",
      "iteration: 165050 loss: 0.0023 lr: 0.02\n",
      "iteration: 165060 loss: 0.0019 lr: 0.02\n",
      "iteration: 165070 loss: 0.0018 lr: 0.02\n",
      "iteration: 165080 loss: 0.0028 lr: 0.02\n",
      "iteration: 165090 loss: 0.0026 lr: 0.02\n",
      "iteration: 165100 loss: 0.0018 lr: 0.02\n",
      "iteration: 165110 loss: 0.0022 lr: 0.02\n",
      "iteration: 165120 loss: 0.0019 lr: 0.02\n",
      "iteration: 165130 loss: 0.0022 lr: 0.02\n",
      "iteration: 165140 loss: 0.0023 lr: 0.02\n",
      "iteration: 165150 loss: 0.0020 lr: 0.02\n",
      "iteration: 165160 loss: 0.0024 lr: 0.02\n",
      "iteration: 165170 loss: 0.0027 lr: 0.02\n",
      "iteration: 165180 loss: 0.0026 lr: 0.02\n",
      "iteration: 165190 loss: 0.0020 lr: 0.02\n",
      "iteration: 165200 loss: 0.0018 lr: 0.02\n",
      "iteration: 165210 loss: 0.0020 lr: 0.02\n",
      "iteration: 165220 loss: 0.0035 lr: 0.02\n",
      "iteration: 165230 loss: 0.0020 lr: 0.02\n",
      "iteration: 165240 loss: 0.0016 lr: 0.02\n",
      "iteration: 165250 loss: 0.0023 lr: 0.02\n",
      "iteration: 165260 loss: 0.0024 lr: 0.02\n",
      "iteration: 165270 loss: 0.0015 lr: 0.02\n",
      "iteration: 165280 loss: 0.0031 lr: 0.02\n",
      "iteration: 165290 loss: 0.0022 lr: 0.02\n",
      "iteration: 165300 loss: 0.0022 lr: 0.02\n",
      "iteration: 165310 loss: 0.0019 lr: 0.02\n",
      "iteration: 165320 loss: 0.0023 lr: 0.02\n",
      "iteration: 165330 loss: 0.0021 lr: 0.02\n",
      "iteration: 165340 loss: 0.0023 lr: 0.02\n",
      "iteration: 165350 loss: 0.0030 lr: 0.02\n",
      "iteration: 165360 loss: 0.0017 lr: 0.02\n",
      "iteration: 165370 loss: 0.0028 lr: 0.02\n",
      "iteration: 165380 loss: 0.0028 lr: 0.02\n",
      "iteration: 165390 loss: 0.0018 lr: 0.02\n",
      "iteration: 165400 loss: 0.0019 lr: 0.02\n",
      "iteration: 165410 loss: 0.0021 lr: 0.02\n",
      "iteration: 165420 loss: 0.0022 lr: 0.02\n",
      "iteration: 165430 loss: 0.0025 lr: 0.02\n",
      "iteration: 165440 loss: 0.0027 lr: 0.02\n",
      "iteration: 165450 loss: 0.0027 lr: 0.02\n",
      "iteration: 165460 loss: 0.0024 lr: 0.02\n",
      "iteration: 165470 loss: 0.0027 lr: 0.02\n",
      "iteration: 165480 loss: 0.0033 lr: 0.02\n",
      "iteration: 165490 loss: 0.0020 lr: 0.02\n",
      "iteration: 165500 loss: 0.0028 lr: 0.02\n",
      "iteration: 165510 loss: 0.0031 lr: 0.02\n",
      "iteration: 165520 loss: 0.0022 lr: 0.02\n",
      "iteration: 165530 loss: 0.0019 lr: 0.02\n",
      "iteration: 165540 loss: 0.0018 lr: 0.02\n",
      "iteration: 165550 loss: 0.0023 lr: 0.02\n",
      "iteration: 165560 loss: 0.0022 lr: 0.02\n",
      "iteration: 165570 loss: 0.0016 lr: 0.02\n",
      "iteration: 165580 loss: 0.0017 lr: 0.02\n",
      "iteration: 165590 loss: 0.0021 lr: 0.02\n",
      "iteration: 165600 loss: 0.0025 lr: 0.02\n",
      "iteration: 165610 loss: 0.0030 lr: 0.02\n",
      "iteration: 165620 loss: 0.0025 lr: 0.02\n",
      "iteration: 165630 loss: 0.0023 lr: 0.02\n",
      "iteration: 165640 loss: 0.0024 lr: 0.02\n",
      "iteration: 165650 loss: 0.0023 lr: 0.02\n",
      "iteration: 165660 loss: 0.0024 lr: 0.02\n",
      "iteration: 165670 loss: 0.0042 lr: 0.02\n",
      "iteration: 165680 loss: 0.0022 lr: 0.02\n",
      "iteration: 165690 loss: 0.0025 lr: 0.02\n",
      "iteration: 165700 loss: 0.0030 lr: 0.02\n",
      "iteration: 165710 loss: 0.0021 lr: 0.02\n",
      "iteration: 165720 loss: 0.0025 lr: 0.02\n",
      "iteration: 165730 loss: 0.0028 lr: 0.02\n",
      "iteration: 165740 loss: 0.0016 lr: 0.02\n",
      "iteration: 165750 loss: 0.0031 lr: 0.02\n",
      "iteration: 165760 loss: 0.0031 lr: 0.02\n",
      "iteration: 165770 loss: 0.0019 lr: 0.02\n",
      "iteration: 165780 loss: 0.0027 lr: 0.02\n",
      "iteration: 165790 loss: 0.0020 lr: 0.02\n",
      "iteration: 165800 loss: 0.0026 lr: 0.02\n",
      "iteration: 165810 loss: 0.0037 lr: 0.02\n",
      "iteration: 165820 loss: 0.0023 lr: 0.02\n",
      "iteration: 165830 loss: 0.0021 lr: 0.02\n",
      "iteration: 165840 loss: 0.0031 lr: 0.02\n",
      "iteration: 165850 loss: 0.0023 lr: 0.02\n",
      "iteration: 165860 loss: 0.0026 lr: 0.02\n",
      "iteration: 165870 loss: 0.0029 lr: 0.02\n",
      "iteration: 165880 loss: 0.0033 lr: 0.02\n",
      "iteration: 165890 loss: 0.0019 lr: 0.02\n",
      "iteration: 165900 loss: 0.0017 lr: 0.02\n",
      "iteration: 165910 loss: 0.0029 lr: 0.02\n",
      "iteration: 165920 loss: 0.0031 lr: 0.02\n",
      "iteration: 165930 loss: 0.0026 lr: 0.02\n",
      "iteration: 165940 loss: 0.0019 lr: 0.02\n",
      "iteration: 165950 loss: 0.0036 lr: 0.02\n",
      "iteration: 165960 loss: 0.0016 lr: 0.02\n",
      "iteration: 165970 loss: 0.0024 lr: 0.02\n",
      "iteration: 165980 loss: 0.0031 lr: 0.02\n",
      "iteration: 165990 loss: 0.0023 lr: 0.02\n",
      "iteration: 166000 loss: 0.0017 lr: 0.02\n",
      "iteration: 166010 loss: 0.0020 lr: 0.02\n",
      "iteration: 166020 loss: 0.0023 lr: 0.02\n",
      "iteration: 166030 loss: 0.0024 lr: 0.02\n",
      "iteration: 166040 loss: 0.0021 lr: 0.02\n",
      "iteration: 166050 loss: 0.0026 lr: 0.02\n",
      "iteration: 166060 loss: 0.0037 lr: 0.02\n",
      "iteration: 166070 loss: 0.0021 lr: 0.02\n",
      "iteration: 166080 loss: 0.0025 lr: 0.02\n",
      "iteration: 166090 loss: 0.0027 lr: 0.02\n",
      "iteration: 166100 loss: 0.0025 lr: 0.02\n",
      "iteration: 166110 loss: 0.0025 lr: 0.02\n",
      "iteration: 166120 loss: 0.0018 lr: 0.02\n",
      "iteration: 166130 loss: 0.0016 lr: 0.02\n",
      "iteration: 166140 loss: 0.0024 lr: 0.02\n",
      "iteration: 166150 loss: 0.0023 lr: 0.02\n",
      "iteration: 166160 loss: 0.0024 lr: 0.02\n",
      "iteration: 166170 loss: 0.0029 lr: 0.02\n",
      "iteration: 166180 loss: 0.0021 lr: 0.02\n",
      "iteration: 166190 loss: 0.0014 lr: 0.02\n",
      "iteration: 166200 loss: 0.0024 lr: 0.02\n",
      "iteration: 166210 loss: 0.0025 lr: 0.02\n",
      "iteration: 166220 loss: 0.0022 lr: 0.02\n",
      "iteration: 166230 loss: 0.0017 lr: 0.02\n",
      "iteration: 166240 loss: 0.0033 lr: 0.02\n",
      "iteration: 166250 loss: 0.0034 lr: 0.02\n",
      "iteration: 166260 loss: 0.0022 lr: 0.02\n",
      "iteration: 166270 loss: 0.0018 lr: 0.02\n",
      "iteration: 166280 loss: 0.0026 lr: 0.02\n",
      "iteration: 166290 loss: 0.0024 lr: 0.02\n",
      "iteration: 166300 loss: 0.0021 lr: 0.02\n",
      "iteration: 166310 loss: 0.0024 lr: 0.02\n",
      "iteration: 166320 loss: 0.0019 lr: 0.02\n",
      "iteration: 166330 loss: 0.0017 lr: 0.02\n",
      "iteration: 166340 loss: 0.0019 lr: 0.02\n",
      "iteration: 166350 loss: 0.0019 lr: 0.02\n",
      "iteration: 166360 loss: 0.0029 lr: 0.02\n",
      "iteration: 166370 loss: 0.0015 lr: 0.02\n",
      "iteration: 166380 loss: 0.0022 lr: 0.02\n",
      "iteration: 166390 loss: 0.0019 lr: 0.02\n",
      "iteration: 166400 loss: 0.0020 lr: 0.02\n",
      "iteration: 166410 loss: 0.0027 lr: 0.02\n",
      "iteration: 166420 loss: 0.0022 lr: 0.02\n",
      "iteration: 166430 loss: 0.0024 lr: 0.02\n",
      "iteration: 166440 loss: 0.0018 lr: 0.02\n",
      "iteration: 166450 loss: 0.0024 lr: 0.02\n",
      "iteration: 166460 loss: 0.0021 lr: 0.02\n",
      "iteration: 166470 loss: 0.0017 lr: 0.02\n",
      "iteration: 166480 loss: 0.0017 lr: 0.02\n",
      "iteration: 166490 loss: 0.0020 lr: 0.02\n",
      "iteration: 166500 loss: 0.0025 lr: 0.02\n",
      "iteration: 166510 loss: 0.0029 lr: 0.02\n",
      "iteration: 166520 loss: 0.0026 lr: 0.02\n",
      "iteration: 166530 loss: 0.0023 lr: 0.02\n",
      "iteration: 166540 loss: 0.0024 lr: 0.02\n",
      "iteration: 166550 loss: 0.0026 lr: 0.02\n",
      "iteration: 166560 loss: 0.0027 lr: 0.02\n",
      "iteration: 166570 loss: 0.0023 lr: 0.02\n",
      "iteration: 166580 loss: 0.0032 lr: 0.02\n",
      "iteration: 166590 loss: 0.0026 lr: 0.02\n",
      "iteration: 166600 loss: 0.0027 lr: 0.02\n",
      "iteration: 166610 loss: 0.0025 lr: 0.02\n",
      "iteration: 166620 loss: 0.0025 lr: 0.02\n",
      "iteration: 166630 loss: 0.0024 lr: 0.02\n",
      "iteration: 166640 loss: 0.0019 lr: 0.02\n",
      "iteration: 166650 loss: 0.0020 lr: 0.02\n",
      "iteration: 166660 loss: 0.0021 lr: 0.02\n",
      "iteration: 166670 loss: 0.0024 lr: 0.02\n",
      "iteration: 166680 loss: 0.0026 lr: 0.02\n",
      "iteration: 166690 loss: 0.0020 lr: 0.02\n",
      "iteration: 166700 loss: 0.0029 lr: 0.02\n",
      "iteration: 166710 loss: 0.0022 lr: 0.02\n",
      "iteration: 166720 loss: 0.0018 lr: 0.02\n",
      "iteration: 166730 loss: 0.0024 lr: 0.02\n",
      "iteration: 166740 loss: 0.0026 lr: 0.02\n",
      "iteration: 166750 loss: 0.0020 lr: 0.02\n",
      "iteration: 166760 loss: 0.0022 lr: 0.02\n",
      "iteration: 166770 loss: 0.0021 lr: 0.02\n",
      "iteration: 166780 loss: 0.0022 lr: 0.02\n",
      "iteration: 166790 loss: 0.0026 lr: 0.02\n",
      "iteration: 166800 loss: 0.0025 lr: 0.02\n",
      "iteration: 166810 loss: 0.0017 lr: 0.02\n",
      "iteration: 166820 loss: 0.0020 lr: 0.02\n",
      "iteration: 166830 loss: 0.0019 lr: 0.02\n",
      "iteration: 166840 loss: 0.0022 lr: 0.02\n",
      "iteration: 166850 loss: 0.0021 lr: 0.02\n",
      "iteration: 166860 loss: 0.0021 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 166870 loss: 0.0020 lr: 0.02\n",
      "iteration: 166880 loss: 0.0023 lr: 0.02\n",
      "iteration: 166890 loss: 0.0023 lr: 0.02\n",
      "iteration: 166900 loss: 0.0022 lr: 0.02\n",
      "iteration: 166910 loss: 0.0023 lr: 0.02\n",
      "iteration: 166920 loss: 0.0023 lr: 0.02\n",
      "iteration: 166930 loss: 0.0025 lr: 0.02\n",
      "iteration: 166940 loss: 0.0016 lr: 0.02\n",
      "iteration: 166950 loss: 0.0028 lr: 0.02\n",
      "iteration: 166960 loss: 0.0018 lr: 0.02\n",
      "iteration: 166970 loss: 0.0016 lr: 0.02\n",
      "iteration: 166980 loss: 0.0026 lr: 0.02\n",
      "iteration: 166990 loss: 0.0019 lr: 0.02\n",
      "iteration: 167000 loss: 0.0024 lr: 0.02\n",
      "iteration: 167010 loss: 0.0028 lr: 0.02\n",
      "iteration: 167020 loss: 0.0028 lr: 0.02\n",
      "iteration: 167030 loss: 0.0028 lr: 0.02\n",
      "iteration: 167040 loss: 0.0035 lr: 0.02\n",
      "iteration: 167050 loss: 0.0020 lr: 0.02\n",
      "iteration: 167060 loss: 0.0029 lr: 0.02\n",
      "iteration: 167070 loss: 0.0029 lr: 0.02\n",
      "iteration: 167080 loss: 0.0019 lr: 0.02\n",
      "iteration: 167090 loss: 0.0021 lr: 0.02\n",
      "iteration: 167100 loss: 0.0022 lr: 0.02\n",
      "iteration: 167110 loss: 0.0045 lr: 0.02\n",
      "iteration: 167120 loss: 0.0024 lr: 0.02\n",
      "iteration: 167130 loss: 0.0023 lr: 0.02\n",
      "iteration: 167140 loss: 0.0026 lr: 0.02\n",
      "iteration: 167150 loss: 0.0022 lr: 0.02\n",
      "iteration: 167160 loss: 0.0029 lr: 0.02\n",
      "iteration: 167170 loss: 0.0018 lr: 0.02\n",
      "iteration: 167180 loss: 0.0022 lr: 0.02\n",
      "iteration: 167190 loss: 0.0029 lr: 0.02\n",
      "iteration: 167200 loss: 0.0023 lr: 0.02\n",
      "iteration: 167210 loss: 0.0021 lr: 0.02\n",
      "iteration: 167220 loss: 0.0017 lr: 0.02\n",
      "iteration: 167230 loss: 0.0020 lr: 0.02\n",
      "iteration: 167240 loss: 0.0023 lr: 0.02\n",
      "iteration: 167250 loss: 0.0023 lr: 0.02\n",
      "iteration: 167260 loss: 0.0017 lr: 0.02\n",
      "iteration: 167270 loss: 0.0022 lr: 0.02\n",
      "iteration: 167280 loss: 0.0024 lr: 0.02\n",
      "iteration: 167290 loss: 0.0021 lr: 0.02\n",
      "iteration: 167300 loss: 0.0033 lr: 0.02\n",
      "iteration: 167310 loss: 0.0022 lr: 0.02\n",
      "iteration: 167320 loss: 0.0029 lr: 0.02\n",
      "iteration: 167330 loss: 0.0017 lr: 0.02\n",
      "iteration: 167340 loss: 0.0020 lr: 0.02\n",
      "iteration: 167350 loss: 0.0030 lr: 0.02\n",
      "iteration: 167360 loss: 0.0022 lr: 0.02\n",
      "iteration: 167370 loss: 0.0015 lr: 0.02\n",
      "iteration: 167380 loss: 0.0030 lr: 0.02\n",
      "iteration: 167390 loss: 0.0025 lr: 0.02\n",
      "iteration: 167400 loss: 0.0031 lr: 0.02\n",
      "iteration: 167410 loss: 0.0036 lr: 0.02\n",
      "iteration: 167420 loss: 0.0021 lr: 0.02\n",
      "iteration: 167430 loss: 0.0031 lr: 0.02\n",
      "iteration: 167440 loss: 0.0018 lr: 0.02\n",
      "iteration: 167450 loss: 0.0024 lr: 0.02\n",
      "iteration: 167460 loss: 0.0029 lr: 0.02\n",
      "iteration: 167470 loss: 0.0022 lr: 0.02\n",
      "iteration: 167480 loss: 0.0016 lr: 0.02\n",
      "iteration: 167490 loss: 0.0026 lr: 0.02\n",
      "iteration: 167500 loss: 0.0018 lr: 0.02\n",
      "iteration: 167510 loss: 0.0030 lr: 0.02\n",
      "iteration: 167520 loss: 0.0023 lr: 0.02\n",
      "iteration: 167530 loss: 0.0024 lr: 0.02\n",
      "iteration: 167540 loss: 0.0038 lr: 0.02\n",
      "iteration: 167550 loss: 0.0016 lr: 0.02\n",
      "iteration: 167560 loss: 0.0023 lr: 0.02\n",
      "iteration: 167570 loss: 0.0025 lr: 0.02\n",
      "iteration: 167580 loss: 0.0026 lr: 0.02\n",
      "iteration: 167590 loss: 0.0022 lr: 0.02\n",
      "iteration: 167600 loss: 0.0024 lr: 0.02\n",
      "iteration: 167610 loss: 0.0022 lr: 0.02\n",
      "iteration: 167620 loss: 0.0018 lr: 0.02\n",
      "iteration: 167630 loss: 0.0020 lr: 0.02\n",
      "iteration: 167640 loss: 0.0019 lr: 0.02\n",
      "iteration: 167650 loss: 0.0024 lr: 0.02\n",
      "iteration: 167660 loss: 0.0024 lr: 0.02\n",
      "iteration: 167670 loss: 0.0021 lr: 0.02\n",
      "iteration: 167680 loss: 0.0025 lr: 0.02\n",
      "iteration: 167690 loss: 0.0024 lr: 0.02\n",
      "iteration: 167700 loss: 0.0021 lr: 0.02\n",
      "iteration: 167710 loss: 0.0020 lr: 0.02\n",
      "iteration: 167720 loss: 0.0017 lr: 0.02\n",
      "iteration: 167730 loss: 0.0017 lr: 0.02\n",
      "iteration: 167740 loss: 0.0023 lr: 0.02\n",
      "iteration: 167750 loss: 0.0029 lr: 0.02\n",
      "iteration: 167760 loss: 0.0030 lr: 0.02\n",
      "iteration: 167770 loss: 0.0026 lr: 0.02\n",
      "iteration: 167780 loss: 0.0031 lr: 0.02\n",
      "iteration: 167790 loss: 0.0027 lr: 0.02\n",
      "iteration: 167800 loss: 0.0029 lr: 0.02\n",
      "iteration: 167810 loss: 0.0022 lr: 0.02\n",
      "iteration: 167820 loss: 0.0027 lr: 0.02\n",
      "iteration: 167830 loss: 0.0032 lr: 0.02\n",
      "iteration: 167840 loss: 0.0020 lr: 0.02\n",
      "iteration: 167850 loss: 0.0021 lr: 0.02\n",
      "iteration: 167860 loss: 0.0024 lr: 0.02\n",
      "iteration: 167870 loss: 0.0023 lr: 0.02\n",
      "iteration: 167880 loss: 0.0028 lr: 0.02\n",
      "iteration: 167890 loss: 0.0029 lr: 0.02\n",
      "iteration: 167900 loss: 0.0024 lr: 0.02\n",
      "iteration: 167910 loss: 0.0023 lr: 0.02\n",
      "iteration: 167920 loss: 0.0020 lr: 0.02\n",
      "iteration: 167930 loss: 0.0020 lr: 0.02\n",
      "iteration: 167940 loss: 0.0018 lr: 0.02\n",
      "iteration: 167950 loss: 0.0019 lr: 0.02\n",
      "iteration: 167960 loss: 0.0021 lr: 0.02\n",
      "iteration: 167970 loss: 0.0028 lr: 0.02\n",
      "iteration: 167980 loss: 0.0024 lr: 0.02\n",
      "iteration: 167990 loss: 0.0025 lr: 0.02\n",
      "iteration: 168000 loss: 0.0030 lr: 0.02\n",
      "iteration: 168010 loss: 0.0021 lr: 0.02\n",
      "iteration: 168020 loss: 0.0020 lr: 0.02\n",
      "iteration: 168030 loss: 0.0020 lr: 0.02\n",
      "iteration: 168040 loss: 0.0018 lr: 0.02\n",
      "iteration: 168050 loss: 0.0022 lr: 0.02\n",
      "iteration: 168060 loss: 0.0023 lr: 0.02\n",
      "iteration: 168070 loss: 0.0023 lr: 0.02\n",
      "iteration: 168080 loss: 0.0020 lr: 0.02\n",
      "iteration: 168090 loss: 0.0022 lr: 0.02\n",
      "iteration: 168100 loss: 0.0020 lr: 0.02\n",
      "iteration: 168110 loss: 0.0041 lr: 0.02\n",
      "iteration: 168120 loss: 0.0027 lr: 0.02\n",
      "iteration: 168130 loss: 0.0022 lr: 0.02\n",
      "iteration: 168140 loss: 0.0022 lr: 0.02\n",
      "iteration: 168150 loss: 0.0022 lr: 0.02\n",
      "iteration: 168160 loss: 0.0013 lr: 0.02\n",
      "iteration: 168170 loss: 0.0025 lr: 0.02\n",
      "iteration: 168180 loss: 0.0021 lr: 0.02\n",
      "iteration: 168190 loss: 0.0022 lr: 0.02\n",
      "iteration: 168200 loss: 0.0019 lr: 0.02\n",
      "iteration: 168210 loss: 0.0025 lr: 0.02\n",
      "iteration: 168220 loss: 0.0024 lr: 0.02\n",
      "iteration: 168230 loss: 0.0033 lr: 0.02\n",
      "iteration: 168240 loss: 0.0021 lr: 0.02\n",
      "iteration: 168250 loss: 0.0022 lr: 0.02\n",
      "iteration: 168260 loss: 0.0024 lr: 0.02\n",
      "iteration: 168270 loss: 0.0034 lr: 0.02\n",
      "iteration: 168280 loss: 0.0026 lr: 0.02\n",
      "iteration: 168290 loss: 0.0028 lr: 0.02\n",
      "iteration: 168300 loss: 0.0025 lr: 0.02\n",
      "iteration: 168310 loss: 0.0022 lr: 0.02\n",
      "iteration: 168320 loss: 0.0038 lr: 0.02\n",
      "iteration: 168330 loss: 0.0030 lr: 0.02\n",
      "iteration: 168340 loss: 0.0022 lr: 0.02\n",
      "iteration: 168350 loss: 0.0025 lr: 0.02\n",
      "iteration: 168360 loss: 0.0026 lr: 0.02\n",
      "iteration: 168370 loss: 0.0023 lr: 0.02\n",
      "iteration: 168380 loss: 0.0031 lr: 0.02\n",
      "iteration: 168390 loss: 0.0026 lr: 0.02\n",
      "iteration: 168400 loss: 0.0026 lr: 0.02\n",
      "iteration: 168410 loss: 0.0035 lr: 0.02\n",
      "iteration: 168420 loss: 0.0034 lr: 0.02\n",
      "iteration: 168430 loss: 0.0022 lr: 0.02\n",
      "iteration: 168440 loss: 0.0027 lr: 0.02\n",
      "iteration: 168450 loss: 0.0029 lr: 0.02\n",
      "iteration: 168460 loss: 0.0022 lr: 0.02\n",
      "iteration: 168470 loss: 0.0025 lr: 0.02\n",
      "iteration: 168480 loss: 0.0014 lr: 0.02\n",
      "iteration: 168490 loss: 0.0022 lr: 0.02\n",
      "iteration: 168500 loss: 0.0022 lr: 0.02\n",
      "iteration: 168510 loss: 0.0023 lr: 0.02\n",
      "iteration: 168520 loss: 0.0021 lr: 0.02\n",
      "iteration: 168530 loss: 0.0025 lr: 0.02\n",
      "iteration: 168540 loss: 0.0023 lr: 0.02\n",
      "iteration: 168550 loss: 0.0024 lr: 0.02\n",
      "iteration: 168560 loss: 0.0022 lr: 0.02\n",
      "iteration: 168570 loss: 0.0028 lr: 0.02\n",
      "iteration: 168580 loss: 0.0018 lr: 0.02\n",
      "iteration: 168590 loss: 0.0023 lr: 0.02\n",
      "iteration: 168600 loss: 0.0024 lr: 0.02\n",
      "iteration: 168610 loss: 0.0030 lr: 0.02\n",
      "iteration: 168620 loss: 0.0028 lr: 0.02\n",
      "iteration: 168630 loss: 0.0016 lr: 0.02\n",
      "iteration: 168640 loss: 0.0022 lr: 0.02\n",
      "iteration: 168650 loss: 0.0023 lr: 0.02\n",
      "iteration: 168660 loss: 0.0032 lr: 0.02\n",
      "iteration: 168670 loss: 0.0020 lr: 0.02\n",
      "iteration: 168680 loss: 0.0027 lr: 0.02\n",
      "iteration: 168690 loss: 0.0027 lr: 0.02\n",
      "iteration: 168700 loss: 0.0028 lr: 0.02\n",
      "iteration: 168710 loss: 0.0015 lr: 0.02\n",
      "iteration: 168720 loss: 0.0039 lr: 0.02\n",
      "iteration: 168730 loss: 0.0025 lr: 0.02\n",
      "iteration: 168740 loss: 0.0022 lr: 0.02\n",
      "iteration: 168750 loss: 0.0019 lr: 0.02\n",
      "iteration: 168760 loss: 0.0023 lr: 0.02\n",
      "iteration: 168770 loss: 0.0026 lr: 0.02\n",
      "iteration: 168780 loss: 0.0020 lr: 0.02\n",
      "iteration: 168790 loss: 0.0016 lr: 0.02\n",
      "iteration: 168800 loss: 0.0015 lr: 0.02\n",
      "iteration: 168810 loss: 0.0027 lr: 0.02\n",
      "iteration: 168820 loss: 0.0022 lr: 0.02\n",
      "iteration: 168830 loss: 0.0030 lr: 0.02\n",
      "iteration: 168840 loss: 0.0022 lr: 0.02\n",
      "iteration: 168850 loss: 0.0027 lr: 0.02\n",
      "iteration: 168860 loss: 0.0031 lr: 0.02\n",
      "iteration: 168870 loss: 0.0038 lr: 0.02\n",
      "iteration: 168880 loss: 0.0031 lr: 0.02\n",
      "iteration: 168890 loss: 0.0016 lr: 0.02\n",
      "iteration: 168900 loss: 0.0023 lr: 0.02\n",
      "iteration: 168910 loss: 0.0025 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 168920 loss: 0.0025 lr: 0.02\n",
      "iteration: 168930 loss: 0.0021 lr: 0.02\n",
      "iteration: 168940 loss: 0.0039 lr: 0.02\n",
      "iteration: 168950 loss: 0.0022 lr: 0.02\n",
      "iteration: 168960 loss: 0.0028 lr: 0.02\n",
      "iteration: 168970 loss: 0.0024 lr: 0.02\n",
      "iteration: 168980 loss: 0.0022 lr: 0.02\n",
      "iteration: 168990 loss: 0.0021 lr: 0.02\n",
      "iteration: 169000 loss: 0.0028 lr: 0.02\n",
      "iteration: 169010 loss: 0.0022 lr: 0.02\n",
      "iteration: 169020 loss: 0.0034 lr: 0.02\n",
      "iteration: 169030 loss: 0.0023 lr: 0.02\n",
      "iteration: 169040 loss: 0.0025 lr: 0.02\n",
      "iteration: 169050 loss: 0.0017 lr: 0.02\n",
      "iteration: 169060 loss: 0.0024 lr: 0.02\n",
      "iteration: 169070 loss: 0.0018 lr: 0.02\n",
      "iteration: 169080 loss: 0.0027 lr: 0.02\n",
      "iteration: 169090 loss: 0.0022 lr: 0.02\n",
      "iteration: 169100 loss: 0.0034 lr: 0.02\n",
      "iteration: 169110 loss: 0.0022 lr: 0.02\n",
      "iteration: 169120 loss: 0.0027 lr: 0.02\n",
      "iteration: 169130 loss: 0.0016 lr: 0.02\n",
      "iteration: 169140 loss: 0.0019 lr: 0.02\n",
      "iteration: 169150 loss: 0.0017 lr: 0.02\n",
      "iteration: 169160 loss: 0.0022 lr: 0.02\n",
      "iteration: 169170 loss: 0.0019 lr: 0.02\n",
      "iteration: 169180 loss: 0.0024 lr: 0.02\n",
      "iteration: 169190 loss: 0.0026 lr: 0.02\n",
      "iteration: 169200 loss: 0.0019 lr: 0.02\n",
      "iteration: 169210 loss: 0.0022 lr: 0.02\n",
      "iteration: 169220 loss: 0.0017 lr: 0.02\n",
      "iteration: 169230 loss: 0.0020 lr: 0.02\n",
      "iteration: 169240 loss: 0.0020 lr: 0.02\n",
      "iteration: 169250 loss: 0.0015 lr: 0.02\n",
      "iteration: 169260 loss: 0.0022 lr: 0.02\n",
      "iteration: 169270 loss: 0.0017 lr: 0.02\n",
      "iteration: 169280 loss: 0.0025 lr: 0.02\n",
      "iteration: 169290 loss: 0.0023 lr: 0.02\n",
      "iteration: 169300 loss: 0.0021 lr: 0.02\n",
      "iteration: 169310 loss: 0.0026 lr: 0.02\n",
      "iteration: 169320 loss: 0.0047 lr: 0.02\n",
      "iteration: 169330 loss: 0.0038 lr: 0.02\n",
      "iteration: 169340 loss: 0.0025 lr: 0.02\n",
      "iteration: 169350 loss: 0.0026 lr: 0.02\n",
      "iteration: 169360 loss: 0.0046 lr: 0.02\n",
      "iteration: 169370 loss: 0.0021 lr: 0.02\n",
      "iteration: 169380 loss: 0.0023 lr: 0.02\n",
      "iteration: 169390 loss: 0.0019 lr: 0.02\n",
      "iteration: 169400 loss: 0.0025 lr: 0.02\n",
      "iteration: 169410 loss: 0.0027 lr: 0.02\n",
      "iteration: 169420 loss: 0.0026 lr: 0.02\n",
      "iteration: 169430 loss: 0.0018 lr: 0.02\n",
      "iteration: 169440 loss: 0.0024 lr: 0.02\n",
      "iteration: 169450 loss: 0.0014 lr: 0.02\n",
      "iteration: 169460 loss: 0.0016 lr: 0.02\n",
      "iteration: 169470 loss: 0.0020 lr: 0.02\n",
      "iteration: 169480 loss: 0.0015 lr: 0.02\n",
      "iteration: 169490 loss: 0.0042 lr: 0.02\n",
      "iteration: 169500 loss: 0.0031 lr: 0.02\n",
      "iteration: 169510 loss: 0.0022 lr: 0.02\n",
      "iteration: 169520 loss: 0.0022 lr: 0.02\n",
      "iteration: 169530 loss: 0.0058 lr: 0.02\n",
      "iteration: 169540 loss: 0.0033 lr: 0.02\n",
      "iteration: 169550 loss: 0.0017 lr: 0.02\n",
      "iteration: 169560 loss: 0.0026 lr: 0.02\n",
      "iteration: 169570 loss: 0.0018 lr: 0.02\n",
      "iteration: 169580 loss: 0.0026 lr: 0.02\n",
      "iteration: 169590 loss: 0.0022 lr: 0.02\n",
      "iteration: 169600 loss: 0.0029 lr: 0.02\n",
      "iteration: 169610 loss: 0.0019 lr: 0.02\n",
      "iteration: 169620 loss: 0.0020 lr: 0.02\n",
      "iteration: 169630 loss: 0.0023 lr: 0.02\n",
      "iteration: 169640 loss: 0.0026 lr: 0.02\n",
      "iteration: 169650 loss: 0.0023 lr: 0.02\n",
      "iteration: 169660 loss: 0.0026 lr: 0.02\n",
      "iteration: 169670 loss: 0.0027 lr: 0.02\n",
      "iteration: 169680 loss: 0.0026 lr: 0.02\n",
      "iteration: 169690 loss: 0.0035 lr: 0.02\n",
      "iteration: 169700 loss: 0.0020 lr: 0.02\n",
      "iteration: 169710 loss: 0.0024 lr: 0.02\n",
      "iteration: 169720 loss: 0.0019 lr: 0.02\n",
      "iteration: 169730 loss: 0.0019 lr: 0.02\n",
      "iteration: 169740 loss: 0.0021 lr: 0.02\n",
      "iteration: 169750 loss: 0.0031 lr: 0.02\n",
      "iteration: 169760 loss: 0.0019 lr: 0.02\n",
      "iteration: 169770 loss: 0.0022 lr: 0.02\n",
      "iteration: 169780 loss: 0.0020 lr: 0.02\n",
      "iteration: 169790 loss: 0.0017 lr: 0.02\n",
      "iteration: 169800 loss: 0.0021 lr: 0.02\n",
      "iteration: 169810 loss: 0.0030 lr: 0.02\n",
      "iteration: 169820 loss: 0.0018 lr: 0.02\n",
      "iteration: 169830 loss: 0.0021 lr: 0.02\n",
      "iteration: 169840 loss: 0.0026 lr: 0.02\n",
      "iteration: 169850 loss: 0.0023 lr: 0.02\n",
      "iteration: 169860 loss: 0.0024 lr: 0.02\n",
      "iteration: 169870 loss: 0.0019 lr: 0.02\n",
      "iteration: 169880 loss: 0.0022 lr: 0.02\n",
      "iteration: 169890 loss: 0.0023 lr: 0.02\n",
      "iteration: 169900 loss: 0.0015 lr: 0.02\n",
      "iteration: 169910 loss: 0.0024 lr: 0.02\n",
      "iteration: 169920 loss: 0.0019 lr: 0.02\n",
      "iteration: 169930 loss: 0.0018 lr: 0.02\n",
      "iteration: 169940 loss: 0.0024 lr: 0.02\n",
      "iteration: 169950 loss: 0.0019 lr: 0.02\n",
      "iteration: 169960 loss: 0.0020 lr: 0.02\n",
      "iteration: 169970 loss: 0.0024 lr: 0.02\n",
      "iteration: 169980 loss: 0.0026 lr: 0.02\n",
      "iteration: 169990 loss: 0.0031 lr: 0.02\n",
      "iteration: 170000 loss: 0.0017 lr: 0.02\n",
      "iteration: 170010 loss: 0.0018 lr: 0.02\n",
      "iteration: 170020 loss: 0.0016 lr: 0.02\n",
      "iteration: 170030 loss: 0.0026 lr: 0.02\n",
      "iteration: 170040 loss: 0.0020 lr: 0.02\n",
      "iteration: 170050 loss: 0.0030 lr: 0.02\n",
      "iteration: 170060 loss: 0.0019 lr: 0.02\n",
      "iteration: 170070 loss: 0.0019 lr: 0.02\n",
      "iteration: 170080 loss: 0.0035 lr: 0.02\n",
      "iteration: 170090 loss: 0.0035 lr: 0.02\n",
      "iteration: 170100 loss: 0.0024 lr: 0.02\n",
      "iteration: 170110 loss: 0.0022 lr: 0.02\n",
      "iteration: 170120 loss: 0.0021 lr: 0.02\n",
      "iteration: 170130 loss: 0.0025 lr: 0.02\n",
      "iteration: 170140 loss: 0.0021 lr: 0.02\n",
      "iteration: 170150 loss: 0.0032 lr: 0.02\n",
      "iteration: 170160 loss: 0.0021 lr: 0.02\n",
      "iteration: 170170 loss: 0.0020 lr: 0.02\n",
      "iteration: 170180 loss: 0.0034 lr: 0.02\n",
      "iteration: 170190 loss: 0.0029 lr: 0.02\n",
      "iteration: 170200 loss: 0.0027 lr: 0.02\n",
      "iteration: 170210 loss: 0.0029 lr: 0.02\n",
      "iteration: 170220 loss: 0.0026 lr: 0.02\n",
      "iteration: 170230 loss: 0.0022 lr: 0.02\n",
      "iteration: 170240 loss: 0.0030 lr: 0.02\n",
      "iteration: 170250 loss: 0.0021 lr: 0.02\n",
      "iteration: 170260 loss: 0.0035 lr: 0.02\n",
      "iteration: 170270 loss: 0.0048 lr: 0.02\n",
      "iteration: 170280 loss: 0.0029 lr: 0.02\n",
      "iteration: 170290 loss: 0.0020 lr: 0.02\n",
      "iteration: 170300 loss: 0.0018 lr: 0.02\n",
      "iteration: 170310 loss: 0.0023 lr: 0.02\n",
      "iteration: 170320 loss: 0.0017 lr: 0.02\n",
      "iteration: 170330 loss: 0.0025 lr: 0.02\n",
      "iteration: 170340 loss: 0.0021 lr: 0.02\n",
      "iteration: 170350 loss: 0.0018 lr: 0.02\n",
      "iteration: 170360 loss: 0.0027 lr: 0.02\n",
      "iteration: 170370 loss: 0.0032 lr: 0.02\n",
      "iteration: 170380 loss: 0.0027 lr: 0.02\n",
      "iteration: 170390 loss: 0.0015 lr: 0.02\n",
      "iteration: 170400 loss: 0.0015 lr: 0.02\n",
      "iteration: 170410 loss: 0.0027 lr: 0.02\n",
      "iteration: 170420 loss: 0.0018 lr: 0.02\n",
      "iteration: 170430 loss: 0.0020 lr: 0.02\n",
      "iteration: 170440 loss: 0.0017 lr: 0.02\n",
      "iteration: 170450 loss: 0.0022 lr: 0.02\n",
      "iteration: 170460 loss: 0.0014 lr: 0.02\n",
      "iteration: 170470 loss: 0.0034 lr: 0.02\n",
      "iteration: 170480 loss: 0.0021 lr: 0.02\n",
      "iteration: 170490 loss: 0.0028 lr: 0.02\n",
      "iteration: 170500 loss: 0.0024 lr: 0.02\n",
      "iteration: 170510 loss: 0.0022 lr: 0.02\n",
      "iteration: 170520 loss: 0.0023 lr: 0.02\n",
      "iteration: 170530 loss: 0.0027 lr: 0.02\n",
      "iteration: 170540 loss: 0.0019 lr: 0.02\n",
      "iteration: 170550 loss: 0.0020 lr: 0.02\n",
      "iteration: 170560 loss: 0.0028 lr: 0.02\n",
      "iteration: 170570 loss: 0.0032 lr: 0.02\n",
      "iteration: 170580 loss: 0.0020 lr: 0.02\n",
      "iteration: 170590 loss: 0.0022 lr: 0.02\n",
      "iteration: 170600 loss: 0.0030 lr: 0.02\n",
      "iteration: 170610 loss: 0.0022 lr: 0.02\n",
      "iteration: 170620 loss: 0.0018 lr: 0.02\n",
      "iteration: 170630 loss: 0.0022 lr: 0.02\n",
      "iteration: 170640 loss: 0.0034 lr: 0.02\n",
      "iteration: 170650 loss: 0.0035 lr: 0.02\n",
      "iteration: 170660 loss: 0.0022 lr: 0.02\n",
      "iteration: 170670 loss: 0.0026 lr: 0.02\n",
      "iteration: 170680 loss: 0.0026 lr: 0.02\n",
      "iteration: 170690 loss: 0.0047 lr: 0.02\n",
      "iteration: 170700 loss: 0.0033 lr: 0.02\n",
      "iteration: 170710 loss: 0.0027 lr: 0.02\n",
      "iteration: 170720 loss: 0.0017 lr: 0.02\n",
      "iteration: 170730 loss: 0.0026 lr: 0.02\n",
      "iteration: 170740 loss: 0.0021 lr: 0.02\n",
      "iteration: 170750 loss: 0.0024 lr: 0.02\n",
      "iteration: 170760 loss: 0.0031 lr: 0.02\n",
      "iteration: 170770 loss: 0.0022 lr: 0.02\n",
      "iteration: 170780 loss: 0.0024 lr: 0.02\n",
      "iteration: 170790 loss: 0.0025 lr: 0.02\n",
      "iteration: 170800 loss: 0.0016 lr: 0.02\n",
      "iteration: 170810 loss: 0.0024 lr: 0.02\n",
      "iteration: 170820 loss: 0.0019 lr: 0.02\n",
      "iteration: 170830 loss: 0.0026 lr: 0.02\n",
      "iteration: 170840 loss: 0.0027 lr: 0.02\n",
      "iteration: 170850 loss: 0.0018 lr: 0.02\n",
      "iteration: 170860 loss: 0.0026 lr: 0.02\n",
      "iteration: 170870 loss: 0.0018 lr: 0.02\n",
      "iteration: 170880 loss: 0.0027 lr: 0.02\n",
      "iteration: 170890 loss: 0.0024 lr: 0.02\n",
      "iteration: 170900 loss: 0.0036 lr: 0.02\n",
      "iteration: 170910 loss: 0.0035 lr: 0.02\n",
      "iteration: 170920 loss: 0.0022 lr: 0.02\n",
      "iteration: 170930 loss: 0.0031 lr: 0.02\n",
      "iteration: 170940 loss: 0.0017 lr: 0.02\n",
      "iteration: 170950 loss: 0.0019 lr: 0.02\n",
      "iteration: 170960 loss: 0.0017 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 170970 loss: 0.0018 lr: 0.02\n",
      "iteration: 170980 loss: 0.0020 lr: 0.02\n",
      "iteration: 170990 loss: 0.0029 lr: 0.02\n",
      "iteration: 171000 loss: 0.0022 lr: 0.02\n",
      "iteration: 171010 loss: 0.0025 lr: 0.02\n",
      "iteration: 171020 loss: 0.0025 lr: 0.02\n",
      "iteration: 171030 loss: 0.0025 lr: 0.02\n",
      "iteration: 171040 loss: 0.0034 lr: 0.02\n",
      "iteration: 171050 loss: 0.0025 lr: 0.02\n",
      "iteration: 171060 loss: 0.0022 lr: 0.02\n",
      "iteration: 171070 loss: 0.0018 lr: 0.02\n",
      "iteration: 171080 loss: 0.0028 lr: 0.02\n",
      "iteration: 171090 loss: 0.0024 lr: 0.02\n",
      "iteration: 171100 loss: 0.0025 lr: 0.02\n",
      "iteration: 171110 loss: 0.0026 lr: 0.02\n",
      "iteration: 171120 loss: 0.0026 lr: 0.02\n",
      "iteration: 171130 loss: 0.0023 lr: 0.02\n",
      "iteration: 171140 loss: 0.0024 lr: 0.02\n",
      "iteration: 171150 loss: 0.0022 lr: 0.02\n",
      "iteration: 171160 loss: 0.0023 lr: 0.02\n",
      "iteration: 171170 loss: 0.0023 lr: 0.02\n",
      "iteration: 171180 loss: 0.0033 lr: 0.02\n",
      "iteration: 171190 loss: 0.0028 lr: 0.02\n",
      "iteration: 171200 loss: 0.0037 lr: 0.02\n",
      "iteration: 171210 loss: 0.0035 lr: 0.02\n",
      "iteration: 171220 loss: 0.0021 lr: 0.02\n",
      "iteration: 171230 loss: 0.0021 lr: 0.02\n",
      "iteration: 171240 loss: 0.0016 lr: 0.02\n",
      "iteration: 171250 loss: 0.0037 lr: 0.02\n",
      "iteration: 171260 loss: 0.0026 lr: 0.02\n",
      "iteration: 171270 loss: 0.0039 lr: 0.02\n",
      "iteration: 171280 loss: 0.0035 lr: 0.02\n",
      "iteration: 171290 loss: 0.0026 lr: 0.02\n",
      "iteration: 171300 loss: 0.0019 lr: 0.02\n",
      "iteration: 171310 loss: 0.0016 lr: 0.02\n",
      "iteration: 171320 loss: 0.0021 lr: 0.02\n",
      "iteration: 171330 loss: 0.0026 lr: 0.02\n",
      "iteration: 171340 loss: 0.0030 lr: 0.02\n",
      "iteration: 171350 loss: 0.0021 lr: 0.02\n",
      "iteration: 171360 loss: 0.0021 lr: 0.02\n",
      "iteration: 171370 loss: 0.0029 lr: 0.02\n",
      "iteration: 171380 loss: 0.0026 lr: 0.02\n",
      "iteration: 171390 loss: 0.0030 lr: 0.02\n",
      "iteration: 171400 loss: 0.0029 lr: 0.02\n",
      "iteration: 171410 loss: 0.0020 lr: 0.02\n",
      "iteration: 171420 loss: 0.0023 lr: 0.02\n",
      "iteration: 171430 loss: 0.0022 lr: 0.02\n",
      "iteration: 171440 loss: 0.0019 lr: 0.02\n",
      "iteration: 171450 loss: 0.0024 lr: 0.02\n",
      "iteration: 171460 loss: 0.0022 lr: 0.02\n",
      "iteration: 171470 loss: 0.0017 lr: 0.02\n",
      "iteration: 171480 loss: 0.0018 lr: 0.02\n",
      "iteration: 171490 loss: 0.0025 lr: 0.02\n",
      "iteration: 171500 loss: 0.0023 lr: 0.02\n",
      "iteration: 171510 loss: 0.0023 lr: 0.02\n",
      "iteration: 171520 loss: 0.0022 lr: 0.02\n",
      "iteration: 171530 loss: 0.0025 lr: 0.02\n",
      "iteration: 171540 loss: 0.0022 lr: 0.02\n",
      "iteration: 171550 loss: 0.0035 lr: 0.02\n",
      "iteration: 171560 loss: 0.0014 lr: 0.02\n",
      "iteration: 171570 loss: 0.0021 lr: 0.02\n",
      "iteration: 171580 loss: 0.0023 lr: 0.02\n",
      "iteration: 171590 loss: 0.0019 lr: 0.02\n",
      "iteration: 171600 loss: 0.0016 lr: 0.02\n",
      "iteration: 171610 loss: 0.0019 lr: 0.02\n",
      "iteration: 171620 loss: 0.0018 lr: 0.02\n",
      "iteration: 171630 loss: 0.0020 lr: 0.02\n",
      "iteration: 171640 loss: 0.0029 lr: 0.02\n",
      "iteration: 171650 loss: 0.0017 lr: 0.02\n",
      "iteration: 171660 loss: 0.0017 lr: 0.02\n",
      "iteration: 171670 loss: 0.0020 lr: 0.02\n",
      "iteration: 171680 loss: 0.0026 lr: 0.02\n",
      "iteration: 171690 loss: 0.0026 lr: 0.02\n",
      "iteration: 171700 loss: 0.0016 lr: 0.02\n",
      "iteration: 171710 loss: 0.0019 lr: 0.02\n",
      "iteration: 171720 loss: 0.0018 lr: 0.02\n",
      "iteration: 171730 loss: 0.0019 lr: 0.02\n",
      "iteration: 171740 loss: 0.0020 lr: 0.02\n",
      "iteration: 171750 loss: 0.0026 lr: 0.02\n",
      "iteration: 171760 loss: 0.0028 lr: 0.02\n",
      "iteration: 171770 loss: 0.0021 lr: 0.02\n",
      "iteration: 171780 loss: 0.0025 lr: 0.02\n",
      "iteration: 171790 loss: 0.0028 lr: 0.02\n",
      "iteration: 171800 loss: 0.0017 lr: 0.02\n",
      "iteration: 171810 loss: 0.0023 lr: 0.02\n",
      "iteration: 171820 loss: 0.0020 lr: 0.02\n",
      "iteration: 171830 loss: 0.0018 lr: 0.02\n",
      "iteration: 171840 loss: 0.0023 lr: 0.02\n",
      "iteration: 171850 loss: 0.0020 lr: 0.02\n",
      "iteration: 171860 loss: 0.0026 lr: 0.02\n",
      "iteration: 171870 loss: 0.0026 lr: 0.02\n",
      "iteration: 171880 loss: 0.0023 lr: 0.02\n",
      "iteration: 171890 loss: 0.0022 lr: 0.02\n",
      "iteration: 171900 loss: 0.0026 lr: 0.02\n",
      "iteration: 171910 loss: 0.0032 lr: 0.02\n",
      "iteration: 171920 loss: 0.0030 lr: 0.02\n",
      "iteration: 171930 loss: 0.0023 lr: 0.02\n",
      "iteration: 171940 loss: 0.0016 lr: 0.02\n",
      "iteration: 171950 loss: 0.0024 lr: 0.02\n",
      "iteration: 171960 loss: 0.0024 lr: 0.02\n",
      "iteration: 171970 loss: 0.0021 lr: 0.02\n",
      "iteration: 171980 loss: 0.0019 lr: 0.02\n",
      "iteration: 171990 loss: 0.0033 lr: 0.02\n",
      "iteration: 172000 loss: 0.0025 lr: 0.02\n",
      "iteration: 172010 loss: 0.0021 lr: 0.02\n",
      "iteration: 172020 loss: 0.0029 lr: 0.02\n",
      "iteration: 172030 loss: 0.0017 lr: 0.02\n",
      "iteration: 172040 loss: 0.0023 lr: 0.02\n",
      "iteration: 172050 loss: 0.0021 lr: 0.02\n",
      "iteration: 172060 loss: 0.0043 lr: 0.02\n",
      "iteration: 172070 loss: 0.0020 lr: 0.02\n",
      "iteration: 172080 loss: 0.0026 lr: 0.02\n",
      "iteration: 172090 loss: 0.0023 lr: 0.02\n",
      "iteration: 172100 loss: 0.0027 lr: 0.02\n",
      "iteration: 172110 loss: 0.0030 lr: 0.02\n",
      "iteration: 172120 loss: 0.0015 lr: 0.02\n",
      "iteration: 172130 loss: 0.0018 lr: 0.02\n",
      "iteration: 172140 loss: 0.0021 lr: 0.02\n",
      "iteration: 172150 loss: 0.0044 lr: 0.02\n",
      "iteration: 172160 loss: 0.0027 lr: 0.02\n",
      "iteration: 172170 loss: 0.0023 lr: 0.02\n",
      "iteration: 172180 loss: 0.0023 lr: 0.02\n",
      "iteration: 172190 loss: 0.0023 lr: 0.02\n",
      "iteration: 172200 loss: 0.0018 lr: 0.02\n",
      "iteration: 172210 loss: 0.0027 lr: 0.02\n",
      "iteration: 172220 loss: 0.0041 lr: 0.02\n",
      "iteration: 172230 loss: 0.0020 lr: 0.02\n",
      "iteration: 172240 loss: 0.0024 lr: 0.02\n",
      "iteration: 172250 loss: 0.0022 lr: 0.02\n",
      "iteration: 172260 loss: 0.0019 lr: 0.02\n",
      "iteration: 172270 loss: 0.0019 lr: 0.02\n",
      "iteration: 172280 loss: 0.0020 lr: 0.02\n",
      "iteration: 172290 loss: 0.0017 lr: 0.02\n",
      "iteration: 172300 loss: 0.0022 lr: 0.02\n",
      "iteration: 172310 loss: 0.0031 lr: 0.02\n",
      "iteration: 172320 loss: 0.0020 lr: 0.02\n",
      "iteration: 172330 loss: 0.0027 lr: 0.02\n",
      "iteration: 172340 loss: 0.0018 lr: 0.02\n",
      "iteration: 172350 loss: 0.0031 lr: 0.02\n",
      "iteration: 172360 loss: 0.0030 lr: 0.02\n",
      "iteration: 172370 loss: 0.0029 lr: 0.02\n",
      "iteration: 172380 loss: 0.0036 lr: 0.02\n",
      "iteration: 172390 loss: 0.0026 lr: 0.02\n",
      "iteration: 172400 loss: 0.0048 lr: 0.02\n",
      "iteration: 172410 loss: 0.0021 lr: 0.02\n",
      "iteration: 172420 loss: 0.0018 lr: 0.02\n",
      "iteration: 172430 loss: 0.0039 lr: 0.02\n",
      "iteration: 172440 loss: 0.0022 lr: 0.02\n",
      "iteration: 172450 loss: 0.0016 lr: 0.02\n",
      "iteration: 172460 loss: 0.0023 lr: 0.02\n",
      "iteration: 172470 loss: 0.0024 lr: 0.02\n",
      "iteration: 172480 loss: 0.0017 lr: 0.02\n",
      "iteration: 172490 loss: 0.0024 lr: 0.02\n",
      "iteration: 172500 loss: 0.0029 lr: 0.02\n",
      "iteration: 172510 loss: 0.0022 lr: 0.02\n",
      "iteration: 172520 loss: 0.0018 lr: 0.02\n",
      "iteration: 172530 loss: 0.0028 lr: 0.02\n",
      "iteration: 172540 loss: 0.0017 lr: 0.02\n",
      "iteration: 172550 loss: 0.0017 lr: 0.02\n",
      "iteration: 172560 loss: 0.0020 lr: 0.02\n",
      "iteration: 172570 loss: 0.0025 lr: 0.02\n",
      "iteration: 172580 loss: 0.0018 lr: 0.02\n",
      "iteration: 172590 loss: 0.0030 lr: 0.02\n",
      "iteration: 172600 loss: 0.0034 lr: 0.02\n",
      "iteration: 172610 loss: 0.0025 lr: 0.02\n",
      "iteration: 172620 loss: 0.0029 lr: 0.02\n",
      "iteration: 172630 loss: 0.0026 lr: 0.02\n",
      "iteration: 172640 loss: 0.0025 lr: 0.02\n",
      "iteration: 172650 loss: 0.0018 lr: 0.02\n",
      "iteration: 172660 loss: 0.0027 lr: 0.02\n",
      "iteration: 172670 loss: 0.0022 lr: 0.02\n",
      "iteration: 172680 loss: 0.0016 lr: 0.02\n",
      "iteration: 172690 loss: 0.0022 lr: 0.02\n",
      "iteration: 172700 loss: 0.0034 lr: 0.02\n",
      "iteration: 172710 loss: 0.0023 lr: 0.02\n",
      "iteration: 172720 loss: 0.0022 lr: 0.02\n",
      "iteration: 172730 loss: 0.0031 lr: 0.02\n",
      "iteration: 172740 loss: 0.0023 lr: 0.02\n",
      "iteration: 172750 loss: 0.0025 lr: 0.02\n",
      "iteration: 172760 loss: 0.0039 lr: 0.02\n",
      "iteration: 172770 loss: 0.0023 lr: 0.02\n",
      "iteration: 172780 loss: 0.0024 lr: 0.02\n",
      "iteration: 172790 loss: 0.0022 lr: 0.02\n",
      "iteration: 172800 loss: 0.0017 lr: 0.02\n",
      "iteration: 172810 loss: 0.0024 lr: 0.02\n",
      "iteration: 172820 loss: 0.0045 lr: 0.02\n",
      "iteration: 172830 loss: 0.0029 lr: 0.02\n",
      "iteration: 172840 loss: 0.0027 lr: 0.02\n",
      "iteration: 172850 loss: 0.0025 lr: 0.02\n",
      "iteration: 172860 loss: 0.0028 lr: 0.02\n",
      "iteration: 172870 loss: 0.0031 lr: 0.02\n",
      "iteration: 172880 loss: 0.0019 lr: 0.02\n",
      "iteration: 172890 loss: 0.0021 lr: 0.02\n",
      "iteration: 172900 loss: 0.0021 lr: 0.02\n",
      "iteration: 172910 loss: 0.0022 lr: 0.02\n",
      "iteration: 172920 loss: 0.0021 lr: 0.02\n",
      "iteration: 172930 loss: 0.0019 lr: 0.02\n",
      "iteration: 172940 loss: 0.0022 lr: 0.02\n",
      "iteration: 172950 loss: 0.0025 lr: 0.02\n",
      "iteration: 172960 loss: 0.0028 lr: 0.02\n",
      "iteration: 172970 loss: 0.0027 lr: 0.02\n",
      "iteration: 172980 loss: 0.0027 lr: 0.02\n",
      "iteration: 172990 loss: 0.0018 lr: 0.02\n",
      "iteration: 173000 loss: 0.0016 lr: 0.02\n",
      "iteration: 173010 loss: 0.0027 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 173020 loss: 0.0017 lr: 0.02\n",
      "iteration: 173030 loss: 0.0027 lr: 0.02\n",
      "iteration: 173040 loss: 0.0023 lr: 0.02\n",
      "iteration: 173050 loss: 0.0024 lr: 0.02\n",
      "iteration: 173060 loss: 0.0029 lr: 0.02\n",
      "iteration: 173070 loss: 0.0028 lr: 0.02\n",
      "iteration: 173080 loss: 0.0016 lr: 0.02\n",
      "iteration: 173090 loss: 0.0024 lr: 0.02\n",
      "iteration: 173100 loss: 0.0024 lr: 0.02\n",
      "iteration: 173110 loss: 0.0042 lr: 0.02\n",
      "iteration: 173120 loss: 0.0024 lr: 0.02\n",
      "iteration: 173130 loss: 0.0039 lr: 0.02\n",
      "iteration: 173140 loss: 0.0032 lr: 0.02\n",
      "iteration: 173150 loss: 0.0022 lr: 0.02\n",
      "iteration: 173160 loss: 0.0026 lr: 0.02\n",
      "iteration: 173170 loss: 0.0038 lr: 0.02\n",
      "iteration: 173180 loss: 0.0023 lr: 0.02\n",
      "iteration: 173190 loss: 0.0039 lr: 0.02\n",
      "iteration: 173200 loss: 0.0022 lr: 0.02\n",
      "iteration: 173210 loss: 0.0025 lr: 0.02\n",
      "iteration: 173220 loss: 0.0038 lr: 0.02\n",
      "iteration: 173230 loss: 0.0019 lr: 0.02\n",
      "iteration: 173240 loss: 0.0020 lr: 0.02\n",
      "iteration: 173250 loss: 0.0017 lr: 0.02\n",
      "iteration: 173260 loss: 0.0029 lr: 0.02\n",
      "iteration: 173270 loss: 0.0034 lr: 0.02\n",
      "iteration: 173280 loss: 0.0021 lr: 0.02\n",
      "iteration: 173290 loss: 0.0039 lr: 0.02\n",
      "iteration: 173300 loss: 0.0021 lr: 0.02\n",
      "iteration: 173310 loss: 0.0023 lr: 0.02\n",
      "iteration: 173320 loss: 0.0028 lr: 0.02\n",
      "iteration: 173330 loss: 0.0023 lr: 0.02\n",
      "iteration: 173340 loss: 0.0026 lr: 0.02\n",
      "iteration: 173350 loss: 0.0027 lr: 0.02\n",
      "iteration: 173360 loss: 0.0020 lr: 0.02\n",
      "iteration: 173370 loss: 0.0019 lr: 0.02\n",
      "iteration: 173380 loss: 0.0024 lr: 0.02\n",
      "iteration: 173390 loss: 0.0035 lr: 0.02\n",
      "iteration: 173400 loss: 0.0015 lr: 0.02\n",
      "iteration: 173410 loss: 0.0033 lr: 0.02\n",
      "iteration: 173420 loss: 0.0032 lr: 0.02\n",
      "iteration: 173430 loss: 0.0022 lr: 0.02\n",
      "iteration: 173440 loss: 0.0019 lr: 0.02\n",
      "iteration: 173450 loss: 0.0023 lr: 0.02\n",
      "iteration: 173460 loss: 0.0032 lr: 0.02\n",
      "iteration: 173470 loss: 0.0021 lr: 0.02\n",
      "iteration: 173480 loss: 0.0022 lr: 0.02\n",
      "iteration: 173490 loss: 0.0026 lr: 0.02\n",
      "iteration: 173500 loss: 0.0024 lr: 0.02\n",
      "iteration: 173510 loss: 0.0021 lr: 0.02\n",
      "iteration: 173520 loss: 0.0018 lr: 0.02\n",
      "iteration: 173530 loss: 0.0025 lr: 0.02\n",
      "iteration: 173540 loss: 0.0026 lr: 0.02\n",
      "iteration: 173550 loss: 0.0024 lr: 0.02\n",
      "iteration: 173560 loss: 0.0025 lr: 0.02\n",
      "iteration: 173570 loss: 0.0024 lr: 0.02\n",
      "iteration: 173580 loss: 0.0036 lr: 0.02\n",
      "iteration: 173590 loss: 0.0032 lr: 0.02\n",
      "iteration: 173600 loss: 0.0022 lr: 0.02\n",
      "iteration: 173610 loss: 0.0025 lr: 0.02\n",
      "iteration: 173620 loss: 0.0018 lr: 0.02\n",
      "iteration: 173630 loss: 0.0022 lr: 0.02\n",
      "iteration: 173640 loss: 0.0017 lr: 0.02\n",
      "iteration: 173650 loss: 0.0020 lr: 0.02\n",
      "iteration: 173660 loss: 0.0054 lr: 0.02\n",
      "iteration: 173670 loss: 0.0047 lr: 0.02\n",
      "iteration: 173680 loss: 0.0027 lr: 0.02\n",
      "iteration: 173690 loss: 0.0027 lr: 0.02\n",
      "iteration: 173700 loss: 0.0031 lr: 0.02\n",
      "iteration: 173710 loss: 0.0024 lr: 0.02\n",
      "iteration: 173720 loss: 0.0020 lr: 0.02\n",
      "iteration: 173730 loss: 0.0028 lr: 0.02\n",
      "iteration: 173740 loss: 0.0024 lr: 0.02\n",
      "iteration: 173750 loss: 0.0028 lr: 0.02\n",
      "iteration: 173760 loss: 0.0032 lr: 0.02\n",
      "iteration: 173770 loss: 0.0025 lr: 0.02\n",
      "iteration: 173780 loss: 0.0018 lr: 0.02\n",
      "iteration: 173790 loss: 0.0032 lr: 0.02\n",
      "iteration: 173800 loss: 0.0019 lr: 0.02\n",
      "iteration: 173810 loss: 0.0025 lr: 0.02\n",
      "iteration: 173820 loss: 0.0026 lr: 0.02\n",
      "iteration: 173830 loss: 0.0020 lr: 0.02\n",
      "iteration: 173840 loss: 0.0021 lr: 0.02\n",
      "iteration: 173850 loss: 0.0016 lr: 0.02\n",
      "iteration: 173860 loss: 0.0021 lr: 0.02\n",
      "iteration: 173870 loss: 0.0025 lr: 0.02\n",
      "iteration: 173880 loss: 0.0026 lr: 0.02\n",
      "iteration: 173890 loss: 0.0022 lr: 0.02\n",
      "iteration: 173900 loss: 0.0022 lr: 0.02\n",
      "iteration: 173910 loss: 0.0020 lr: 0.02\n",
      "iteration: 173920 loss: 0.0027 lr: 0.02\n",
      "iteration: 173930 loss: 0.0019 lr: 0.02\n",
      "iteration: 173940 loss: 0.0016 lr: 0.02\n",
      "iteration: 173950 loss: 0.0023 lr: 0.02\n",
      "iteration: 173960 loss: 0.0024 lr: 0.02\n",
      "iteration: 173970 loss: 0.0020 lr: 0.02\n",
      "iteration: 173980 loss: 0.0020 lr: 0.02\n",
      "iteration: 173990 loss: 0.0016 lr: 0.02\n",
      "iteration: 174000 loss: 0.0022 lr: 0.02\n",
      "iteration: 174010 loss: 0.0033 lr: 0.02\n",
      "iteration: 174020 loss: 0.0026 lr: 0.02\n",
      "iteration: 174030 loss: 0.0021 lr: 0.02\n",
      "iteration: 174040 loss: 0.0021 lr: 0.02\n",
      "iteration: 174050 loss: 0.0016 lr: 0.02\n",
      "iteration: 174060 loss: 0.0030 lr: 0.02\n",
      "iteration: 174070 loss: 0.0023 lr: 0.02\n",
      "iteration: 174080 loss: 0.0025 lr: 0.02\n",
      "iteration: 174090 loss: 0.0018 lr: 0.02\n",
      "iteration: 174100 loss: 0.0016 lr: 0.02\n",
      "iteration: 174110 loss: 0.0020 lr: 0.02\n",
      "iteration: 174120 loss: 0.0035 lr: 0.02\n",
      "iteration: 174130 loss: 0.0030 lr: 0.02\n",
      "iteration: 174140 loss: 0.0016 lr: 0.02\n",
      "iteration: 174150 loss: 0.0023 lr: 0.02\n",
      "iteration: 174160 loss: 0.0032 lr: 0.02\n",
      "iteration: 174170 loss: 0.0018 lr: 0.02\n",
      "iteration: 174180 loss: 0.0022 lr: 0.02\n",
      "iteration: 174190 loss: 0.0031 lr: 0.02\n",
      "iteration: 174200 loss: 0.0025 lr: 0.02\n",
      "iteration: 174210 loss: 0.0020 lr: 0.02\n",
      "iteration: 174220 loss: 0.0044 lr: 0.02\n",
      "iteration: 174230 loss: 0.0023 lr: 0.02\n",
      "iteration: 174240 loss: 0.0025 lr: 0.02\n",
      "iteration: 174250 loss: 0.0018 lr: 0.02\n",
      "iteration: 174260 loss: 0.0034 lr: 0.02\n",
      "iteration: 174270 loss: 0.0027 lr: 0.02\n",
      "iteration: 174280 loss: 0.0026 lr: 0.02\n",
      "iteration: 174290 loss: 0.0032 lr: 0.02\n",
      "iteration: 174300 loss: 0.0021 lr: 0.02\n",
      "iteration: 174310 loss: 0.0027 lr: 0.02\n",
      "iteration: 174320 loss: 0.0028 lr: 0.02\n",
      "iteration: 174330 loss: 0.0020 lr: 0.02\n",
      "iteration: 174340 loss: 0.0029 lr: 0.02\n",
      "iteration: 174350 loss: 0.0018 lr: 0.02\n",
      "iteration: 174360 loss: 0.0015 lr: 0.02\n",
      "iteration: 174370 loss: 0.0024 lr: 0.02\n",
      "iteration: 174380 loss: 0.0024 lr: 0.02\n",
      "iteration: 174390 loss: 0.0023 lr: 0.02\n",
      "iteration: 174400 loss: 0.0025 lr: 0.02\n",
      "iteration: 174410 loss: 0.0016 lr: 0.02\n",
      "iteration: 174420 loss: 0.0024 lr: 0.02\n",
      "iteration: 174430 loss: 0.0014 lr: 0.02\n",
      "iteration: 174440 loss: 0.0022 lr: 0.02\n",
      "iteration: 174450 loss: 0.0022 lr: 0.02\n",
      "iteration: 174460 loss: 0.0020 lr: 0.02\n",
      "iteration: 174470 loss: 0.0022 lr: 0.02\n",
      "iteration: 174480 loss: 0.0018 lr: 0.02\n",
      "iteration: 174490 loss: 0.0036 lr: 0.02\n",
      "iteration: 174500 loss: 0.0026 lr: 0.02\n",
      "iteration: 174510 loss: 0.0019 lr: 0.02\n",
      "iteration: 174520 loss: 0.0032 lr: 0.02\n",
      "iteration: 174530 loss: 0.0016 lr: 0.02\n",
      "iteration: 174540 loss: 0.0021 lr: 0.02\n",
      "iteration: 174550 loss: 0.0014 lr: 0.02\n",
      "iteration: 174560 loss: 0.0016 lr: 0.02\n",
      "iteration: 174570 loss: 0.0024 lr: 0.02\n",
      "iteration: 174580 loss: 0.0023 lr: 0.02\n",
      "iteration: 174590 loss: 0.0024 lr: 0.02\n",
      "iteration: 174600 loss: 0.0034 lr: 0.02\n",
      "iteration: 174610 loss: 0.0022 lr: 0.02\n",
      "iteration: 174620 loss: 0.0020 lr: 0.02\n",
      "iteration: 174630 loss: 0.0019 lr: 0.02\n",
      "iteration: 174640 loss: 0.0027 lr: 0.02\n",
      "iteration: 174650 loss: 0.0029 lr: 0.02\n",
      "iteration: 174660 loss: 0.0020 lr: 0.02\n",
      "iteration: 174670 loss: 0.0023 lr: 0.02\n",
      "iteration: 174680 loss: 0.0026 lr: 0.02\n",
      "iteration: 174690 loss: 0.0020 lr: 0.02\n",
      "iteration: 174700 loss: 0.0025 lr: 0.02\n",
      "iteration: 174710 loss: 0.0023 lr: 0.02\n",
      "iteration: 174720 loss: 0.0029 lr: 0.02\n",
      "iteration: 174730 loss: 0.0024 lr: 0.02\n",
      "iteration: 174740 loss: 0.0027 lr: 0.02\n",
      "iteration: 174750 loss: 0.0023 lr: 0.02\n",
      "iteration: 174760 loss: 0.0030 lr: 0.02\n",
      "iteration: 174770 loss: 0.0018 lr: 0.02\n",
      "iteration: 174780 loss: 0.0021 lr: 0.02\n",
      "iteration: 174790 loss: 0.0020 lr: 0.02\n",
      "iteration: 174800 loss: 0.0027 lr: 0.02\n",
      "iteration: 174810 loss: 0.0019 lr: 0.02\n",
      "iteration: 174820 loss: 0.0020 lr: 0.02\n",
      "iteration: 174830 loss: 0.0020 lr: 0.02\n",
      "iteration: 174840 loss: 0.0018 lr: 0.02\n",
      "iteration: 174850 loss: 0.0029 lr: 0.02\n",
      "iteration: 174860 loss: 0.0021 lr: 0.02\n",
      "iteration: 174870 loss: 0.0030 lr: 0.02\n",
      "iteration: 174880 loss: 0.0022 lr: 0.02\n",
      "iteration: 174890 loss: 0.0020 lr: 0.02\n",
      "iteration: 174900 loss: 0.0019 lr: 0.02\n",
      "iteration: 174910 loss: 0.0015 lr: 0.02\n",
      "iteration: 174920 loss: 0.0019 lr: 0.02\n",
      "iteration: 174930 loss: 0.0035 lr: 0.02\n",
      "iteration: 174940 loss: 0.0017 lr: 0.02\n",
      "iteration: 174950 loss: 0.0021 lr: 0.02\n",
      "iteration: 174960 loss: 0.0024 lr: 0.02\n",
      "iteration: 174970 loss: 0.0021 lr: 0.02\n",
      "iteration: 174980 loss: 0.0022 lr: 0.02\n",
      "iteration: 174990 loss: 0.0016 lr: 0.02\n",
      "iteration: 175000 loss: 0.0028 lr: 0.02\n",
      "iteration: 175010 loss: 0.0018 lr: 0.02\n",
      "iteration: 175020 loss: 0.0022 lr: 0.02\n",
      "iteration: 175030 loss: 0.0022 lr: 0.02\n",
      "iteration: 175040 loss: 0.0021 lr: 0.02\n",
      "iteration: 175050 loss: 0.0034 lr: 0.02\n",
      "iteration: 175060 loss: 0.0018 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 175070 loss: 0.0019 lr: 0.02\n",
      "iteration: 175080 loss: 0.0025 lr: 0.02\n",
      "iteration: 175090 loss: 0.0033 lr: 0.02\n",
      "iteration: 175100 loss: 0.0025 lr: 0.02\n",
      "iteration: 175110 loss: 0.0030 lr: 0.02\n",
      "iteration: 175120 loss: 0.0018 lr: 0.02\n",
      "iteration: 175130 loss: 0.0017 lr: 0.02\n",
      "iteration: 175140 loss: 0.0026 lr: 0.02\n",
      "iteration: 175150 loss: 0.0016 lr: 0.02\n",
      "iteration: 175160 loss: 0.0021 lr: 0.02\n",
      "iteration: 175170 loss: 0.0018 lr: 0.02\n",
      "iteration: 175180 loss: 0.0025 lr: 0.02\n",
      "iteration: 175190 loss: 0.0022 lr: 0.02\n",
      "iteration: 175200 loss: 0.0026 lr: 0.02\n",
      "iteration: 175210 loss: 0.0020 lr: 0.02\n",
      "iteration: 175220 loss: 0.0027 lr: 0.02\n",
      "iteration: 175230 loss: 0.0021 lr: 0.02\n",
      "iteration: 175240 loss: 0.0022 lr: 0.02\n",
      "iteration: 175250 loss: 0.0020 lr: 0.02\n",
      "iteration: 175260 loss: 0.0026 lr: 0.02\n",
      "iteration: 175270 loss: 0.0036 lr: 0.02\n",
      "iteration: 175280 loss: 0.0027 lr: 0.02\n",
      "iteration: 175290 loss: 0.0016 lr: 0.02\n",
      "iteration: 175300 loss: 0.0021 lr: 0.02\n",
      "iteration: 175310 loss: 0.0034 lr: 0.02\n",
      "iteration: 175320 loss: 0.0021 lr: 0.02\n",
      "iteration: 175330 loss: 0.0024 lr: 0.02\n",
      "iteration: 175340 loss: 0.0017 lr: 0.02\n",
      "iteration: 175350 loss: 0.0020 lr: 0.02\n",
      "iteration: 175360 loss: 0.0016 lr: 0.02\n",
      "iteration: 175370 loss: 0.0022 lr: 0.02\n",
      "iteration: 175380 loss: 0.0018 lr: 0.02\n",
      "iteration: 175390 loss: 0.0020 lr: 0.02\n",
      "iteration: 175400 loss: 0.0020 lr: 0.02\n",
      "iteration: 175410 loss: 0.0017 lr: 0.02\n",
      "iteration: 175420 loss: 0.0016 lr: 0.02\n",
      "iteration: 175430 loss: 0.0020 lr: 0.02\n",
      "iteration: 175440 loss: 0.0021 lr: 0.02\n",
      "iteration: 175450 loss: 0.0023 lr: 0.02\n",
      "iteration: 175460 loss: 0.0026 lr: 0.02\n",
      "iteration: 175470 loss: 0.0020 lr: 0.02\n",
      "iteration: 175480 loss: 0.0024 lr: 0.02\n",
      "iteration: 175490 loss: 0.0022 lr: 0.02\n",
      "iteration: 175500 loss: 0.0017 lr: 0.02\n",
      "iteration: 175510 loss: 0.0019 lr: 0.02\n",
      "iteration: 175520 loss: 0.0024 lr: 0.02\n",
      "iteration: 175530 loss: 0.0024 lr: 0.02\n",
      "iteration: 175540 loss: 0.0023 lr: 0.02\n",
      "iteration: 175550 loss: 0.0022 lr: 0.02\n",
      "iteration: 175560 loss: 0.0025 lr: 0.02\n",
      "iteration: 175570 loss: 0.0017 lr: 0.02\n",
      "iteration: 175580 loss: 0.0033 lr: 0.02\n",
      "iteration: 175590 loss: 0.0023 lr: 0.02\n",
      "iteration: 175600 loss: 0.0020 lr: 0.02\n",
      "iteration: 175610 loss: 0.0018 lr: 0.02\n",
      "iteration: 175620 loss: 0.0023 lr: 0.02\n",
      "iteration: 175630 loss: 0.0035 lr: 0.02\n",
      "iteration: 175640 loss: 0.0042 lr: 0.02\n",
      "iteration: 175650 loss: 0.0019 lr: 0.02\n",
      "iteration: 175660 loss: 0.0021 lr: 0.02\n",
      "iteration: 175670 loss: 0.0025 lr: 0.02\n",
      "iteration: 175680 loss: 0.0019 lr: 0.02\n",
      "iteration: 175690 loss: 0.0017 lr: 0.02\n",
      "iteration: 175700 loss: 0.0019 lr: 0.02\n",
      "iteration: 175710 loss: 0.0020 lr: 0.02\n",
      "iteration: 175720 loss: 0.0026 lr: 0.02\n",
      "iteration: 175730 loss: 0.0028 lr: 0.02\n",
      "iteration: 175740 loss: 0.0032 lr: 0.02\n",
      "iteration: 175750 loss: 0.0020 lr: 0.02\n",
      "iteration: 175760 loss: 0.0021 lr: 0.02\n",
      "iteration: 175770 loss: 0.0033 lr: 0.02\n",
      "iteration: 175780 loss: 0.0028 lr: 0.02\n",
      "iteration: 175790 loss: 0.0023 lr: 0.02\n",
      "iteration: 175800 loss: 0.0019 lr: 0.02\n",
      "iteration: 175810 loss: 0.0018 lr: 0.02\n",
      "iteration: 175820 loss: 0.0020 lr: 0.02\n",
      "iteration: 175830 loss: 0.0023 lr: 0.02\n",
      "iteration: 175840 loss: 0.0023 lr: 0.02\n",
      "iteration: 175850 loss: 0.0023 lr: 0.02\n",
      "iteration: 175860 loss: 0.0020 lr: 0.02\n",
      "iteration: 175870 loss: 0.0020 lr: 0.02\n",
      "iteration: 175880 loss: 0.0023 lr: 0.02\n",
      "iteration: 175890 loss: 0.0030 lr: 0.02\n",
      "iteration: 175900 loss: 0.0023 lr: 0.02\n",
      "iteration: 175910 loss: 0.0038 lr: 0.02\n",
      "iteration: 175920 loss: 0.0018 lr: 0.02\n",
      "iteration: 175930 loss: 0.0024 lr: 0.02\n",
      "iteration: 175940 loss: 0.0020 lr: 0.02\n",
      "iteration: 175950 loss: 0.0018 lr: 0.02\n",
      "iteration: 175960 loss: 0.0024 lr: 0.02\n",
      "iteration: 175970 loss: 0.0019 lr: 0.02\n",
      "iteration: 175980 loss: 0.0022 lr: 0.02\n",
      "iteration: 175990 loss: 0.0016 lr: 0.02\n",
      "iteration: 176000 loss: 0.0031 lr: 0.02\n",
      "iteration: 176010 loss: 0.0035 lr: 0.02\n",
      "iteration: 176020 loss: 0.0023 lr: 0.02\n",
      "iteration: 176030 loss: 0.0017 lr: 0.02\n",
      "iteration: 176040 loss: 0.0034 lr: 0.02\n",
      "iteration: 176050 loss: 0.0021 lr: 0.02\n",
      "iteration: 176060 loss: 0.0022 lr: 0.02\n",
      "iteration: 176070 loss: 0.0026 lr: 0.02\n",
      "iteration: 176080 loss: 0.0023 lr: 0.02\n",
      "iteration: 176090 loss: 0.0026 lr: 0.02\n",
      "iteration: 176100 loss: 0.0020 lr: 0.02\n",
      "iteration: 176110 loss: 0.0019 lr: 0.02\n",
      "iteration: 176120 loss: 0.0034 lr: 0.02\n",
      "iteration: 176130 loss: 0.0029 lr: 0.02\n",
      "iteration: 176140 loss: 0.0019 lr: 0.02\n",
      "iteration: 176150 loss: 0.0019 lr: 0.02\n",
      "iteration: 176160 loss: 0.0024 lr: 0.02\n",
      "iteration: 176170 loss: 0.0023 lr: 0.02\n",
      "iteration: 176180 loss: 0.0027 lr: 0.02\n",
      "iteration: 176190 loss: 0.0012 lr: 0.02\n",
      "iteration: 176200 loss: 0.0026 lr: 0.02\n",
      "iteration: 176210 loss: 0.0021 lr: 0.02\n",
      "iteration: 176220 loss: 0.0019 lr: 0.02\n",
      "iteration: 176230 loss: 0.0022 lr: 0.02\n",
      "iteration: 176240 loss: 0.0024 lr: 0.02\n",
      "iteration: 176250 loss: 0.0021 lr: 0.02\n",
      "iteration: 176260 loss: 0.0029 lr: 0.02\n",
      "iteration: 176270 loss: 0.0014 lr: 0.02\n",
      "iteration: 176280 loss: 0.0028 lr: 0.02\n",
      "iteration: 176290 loss: 0.0016 lr: 0.02\n",
      "iteration: 176300 loss: 0.0023 lr: 0.02\n",
      "iteration: 176310 loss: 0.0030 lr: 0.02\n",
      "iteration: 176320 loss: 0.0025 lr: 0.02\n",
      "iteration: 176330 loss: 0.0022 lr: 0.02\n",
      "iteration: 176340 loss: 0.0018 lr: 0.02\n",
      "iteration: 176350 loss: 0.0016 lr: 0.02\n",
      "iteration: 176360 loss: 0.0015 lr: 0.02\n",
      "iteration: 176370 loss: 0.0017 lr: 0.02\n",
      "iteration: 176380 loss: 0.0024 lr: 0.02\n",
      "iteration: 176390 loss: 0.0027 lr: 0.02\n",
      "iteration: 176400 loss: 0.0024 lr: 0.02\n",
      "iteration: 176410 loss: 0.0024 lr: 0.02\n",
      "iteration: 176420 loss: 0.0012 lr: 0.02\n",
      "iteration: 176430 loss: 0.0028 lr: 0.02\n",
      "iteration: 176440 loss: 0.0024 lr: 0.02\n",
      "iteration: 176450 loss: 0.0024 lr: 0.02\n",
      "iteration: 176460 loss: 0.0023 lr: 0.02\n",
      "iteration: 176470 loss: 0.0026 lr: 0.02\n",
      "iteration: 176480 loss: 0.0023 lr: 0.02\n",
      "iteration: 176490 loss: 0.0020 lr: 0.02\n",
      "iteration: 176500 loss: 0.0015 lr: 0.02\n",
      "iteration: 176510 loss: 0.0022 lr: 0.02\n",
      "iteration: 176520 loss: 0.0018 lr: 0.02\n",
      "iteration: 176530 loss: 0.0025 lr: 0.02\n",
      "iteration: 176540 loss: 0.0028 lr: 0.02\n",
      "iteration: 176550 loss: 0.0024 lr: 0.02\n",
      "iteration: 176560 loss: 0.0023 lr: 0.02\n",
      "iteration: 176570 loss: 0.0029 lr: 0.02\n",
      "iteration: 176580 loss: 0.0016 lr: 0.02\n",
      "iteration: 176590 loss: 0.0028 lr: 0.02\n",
      "iteration: 176600 loss: 0.0025 lr: 0.02\n",
      "iteration: 176610 loss: 0.0028 lr: 0.02\n",
      "iteration: 176620 loss: 0.0037 lr: 0.02\n",
      "iteration: 176630 loss: 0.0027 lr: 0.02\n",
      "iteration: 176640 loss: 0.0020 lr: 0.02\n",
      "iteration: 176650 loss: 0.0021 lr: 0.02\n",
      "iteration: 176660 loss: 0.0028 lr: 0.02\n",
      "iteration: 176670 loss: 0.0018 lr: 0.02\n",
      "iteration: 176680 loss: 0.0012 lr: 0.02\n",
      "iteration: 176690 loss: 0.0028 lr: 0.02\n",
      "iteration: 176700 loss: 0.0020 lr: 0.02\n",
      "iteration: 176710 loss: 0.0037 lr: 0.02\n",
      "iteration: 176720 loss: 0.0020 lr: 0.02\n",
      "iteration: 176730 loss: 0.0024 lr: 0.02\n",
      "iteration: 176740 loss: 0.0031 lr: 0.02\n",
      "iteration: 176750 loss: 0.0023 lr: 0.02\n",
      "iteration: 176760 loss: 0.0024 lr: 0.02\n",
      "iteration: 176770 loss: 0.0022 lr: 0.02\n",
      "iteration: 176780 loss: 0.0024 lr: 0.02\n",
      "iteration: 176790 loss: 0.0023 lr: 0.02\n",
      "iteration: 176800 loss: 0.0025 lr: 0.02\n",
      "iteration: 176810 loss: 0.0031 lr: 0.02\n",
      "iteration: 176820 loss: 0.0022 lr: 0.02\n",
      "iteration: 176830 loss: 0.0018 lr: 0.02\n",
      "iteration: 176840 loss: 0.0029 lr: 0.02\n",
      "iteration: 176850 loss: 0.0026 lr: 0.02\n",
      "iteration: 176860 loss: 0.0021 lr: 0.02\n",
      "iteration: 176870 loss: 0.0029 lr: 0.02\n",
      "iteration: 176880 loss: 0.0041 lr: 0.02\n",
      "iteration: 176890 loss: 0.0012 lr: 0.02\n",
      "iteration: 176900 loss: 0.0025 lr: 0.02\n",
      "iteration: 176910 loss: 0.0023 lr: 0.02\n",
      "iteration: 176920 loss: 0.0028 lr: 0.02\n",
      "iteration: 176930 loss: 0.0025 lr: 0.02\n",
      "iteration: 176940 loss: 0.0026 lr: 0.02\n",
      "iteration: 176950 loss: 0.0015 lr: 0.02\n",
      "iteration: 176960 loss: 0.0021 lr: 0.02\n",
      "iteration: 176970 loss: 0.0025 lr: 0.02\n",
      "iteration: 176980 loss: 0.0023 lr: 0.02\n",
      "iteration: 176990 loss: 0.0022 lr: 0.02\n",
      "iteration: 177000 loss: 0.0019 lr: 0.02\n",
      "iteration: 177010 loss: 0.0019 lr: 0.02\n",
      "iteration: 177020 loss: 0.0018 lr: 0.02\n",
      "iteration: 177030 loss: 0.0017 lr: 0.02\n",
      "iteration: 177040 loss: 0.0027 lr: 0.02\n",
      "iteration: 177050 loss: 0.0024 lr: 0.02\n",
      "iteration: 177060 loss: 0.0024 lr: 0.02\n",
      "iteration: 177070 loss: 0.0022 lr: 0.02\n",
      "iteration: 177080 loss: 0.0034 lr: 0.02\n",
      "iteration: 177090 loss: 0.0026 lr: 0.02\n",
      "iteration: 177100 loss: 0.0034 lr: 0.02\n",
      "iteration: 177110 loss: 0.0017 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 177120 loss: 0.0022 lr: 0.02\n",
      "iteration: 177130 loss: 0.0028 lr: 0.02\n",
      "iteration: 177140 loss: 0.0021 lr: 0.02\n",
      "iteration: 177150 loss: 0.0019 lr: 0.02\n",
      "iteration: 177160 loss: 0.0020 lr: 0.02\n",
      "iteration: 177170 loss: 0.0020 lr: 0.02\n",
      "iteration: 177180 loss: 0.0021 lr: 0.02\n",
      "iteration: 177190 loss: 0.0022 lr: 0.02\n",
      "iteration: 177200 loss: 0.0017 lr: 0.02\n",
      "iteration: 177210 loss: 0.0017 lr: 0.02\n",
      "iteration: 177220 loss: 0.0018 lr: 0.02\n",
      "iteration: 177230 loss: 0.0017 lr: 0.02\n",
      "iteration: 177240 loss: 0.0026 lr: 0.02\n",
      "iteration: 177250 loss: 0.0018 lr: 0.02\n",
      "iteration: 177260 loss: 0.0019 lr: 0.02\n",
      "iteration: 177270 loss: 0.0017 lr: 0.02\n",
      "iteration: 177280 loss: 0.0037 lr: 0.02\n",
      "iteration: 177290 loss: 0.0019 lr: 0.02\n",
      "iteration: 177300 loss: 0.0022 lr: 0.02\n",
      "iteration: 177310 loss: 0.0022 lr: 0.02\n",
      "iteration: 177320 loss: 0.0023 lr: 0.02\n",
      "iteration: 177330 loss: 0.0018 lr: 0.02\n",
      "iteration: 177340 loss: 0.0020 lr: 0.02\n",
      "iteration: 177350 loss: 0.0024 lr: 0.02\n",
      "iteration: 177360 loss: 0.0026 lr: 0.02\n",
      "iteration: 177370 loss: 0.0017 lr: 0.02\n",
      "iteration: 177380 loss: 0.0025 lr: 0.02\n",
      "iteration: 177390 loss: 0.0033 lr: 0.02\n",
      "iteration: 177400 loss: 0.0024 lr: 0.02\n",
      "iteration: 177410 loss: 0.0020 lr: 0.02\n",
      "iteration: 177420 loss: 0.0021 lr: 0.02\n",
      "iteration: 177430 loss: 0.0024 lr: 0.02\n",
      "iteration: 177440 loss: 0.0016 lr: 0.02\n",
      "iteration: 177450 loss: 0.0026 lr: 0.02\n",
      "iteration: 177460 loss: 0.0019 lr: 0.02\n",
      "iteration: 177470 loss: 0.0026 lr: 0.02\n",
      "iteration: 177480 loss: 0.0019 lr: 0.02\n",
      "iteration: 177490 loss: 0.0018 lr: 0.02\n",
      "iteration: 177500 loss: 0.0026 lr: 0.02\n",
      "iteration: 177510 loss: 0.0027 lr: 0.02\n",
      "iteration: 177520 loss: 0.0017 lr: 0.02\n",
      "iteration: 177530 loss: 0.0014 lr: 0.02\n",
      "iteration: 177540 loss: 0.0026 lr: 0.02\n",
      "iteration: 177550 loss: 0.0017 lr: 0.02\n",
      "iteration: 177560 loss: 0.0017 lr: 0.02\n",
      "iteration: 177570 loss: 0.0026 lr: 0.02\n",
      "iteration: 177580 loss: 0.0026 lr: 0.02\n",
      "iteration: 177590 loss: 0.0017 lr: 0.02\n",
      "iteration: 177600 loss: 0.0023 lr: 0.02\n",
      "iteration: 177610 loss: 0.0028 lr: 0.02\n",
      "iteration: 177620 loss: 0.0024 lr: 0.02\n",
      "iteration: 177630 loss: 0.0017 lr: 0.02\n",
      "iteration: 177640 loss: 0.0031 lr: 0.02\n",
      "iteration: 177650 loss: 0.0023 lr: 0.02\n",
      "iteration: 177660 loss: 0.0027 lr: 0.02\n",
      "iteration: 177670 loss: 0.0025 lr: 0.02\n",
      "iteration: 177680 loss: 0.0016 lr: 0.02\n",
      "iteration: 177690 loss: 0.0029 lr: 0.02\n",
      "iteration: 177700 loss: 0.0020 lr: 0.02\n",
      "iteration: 177710 loss: 0.0024 lr: 0.02\n",
      "iteration: 177720 loss: 0.0022 lr: 0.02\n",
      "iteration: 177730 loss: 0.0020 lr: 0.02\n",
      "iteration: 177740 loss: 0.0026 lr: 0.02\n",
      "iteration: 177750 loss: 0.0024 lr: 0.02\n",
      "iteration: 177760 loss: 0.0032 lr: 0.02\n",
      "iteration: 177770 loss: 0.0031 lr: 0.02\n",
      "iteration: 177780 loss: 0.0022 lr: 0.02\n",
      "iteration: 177790 loss: 0.0028 lr: 0.02\n",
      "iteration: 177800 loss: 0.0027 lr: 0.02\n",
      "iteration: 177810 loss: 0.0019 lr: 0.02\n",
      "iteration: 177820 loss: 0.0021 lr: 0.02\n",
      "iteration: 177830 loss: 0.0017 lr: 0.02\n",
      "iteration: 177840 loss: 0.0023 lr: 0.02\n",
      "iteration: 177850 loss: 0.0025 lr: 0.02\n",
      "iteration: 177860 loss: 0.0014 lr: 0.02\n",
      "iteration: 177870 loss: 0.0020 lr: 0.02\n",
      "iteration: 177880 loss: 0.0018 lr: 0.02\n",
      "iteration: 177890 loss: 0.0028 lr: 0.02\n",
      "iteration: 177900 loss: 0.0019 lr: 0.02\n",
      "iteration: 177910 loss: 0.0023 lr: 0.02\n",
      "iteration: 177920 loss: 0.0032 lr: 0.02\n",
      "iteration: 177930 loss: 0.0022 lr: 0.02\n",
      "iteration: 177940 loss: 0.0021 lr: 0.02\n",
      "iteration: 177950 loss: 0.0022 lr: 0.02\n",
      "iteration: 177960 loss: 0.0017 lr: 0.02\n",
      "iteration: 177970 loss: 0.0024 lr: 0.02\n",
      "iteration: 177980 loss: 0.0023 lr: 0.02\n",
      "iteration: 177990 loss: 0.0017 lr: 0.02\n",
      "iteration: 178000 loss: 0.0019 lr: 0.02\n",
      "iteration: 178010 loss: 0.0021 lr: 0.02\n",
      "iteration: 178020 loss: 0.0030 lr: 0.02\n",
      "iteration: 178030 loss: 0.0021 lr: 0.02\n",
      "iteration: 178040 loss: 0.0019 lr: 0.02\n",
      "iteration: 178050 loss: 0.0026 lr: 0.02\n",
      "iteration: 178060 loss: 0.0029 lr: 0.02\n",
      "iteration: 178070 loss: 0.0024 lr: 0.02\n",
      "iteration: 178080 loss: 0.0024 lr: 0.02\n",
      "iteration: 178090 loss: 0.0021 lr: 0.02\n",
      "iteration: 178100 loss: 0.0030 lr: 0.02\n",
      "iteration: 178110 loss: 0.0031 lr: 0.02\n",
      "iteration: 178120 loss: 0.0023 lr: 0.02\n",
      "iteration: 178130 loss: 0.0022 lr: 0.02\n",
      "iteration: 178140 loss: 0.0029 lr: 0.02\n",
      "iteration: 178150 loss: 0.0047 lr: 0.02\n",
      "iteration: 178160 loss: 0.0017 lr: 0.02\n",
      "iteration: 178170 loss: 0.0023 lr: 0.02\n",
      "iteration: 178180 loss: 0.0033 lr: 0.02\n",
      "iteration: 178190 loss: 0.0030 lr: 0.02\n",
      "iteration: 178200 loss: 0.0023 lr: 0.02\n",
      "iteration: 178210 loss: 0.0019 lr: 0.02\n",
      "iteration: 178220 loss: 0.0025 lr: 0.02\n",
      "iteration: 178230 loss: 0.0022 lr: 0.02\n",
      "iteration: 178240 loss: 0.0020 lr: 0.02\n",
      "iteration: 178250 loss: 0.0015 lr: 0.02\n",
      "iteration: 178260 loss: 0.0018 lr: 0.02\n",
      "iteration: 178270 loss: 0.0016 lr: 0.02\n",
      "iteration: 178280 loss: 0.0021 lr: 0.02\n",
      "iteration: 178290 loss: 0.0026 lr: 0.02\n",
      "iteration: 178300 loss: 0.0024 lr: 0.02\n",
      "iteration: 178310 loss: 0.0022 lr: 0.02\n",
      "iteration: 178320 loss: 0.0022 lr: 0.02\n",
      "iteration: 178330 loss: 0.0017 lr: 0.02\n",
      "iteration: 178340 loss: 0.0021 lr: 0.02\n",
      "iteration: 178350 loss: 0.0024 lr: 0.02\n",
      "iteration: 178360 loss: 0.0035 lr: 0.02\n",
      "iteration: 178370 loss: 0.0025 lr: 0.02\n",
      "iteration: 178380 loss: 0.0027 lr: 0.02\n",
      "iteration: 178390 loss: 0.0030 lr: 0.02\n",
      "iteration: 178400 loss: 0.0018 lr: 0.02\n",
      "iteration: 178410 loss: 0.0033 lr: 0.02\n",
      "iteration: 178420 loss: 0.0021 lr: 0.02\n",
      "iteration: 178430 loss: 0.0028 lr: 0.02\n",
      "iteration: 178440 loss: 0.0016 lr: 0.02\n",
      "iteration: 178450 loss: 0.0021 lr: 0.02\n",
      "iteration: 178460 loss: 0.0018 lr: 0.02\n",
      "iteration: 178470 loss: 0.0017 lr: 0.02\n",
      "iteration: 178480 loss: 0.0020 lr: 0.02\n",
      "iteration: 178490 loss: 0.0019 lr: 0.02\n",
      "iteration: 178500 loss: 0.0019 lr: 0.02\n",
      "iteration: 178510 loss: 0.0032 lr: 0.02\n",
      "iteration: 178520 loss: 0.0025 lr: 0.02\n",
      "iteration: 178530 loss: 0.0022 lr: 0.02\n",
      "iteration: 178540 loss: 0.0021 lr: 0.02\n",
      "iteration: 178550 loss: 0.0024 lr: 0.02\n",
      "iteration: 178560 loss: 0.0023 lr: 0.02\n",
      "iteration: 178570 loss: 0.0032 lr: 0.02\n",
      "iteration: 178580 loss: 0.0024 lr: 0.02\n",
      "iteration: 178590 loss: 0.0027 lr: 0.02\n",
      "iteration: 178600 loss: 0.0027 lr: 0.02\n",
      "iteration: 178610 loss: 0.0028 lr: 0.02\n",
      "iteration: 178620 loss: 0.0026 lr: 0.02\n",
      "iteration: 178630 loss: 0.0021 lr: 0.02\n",
      "iteration: 178640 loss: 0.0024 lr: 0.02\n",
      "iteration: 178650 loss: 0.0028 lr: 0.02\n",
      "iteration: 178660 loss: 0.0020 lr: 0.02\n",
      "iteration: 178670 loss: 0.0025 lr: 0.02\n",
      "iteration: 178680 loss: 0.0014 lr: 0.02\n",
      "iteration: 178690 loss: 0.0018 lr: 0.02\n",
      "iteration: 178700 loss: 0.0025 lr: 0.02\n",
      "iteration: 178710 loss: 0.0019 lr: 0.02\n",
      "iteration: 178720 loss: 0.0018 lr: 0.02\n",
      "iteration: 178730 loss: 0.0022 lr: 0.02\n",
      "iteration: 178740 loss: 0.0024 lr: 0.02\n",
      "iteration: 178750 loss: 0.0033 lr: 0.02\n",
      "iteration: 178760 loss: 0.0028 lr: 0.02\n",
      "iteration: 178770 loss: 0.0024 lr: 0.02\n",
      "iteration: 178780 loss: 0.0025 lr: 0.02\n",
      "iteration: 178790 loss: 0.0015 lr: 0.02\n",
      "iteration: 178800 loss: 0.0028 lr: 0.02\n",
      "iteration: 178810 loss: 0.0021 lr: 0.02\n",
      "iteration: 178820 loss: 0.0028 lr: 0.02\n",
      "iteration: 178830 loss: 0.0023 lr: 0.02\n",
      "iteration: 178840 loss: 0.0025 lr: 0.02\n",
      "iteration: 178850 loss: 0.0029 lr: 0.02\n",
      "iteration: 178860 loss: 0.0028 lr: 0.02\n",
      "iteration: 178870 loss: 0.0019 lr: 0.02\n",
      "iteration: 178880 loss: 0.0018 lr: 0.02\n",
      "iteration: 178890 loss: 0.0020 lr: 0.02\n",
      "iteration: 178900 loss: 0.0021 lr: 0.02\n",
      "iteration: 178910 loss: 0.0029 lr: 0.02\n",
      "iteration: 178920 loss: 0.0019 lr: 0.02\n",
      "iteration: 178930 loss: 0.0025 lr: 0.02\n",
      "iteration: 178940 loss: 0.0017 lr: 0.02\n",
      "iteration: 178950 loss: 0.0026 lr: 0.02\n",
      "iteration: 178960 loss: 0.0034 lr: 0.02\n",
      "iteration: 178970 loss: 0.0023 lr: 0.02\n",
      "iteration: 178980 loss: 0.0023 lr: 0.02\n",
      "iteration: 178990 loss: 0.0021 lr: 0.02\n",
      "iteration: 179000 loss: 0.0036 lr: 0.02\n",
      "iteration: 179010 loss: 0.0034 lr: 0.02\n",
      "iteration: 179020 loss: 0.0023 lr: 0.02\n",
      "iteration: 179030 loss: 0.0019 lr: 0.02\n",
      "iteration: 179040 loss: 0.0021 lr: 0.02\n",
      "iteration: 179050 loss: 0.0017 lr: 0.02\n",
      "iteration: 179060 loss: 0.0039 lr: 0.02\n",
      "iteration: 179070 loss: 0.0024 lr: 0.02\n",
      "iteration: 179080 loss: 0.0019 lr: 0.02\n",
      "iteration: 179090 loss: 0.0020 lr: 0.02\n",
      "iteration: 179100 loss: 0.0025 lr: 0.02\n",
      "iteration: 179110 loss: 0.0022 lr: 0.02\n",
      "iteration: 179120 loss: 0.0028 lr: 0.02\n",
      "iteration: 179130 loss: 0.0014 lr: 0.02\n",
      "iteration: 179140 loss: 0.0021 lr: 0.02\n",
      "iteration: 179150 loss: 0.0019 lr: 0.02\n",
      "iteration: 179160 loss: 0.0028 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 179170 loss: 0.0018 lr: 0.02\n",
      "iteration: 179180 loss: 0.0027 lr: 0.02\n",
      "iteration: 179190 loss: 0.0016 lr: 0.02\n",
      "iteration: 179200 loss: 0.0026 lr: 0.02\n",
      "iteration: 179210 loss: 0.0018 lr: 0.02\n",
      "iteration: 179220 loss: 0.0031 lr: 0.02\n",
      "iteration: 179230 loss: 0.0041 lr: 0.02\n",
      "iteration: 179240 loss: 0.0019 lr: 0.02\n",
      "iteration: 179250 loss: 0.0026 lr: 0.02\n",
      "iteration: 179260 loss: 0.0029 lr: 0.02\n",
      "iteration: 179270 loss: 0.0025 lr: 0.02\n",
      "iteration: 179280 loss: 0.0025 lr: 0.02\n",
      "iteration: 179290 loss: 0.0021 lr: 0.02\n",
      "iteration: 179300 loss: 0.0022 lr: 0.02\n",
      "iteration: 179310 loss: 0.0029 lr: 0.02\n",
      "iteration: 179320 loss: 0.0019 lr: 0.02\n",
      "iteration: 179330 loss: 0.0021 lr: 0.02\n",
      "iteration: 179340 loss: 0.0032 lr: 0.02\n",
      "iteration: 179350 loss: 0.0027 lr: 0.02\n",
      "iteration: 179360 loss: 0.0033 lr: 0.02\n",
      "iteration: 179370 loss: 0.0030 lr: 0.02\n",
      "iteration: 179380 loss: 0.0017 lr: 0.02\n",
      "iteration: 179390 loss: 0.0025 lr: 0.02\n",
      "iteration: 179400 loss: 0.0030 lr: 0.02\n",
      "iteration: 179410 loss: 0.0022 lr: 0.02\n",
      "iteration: 179420 loss: 0.0029 lr: 0.02\n",
      "iteration: 179430 loss: 0.0024 lr: 0.02\n",
      "iteration: 179440 loss: 0.0028 lr: 0.02\n",
      "iteration: 179450 loss: 0.0017 lr: 0.02\n",
      "iteration: 179460 loss: 0.0031 lr: 0.02\n",
      "iteration: 179470 loss: 0.0025 lr: 0.02\n",
      "iteration: 179480 loss: 0.0028 lr: 0.02\n",
      "iteration: 179490 loss: 0.0021 lr: 0.02\n",
      "iteration: 179500 loss: 0.0030 lr: 0.02\n",
      "iteration: 179510 loss: 0.0020 lr: 0.02\n",
      "iteration: 179520 loss: 0.0023 lr: 0.02\n",
      "iteration: 179530 loss: 0.0016 lr: 0.02\n",
      "iteration: 179540 loss: 0.0021 lr: 0.02\n",
      "iteration: 179550 loss: 0.0022 lr: 0.02\n",
      "iteration: 179560 loss: 0.0021 lr: 0.02\n",
      "iteration: 179570 loss: 0.0019 lr: 0.02\n",
      "iteration: 179580 loss: 0.0034 lr: 0.02\n",
      "iteration: 179590 loss: 0.0023 lr: 0.02\n",
      "iteration: 179600 loss: 0.0025 lr: 0.02\n",
      "iteration: 179610 loss: 0.0028 lr: 0.02\n",
      "iteration: 179620 loss: 0.0022 lr: 0.02\n",
      "iteration: 179630 loss: 0.0032 lr: 0.02\n",
      "iteration: 179640 loss: 0.0022 lr: 0.02\n",
      "iteration: 179650 loss: 0.0027 lr: 0.02\n",
      "iteration: 179660 loss: 0.0019 lr: 0.02\n",
      "iteration: 179670 loss: 0.0021 lr: 0.02\n",
      "iteration: 179680 loss: 0.0022 lr: 0.02\n",
      "iteration: 179690 loss: 0.0015 lr: 0.02\n",
      "iteration: 179700 loss: 0.0015 lr: 0.02\n",
      "iteration: 179710 loss: 0.0034 lr: 0.02\n",
      "iteration: 179720 loss: 0.0014 lr: 0.02\n",
      "iteration: 179730 loss: 0.0019 lr: 0.02\n",
      "iteration: 179740 loss: 0.0021 lr: 0.02\n",
      "iteration: 179750 loss: 0.0024 lr: 0.02\n",
      "iteration: 179760 loss: 0.0026 lr: 0.02\n",
      "iteration: 179770 loss: 0.0022 lr: 0.02\n",
      "iteration: 179780 loss: 0.0017 lr: 0.02\n",
      "iteration: 179790 loss: 0.0025 lr: 0.02\n",
      "iteration: 179800 loss: 0.0028 lr: 0.02\n",
      "iteration: 179810 loss: 0.0017 lr: 0.02\n",
      "iteration: 179820 loss: 0.0028 lr: 0.02\n",
      "iteration: 179830 loss: 0.0018 lr: 0.02\n",
      "iteration: 179840 loss: 0.0018 lr: 0.02\n",
      "iteration: 179850 loss: 0.0018 lr: 0.02\n",
      "iteration: 179860 loss: 0.0019 lr: 0.02\n",
      "iteration: 179870 loss: 0.0023 lr: 0.02\n",
      "iteration: 179880 loss: 0.0022 lr: 0.02\n",
      "iteration: 179890 loss: 0.0031 lr: 0.02\n",
      "iteration: 179900 loss: 0.0022 lr: 0.02\n",
      "iteration: 179910 loss: 0.0031 lr: 0.02\n",
      "iteration: 179920 loss: 0.0020 lr: 0.02\n",
      "iteration: 179930 loss: 0.0026 lr: 0.02\n",
      "iteration: 179940 loss: 0.0025 lr: 0.02\n",
      "iteration: 179950 loss: 0.0015 lr: 0.02\n",
      "iteration: 179960 loss: 0.0022 lr: 0.02\n",
      "iteration: 179970 loss: 0.0016 lr: 0.02\n",
      "iteration: 179980 loss: 0.0018 lr: 0.02\n",
      "iteration: 179990 loss: 0.0018 lr: 0.02\n",
      "iteration: 180000 loss: 0.0024 lr: 0.02\n",
      "iteration: 180010 loss: 0.0029 lr: 0.02\n",
      "iteration: 180020 loss: 0.0014 lr: 0.02\n",
      "iteration: 180030 loss: 0.0023 lr: 0.02\n",
      "iteration: 180040 loss: 0.0030 lr: 0.02\n",
      "iteration: 180050 loss: 0.0021 lr: 0.02\n",
      "iteration: 180060 loss: 0.0033 lr: 0.02\n",
      "iteration: 180070 loss: 0.0022 lr: 0.02\n",
      "iteration: 180080 loss: 0.0019 lr: 0.02\n",
      "iteration: 180090 loss: 0.0023 lr: 0.02\n",
      "iteration: 180100 loss: 0.0018 lr: 0.02\n",
      "iteration: 180110 loss: 0.0017 lr: 0.02\n",
      "iteration: 180120 loss: 0.0018 lr: 0.02\n",
      "iteration: 180130 loss: 0.0025 lr: 0.02\n",
      "iteration: 180140 loss: 0.0019 lr: 0.02\n",
      "iteration: 180150 loss: 0.0024 lr: 0.02\n",
      "iteration: 180160 loss: 0.0026 lr: 0.02\n",
      "iteration: 180170 loss: 0.0028 lr: 0.02\n",
      "iteration: 180180 loss: 0.0022 lr: 0.02\n",
      "iteration: 180190 loss: 0.0020 lr: 0.02\n",
      "iteration: 180200 loss: 0.0023 lr: 0.02\n",
      "iteration: 180210 loss: 0.0023 lr: 0.02\n",
      "iteration: 180220 loss: 0.0022 lr: 0.02\n",
      "iteration: 180230 loss: 0.0025 lr: 0.02\n",
      "iteration: 180240 loss: 0.0021 lr: 0.02\n",
      "iteration: 180250 loss: 0.0032 lr: 0.02\n",
      "iteration: 180260 loss: 0.0019 lr: 0.02\n",
      "iteration: 180270 loss: 0.0009 lr: 0.02\n",
      "iteration: 180280 loss: 0.0020 lr: 0.02\n",
      "iteration: 180290 loss: 0.0017 lr: 0.02\n",
      "iteration: 180300 loss: 0.0032 lr: 0.02\n",
      "iteration: 180310 loss: 0.0022 lr: 0.02\n",
      "iteration: 180320 loss: 0.0028 lr: 0.02\n",
      "iteration: 180330 loss: 0.0024 lr: 0.02\n",
      "iteration: 180340 loss: 0.0019 lr: 0.02\n",
      "iteration: 180350 loss: 0.0021 lr: 0.02\n",
      "iteration: 180360 loss: 0.0044 lr: 0.02\n",
      "iteration: 180370 loss: 0.0017 lr: 0.02\n",
      "iteration: 180380 loss: 0.0018 lr: 0.02\n",
      "iteration: 180390 loss: 0.0021 lr: 0.02\n",
      "iteration: 180400 loss: 0.0023 lr: 0.02\n",
      "iteration: 180410 loss: 0.0018 lr: 0.02\n",
      "iteration: 180420 loss: 0.0021 lr: 0.02\n",
      "iteration: 180430 loss: 0.0016 lr: 0.02\n",
      "iteration: 180440 loss: 0.0027 lr: 0.02\n",
      "iteration: 180450 loss: 0.0032 lr: 0.02\n",
      "iteration: 180460 loss: 0.0035 lr: 0.02\n",
      "iteration: 180470 loss: 0.0033 lr: 0.02\n",
      "iteration: 180480 loss: 0.0023 lr: 0.02\n",
      "iteration: 180490 loss: 0.0039 lr: 0.02\n",
      "iteration: 180500 loss: 0.0026 lr: 0.02\n",
      "iteration: 180510 loss: 0.0016 lr: 0.02\n",
      "iteration: 180520 loss: 0.0021 lr: 0.02\n",
      "iteration: 180530 loss: 0.0039 lr: 0.02\n",
      "iteration: 180540 loss: 0.0021 lr: 0.02\n",
      "iteration: 180550 loss: 0.0019 lr: 0.02\n",
      "iteration: 180560 loss: 0.0020 lr: 0.02\n",
      "iteration: 180570 loss: 0.0024 lr: 0.02\n",
      "iteration: 180580 loss: 0.0028 lr: 0.02\n",
      "iteration: 180590 loss: 0.0019 lr: 0.02\n",
      "iteration: 180600 loss: 0.0026 lr: 0.02\n",
      "iteration: 180610 loss: 0.0026 lr: 0.02\n",
      "iteration: 180620 loss: 0.0018 lr: 0.02\n",
      "iteration: 180630 loss: 0.0015 lr: 0.02\n",
      "iteration: 180640 loss: 0.0017 lr: 0.02\n",
      "iteration: 180650 loss: 0.0022 lr: 0.02\n",
      "iteration: 180660 loss: 0.0024 lr: 0.02\n",
      "iteration: 180670 loss: 0.0020 lr: 0.02\n",
      "iteration: 180680 loss: 0.0023 lr: 0.02\n",
      "iteration: 180690 loss: 0.0024 lr: 0.02\n",
      "iteration: 180700 loss: 0.0018 lr: 0.02\n",
      "iteration: 180710 loss: 0.0023 lr: 0.02\n",
      "iteration: 180720 loss: 0.0025 lr: 0.02\n",
      "iteration: 180730 loss: 0.0022 lr: 0.02\n",
      "iteration: 180740 loss: 0.0022 lr: 0.02\n",
      "iteration: 180750 loss: 0.0013 lr: 0.02\n",
      "iteration: 180760 loss: 0.0026 lr: 0.02\n",
      "iteration: 180770 loss: 0.0025 lr: 0.02\n",
      "iteration: 180780 loss: 0.0025 lr: 0.02\n",
      "iteration: 180790 loss: 0.0019 lr: 0.02\n",
      "iteration: 180800 loss: 0.0022 lr: 0.02\n",
      "iteration: 180810 loss: 0.0017 lr: 0.02\n",
      "iteration: 180820 loss: 0.0019 lr: 0.02\n",
      "iteration: 180830 loss: 0.0020 lr: 0.02\n",
      "iteration: 180840 loss: 0.0021 lr: 0.02\n",
      "iteration: 180850 loss: 0.0030 lr: 0.02\n",
      "iteration: 180860 loss: 0.0025 lr: 0.02\n",
      "iteration: 180870 loss: 0.0018 lr: 0.02\n",
      "iteration: 180880 loss: 0.0029 lr: 0.02\n",
      "iteration: 180890 loss: 0.0015 lr: 0.02\n",
      "iteration: 180900 loss: 0.0018 lr: 0.02\n",
      "iteration: 180910 loss: 0.0033 lr: 0.02\n",
      "iteration: 180920 loss: 0.0024 lr: 0.02\n",
      "iteration: 180930 loss: 0.0019 lr: 0.02\n",
      "iteration: 180940 loss: 0.0018 lr: 0.02\n",
      "iteration: 180950 loss: 0.0019 lr: 0.02\n",
      "iteration: 180960 loss: 0.0021 lr: 0.02\n",
      "iteration: 180970 loss: 0.0019 lr: 0.02\n",
      "iteration: 180980 loss: 0.0019 lr: 0.02\n",
      "iteration: 180990 loss: 0.0027 lr: 0.02\n",
      "iteration: 181000 loss: 0.0016 lr: 0.02\n",
      "iteration: 181010 loss: 0.0026 lr: 0.02\n",
      "iteration: 181020 loss: 0.0030 lr: 0.02\n",
      "iteration: 181030 loss: 0.0025 lr: 0.02\n",
      "iteration: 181040 loss: 0.0022 lr: 0.02\n",
      "iteration: 181050 loss: 0.0026 lr: 0.02\n",
      "iteration: 181060 loss: 0.0023 lr: 0.02\n",
      "iteration: 181070 loss: 0.0027 lr: 0.02\n",
      "iteration: 181080 loss: 0.0025 lr: 0.02\n",
      "iteration: 181090 loss: 0.0025 lr: 0.02\n",
      "iteration: 181100 loss: 0.0019 lr: 0.02\n",
      "iteration: 181110 loss: 0.0015 lr: 0.02\n",
      "iteration: 181120 loss: 0.0016 lr: 0.02\n",
      "iteration: 181130 loss: 0.0021 lr: 0.02\n",
      "iteration: 181140 loss: 0.0028 lr: 0.02\n",
      "iteration: 181150 loss: 0.0017 lr: 0.02\n",
      "iteration: 181160 loss: 0.0018 lr: 0.02\n",
      "iteration: 181170 loss: 0.0020 lr: 0.02\n",
      "iteration: 181180 loss: 0.0019 lr: 0.02\n",
      "iteration: 181190 loss: 0.0033 lr: 0.02\n",
      "iteration: 181200 loss: 0.0026 lr: 0.02\n",
      "iteration: 181210 loss: 0.0016 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 181220 loss: 0.0030 lr: 0.02\n",
      "iteration: 181230 loss: 0.0029 lr: 0.02\n",
      "iteration: 181240 loss: 0.0025 lr: 0.02\n",
      "iteration: 181250 loss: 0.0027 lr: 0.02\n",
      "iteration: 181260 loss: 0.0031 lr: 0.02\n",
      "iteration: 181270 loss: 0.0030 lr: 0.02\n",
      "iteration: 181280 loss: 0.0020 lr: 0.02\n",
      "iteration: 181290 loss: 0.0026 lr: 0.02\n",
      "iteration: 181300 loss: 0.0021 lr: 0.02\n",
      "iteration: 181310 loss: 0.0035 lr: 0.02\n",
      "iteration: 181320 loss: 0.0030 lr: 0.02\n",
      "iteration: 181330 loss: 0.0020 lr: 0.02\n",
      "iteration: 181340 loss: 0.0019 lr: 0.02\n",
      "iteration: 181350 loss: 0.0030 lr: 0.02\n",
      "iteration: 181360 loss: 0.0030 lr: 0.02\n",
      "iteration: 181370 loss: 0.0026 lr: 0.02\n",
      "iteration: 181380 loss: 0.0034 lr: 0.02\n",
      "iteration: 181390 loss: 0.0034 lr: 0.02\n",
      "iteration: 181400 loss: 0.0023 lr: 0.02\n",
      "iteration: 181410 loss: 0.0018 lr: 0.02\n",
      "iteration: 181420 loss: 0.0023 lr: 0.02\n",
      "iteration: 181430 loss: 0.0019 lr: 0.02\n",
      "iteration: 181440 loss: 0.0015 lr: 0.02\n",
      "iteration: 181450 loss: 0.0017 lr: 0.02\n",
      "iteration: 181460 loss: 0.0019 lr: 0.02\n",
      "iteration: 181470 loss: 0.0016 lr: 0.02\n",
      "iteration: 181480 loss: 0.0020 lr: 0.02\n",
      "iteration: 181490 loss: 0.0020 lr: 0.02\n",
      "iteration: 181500 loss: 0.0017 lr: 0.02\n",
      "iteration: 181510 loss: 0.0020 lr: 0.02\n",
      "iteration: 181520 loss: 0.0024 lr: 0.02\n",
      "iteration: 181530 loss: 0.0022 lr: 0.02\n",
      "iteration: 181540 loss: 0.0040 lr: 0.02\n",
      "iteration: 181550 loss: 0.0024 lr: 0.02\n",
      "iteration: 181560 loss: 0.0028 lr: 0.02\n",
      "iteration: 181570 loss: 0.0032 lr: 0.02\n",
      "iteration: 181580 loss: 0.0017 lr: 0.02\n",
      "iteration: 181590 loss: 0.0025 lr: 0.02\n",
      "iteration: 181600 loss: 0.0032 lr: 0.02\n",
      "iteration: 181610 loss: 0.0022 lr: 0.02\n",
      "iteration: 181620 loss: 0.0023 lr: 0.02\n",
      "iteration: 181630 loss: 0.0021 lr: 0.02\n",
      "iteration: 181640 loss: 0.0022 lr: 0.02\n",
      "iteration: 181650 loss: 0.0034 lr: 0.02\n",
      "iteration: 181660 loss: 0.0023 lr: 0.02\n",
      "iteration: 181670 loss: 0.0020 lr: 0.02\n",
      "iteration: 181680 loss: 0.0023 lr: 0.02\n",
      "iteration: 181690 loss: 0.0021 lr: 0.02\n",
      "iteration: 181700 loss: 0.0026 lr: 0.02\n",
      "iteration: 181710 loss: 0.0020 lr: 0.02\n",
      "iteration: 181720 loss: 0.0027 lr: 0.02\n",
      "iteration: 181730 loss: 0.0022 lr: 0.02\n",
      "iteration: 181740 loss: 0.0032 lr: 0.02\n",
      "iteration: 181750 loss: 0.0025 lr: 0.02\n",
      "iteration: 181760 loss: 0.0026 lr: 0.02\n",
      "iteration: 181770 loss: 0.0017 lr: 0.02\n",
      "iteration: 181780 loss: 0.0019 lr: 0.02\n",
      "iteration: 181790 loss: 0.0028 lr: 0.02\n",
      "iteration: 181800 loss: 0.0029 lr: 0.02\n",
      "iteration: 181810 loss: 0.0024 lr: 0.02\n",
      "iteration: 181820 loss: 0.0018 lr: 0.02\n",
      "iteration: 181830 loss: 0.0021 lr: 0.02\n",
      "iteration: 181840 loss: 0.0020 lr: 0.02\n",
      "iteration: 181850 loss: 0.0027 lr: 0.02\n",
      "iteration: 181860 loss: 0.0020 lr: 0.02\n",
      "iteration: 181870 loss: 0.0023 lr: 0.02\n",
      "iteration: 181880 loss: 0.0019 lr: 0.02\n",
      "iteration: 181890 loss: 0.0021 lr: 0.02\n",
      "iteration: 181900 loss: 0.0023 lr: 0.02\n",
      "iteration: 181910 loss: 0.0022 lr: 0.02\n",
      "iteration: 181920 loss: 0.0019 lr: 0.02\n",
      "iteration: 181930 loss: 0.0022 lr: 0.02\n",
      "iteration: 181940 loss: 0.0018 lr: 0.02\n",
      "iteration: 181950 loss: 0.0016 lr: 0.02\n",
      "iteration: 181960 loss: 0.0017 lr: 0.02\n",
      "iteration: 181970 loss: 0.0015 lr: 0.02\n",
      "iteration: 181980 loss: 0.0013 lr: 0.02\n",
      "iteration: 181990 loss: 0.0026 lr: 0.02\n",
      "iteration: 182000 loss: 0.0020 lr: 0.02\n",
      "iteration: 182010 loss: 0.0026 lr: 0.02\n",
      "iteration: 182020 loss: 0.0020 lr: 0.02\n",
      "iteration: 182030 loss: 0.0017 lr: 0.02\n",
      "iteration: 182040 loss: 0.0023 lr: 0.02\n",
      "iteration: 182050 loss: 0.0026 lr: 0.02\n",
      "iteration: 182060 loss: 0.0028 lr: 0.02\n",
      "iteration: 182070 loss: 0.0022 lr: 0.02\n",
      "iteration: 182080 loss: 0.0020 lr: 0.02\n",
      "iteration: 182090 loss: 0.0020 lr: 0.02\n",
      "iteration: 182100 loss: 0.0029 lr: 0.02\n",
      "iteration: 182110 loss: 0.0018 lr: 0.02\n",
      "iteration: 182120 loss: 0.0021 lr: 0.02\n",
      "iteration: 182130 loss: 0.0018 lr: 0.02\n",
      "iteration: 182140 loss: 0.0022 lr: 0.02\n",
      "iteration: 182150 loss: 0.0016 lr: 0.02\n",
      "iteration: 182160 loss: 0.0018 lr: 0.02\n",
      "iteration: 182170 loss: 0.0021 lr: 0.02\n",
      "iteration: 182180 loss: 0.0022 lr: 0.02\n",
      "iteration: 182190 loss: 0.0017 lr: 0.02\n",
      "iteration: 182200 loss: 0.0026 lr: 0.02\n",
      "iteration: 182210 loss: 0.0015 lr: 0.02\n",
      "iteration: 182220 loss: 0.0020 lr: 0.02\n",
      "iteration: 182230 loss: 0.0032 lr: 0.02\n",
      "iteration: 182240 loss: 0.0020 lr: 0.02\n",
      "iteration: 182250 loss: 0.0020 lr: 0.02\n",
      "iteration: 182260 loss: 0.0021 lr: 0.02\n",
      "iteration: 182270 loss: 0.0020 lr: 0.02\n",
      "iteration: 182280 loss: 0.0031 lr: 0.02\n",
      "iteration: 182290 loss: 0.0024 lr: 0.02\n",
      "iteration: 182300 loss: 0.0025 lr: 0.02\n",
      "iteration: 182310 loss: 0.0020 lr: 0.02\n",
      "iteration: 182320 loss: 0.0014 lr: 0.02\n",
      "iteration: 182330 loss: 0.0019 lr: 0.02\n",
      "iteration: 182340 loss: 0.0019 lr: 0.02\n",
      "iteration: 182350 loss: 0.0018 lr: 0.02\n",
      "iteration: 182360 loss: 0.0015 lr: 0.02\n",
      "iteration: 182370 loss: 0.0039 lr: 0.02\n",
      "iteration: 182380 loss: 0.0019 lr: 0.02\n",
      "iteration: 182390 loss: 0.0019 lr: 0.02\n",
      "iteration: 182400 loss: 0.0022 lr: 0.02\n",
      "iteration: 182410 loss: 0.0030 lr: 0.02\n",
      "iteration: 182420 loss: 0.0022 lr: 0.02\n",
      "iteration: 182430 loss: 0.0029 lr: 0.02\n",
      "iteration: 182440 loss: 0.0017 lr: 0.02\n",
      "iteration: 182450 loss: 0.0019 lr: 0.02\n",
      "iteration: 182460 loss: 0.0023 lr: 0.02\n",
      "iteration: 182470 loss: 0.0022 lr: 0.02\n",
      "iteration: 182480 loss: 0.0019 lr: 0.02\n",
      "iteration: 182490 loss: 0.0022 lr: 0.02\n",
      "iteration: 182500 loss: 0.0013 lr: 0.02\n",
      "iteration: 182510 loss: 0.0016 lr: 0.02\n",
      "iteration: 182520 loss: 0.0024 lr: 0.02\n",
      "iteration: 182530 loss: 0.0029 lr: 0.02\n",
      "iteration: 182540 loss: 0.0021 lr: 0.02\n",
      "iteration: 182550 loss: 0.0021 lr: 0.02\n",
      "iteration: 182560 loss: 0.0029 lr: 0.02\n",
      "iteration: 182570 loss: 0.0021 lr: 0.02\n",
      "iteration: 182580 loss: 0.0020 lr: 0.02\n",
      "iteration: 182590 loss: 0.0023 lr: 0.02\n",
      "iteration: 182600 loss: 0.0025 lr: 0.02\n",
      "iteration: 182610 loss: 0.0022 lr: 0.02\n",
      "iteration: 182620 loss: 0.0027 lr: 0.02\n",
      "iteration: 182630 loss: 0.0017 lr: 0.02\n",
      "iteration: 182640 loss: 0.0037 lr: 0.02\n",
      "iteration: 182650 loss: 0.0022 lr: 0.02\n",
      "iteration: 182660 loss: 0.0022 lr: 0.02\n",
      "iteration: 182670 loss: 0.0020 lr: 0.02\n",
      "iteration: 182680 loss: 0.0025 lr: 0.02\n",
      "iteration: 182690 loss: 0.0023 lr: 0.02\n",
      "iteration: 182700 loss: 0.0020 lr: 0.02\n",
      "iteration: 182710 loss: 0.0020 lr: 0.02\n",
      "iteration: 182720 loss: 0.0018 lr: 0.02\n",
      "iteration: 182730 loss: 0.0026 lr: 0.02\n",
      "iteration: 182740 loss: 0.0021 lr: 0.02\n",
      "iteration: 182750 loss: 0.0017 lr: 0.02\n",
      "iteration: 182760 loss: 0.0016 lr: 0.02\n",
      "iteration: 182770 loss: 0.0020 lr: 0.02\n",
      "iteration: 182780 loss: 0.0018 lr: 0.02\n",
      "iteration: 182790 loss: 0.0029 lr: 0.02\n",
      "iteration: 182800 loss: 0.0031 lr: 0.02\n",
      "iteration: 182810 loss: 0.0025 lr: 0.02\n",
      "iteration: 182820 loss: 0.0032 lr: 0.02\n",
      "iteration: 182830 loss: 0.0024 lr: 0.02\n",
      "iteration: 182840 loss: 0.0020 lr: 0.02\n",
      "iteration: 182850 loss: 0.0031 lr: 0.02\n",
      "iteration: 182860 loss: 0.0032 lr: 0.02\n",
      "iteration: 182870 loss: 0.0025 lr: 0.02\n",
      "iteration: 182880 loss: 0.0020 lr: 0.02\n",
      "iteration: 182890 loss: 0.0024 lr: 0.02\n",
      "iteration: 182900 loss: 0.0027 lr: 0.02\n",
      "iteration: 182910 loss: 0.0031 lr: 0.02\n",
      "iteration: 182920 loss: 0.0030 lr: 0.02\n",
      "iteration: 182930 loss: 0.0030 lr: 0.02\n",
      "iteration: 182940 loss: 0.0027 lr: 0.02\n",
      "iteration: 182950 loss: 0.0028 lr: 0.02\n",
      "iteration: 182960 loss: 0.0018 lr: 0.02\n",
      "iteration: 182970 loss: 0.0023 lr: 0.02\n",
      "iteration: 182980 loss: 0.0022 lr: 0.02\n",
      "iteration: 182990 loss: 0.0025 lr: 0.02\n",
      "iteration: 183000 loss: 0.0021 lr: 0.02\n",
      "iteration: 183010 loss: 0.0018 lr: 0.02\n",
      "iteration: 183020 loss: 0.0024 lr: 0.02\n",
      "iteration: 183030 loss: 0.0024 lr: 0.02\n",
      "iteration: 183040 loss: 0.0015 lr: 0.02\n",
      "iteration: 183050 loss: 0.0018 lr: 0.02\n",
      "iteration: 183060 loss: 0.0023 lr: 0.02\n",
      "iteration: 183070 loss: 0.0025 lr: 0.02\n",
      "iteration: 183080 loss: 0.0028 lr: 0.02\n",
      "iteration: 183090 loss: 0.0022 lr: 0.02\n",
      "iteration: 183100 loss: 0.0023 lr: 0.02\n",
      "iteration: 183110 loss: 0.0022 lr: 0.02\n",
      "iteration: 183120 loss: 0.0019 lr: 0.02\n",
      "iteration: 183130 loss: 0.0021 lr: 0.02\n",
      "iteration: 183140 loss: 0.0020 lr: 0.02\n",
      "iteration: 183150 loss: 0.0027 lr: 0.02\n",
      "iteration: 183160 loss: 0.0028 lr: 0.02\n",
      "iteration: 183170 loss: 0.0028 lr: 0.02\n",
      "iteration: 183180 loss: 0.0027 lr: 0.02\n",
      "iteration: 183190 loss: 0.0028 lr: 0.02\n",
      "iteration: 183200 loss: 0.0034 lr: 0.02\n",
      "iteration: 183210 loss: 0.0020 lr: 0.02\n",
      "iteration: 183220 loss: 0.0019 lr: 0.02\n",
      "iteration: 183230 loss: 0.0021 lr: 0.02\n",
      "iteration: 183240 loss: 0.0021 lr: 0.02\n",
      "iteration: 183250 loss: 0.0016 lr: 0.02\n",
      "iteration: 183260 loss: 0.0028 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 183270 loss: 0.0027 lr: 0.02\n",
      "iteration: 183280 loss: 0.0026 lr: 0.02\n",
      "iteration: 183290 loss: 0.0033 lr: 0.02\n",
      "iteration: 183300 loss: 0.0037 lr: 0.02\n",
      "iteration: 183310 loss: 0.0023 lr: 0.02\n",
      "iteration: 183320 loss: 0.0016 lr: 0.02\n",
      "iteration: 183330 loss: 0.0025 lr: 0.02\n",
      "iteration: 183340 loss: 0.0031 lr: 0.02\n",
      "iteration: 183350 loss: 0.0024 lr: 0.02\n",
      "iteration: 183360 loss: 0.0016 lr: 0.02\n",
      "iteration: 183370 loss: 0.0027 lr: 0.02\n",
      "iteration: 183380 loss: 0.0029 lr: 0.02\n",
      "iteration: 183390 loss: 0.0023 lr: 0.02\n",
      "iteration: 183400 loss: 0.0027 lr: 0.02\n",
      "iteration: 183410 loss: 0.0021 lr: 0.02\n",
      "iteration: 183420 loss: 0.0017 lr: 0.02\n",
      "iteration: 183430 loss: 0.0029 lr: 0.02\n",
      "iteration: 183440 loss: 0.0027 lr: 0.02\n",
      "iteration: 183450 loss: 0.0015 lr: 0.02\n",
      "iteration: 183460 loss: 0.0019 lr: 0.02\n",
      "iteration: 183470 loss: 0.0021 lr: 0.02\n",
      "iteration: 183480 loss: 0.0021 lr: 0.02\n",
      "iteration: 183490 loss: 0.0024 lr: 0.02\n",
      "iteration: 183500 loss: 0.0021 lr: 0.02\n",
      "iteration: 183510 loss: 0.0020 lr: 0.02\n",
      "iteration: 183520 loss: 0.0023 lr: 0.02\n",
      "iteration: 183530 loss: 0.0023 lr: 0.02\n",
      "iteration: 183540 loss: 0.0020 lr: 0.02\n",
      "iteration: 183550 loss: 0.0017 lr: 0.02\n",
      "iteration: 183560 loss: 0.0020 lr: 0.02\n",
      "iteration: 183570 loss: 0.0022 lr: 0.02\n",
      "iteration: 183580 loss: 0.0022 lr: 0.02\n",
      "iteration: 183590 loss: 0.0037 lr: 0.02\n",
      "iteration: 183600 loss: 0.0018 lr: 0.02\n",
      "iteration: 183610 loss: 0.0024 lr: 0.02\n",
      "iteration: 183620 loss: 0.0035 lr: 0.02\n",
      "iteration: 183630 loss: 0.0034 lr: 0.02\n",
      "iteration: 183640 loss: 0.0027 lr: 0.02\n",
      "iteration: 183650 loss: 0.0024 lr: 0.02\n",
      "iteration: 183660 loss: 0.0026 lr: 0.02\n",
      "iteration: 183670 loss: 0.0015 lr: 0.02\n",
      "iteration: 183680 loss: 0.0032 lr: 0.02\n",
      "iteration: 183690 loss: 0.0019 lr: 0.02\n",
      "iteration: 183700 loss: 0.0017 lr: 0.02\n",
      "iteration: 183710 loss: 0.0023 lr: 0.02\n",
      "iteration: 183720 loss: 0.0019 lr: 0.02\n",
      "iteration: 183730 loss: 0.0020 lr: 0.02\n",
      "iteration: 183740 loss: 0.0018 lr: 0.02\n",
      "iteration: 183750 loss: 0.0018 lr: 0.02\n",
      "iteration: 183760 loss: 0.0019 lr: 0.02\n",
      "iteration: 183770 loss: 0.0018 lr: 0.02\n",
      "iteration: 183780 loss: 0.0020 lr: 0.02\n",
      "iteration: 183790 loss: 0.0027 lr: 0.02\n",
      "iteration: 183800 loss: 0.0031 lr: 0.02\n",
      "iteration: 183810 loss: 0.0046 lr: 0.02\n",
      "iteration: 183820 loss: 0.0024 lr: 0.02\n",
      "iteration: 183830 loss: 0.0023 lr: 0.02\n",
      "iteration: 183840 loss: 0.0021 lr: 0.02\n",
      "iteration: 183850 loss: 0.0018 lr: 0.02\n",
      "iteration: 183860 loss: 0.0020 lr: 0.02\n",
      "iteration: 183870 loss: 0.0026 lr: 0.02\n",
      "iteration: 183880 loss: 0.0020 lr: 0.02\n",
      "iteration: 183890 loss: 0.0019 lr: 0.02\n",
      "iteration: 183900 loss: 0.0026 lr: 0.02\n",
      "iteration: 183910 loss: 0.0026 lr: 0.02\n",
      "iteration: 183920 loss: 0.0015 lr: 0.02\n",
      "iteration: 183930 loss: 0.0023 lr: 0.02\n",
      "iteration: 183940 loss: 0.0016 lr: 0.02\n",
      "iteration: 183950 loss: 0.0018 lr: 0.02\n",
      "iteration: 183960 loss: 0.0031 lr: 0.02\n",
      "iteration: 183970 loss: 0.0022 lr: 0.02\n",
      "iteration: 183980 loss: 0.0024 lr: 0.02\n",
      "iteration: 183990 loss: 0.0026 lr: 0.02\n",
      "iteration: 184000 loss: 0.0030 lr: 0.02\n",
      "iteration: 184010 loss: 0.0039 lr: 0.02\n",
      "iteration: 184020 loss: 0.0019 lr: 0.02\n",
      "iteration: 184030 loss: 0.0024 lr: 0.02\n",
      "iteration: 184040 loss: 0.0014 lr: 0.02\n",
      "iteration: 184050 loss: 0.0013 lr: 0.02\n",
      "iteration: 184060 loss: 0.0025 lr: 0.02\n",
      "iteration: 184070 loss: 0.0026 lr: 0.02\n",
      "iteration: 184080 loss: 0.0019 lr: 0.02\n",
      "iteration: 184090 loss: 0.0021 lr: 0.02\n",
      "iteration: 184100 loss: 0.0018 lr: 0.02\n",
      "iteration: 184110 loss: 0.0031 lr: 0.02\n",
      "iteration: 184120 loss: 0.0017 lr: 0.02\n",
      "iteration: 184130 loss: 0.0026 lr: 0.02\n",
      "iteration: 184140 loss: 0.0022 lr: 0.02\n",
      "iteration: 184150 loss: 0.0027 lr: 0.02\n",
      "iteration: 184160 loss: 0.0024 lr: 0.02\n",
      "iteration: 184170 loss: 0.0026 lr: 0.02\n",
      "iteration: 184180 loss: 0.0026 lr: 0.02\n",
      "iteration: 184190 loss: 0.0021 lr: 0.02\n",
      "iteration: 184200 loss: 0.0025 lr: 0.02\n",
      "iteration: 184210 loss: 0.0023 lr: 0.02\n",
      "iteration: 184220 loss: 0.0033 lr: 0.02\n",
      "iteration: 184230 loss: 0.0022 lr: 0.02\n",
      "iteration: 184240 loss: 0.0019 lr: 0.02\n",
      "iteration: 184250 loss: 0.0023 lr: 0.02\n",
      "iteration: 184260 loss: 0.0026 lr: 0.02\n",
      "iteration: 184270 loss: 0.0021 lr: 0.02\n",
      "iteration: 184280 loss: 0.0026 lr: 0.02\n",
      "iteration: 184290 loss: 0.0023 lr: 0.02\n",
      "iteration: 184300 loss: 0.0017 lr: 0.02\n",
      "iteration: 184310 loss: 0.0019 lr: 0.02\n",
      "iteration: 184320 loss: 0.0021 lr: 0.02\n",
      "iteration: 184330 loss: 0.0016 lr: 0.02\n",
      "iteration: 184340 loss: 0.0025 lr: 0.02\n",
      "iteration: 184350 loss: 0.0025 lr: 0.02\n",
      "iteration: 184360 loss: 0.0024 lr: 0.02\n",
      "iteration: 184370 loss: 0.0024 lr: 0.02\n",
      "iteration: 184380 loss: 0.0026 lr: 0.02\n",
      "iteration: 184390 loss: 0.0021 lr: 0.02\n",
      "iteration: 184400 loss: 0.0019 lr: 0.02\n",
      "iteration: 184410 loss: 0.0024 lr: 0.02\n",
      "iteration: 184420 loss: 0.0019 lr: 0.02\n",
      "iteration: 184430 loss: 0.0027 lr: 0.02\n",
      "iteration: 184440 loss: 0.0023 lr: 0.02\n",
      "iteration: 184450 loss: 0.0021 lr: 0.02\n",
      "iteration: 184460 loss: 0.0047 lr: 0.02\n",
      "iteration: 184470 loss: 0.0020 lr: 0.02\n",
      "iteration: 184480 loss: 0.0027 lr: 0.02\n",
      "iteration: 184490 loss: 0.0020 lr: 0.02\n",
      "iteration: 184500 loss: 0.0028 lr: 0.02\n",
      "iteration: 184510 loss: 0.0019 lr: 0.02\n",
      "iteration: 184520 loss: 0.0023 lr: 0.02\n",
      "iteration: 184530 loss: 0.0030 lr: 0.02\n",
      "iteration: 184540 loss: 0.0021 lr: 0.02\n",
      "iteration: 184550 loss: 0.0019 lr: 0.02\n",
      "iteration: 184560 loss: 0.0023 lr: 0.02\n",
      "iteration: 184570 loss: 0.0026 lr: 0.02\n",
      "iteration: 184580 loss: 0.0040 lr: 0.02\n",
      "iteration: 184590 loss: 0.0026 lr: 0.02\n",
      "iteration: 184600 loss: 0.0022 lr: 0.02\n",
      "iteration: 184610 loss: 0.0028 lr: 0.02\n",
      "iteration: 184620 loss: 0.0019 lr: 0.02\n",
      "iteration: 184630 loss: 0.0018 lr: 0.02\n",
      "iteration: 184640 loss: 0.0017 lr: 0.02\n",
      "iteration: 184650 loss: 0.0019 lr: 0.02\n",
      "iteration: 184660 loss: 0.0020 lr: 0.02\n",
      "iteration: 184670 loss: 0.0026 lr: 0.02\n",
      "iteration: 184680 loss: 0.0023 lr: 0.02\n",
      "iteration: 184690 loss: 0.0026 lr: 0.02\n",
      "iteration: 184700 loss: 0.0023 lr: 0.02\n",
      "iteration: 184710 loss: 0.0020 lr: 0.02\n",
      "iteration: 184720 loss: 0.0019 lr: 0.02\n",
      "iteration: 184730 loss: 0.0023 lr: 0.02\n",
      "iteration: 184740 loss: 0.0016 lr: 0.02\n",
      "iteration: 184750 loss: 0.0020 lr: 0.02\n",
      "iteration: 184760 loss: 0.0019 lr: 0.02\n",
      "iteration: 184770 loss: 0.0016 lr: 0.02\n",
      "iteration: 184780 loss: 0.0020 lr: 0.02\n",
      "iteration: 184790 loss: 0.0019 lr: 0.02\n",
      "iteration: 184800 loss: 0.0019 lr: 0.02\n",
      "iteration: 184810 loss: 0.0023 lr: 0.02\n",
      "iteration: 184820 loss: 0.0026 lr: 0.02\n",
      "iteration: 184830 loss: 0.0025 lr: 0.02\n",
      "iteration: 184840 loss: 0.0028 lr: 0.02\n",
      "iteration: 184850 loss: 0.0017 lr: 0.02\n",
      "iteration: 184860 loss: 0.0021 lr: 0.02\n",
      "iteration: 184870 loss: 0.0020 lr: 0.02\n",
      "iteration: 184880 loss: 0.0014 lr: 0.02\n",
      "iteration: 184890 loss: 0.0022 lr: 0.02\n",
      "iteration: 184900 loss: 0.0022 lr: 0.02\n",
      "iteration: 184910 loss: 0.0018 lr: 0.02\n",
      "iteration: 184920 loss: 0.0024 lr: 0.02\n",
      "iteration: 184930 loss: 0.0024 lr: 0.02\n",
      "iteration: 184940 loss: 0.0015 lr: 0.02\n",
      "iteration: 184950 loss: 0.0021 lr: 0.02\n",
      "iteration: 184960 loss: 0.0016 lr: 0.02\n",
      "iteration: 184970 loss: 0.0021 lr: 0.02\n",
      "iteration: 184980 loss: 0.0024 lr: 0.02\n",
      "iteration: 184990 loss: 0.0024 lr: 0.02\n",
      "iteration: 185000 loss: 0.0031 lr: 0.02\n",
      "iteration: 185010 loss: 0.0034 lr: 0.02\n",
      "iteration: 185020 loss: 0.0025 lr: 0.02\n",
      "iteration: 185030 loss: 0.0024 lr: 0.02\n",
      "iteration: 185040 loss: 0.0022 lr: 0.02\n",
      "iteration: 185050 loss: 0.0021 lr: 0.02\n",
      "iteration: 185060 loss: 0.0028 lr: 0.02\n",
      "iteration: 185070 loss: 0.0024 lr: 0.02\n",
      "iteration: 185080 loss: 0.0017 lr: 0.02\n",
      "iteration: 185090 loss: 0.0015 lr: 0.02\n",
      "iteration: 185100 loss: 0.0026 lr: 0.02\n",
      "iteration: 185110 loss: 0.0024 lr: 0.02\n",
      "iteration: 185120 loss: 0.0029 lr: 0.02\n",
      "iteration: 185130 loss: 0.0022 lr: 0.02\n",
      "iteration: 185140 loss: 0.0037 lr: 0.02\n",
      "iteration: 185150 loss: 0.0023 lr: 0.02\n",
      "iteration: 185160 loss: 0.0022 lr: 0.02\n",
      "iteration: 185170 loss: 0.0037 lr: 0.02\n",
      "iteration: 185180 loss: 0.0022 lr: 0.02\n",
      "iteration: 185190 loss: 0.0022 lr: 0.02\n",
      "iteration: 185200 loss: 0.0029 lr: 0.02\n",
      "iteration: 185210 loss: 0.0024 lr: 0.02\n",
      "iteration: 185220 loss: 0.0024 lr: 0.02\n",
      "iteration: 185230 loss: 0.0020 lr: 0.02\n",
      "iteration: 185240 loss: 0.0017 lr: 0.02\n",
      "iteration: 185250 loss: 0.0022 lr: 0.02\n",
      "iteration: 185260 loss: 0.0026 lr: 0.02\n",
      "iteration: 185270 loss: 0.0018 lr: 0.02\n",
      "iteration: 185280 loss: 0.0023 lr: 0.02\n",
      "iteration: 185290 loss: 0.0020 lr: 0.02\n",
      "iteration: 185300 loss: 0.0019 lr: 0.02\n",
      "iteration: 185310 loss: 0.0025 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 185320 loss: 0.0021 lr: 0.02\n",
      "iteration: 185330 loss: 0.0021 lr: 0.02\n",
      "iteration: 185340 loss: 0.0019 lr: 0.02\n",
      "iteration: 185350 loss: 0.0022 lr: 0.02\n",
      "iteration: 185360 loss: 0.0028 lr: 0.02\n",
      "iteration: 185370 loss: 0.0026 lr: 0.02\n",
      "iteration: 185380 loss: 0.0018 lr: 0.02\n",
      "iteration: 185390 loss: 0.0024 lr: 0.02\n",
      "iteration: 185400 loss: 0.0024 lr: 0.02\n",
      "iteration: 185410 loss: 0.0019 lr: 0.02\n",
      "iteration: 185420 loss: 0.0015 lr: 0.02\n",
      "iteration: 185430 loss: 0.0017 lr: 0.02\n",
      "iteration: 185440 loss: 0.0014 lr: 0.02\n",
      "iteration: 185450 loss: 0.0013 lr: 0.02\n",
      "iteration: 185460 loss: 0.0026 lr: 0.02\n",
      "iteration: 185470 loss: 0.0021 lr: 0.02\n",
      "iteration: 185480 loss: 0.0024 lr: 0.02\n",
      "iteration: 185490 loss: 0.0024 lr: 0.02\n",
      "iteration: 185500 loss: 0.0021 lr: 0.02\n",
      "iteration: 185510 loss: 0.0026 lr: 0.02\n",
      "iteration: 185520 loss: 0.0030 lr: 0.02\n",
      "iteration: 185530 loss: 0.0018 lr: 0.02\n",
      "iteration: 185540 loss: 0.0027 lr: 0.02\n",
      "iteration: 185550 loss: 0.0019 lr: 0.02\n",
      "iteration: 185560 loss: 0.0029 lr: 0.02\n",
      "iteration: 185570 loss: 0.0023 lr: 0.02\n",
      "iteration: 185580 loss: 0.0030 lr: 0.02\n",
      "iteration: 185590 loss: 0.0022 lr: 0.02\n",
      "iteration: 185600 loss: 0.0022 lr: 0.02\n",
      "iteration: 185610 loss: 0.0028 lr: 0.02\n",
      "iteration: 185620 loss: 0.0024 lr: 0.02\n",
      "iteration: 185630 loss: 0.0024 lr: 0.02\n",
      "iteration: 185640 loss: 0.0021 lr: 0.02\n",
      "iteration: 185650 loss: 0.0018 lr: 0.02\n",
      "iteration: 185660 loss: 0.0030 lr: 0.02\n",
      "iteration: 185670 loss: 0.0030 lr: 0.02\n",
      "iteration: 185680 loss: 0.0026 lr: 0.02\n",
      "iteration: 185690 loss: 0.0021 lr: 0.02\n",
      "iteration: 185700 loss: 0.0025 lr: 0.02\n",
      "iteration: 185710 loss: 0.0032 lr: 0.02\n",
      "iteration: 185720 loss: 0.0020 lr: 0.02\n",
      "iteration: 185730 loss: 0.0020 lr: 0.02\n",
      "iteration: 185740 loss: 0.0028 lr: 0.02\n",
      "iteration: 185750 loss: 0.0014 lr: 0.02\n",
      "iteration: 185760 loss: 0.0025 lr: 0.02\n",
      "iteration: 185770 loss: 0.0014 lr: 0.02\n",
      "iteration: 185780 loss: 0.0020 lr: 0.02\n",
      "iteration: 185790 loss: 0.0025 lr: 0.02\n",
      "iteration: 185800 loss: 0.0023 lr: 0.02\n",
      "iteration: 185810 loss: 0.0020 lr: 0.02\n",
      "iteration: 185820 loss: 0.0019 lr: 0.02\n",
      "iteration: 185830 loss: 0.0019 lr: 0.02\n",
      "iteration: 185840 loss: 0.0026 lr: 0.02\n",
      "iteration: 185850 loss: 0.0028 lr: 0.02\n",
      "iteration: 185860 loss: 0.0021 lr: 0.02\n",
      "iteration: 185870 loss: 0.0024 lr: 0.02\n",
      "iteration: 185880 loss: 0.0025 lr: 0.02\n",
      "iteration: 185890 loss: 0.0019 lr: 0.02\n",
      "iteration: 185900 loss: 0.0024 lr: 0.02\n",
      "iteration: 185910 loss: 0.0024 lr: 0.02\n",
      "iteration: 185920 loss: 0.0021 lr: 0.02\n",
      "iteration: 185930 loss: 0.0021 lr: 0.02\n",
      "iteration: 185940 loss: 0.0028 lr: 0.02\n",
      "iteration: 185950 loss: 0.0020 lr: 0.02\n",
      "iteration: 185960 loss: 0.0018 lr: 0.02\n",
      "iteration: 185970 loss: 0.0022 lr: 0.02\n",
      "iteration: 185980 loss: 0.0017 lr: 0.02\n",
      "iteration: 185990 loss: 0.0020 lr: 0.02\n",
      "iteration: 186000 loss: 0.0020 lr: 0.02\n",
      "iteration: 186010 loss: 0.0025 lr: 0.02\n",
      "iteration: 186020 loss: 0.0034 lr: 0.02\n",
      "iteration: 186030 loss: 0.0028 lr: 0.02\n",
      "iteration: 186040 loss: 0.0020 lr: 0.02\n",
      "iteration: 186050 loss: 0.0022 lr: 0.02\n",
      "iteration: 186060 loss: 0.0018 lr: 0.02\n",
      "iteration: 186070 loss: 0.0024 lr: 0.02\n",
      "iteration: 186080 loss: 0.0021 lr: 0.02\n",
      "iteration: 186090 loss: 0.0023 lr: 0.02\n",
      "iteration: 186100 loss: 0.0020 lr: 0.02\n",
      "iteration: 186110 loss: 0.0013 lr: 0.02\n",
      "iteration: 186120 loss: 0.0025 lr: 0.02\n",
      "iteration: 186130 loss: 0.0022 lr: 0.02\n",
      "iteration: 186140 loss: 0.0023 lr: 0.02\n",
      "iteration: 186150 loss: 0.0021 lr: 0.02\n",
      "iteration: 186160 loss: 0.0022 lr: 0.02\n",
      "iteration: 186170 loss: 0.0018 lr: 0.02\n",
      "iteration: 186180 loss: 0.0020 lr: 0.02\n",
      "iteration: 186190 loss: 0.0029 lr: 0.02\n",
      "iteration: 186200 loss: 0.0022 lr: 0.02\n",
      "iteration: 186210 loss: 0.0017 lr: 0.02\n",
      "iteration: 186220 loss: 0.0021 lr: 0.02\n",
      "iteration: 186230 loss: 0.0029 lr: 0.02\n",
      "iteration: 186240 loss: 0.0020 lr: 0.02\n",
      "iteration: 186250 loss: 0.0029 lr: 0.02\n",
      "iteration: 186260 loss: 0.0018 lr: 0.02\n",
      "iteration: 186270 loss: 0.0027 lr: 0.02\n",
      "iteration: 186280 loss: 0.0027 lr: 0.02\n",
      "iteration: 186290 loss: 0.0022 lr: 0.02\n",
      "iteration: 186300 loss: 0.0019 lr: 0.02\n",
      "iteration: 186310 loss: 0.0033 lr: 0.02\n",
      "iteration: 186320 loss: 0.0022 lr: 0.02\n",
      "iteration: 186330 loss: 0.0023 lr: 0.02\n",
      "iteration: 186340 loss: 0.0028 lr: 0.02\n",
      "iteration: 186350 loss: 0.0024 lr: 0.02\n",
      "iteration: 186360 loss: 0.0020 lr: 0.02\n",
      "iteration: 186370 loss: 0.0026 lr: 0.02\n",
      "iteration: 186380 loss: 0.0023 lr: 0.02\n",
      "iteration: 186390 loss: 0.0026 lr: 0.02\n",
      "iteration: 186400 loss: 0.0019 lr: 0.02\n",
      "iteration: 186410 loss: 0.0022 lr: 0.02\n",
      "iteration: 186420 loss: 0.0018 lr: 0.02\n",
      "iteration: 186430 loss: 0.0016 lr: 0.02\n",
      "iteration: 186440 loss: 0.0026 lr: 0.02\n",
      "iteration: 186450 loss: 0.0024 lr: 0.02\n",
      "iteration: 186460 loss: 0.0020 lr: 0.02\n",
      "iteration: 186470 loss: 0.0022 lr: 0.02\n",
      "iteration: 186480 loss: 0.0019 lr: 0.02\n",
      "iteration: 186490 loss: 0.0027 lr: 0.02\n",
      "iteration: 186500 loss: 0.0015 lr: 0.02\n",
      "iteration: 186510 loss: 0.0022 lr: 0.02\n",
      "iteration: 186520 loss: 0.0035 lr: 0.02\n",
      "iteration: 186530 loss: 0.0025 lr: 0.02\n",
      "iteration: 186540 loss: 0.0014 lr: 0.02\n",
      "iteration: 186550 loss: 0.0022 lr: 0.02\n",
      "iteration: 186560 loss: 0.0018 lr: 0.02\n",
      "iteration: 186570 loss: 0.0032 lr: 0.02\n",
      "iteration: 186580 loss: 0.0027 lr: 0.02\n",
      "iteration: 186590 loss: 0.0018 lr: 0.02\n",
      "iteration: 186600 loss: 0.0019 lr: 0.02\n",
      "iteration: 186610 loss: 0.0026 lr: 0.02\n",
      "iteration: 186620 loss: 0.0023 lr: 0.02\n",
      "iteration: 186630 loss: 0.0020 lr: 0.02\n",
      "iteration: 186640 loss: 0.0027 lr: 0.02\n",
      "iteration: 186650 loss: 0.0017 lr: 0.02\n",
      "iteration: 186660 loss: 0.0020 lr: 0.02\n",
      "iteration: 186670 loss: 0.0018 lr: 0.02\n",
      "iteration: 186680 loss: 0.0018 lr: 0.02\n",
      "iteration: 186690 loss: 0.0023 lr: 0.02\n",
      "iteration: 186700 loss: 0.0023 lr: 0.02\n",
      "iteration: 186710 loss: 0.0019 lr: 0.02\n",
      "iteration: 186720 loss: 0.0017 lr: 0.02\n",
      "iteration: 186730 loss: 0.0029 lr: 0.02\n",
      "iteration: 186740 loss: 0.0027 lr: 0.02\n",
      "iteration: 186750 loss: 0.0024 lr: 0.02\n",
      "iteration: 186760 loss: 0.0021 lr: 0.02\n",
      "iteration: 186770 loss: 0.0025 lr: 0.02\n",
      "iteration: 186780 loss: 0.0025 lr: 0.02\n",
      "iteration: 186790 loss: 0.0020 lr: 0.02\n",
      "iteration: 186800 loss: 0.0025 lr: 0.02\n",
      "iteration: 186810 loss: 0.0023 lr: 0.02\n",
      "iteration: 186820 loss: 0.0023 lr: 0.02\n",
      "iteration: 186830 loss: 0.0019 lr: 0.02\n",
      "iteration: 186840 loss: 0.0022 lr: 0.02\n",
      "iteration: 186850 loss: 0.0025 lr: 0.02\n",
      "iteration: 186860 loss: 0.0020 lr: 0.02\n",
      "iteration: 186870 loss: 0.0032 lr: 0.02\n",
      "iteration: 186880 loss: 0.0013 lr: 0.02\n",
      "iteration: 186890 loss: 0.0017 lr: 0.02\n",
      "iteration: 186900 loss: 0.0026 lr: 0.02\n",
      "iteration: 186910 loss: 0.0032 lr: 0.02\n",
      "iteration: 186920 loss: 0.0017 lr: 0.02\n",
      "iteration: 186930 loss: 0.0019 lr: 0.02\n",
      "iteration: 186940 loss: 0.0017 lr: 0.02\n",
      "iteration: 186950 loss: 0.0031 lr: 0.02\n",
      "iteration: 186960 loss: 0.0021 lr: 0.02\n",
      "iteration: 186970 loss: 0.0027 lr: 0.02\n",
      "iteration: 186980 loss: 0.0019 lr: 0.02\n",
      "iteration: 186990 loss: 0.0022 lr: 0.02\n",
      "iteration: 187000 loss: 0.0019 lr: 0.02\n",
      "iteration: 187010 loss: 0.0026 lr: 0.02\n",
      "iteration: 187020 loss: 0.0020 lr: 0.02\n",
      "iteration: 187030 loss: 0.0016 lr: 0.02\n",
      "iteration: 187040 loss: 0.0021 lr: 0.02\n",
      "iteration: 187050 loss: 0.0016 lr: 0.02\n",
      "iteration: 187060 loss: 0.0021 lr: 0.02\n",
      "iteration: 187070 loss: 0.0035 lr: 0.02\n",
      "iteration: 187080 loss: 0.0030 lr: 0.02\n",
      "iteration: 187090 loss: 0.0026 lr: 0.02\n",
      "iteration: 187100 loss: 0.0024 lr: 0.02\n",
      "iteration: 187110 loss: 0.0017 lr: 0.02\n",
      "iteration: 187120 loss: 0.0023 lr: 0.02\n",
      "iteration: 187130 loss: 0.0020 lr: 0.02\n",
      "iteration: 187140 loss: 0.0024 lr: 0.02\n",
      "iteration: 187150 loss: 0.0018 lr: 0.02\n",
      "iteration: 187160 loss: 0.0019 lr: 0.02\n",
      "iteration: 187170 loss: 0.0019 lr: 0.02\n",
      "iteration: 187180 loss: 0.0016 lr: 0.02\n",
      "iteration: 187190 loss: 0.0020 lr: 0.02\n",
      "iteration: 187200 loss: 0.0027 lr: 0.02\n",
      "iteration: 187210 loss: 0.0022 lr: 0.02\n",
      "iteration: 187220 loss: 0.0020 lr: 0.02\n",
      "iteration: 187230 loss: 0.0018 lr: 0.02\n",
      "iteration: 187240 loss: 0.0022 lr: 0.02\n",
      "iteration: 187250 loss: 0.0017 lr: 0.02\n",
      "iteration: 187260 loss: 0.0017 lr: 0.02\n",
      "iteration: 187270 loss: 0.0016 lr: 0.02\n",
      "iteration: 187280 loss: 0.0021 lr: 0.02\n",
      "iteration: 187290 loss: 0.0016 lr: 0.02\n",
      "iteration: 187300 loss: 0.0025 lr: 0.02\n",
      "iteration: 187310 loss: 0.0017 lr: 0.02\n",
      "iteration: 187320 loss: 0.0021 lr: 0.02\n",
      "iteration: 187330 loss: 0.0024 lr: 0.02\n",
      "iteration: 187340 loss: 0.0018 lr: 0.02\n",
      "iteration: 187350 loss: 0.0032 lr: 0.02\n",
      "iteration: 187360 loss: 0.0020 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 187370 loss: 0.0023 lr: 0.02\n",
      "iteration: 187380 loss: 0.0024 lr: 0.02\n",
      "iteration: 187390 loss: 0.0019 lr: 0.02\n",
      "iteration: 187400 loss: 0.0031 lr: 0.02\n",
      "iteration: 187410 loss: 0.0031 lr: 0.02\n",
      "iteration: 187420 loss: 0.0024 lr: 0.02\n",
      "iteration: 187430 loss: 0.0020 lr: 0.02\n",
      "iteration: 187440 loss: 0.0024 lr: 0.02\n",
      "iteration: 187450 loss: 0.0027 lr: 0.02\n",
      "iteration: 187460 loss: 0.0033 lr: 0.02\n",
      "iteration: 187470 loss: 0.0018 lr: 0.02\n",
      "iteration: 187480 loss: 0.0024 lr: 0.02\n",
      "iteration: 187490 loss: 0.0028 lr: 0.02\n",
      "iteration: 187500 loss: 0.0020 lr: 0.02\n",
      "iteration: 187510 loss: 0.0025 lr: 0.02\n",
      "iteration: 187520 loss: 0.0031 lr: 0.02\n",
      "iteration: 187530 loss: 0.0027 lr: 0.02\n",
      "iteration: 187540 loss: 0.0025 lr: 0.02\n",
      "iteration: 187550 loss: 0.0018 lr: 0.02\n",
      "iteration: 187560 loss: 0.0022 lr: 0.02\n",
      "iteration: 187570 loss: 0.0026 lr: 0.02\n",
      "iteration: 187580 loss: 0.0020 lr: 0.02\n",
      "iteration: 187590 loss: 0.0021 lr: 0.02\n",
      "iteration: 187600 loss: 0.0022 lr: 0.02\n",
      "iteration: 187610 loss: 0.0018 lr: 0.02\n",
      "iteration: 187620 loss: 0.0024 lr: 0.02\n",
      "iteration: 187630 loss: 0.0025 lr: 0.02\n",
      "iteration: 187640 loss: 0.0038 lr: 0.02\n",
      "iteration: 187650 loss: 0.0014 lr: 0.02\n",
      "iteration: 187660 loss: 0.0015 lr: 0.02\n",
      "iteration: 187670 loss: 0.0023 lr: 0.02\n",
      "iteration: 187680 loss: 0.0019 lr: 0.02\n",
      "iteration: 187690 loss: 0.0033 lr: 0.02\n",
      "iteration: 187700 loss: 0.0026 lr: 0.02\n",
      "iteration: 187710 loss: 0.0028 lr: 0.02\n",
      "iteration: 187720 loss: 0.0018 lr: 0.02\n",
      "iteration: 187730 loss: 0.0020 lr: 0.02\n",
      "iteration: 187740 loss: 0.0021 lr: 0.02\n",
      "iteration: 187750 loss: 0.0022 lr: 0.02\n",
      "iteration: 187760 loss: 0.0025 lr: 0.02\n",
      "iteration: 187770 loss: 0.0023 lr: 0.02\n",
      "iteration: 187780 loss: 0.0020 lr: 0.02\n",
      "iteration: 187790 loss: 0.0015 lr: 0.02\n",
      "iteration: 187800 loss: 0.0026 lr: 0.02\n",
      "iteration: 187810 loss: 0.0021 lr: 0.02\n",
      "iteration: 187820 loss: 0.0022 lr: 0.02\n",
      "iteration: 187830 loss: 0.0016 lr: 0.02\n",
      "iteration: 187840 loss: 0.0017 lr: 0.02\n",
      "iteration: 187850 loss: 0.0014 lr: 0.02\n",
      "iteration: 187860 loss: 0.0025 lr: 0.02\n",
      "iteration: 187870 loss: 0.0021 lr: 0.02\n",
      "iteration: 187880 loss: 0.0016 lr: 0.02\n",
      "iteration: 187890 loss: 0.0023 lr: 0.02\n",
      "iteration: 187900 loss: 0.0030 lr: 0.02\n",
      "iteration: 187910 loss: 0.0027 lr: 0.02\n",
      "iteration: 187920 loss: 0.0020 lr: 0.02\n",
      "iteration: 187930 loss: 0.0025 lr: 0.02\n",
      "iteration: 187940 loss: 0.0020 lr: 0.02\n",
      "iteration: 187950 loss: 0.0022 lr: 0.02\n",
      "iteration: 187960 loss: 0.0028 lr: 0.02\n",
      "iteration: 187970 loss: 0.0015 lr: 0.02\n",
      "iteration: 187980 loss: 0.0018 lr: 0.02\n",
      "iteration: 187990 loss: 0.0027 lr: 0.02\n",
      "iteration: 188000 loss: 0.0025 lr: 0.02\n",
      "iteration: 188010 loss: 0.0022 lr: 0.02\n",
      "iteration: 188020 loss: 0.0021 lr: 0.02\n",
      "iteration: 188030 loss: 0.0016 lr: 0.02\n",
      "iteration: 188040 loss: 0.0024 lr: 0.02\n",
      "iteration: 188050 loss: 0.0028 lr: 0.02\n",
      "iteration: 188060 loss: 0.0020 lr: 0.02\n",
      "iteration: 188070 loss: 0.0024 lr: 0.02\n",
      "iteration: 188080 loss: 0.0016 lr: 0.02\n",
      "iteration: 188090 loss: 0.0023 lr: 0.02\n",
      "iteration: 188100 loss: 0.0018 lr: 0.02\n",
      "iteration: 188110 loss: 0.0024 lr: 0.02\n",
      "iteration: 188120 loss: 0.0016 lr: 0.02\n",
      "iteration: 188130 loss: 0.0023 lr: 0.02\n",
      "iteration: 188140 loss: 0.0023 lr: 0.02\n",
      "iteration: 188150 loss: 0.0022 lr: 0.02\n",
      "iteration: 188160 loss: 0.0014 lr: 0.02\n",
      "iteration: 188170 loss: 0.0016 lr: 0.02\n",
      "iteration: 188180 loss: 0.0019 lr: 0.02\n",
      "iteration: 188190 loss: 0.0016 lr: 0.02\n",
      "iteration: 188200 loss: 0.0019 lr: 0.02\n",
      "iteration: 188210 loss: 0.0029 lr: 0.02\n",
      "iteration: 188220 loss: 0.0013 lr: 0.02\n",
      "iteration: 188230 loss: 0.0019 lr: 0.02\n",
      "iteration: 188240 loss: 0.0023 lr: 0.02\n",
      "iteration: 188250 loss: 0.0021 lr: 0.02\n",
      "iteration: 188260 loss: 0.0034 lr: 0.02\n",
      "iteration: 188270 loss: 0.0017 lr: 0.02\n",
      "iteration: 188280 loss: 0.0028 lr: 0.02\n",
      "iteration: 188290 loss: 0.0022 lr: 0.02\n",
      "iteration: 188300 loss: 0.0025 lr: 0.02\n",
      "iteration: 188310 loss: 0.0025 lr: 0.02\n",
      "iteration: 188320 loss: 0.0034 lr: 0.02\n",
      "iteration: 188330 loss: 0.0017 lr: 0.02\n",
      "iteration: 188340 loss: 0.0029 lr: 0.02\n",
      "iteration: 188350 loss: 0.0022 lr: 0.02\n",
      "iteration: 188360 loss: 0.0016 lr: 0.02\n",
      "iteration: 188370 loss: 0.0025 lr: 0.02\n",
      "iteration: 188380 loss: 0.0024 lr: 0.02\n",
      "iteration: 188390 loss: 0.0042 lr: 0.02\n",
      "iteration: 188400 loss: 0.0023 lr: 0.02\n",
      "iteration: 188410 loss: 0.0041 lr: 0.02\n",
      "iteration: 188420 loss: 0.0022 lr: 0.02\n",
      "iteration: 188430 loss: 0.0024 lr: 0.02\n",
      "iteration: 188440 loss: 0.0018 lr: 0.02\n",
      "iteration: 188450 loss: 0.0018 lr: 0.02\n",
      "iteration: 188460 loss: 0.0024 lr: 0.02\n",
      "iteration: 188470 loss: 0.0019 lr: 0.02\n",
      "iteration: 188480 loss: 0.0021 lr: 0.02\n",
      "iteration: 188490 loss: 0.0027 lr: 0.02\n",
      "iteration: 188500 loss: 0.0015 lr: 0.02\n",
      "iteration: 188510 loss: 0.0021 lr: 0.02\n",
      "iteration: 188520 loss: 0.0019 lr: 0.02\n",
      "iteration: 188530 loss: 0.0017 lr: 0.02\n",
      "iteration: 188540 loss: 0.0019 lr: 0.02\n",
      "iteration: 188550 loss: 0.0015 lr: 0.02\n",
      "iteration: 188560 loss: 0.0021 lr: 0.02\n",
      "iteration: 188570 loss: 0.0029 lr: 0.02\n",
      "iteration: 188580 loss: 0.0020 lr: 0.02\n",
      "iteration: 188590 loss: 0.0025 lr: 0.02\n",
      "iteration: 188600 loss: 0.0026 lr: 0.02\n",
      "iteration: 188610 loss: 0.0018 lr: 0.02\n",
      "iteration: 188620 loss: 0.0027 lr: 0.02\n",
      "iteration: 188630 loss: 0.0024 lr: 0.02\n",
      "iteration: 188640 loss: 0.0020 lr: 0.02\n",
      "iteration: 188650 loss: 0.0021 lr: 0.02\n",
      "iteration: 188660 loss: 0.0016 lr: 0.02\n",
      "iteration: 188670 loss: 0.0027 lr: 0.02\n",
      "iteration: 188680 loss: 0.0019 lr: 0.02\n",
      "iteration: 188690 loss: 0.0021 lr: 0.02\n",
      "iteration: 188700 loss: 0.0033 lr: 0.02\n",
      "iteration: 188710 loss: 0.0023 lr: 0.02\n",
      "iteration: 188720 loss: 0.0017 lr: 0.02\n",
      "iteration: 188730 loss: 0.0021 lr: 0.02\n",
      "iteration: 188740 loss: 0.0021 lr: 0.02\n",
      "iteration: 188750 loss: 0.0025 lr: 0.02\n",
      "iteration: 188760 loss: 0.0024 lr: 0.02\n",
      "iteration: 188770 loss: 0.0031 lr: 0.02\n",
      "iteration: 188780 loss: 0.0016 lr: 0.02\n",
      "iteration: 188790 loss: 0.0016 lr: 0.02\n",
      "iteration: 188800 loss: 0.0025 lr: 0.02\n",
      "iteration: 188810 loss: 0.0025 lr: 0.02\n",
      "iteration: 188820 loss: 0.0028 lr: 0.02\n",
      "iteration: 188830 loss: 0.0031 lr: 0.02\n",
      "iteration: 188840 loss: 0.0023 lr: 0.02\n",
      "iteration: 188850 loss: 0.0029 lr: 0.02\n",
      "iteration: 188860 loss: 0.0025 lr: 0.02\n",
      "iteration: 188870 loss: 0.0034 lr: 0.02\n",
      "iteration: 188880 loss: 0.0024 lr: 0.02\n",
      "iteration: 188890 loss: 0.0017 lr: 0.02\n",
      "iteration: 188900 loss: 0.0022 lr: 0.02\n",
      "iteration: 188910 loss: 0.0028 lr: 0.02\n",
      "iteration: 188920 loss: 0.0023 lr: 0.02\n",
      "iteration: 188930 loss: 0.0020 lr: 0.02\n",
      "iteration: 188940 loss: 0.0029 lr: 0.02\n",
      "iteration: 188950 loss: 0.0025 lr: 0.02\n",
      "iteration: 188960 loss: 0.0017 lr: 0.02\n",
      "iteration: 188970 loss: 0.0019 lr: 0.02\n",
      "iteration: 188980 loss: 0.0019 lr: 0.02\n",
      "iteration: 188990 loss: 0.0024 lr: 0.02\n",
      "iteration: 189000 loss: 0.0038 lr: 0.02\n",
      "iteration: 189010 loss: 0.0027 lr: 0.02\n",
      "iteration: 189020 loss: 0.0018 lr: 0.02\n",
      "iteration: 189030 loss: 0.0028 lr: 0.02\n",
      "iteration: 189040 loss: 0.0024 lr: 0.02\n",
      "iteration: 189050 loss: 0.0016 lr: 0.02\n",
      "iteration: 189060 loss: 0.0022 lr: 0.02\n",
      "iteration: 189070 loss: 0.0028 lr: 0.02\n",
      "iteration: 189080 loss: 0.0019 lr: 0.02\n",
      "iteration: 189090 loss: 0.0020 lr: 0.02\n",
      "iteration: 189100 loss: 0.0022 lr: 0.02\n",
      "iteration: 189110 loss: 0.0021 lr: 0.02\n",
      "iteration: 189120 loss: 0.0017 lr: 0.02\n",
      "iteration: 189130 loss: 0.0023 lr: 0.02\n",
      "iteration: 189140 loss: 0.0016 lr: 0.02\n",
      "iteration: 189150 loss: 0.0021 lr: 0.02\n",
      "iteration: 189160 loss: 0.0020 lr: 0.02\n",
      "iteration: 189170 loss: 0.0021 lr: 0.02\n",
      "iteration: 189180 loss: 0.0034 lr: 0.02\n",
      "iteration: 189190 loss: 0.0020 lr: 0.02\n",
      "iteration: 189200 loss: 0.0015 lr: 0.02\n",
      "iteration: 189210 loss: 0.0024 lr: 0.02\n",
      "iteration: 189220 loss: 0.0022 lr: 0.02\n",
      "iteration: 189230 loss: 0.0036 lr: 0.02\n",
      "iteration: 189240 loss: 0.0022 lr: 0.02\n",
      "iteration: 189250 loss: 0.0021 lr: 0.02\n",
      "iteration: 189260 loss: 0.0019 lr: 0.02\n",
      "iteration: 189270 loss: 0.0024 lr: 0.02\n",
      "iteration: 189280 loss: 0.0018 lr: 0.02\n",
      "iteration: 189290 loss: 0.0042 lr: 0.02\n",
      "iteration: 189300 loss: 0.0026 lr: 0.02\n",
      "iteration: 189310 loss: 0.0024 lr: 0.02\n",
      "iteration: 189320 loss: 0.0021 lr: 0.02\n",
      "iteration: 189330 loss: 0.0027 lr: 0.02\n",
      "iteration: 189340 loss: 0.0017 lr: 0.02\n",
      "iteration: 189350 loss: 0.0019 lr: 0.02\n",
      "iteration: 189360 loss: 0.0028 lr: 0.02\n",
      "iteration: 189370 loss: 0.0021 lr: 0.02\n",
      "iteration: 189380 loss: 0.0031 lr: 0.02\n",
      "iteration: 189390 loss: 0.0029 lr: 0.02\n",
      "iteration: 189400 loss: 0.0018 lr: 0.02\n",
      "iteration: 189410 loss: 0.0021 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 189420 loss: 0.0026 lr: 0.02\n",
      "iteration: 189430 loss: 0.0022 lr: 0.02\n",
      "iteration: 189440 loss: 0.0023 lr: 0.02\n",
      "iteration: 189450 loss: 0.0036 lr: 0.02\n",
      "iteration: 189460 loss: 0.0021 lr: 0.02\n",
      "iteration: 189470 loss: 0.0024 lr: 0.02\n",
      "iteration: 189480 loss: 0.0027 lr: 0.02\n",
      "iteration: 189490 loss: 0.0027 lr: 0.02\n",
      "iteration: 189500 loss: 0.0020 lr: 0.02\n",
      "iteration: 189510 loss: 0.0031 lr: 0.02\n",
      "iteration: 189520 loss: 0.0020 lr: 0.02\n",
      "iteration: 189530 loss: 0.0017 lr: 0.02\n",
      "iteration: 189540 loss: 0.0021 lr: 0.02\n",
      "iteration: 189550 loss: 0.0022 lr: 0.02\n",
      "iteration: 189560 loss: 0.0025 lr: 0.02\n",
      "iteration: 189570 loss: 0.0046 lr: 0.02\n",
      "iteration: 189580 loss: 0.0020 lr: 0.02\n",
      "iteration: 189590 loss: 0.0016 lr: 0.02\n",
      "iteration: 189600 loss: 0.0035 lr: 0.02\n",
      "iteration: 189610 loss: 0.0022 lr: 0.02\n",
      "iteration: 189620 loss: 0.0031 lr: 0.02\n",
      "iteration: 189630 loss: 0.0020 lr: 0.02\n",
      "iteration: 189640 loss: 0.0018 lr: 0.02\n",
      "iteration: 189650 loss: 0.0022 lr: 0.02\n",
      "iteration: 189660 loss: 0.0023 lr: 0.02\n",
      "iteration: 189670 loss: 0.0024 lr: 0.02\n",
      "iteration: 189680 loss: 0.0024 lr: 0.02\n",
      "iteration: 189690 loss: 0.0024 lr: 0.02\n",
      "iteration: 189700 loss: 0.0033 lr: 0.02\n",
      "iteration: 189710 loss: 0.0030 lr: 0.02\n",
      "iteration: 189720 loss: 0.0024 lr: 0.02\n",
      "iteration: 189730 loss: 0.0021 lr: 0.02\n",
      "iteration: 189740 loss: 0.0017 lr: 0.02\n",
      "iteration: 189750 loss: 0.0047 lr: 0.02\n",
      "iteration: 189760 loss: 0.0028 lr: 0.02\n",
      "iteration: 189770 loss: 0.0017 lr: 0.02\n",
      "iteration: 189780 loss: 0.0030 lr: 0.02\n",
      "iteration: 189790 loss: 0.0026 lr: 0.02\n",
      "iteration: 189800 loss: 0.0027 lr: 0.02\n",
      "iteration: 189810 loss: 0.0020 lr: 0.02\n",
      "iteration: 189820 loss: 0.0017 lr: 0.02\n",
      "iteration: 189830 loss: 0.0027 lr: 0.02\n",
      "iteration: 189840 loss: 0.0029 lr: 0.02\n",
      "iteration: 189850 loss: 0.0022 lr: 0.02\n",
      "iteration: 189860 loss: 0.0022 lr: 0.02\n",
      "iteration: 189870 loss: 0.0017 lr: 0.02\n",
      "iteration: 189880 loss: 0.0022 lr: 0.02\n",
      "iteration: 189890 loss: 0.0018 lr: 0.02\n",
      "iteration: 189900 loss: 0.0018 lr: 0.02\n",
      "iteration: 189910 loss: 0.0019 lr: 0.02\n",
      "iteration: 189920 loss: 0.0026 lr: 0.02\n",
      "iteration: 189930 loss: 0.0021 lr: 0.02\n",
      "iteration: 189940 loss: 0.0020 lr: 0.02\n",
      "iteration: 189950 loss: 0.0022 lr: 0.02\n",
      "iteration: 189960 loss: 0.0023 lr: 0.02\n",
      "iteration: 189970 loss: 0.0023 lr: 0.02\n",
      "iteration: 189980 loss: 0.0022 lr: 0.02\n",
      "iteration: 189990 loss: 0.0025 lr: 0.02\n",
      "iteration: 190000 loss: 0.0027 lr: 0.02\n",
      "iteration: 190010 loss: 0.0026 lr: 0.02\n",
      "iteration: 190020 loss: 0.0036 lr: 0.02\n",
      "iteration: 190030 loss: 0.0018 lr: 0.02\n",
      "iteration: 190040 loss: 0.0020 lr: 0.02\n",
      "iteration: 190050 loss: 0.0024 lr: 0.02\n",
      "iteration: 190060 loss: 0.0033 lr: 0.02\n",
      "iteration: 190070 loss: 0.0030 lr: 0.02\n",
      "iteration: 190080 loss: 0.0015 lr: 0.02\n",
      "iteration: 190090 loss: 0.0029 lr: 0.02\n",
      "iteration: 190100 loss: 0.0024 lr: 0.02\n",
      "iteration: 190110 loss: 0.0034 lr: 0.02\n",
      "iteration: 190120 loss: 0.0028 lr: 0.02\n",
      "iteration: 190130 loss: 0.0019 lr: 0.02\n",
      "iteration: 190140 loss: 0.0030 lr: 0.02\n",
      "iteration: 190150 loss: 0.0014 lr: 0.02\n",
      "iteration: 190160 loss: 0.0025 lr: 0.02\n",
      "iteration: 190170 loss: 0.0020 lr: 0.02\n",
      "iteration: 190180 loss: 0.0020 lr: 0.02\n",
      "iteration: 190190 loss: 0.0028 lr: 0.02\n",
      "iteration: 190200 loss: 0.0022 lr: 0.02\n",
      "iteration: 190210 loss: 0.0020 lr: 0.02\n",
      "iteration: 190220 loss: 0.0023 lr: 0.02\n",
      "iteration: 190230 loss: 0.0033 lr: 0.02\n",
      "iteration: 190240 loss: 0.0018 lr: 0.02\n",
      "iteration: 190250 loss: 0.0015 lr: 0.02\n",
      "iteration: 190260 loss: 0.0018 lr: 0.02\n",
      "iteration: 190270 loss: 0.0018 lr: 0.02\n",
      "iteration: 190280 loss: 0.0022 lr: 0.02\n",
      "iteration: 190290 loss: 0.0020 lr: 0.02\n",
      "iteration: 190300 loss: 0.0020 lr: 0.02\n",
      "iteration: 190310 loss: 0.0021 lr: 0.02\n",
      "iteration: 190320 loss: 0.0019 lr: 0.02\n",
      "iteration: 190330 loss: 0.0019 lr: 0.02\n",
      "iteration: 190340 loss: 0.0023 lr: 0.02\n",
      "iteration: 190350 loss: 0.0022 lr: 0.02\n",
      "iteration: 190360 loss: 0.0023 lr: 0.02\n",
      "iteration: 190370 loss: 0.0019 lr: 0.02\n",
      "iteration: 190380 loss: 0.0018 lr: 0.02\n",
      "iteration: 190390 loss: 0.0023 lr: 0.02\n",
      "iteration: 190400 loss: 0.0016 lr: 0.02\n",
      "iteration: 190410 loss: 0.0044 lr: 0.02\n",
      "iteration: 190420 loss: 0.0027 lr: 0.02\n",
      "iteration: 190430 loss: 0.0022 lr: 0.02\n",
      "iteration: 190440 loss: 0.0024 lr: 0.02\n",
      "iteration: 190450 loss: 0.0020 lr: 0.02\n",
      "iteration: 190460 loss: 0.0024 lr: 0.02\n",
      "iteration: 190470 loss: 0.0021 lr: 0.02\n",
      "iteration: 190480 loss: 0.0015 lr: 0.02\n",
      "iteration: 190490 loss: 0.0023 lr: 0.02\n",
      "iteration: 190500 loss: 0.0023 lr: 0.02\n",
      "iteration: 190510 loss: 0.0020 lr: 0.02\n",
      "iteration: 190520 loss: 0.0021 lr: 0.02\n",
      "iteration: 190530 loss: 0.0018 lr: 0.02\n",
      "iteration: 190540 loss: 0.0021 lr: 0.02\n",
      "iteration: 190550 loss: 0.0030 lr: 0.02\n",
      "iteration: 190560 loss: 0.0015 lr: 0.02\n",
      "iteration: 190570 loss: 0.0021 lr: 0.02\n",
      "iteration: 190580 loss: 0.0022 lr: 0.02\n",
      "iteration: 190590 loss: 0.0017 lr: 0.02\n",
      "iteration: 190600 loss: 0.0016 lr: 0.02\n",
      "iteration: 190610 loss: 0.0022 lr: 0.02\n",
      "iteration: 190620 loss: 0.0026 lr: 0.02\n",
      "iteration: 190630 loss: 0.0033 lr: 0.02\n",
      "iteration: 190640 loss: 0.0025 lr: 0.02\n",
      "iteration: 190650 loss: 0.0024 lr: 0.02\n",
      "iteration: 190660 loss: 0.0048 lr: 0.02\n",
      "iteration: 190670 loss: 0.0019 lr: 0.02\n",
      "iteration: 190680 loss: 0.0017 lr: 0.02\n",
      "iteration: 190690 loss: 0.0039 lr: 0.02\n",
      "iteration: 190700 loss: 0.0019 lr: 0.02\n",
      "iteration: 190710 loss: 0.0019 lr: 0.02\n",
      "iteration: 190720 loss: 0.0021 lr: 0.02\n",
      "iteration: 190730 loss: 0.0020 lr: 0.02\n",
      "iteration: 190740 loss: 0.0032 lr: 0.02\n",
      "iteration: 190750 loss: 0.0016 lr: 0.02\n",
      "iteration: 190760 loss: 0.0027 lr: 0.02\n",
      "iteration: 190770 loss: 0.0029 lr: 0.02\n",
      "iteration: 190780 loss: 0.0019 lr: 0.02\n",
      "iteration: 190790 loss: 0.0029 lr: 0.02\n",
      "iteration: 190800 loss: 0.0015 lr: 0.02\n",
      "iteration: 190810 loss: 0.0038 lr: 0.02\n",
      "iteration: 190820 loss: 0.0022 lr: 0.02\n",
      "iteration: 190830 loss: 0.0032 lr: 0.02\n",
      "iteration: 190840 loss: 0.0024 lr: 0.02\n",
      "iteration: 190850 loss: 0.0026 lr: 0.02\n",
      "iteration: 190860 loss: 0.0033 lr: 0.02\n",
      "iteration: 190870 loss: 0.0030 lr: 0.02\n",
      "iteration: 190880 loss: 0.0019 lr: 0.02\n",
      "iteration: 190890 loss: 0.0025 lr: 0.02\n",
      "iteration: 190900 loss: 0.0020 lr: 0.02\n",
      "iteration: 190910 loss: 0.0022 lr: 0.02\n",
      "iteration: 190920 loss: 0.0038 lr: 0.02\n",
      "iteration: 190930 loss: 0.0022 lr: 0.02\n",
      "iteration: 190940 loss: 0.0025 lr: 0.02\n",
      "iteration: 190950 loss: 0.0024 lr: 0.02\n",
      "iteration: 190960 loss: 0.0024 lr: 0.02\n",
      "iteration: 190970 loss: 0.0024 lr: 0.02\n",
      "iteration: 190980 loss: 0.0021 lr: 0.02\n",
      "iteration: 190990 loss: 0.0019 lr: 0.02\n",
      "iteration: 191000 loss: 0.0020 lr: 0.02\n",
      "iteration: 191010 loss: 0.0019 lr: 0.02\n",
      "iteration: 191020 loss: 0.0023 lr: 0.02\n",
      "iteration: 191030 loss: 0.0030 lr: 0.02\n",
      "iteration: 191040 loss: 0.0026 lr: 0.02\n",
      "iteration: 191050 loss: 0.0027 lr: 0.02\n",
      "iteration: 191060 loss: 0.0020 lr: 0.02\n",
      "iteration: 191070 loss: 0.0025 lr: 0.02\n",
      "iteration: 191080 loss: 0.0027 lr: 0.02\n",
      "iteration: 191090 loss: 0.0022 lr: 0.02\n",
      "iteration: 191100 loss: 0.0017 lr: 0.02\n",
      "iteration: 191110 loss: 0.0024 lr: 0.02\n",
      "iteration: 191120 loss: 0.0016 lr: 0.02\n",
      "iteration: 191130 loss: 0.0025 lr: 0.02\n",
      "iteration: 191140 loss: 0.0026 lr: 0.02\n",
      "iteration: 191150 loss: 0.0037 lr: 0.02\n",
      "iteration: 191160 loss: 0.0035 lr: 0.02\n",
      "iteration: 191170 loss: 0.0024 lr: 0.02\n",
      "iteration: 191180 loss: 0.0022 lr: 0.02\n",
      "iteration: 191190 loss: 0.0021 lr: 0.02\n",
      "iteration: 191200 loss: 0.0024 lr: 0.02\n",
      "iteration: 191210 loss: 0.0016 lr: 0.02\n",
      "iteration: 191220 loss: 0.0024 lr: 0.02\n",
      "iteration: 191230 loss: 0.0023 lr: 0.02\n",
      "iteration: 191240 loss: 0.0019 lr: 0.02\n",
      "iteration: 191250 loss: 0.0023 lr: 0.02\n",
      "iteration: 191260 loss: 0.0014 lr: 0.02\n",
      "iteration: 191270 loss: 0.0029 lr: 0.02\n",
      "iteration: 191280 loss: 0.0020 lr: 0.02\n",
      "iteration: 191290 loss: 0.0021 lr: 0.02\n",
      "iteration: 191300 loss: 0.0029 lr: 0.02\n",
      "iteration: 191310 loss: 0.0023 lr: 0.02\n",
      "iteration: 191320 loss: 0.0024 lr: 0.02\n",
      "iteration: 191330 loss: 0.0023 lr: 0.02\n",
      "iteration: 191340 loss: 0.0022 lr: 0.02\n",
      "iteration: 191350 loss: 0.0021 lr: 0.02\n",
      "iteration: 191360 loss: 0.0018 lr: 0.02\n",
      "iteration: 191370 loss: 0.0017 lr: 0.02\n",
      "iteration: 191380 loss: 0.0016 lr: 0.02\n",
      "iteration: 191390 loss: 0.0022 lr: 0.02\n",
      "iteration: 191400 loss: 0.0015 lr: 0.02\n",
      "iteration: 191410 loss: 0.0020 lr: 0.02\n",
      "iteration: 191420 loss: 0.0021 lr: 0.02\n",
      "iteration: 191430 loss: 0.0026 lr: 0.02\n",
      "iteration: 191440 loss: 0.0020 lr: 0.02\n",
      "iteration: 191450 loss: 0.0013 lr: 0.02\n",
      "iteration: 191460 loss: 0.0017 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 191470 loss: 0.0025 lr: 0.02\n",
      "iteration: 191480 loss: 0.0018 lr: 0.02\n",
      "iteration: 191490 loss: 0.0018 lr: 0.02\n",
      "iteration: 191500 loss: 0.0016 lr: 0.02\n",
      "iteration: 191510 loss: 0.0038 lr: 0.02\n",
      "iteration: 191520 loss: 0.0019 lr: 0.02\n",
      "iteration: 191530 loss: 0.0018 lr: 0.02\n",
      "iteration: 191540 loss: 0.0044 lr: 0.02\n",
      "iteration: 191550 loss: 0.0021 lr: 0.02\n",
      "iteration: 191560 loss: 0.0023 lr: 0.02\n",
      "iteration: 191570 loss: 0.0020 lr: 0.02\n",
      "iteration: 191580 loss: 0.0020 lr: 0.02\n",
      "iteration: 191590 loss: 0.0020 lr: 0.02\n",
      "iteration: 191600 loss: 0.0016 lr: 0.02\n",
      "iteration: 191610 loss: 0.0019 lr: 0.02\n",
      "iteration: 191620 loss: 0.0022 lr: 0.02\n",
      "iteration: 191630 loss: 0.0017 lr: 0.02\n",
      "iteration: 191640 loss: 0.0019 lr: 0.02\n",
      "iteration: 191650 loss: 0.0018 lr: 0.02\n",
      "iteration: 191660 loss: 0.0016 lr: 0.02\n",
      "iteration: 191670 loss: 0.0017 lr: 0.02\n",
      "iteration: 191680 loss: 0.0019 lr: 0.02\n",
      "iteration: 191690 loss: 0.0018 lr: 0.02\n",
      "iteration: 191700 loss: 0.0016 lr: 0.02\n",
      "iteration: 191710 loss: 0.0030 lr: 0.02\n",
      "iteration: 191720 loss: 0.0015 lr: 0.02\n",
      "iteration: 191730 loss: 0.0021 lr: 0.02\n",
      "iteration: 191740 loss: 0.0030 lr: 0.02\n",
      "iteration: 191750 loss: 0.0015 lr: 0.02\n",
      "iteration: 191760 loss: 0.0019 lr: 0.02\n",
      "iteration: 191770 loss: 0.0020 lr: 0.02\n",
      "iteration: 191780 loss: 0.0036 lr: 0.02\n",
      "iteration: 191790 loss: 0.0022 lr: 0.02\n",
      "iteration: 191800 loss: 0.0027 lr: 0.02\n",
      "iteration: 191810 loss: 0.0019 lr: 0.02\n",
      "iteration: 191820 loss: 0.0022 lr: 0.02\n",
      "iteration: 191830 loss: 0.0024 lr: 0.02\n",
      "iteration: 191840 loss: 0.0026 lr: 0.02\n",
      "iteration: 191850 loss: 0.0018 lr: 0.02\n",
      "iteration: 191860 loss: 0.0028 lr: 0.02\n",
      "iteration: 191870 loss: 0.0030 lr: 0.02\n",
      "iteration: 191880 loss: 0.0026 lr: 0.02\n",
      "iteration: 191890 loss: 0.0031 lr: 0.02\n",
      "iteration: 191900 loss: 0.0021 lr: 0.02\n",
      "iteration: 191910 loss: 0.0028 lr: 0.02\n",
      "iteration: 191920 loss: 0.0019 lr: 0.02\n",
      "iteration: 191930 loss: 0.0031 lr: 0.02\n",
      "iteration: 191940 loss: 0.0025 lr: 0.02\n",
      "iteration: 191950 loss: 0.0032 lr: 0.02\n",
      "iteration: 191960 loss: 0.0021 lr: 0.02\n",
      "iteration: 191970 loss: 0.0020 lr: 0.02\n",
      "iteration: 191980 loss: 0.0024 lr: 0.02\n",
      "iteration: 191990 loss: 0.0039 lr: 0.02\n",
      "iteration: 192000 loss: 0.0021 lr: 0.02\n",
      "iteration: 192010 loss: 0.0016 lr: 0.02\n",
      "iteration: 192020 loss: 0.0023 lr: 0.02\n",
      "iteration: 192030 loss: 0.0023 lr: 0.02\n",
      "iteration: 192040 loss: 0.0024 lr: 0.02\n",
      "iteration: 192050 loss: 0.0032 lr: 0.02\n",
      "iteration: 192060 loss: 0.0027 lr: 0.02\n",
      "iteration: 192070 loss: 0.0018 lr: 0.02\n",
      "iteration: 192080 loss: 0.0018 lr: 0.02\n",
      "iteration: 192090 loss: 0.0030 lr: 0.02\n",
      "iteration: 192100 loss: 0.0021 lr: 0.02\n",
      "iteration: 192110 loss: 0.0031 lr: 0.02\n",
      "iteration: 192120 loss: 0.0023 lr: 0.02\n",
      "iteration: 192130 loss: 0.0020 lr: 0.02\n",
      "iteration: 192140 loss: 0.0025 lr: 0.02\n",
      "iteration: 192150 loss: 0.0024 lr: 0.02\n",
      "iteration: 192160 loss: 0.0016 lr: 0.02\n",
      "iteration: 192170 loss: 0.0013 lr: 0.02\n",
      "iteration: 192180 loss: 0.0024 lr: 0.02\n",
      "iteration: 192190 loss: 0.0028 lr: 0.02\n",
      "iteration: 192200 loss: 0.0020 lr: 0.02\n",
      "iteration: 192210 loss: 0.0022 lr: 0.02\n",
      "iteration: 192220 loss: 0.0022 lr: 0.02\n",
      "iteration: 192230 loss: 0.0029 lr: 0.02\n",
      "iteration: 192240 loss: 0.0022 lr: 0.02\n",
      "iteration: 192250 loss: 0.0016 lr: 0.02\n",
      "iteration: 192260 loss: 0.0026 lr: 0.02\n",
      "iteration: 192270 loss: 0.0022 lr: 0.02\n",
      "iteration: 192280 loss: 0.0019 lr: 0.02\n",
      "iteration: 192290 loss: 0.0020 lr: 0.02\n",
      "iteration: 192300 loss: 0.0023 lr: 0.02\n",
      "iteration: 192310 loss: 0.0020 lr: 0.02\n",
      "iteration: 192320 loss: 0.0025 lr: 0.02\n",
      "iteration: 192330 loss: 0.0032 lr: 0.02\n",
      "iteration: 192340 loss: 0.0026 lr: 0.02\n",
      "iteration: 192350 loss: 0.0017 lr: 0.02\n",
      "iteration: 192360 loss: 0.0024 lr: 0.02\n",
      "iteration: 192370 loss: 0.0021 lr: 0.02\n",
      "iteration: 192380 loss: 0.0020 lr: 0.02\n",
      "iteration: 192390 loss: 0.0030 lr: 0.02\n",
      "iteration: 192400 loss: 0.0025 lr: 0.02\n",
      "iteration: 192410 loss: 0.0014 lr: 0.02\n",
      "iteration: 192420 loss: 0.0032 lr: 0.02\n",
      "iteration: 192430 loss: 0.0020 lr: 0.02\n",
      "iteration: 192440 loss: 0.0021 lr: 0.02\n",
      "iteration: 192450 loss: 0.0026 lr: 0.02\n",
      "iteration: 192460 loss: 0.0023 lr: 0.02\n",
      "iteration: 192470 loss: 0.0027 lr: 0.02\n",
      "iteration: 192480 loss: 0.0022 lr: 0.02\n",
      "iteration: 192490 loss: 0.0019 lr: 0.02\n",
      "iteration: 192500 loss: 0.0015 lr: 0.02\n",
      "iteration: 192510 loss: 0.0021 lr: 0.02\n",
      "iteration: 192520 loss: 0.0028 lr: 0.02\n",
      "iteration: 192530 loss: 0.0028 lr: 0.02\n",
      "iteration: 192540 loss: 0.0015 lr: 0.02\n",
      "iteration: 192550 loss: 0.0022 lr: 0.02\n",
      "iteration: 192560 loss: 0.0019 lr: 0.02\n",
      "iteration: 192570 loss: 0.0023 lr: 0.02\n",
      "iteration: 192580 loss: 0.0012 lr: 0.02\n",
      "iteration: 192590 loss: 0.0021 lr: 0.02\n",
      "iteration: 192600 loss: 0.0025 lr: 0.02\n",
      "iteration: 192610 loss: 0.0021 lr: 0.02\n",
      "iteration: 192620 loss: 0.0023 lr: 0.02\n",
      "iteration: 192630 loss: 0.0023 lr: 0.02\n",
      "iteration: 192640 loss: 0.0022 lr: 0.02\n",
      "iteration: 192650 loss: 0.0021 lr: 0.02\n",
      "iteration: 192660 loss: 0.0021 lr: 0.02\n",
      "iteration: 192670 loss: 0.0019 lr: 0.02\n",
      "iteration: 192680 loss: 0.0015 lr: 0.02\n",
      "iteration: 192690 loss: 0.0025 lr: 0.02\n",
      "iteration: 192700 loss: 0.0032 lr: 0.02\n",
      "iteration: 192710 loss: 0.0016 lr: 0.02\n",
      "iteration: 192720 loss: 0.0021 lr: 0.02\n",
      "iteration: 192730 loss: 0.0016 lr: 0.02\n",
      "iteration: 192740 loss: 0.0018 lr: 0.02\n",
      "iteration: 192750 loss: 0.0021 lr: 0.02\n",
      "iteration: 192760 loss: 0.0022 lr: 0.02\n",
      "iteration: 192770 loss: 0.0018 lr: 0.02\n",
      "iteration: 192780 loss: 0.0020 lr: 0.02\n",
      "iteration: 192790 loss: 0.0018 lr: 0.02\n",
      "iteration: 192800 loss: 0.0030 lr: 0.02\n",
      "iteration: 192810 loss: 0.0021 lr: 0.02\n",
      "iteration: 192820 loss: 0.0020 lr: 0.02\n",
      "iteration: 192830 loss: 0.0016 lr: 0.02\n",
      "iteration: 192840 loss: 0.0019 lr: 0.02\n",
      "iteration: 192850 loss: 0.0023 lr: 0.02\n",
      "iteration: 192860 loss: 0.0022 lr: 0.02\n",
      "iteration: 192870 loss: 0.0021 lr: 0.02\n",
      "iteration: 192880 loss: 0.0023 lr: 0.02\n",
      "iteration: 192890 loss: 0.0024 lr: 0.02\n",
      "iteration: 192900 loss: 0.0017 lr: 0.02\n",
      "iteration: 192910 loss: 0.0023 lr: 0.02\n",
      "iteration: 192920 loss: 0.0018 lr: 0.02\n",
      "iteration: 192930 loss: 0.0024 lr: 0.02\n",
      "iteration: 192940 loss: 0.0019 lr: 0.02\n",
      "iteration: 192950 loss: 0.0013 lr: 0.02\n",
      "iteration: 192960 loss: 0.0020 lr: 0.02\n",
      "iteration: 192970 loss: 0.0018 lr: 0.02\n",
      "iteration: 192980 loss: 0.0026 lr: 0.02\n",
      "iteration: 192990 loss: 0.0020 lr: 0.02\n",
      "iteration: 193000 loss: 0.0021 lr: 0.02\n",
      "iteration: 193010 loss: 0.0022 lr: 0.02\n",
      "iteration: 193020 loss: 0.0027 lr: 0.02\n",
      "iteration: 193030 loss: 0.0021 lr: 0.02\n",
      "iteration: 193040 loss: 0.0022 lr: 0.02\n",
      "iteration: 193050 loss: 0.0013 lr: 0.02\n",
      "iteration: 193060 loss: 0.0022 lr: 0.02\n",
      "iteration: 193070 loss: 0.0023 lr: 0.02\n",
      "iteration: 193080 loss: 0.0020 lr: 0.02\n",
      "iteration: 193090 loss: 0.0021 lr: 0.02\n",
      "iteration: 193100 loss: 0.0030 lr: 0.02\n",
      "iteration: 193110 loss: 0.0027 lr: 0.02\n",
      "iteration: 193120 loss: 0.0019 lr: 0.02\n",
      "iteration: 193130 loss: 0.0026 lr: 0.02\n",
      "iteration: 193140 loss: 0.0022 lr: 0.02\n",
      "iteration: 193150 loss: 0.0034 lr: 0.02\n",
      "iteration: 193160 loss: 0.0024 lr: 0.02\n",
      "iteration: 193170 loss: 0.0020 lr: 0.02\n",
      "iteration: 193180 loss: 0.0024 lr: 0.02\n",
      "iteration: 193190 loss: 0.0023 lr: 0.02\n",
      "iteration: 193200 loss: 0.0024 lr: 0.02\n",
      "iteration: 193210 loss: 0.0032 lr: 0.02\n",
      "iteration: 193220 loss: 0.0024 lr: 0.02\n",
      "iteration: 193230 loss: 0.0034 lr: 0.02\n",
      "iteration: 193240 loss: 0.0021 lr: 0.02\n",
      "iteration: 193250 loss: 0.0017 lr: 0.02\n",
      "iteration: 193260 loss: 0.0025 lr: 0.02\n",
      "iteration: 193270 loss: 0.0021 lr: 0.02\n",
      "iteration: 193280 loss: 0.0019 lr: 0.02\n",
      "iteration: 193290 loss: 0.0016 lr: 0.02\n",
      "iteration: 193300 loss: 0.0022 lr: 0.02\n",
      "iteration: 193310 loss: 0.0024 lr: 0.02\n",
      "iteration: 193320 loss: 0.0018 lr: 0.02\n",
      "iteration: 193330 loss: 0.0030 lr: 0.02\n",
      "iteration: 193340 loss: 0.0025 lr: 0.02\n",
      "iteration: 193350 loss: 0.0019 lr: 0.02\n",
      "iteration: 193360 loss: 0.0017 lr: 0.02\n",
      "iteration: 193370 loss: 0.0022 lr: 0.02\n",
      "iteration: 193380 loss: 0.0032 lr: 0.02\n",
      "iteration: 193390 loss: 0.0019 lr: 0.02\n",
      "iteration: 193400 loss: 0.0015 lr: 0.02\n",
      "iteration: 193410 loss: 0.0023 lr: 0.02\n",
      "iteration: 193420 loss: 0.0026 lr: 0.02\n",
      "iteration: 193430 loss: 0.0020 lr: 0.02\n",
      "iteration: 193440 loss: 0.0035 lr: 0.02\n",
      "iteration: 193450 loss: 0.0023 lr: 0.02\n",
      "iteration: 193460 loss: 0.0022 lr: 0.02\n",
      "iteration: 193470 loss: 0.0016 lr: 0.02\n",
      "iteration: 193480 loss: 0.0024 lr: 0.02\n",
      "iteration: 193490 loss: 0.0015 lr: 0.02\n",
      "iteration: 193500 loss: 0.0020 lr: 0.02\n",
      "iteration: 193510 loss: 0.0019 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 193520 loss: 0.0014 lr: 0.02\n",
      "iteration: 193530 loss: 0.0030 lr: 0.02\n",
      "iteration: 193540 loss: 0.0017 lr: 0.02\n",
      "iteration: 193550 loss: 0.0028 lr: 0.02\n",
      "iteration: 193560 loss: 0.0019 lr: 0.02\n",
      "iteration: 193570 loss: 0.0018 lr: 0.02\n",
      "iteration: 193580 loss: 0.0015 lr: 0.02\n",
      "iteration: 193590 loss: 0.0033 lr: 0.02\n",
      "iteration: 193600 loss: 0.0020 lr: 0.02\n",
      "iteration: 193610 loss: 0.0019 lr: 0.02\n",
      "iteration: 193620 loss: 0.0023 lr: 0.02\n",
      "iteration: 193630 loss: 0.0019 lr: 0.02\n",
      "iteration: 193640 loss: 0.0019 lr: 0.02\n",
      "iteration: 193650 loss: 0.0032 lr: 0.02\n",
      "iteration: 193660 loss: 0.0017 lr: 0.02\n",
      "iteration: 193670 loss: 0.0019 lr: 0.02\n",
      "iteration: 193680 loss: 0.0018 lr: 0.02\n",
      "iteration: 193690 loss: 0.0015 lr: 0.02\n",
      "iteration: 193700 loss: 0.0021 lr: 0.02\n",
      "iteration: 193710 loss: 0.0020 lr: 0.02\n",
      "iteration: 193720 loss: 0.0023 lr: 0.02\n",
      "iteration: 193730 loss: 0.0026 lr: 0.02\n",
      "iteration: 193740 loss: 0.0018 lr: 0.02\n",
      "iteration: 193750 loss: 0.0019 lr: 0.02\n",
      "iteration: 193760 loss: 0.0018 lr: 0.02\n",
      "iteration: 193770 loss: 0.0021 lr: 0.02\n",
      "iteration: 193780 loss: 0.0020 lr: 0.02\n",
      "iteration: 193790 loss: 0.0023 lr: 0.02\n",
      "iteration: 193800 loss: 0.0025 lr: 0.02\n",
      "iteration: 193810 loss: 0.0022 lr: 0.02\n",
      "iteration: 193820 loss: 0.0026 lr: 0.02\n",
      "iteration: 193830 loss: 0.0020 lr: 0.02\n",
      "iteration: 193840 loss: 0.0025 lr: 0.02\n",
      "iteration: 193850 loss: 0.0028 lr: 0.02\n",
      "iteration: 193860 loss: 0.0022 lr: 0.02\n",
      "iteration: 193870 loss: 0.0023 lr: 0.02\n",
      "iteration: 193880 loss: 0.0019 lr: 0.02\n",
      "iteration: 193890 loss: 0.0019 lr: 0.02\n",
      "iteration: 193900 loss: 0.0024 lr: 0.02\n",
      "iteration: 193910 loss: 0.0023 lr: 0.02\n",
      "iteration: 193920 loss: 0.0020 lr: 0.02\n",
      "iteration: 193930 loss: 0.0019 lr: 0.02\n",
      "iteration: 193940 loss: 0.0018 lr: 0.02\n",
      "iteration: 193950 loss: 0.0025 lr: 0.02\n",
      "iteration: 193960 loss: 0.0021 lr: 0.02\n",
      "iteration: 193970 loss: 0.0022 lr: 0.02\n",
      "iteration: 193980 loss: 0.0030 lr: 0.02\n",
      "iteration: 193990 loss: 0.0016 lr: 0.02\n",
      "iteration: 194000 loss: 0.0018 lr: 0.02\n",
      "iteration: 194010 loss: 0.0019 lr: 0.02\n",
      "iteration: 194020 loss: 0.0024 lr: 0.02\n",
      "iteration: 194030 loss: 0.0022 lr: 0.02\n",
      "iteration: 194040 loss: 0.0023 lr: 0.02\n",
      "iteration: 194050 loss: 0.0026 lr: 0.02\n",
      "iteration: 194060 loss: 0.0019 lr: 0.02\n",
      "iteration: 194070 loss: 0.0020 lr: 0.02\n",
      "iteration: 194080 loss: 0.0015 lr: 0.02\n",
      "iteration: 194090 loss: 0.0027 lr: 0.02\n",
      "iteration: 194100 loss: 0.0019 lr: 0.02\n",
      "iteration: 194110 loss: 0.0029 lr: 0.02\n",
      "iteration: 194120 loss: 0.0016 lr: 0.02\n",
      "iteration: 194130 loss: 0.0024 lr: 0.02\n",
      "iteration: 194140 loss: 0.0019 lr: 0.02\n",
      "iteration: 194150 loss: 0.0016 lr: 0.02\n",
      "iteration: 194160 loss: 0.0022 lr: 0.02\n",
      "iteration: 194170 loss: 0.0023 lr: 0.02\n",
      "iteration: 194180 loss: 0.0013 lr: 0.02\n",
      "iteration: 194190 loss: 0.0023 lr: 0.02\n",
      "iteration: 194200 loss: 0.0027 lr: 0.02\n",
      "iteration: 194210 loss: 0.0041 lr: 0.02\n",
      "iteration: 194220 loss: 0.0019 lr: 0.02\n",
      "iteration: 194230 loss: 0.0021 lr: 0.02\n",
      "iteration: 194240 loss: 0.0033 lr: 0.02\n",
      "iteration: 194250 loss: 0.0020 lr: 0.02\n",
      "iteration: 194260 loss: 0.0042 lr: 0.02\n",
      "iteration: 194270 loss: 0.0023 lr: 0.02\n",
      "iteration: 194280 loss: 0.0022 lr: 0.02\n",
      "iteration: 194290 loss: 0.0029 lr: 0.02\n",
      "iteration: 194300 loss: 0.0020 lr: 0.02\n",
      "iteration: 194310 loss: 0.0024 lr: 0.02\n",
      "iteration: 194320 loss: 0.0032 lr: 0.02\n",
      "iteration: 194330 loss: 0.0017 lr: 0.02\n",
      "iteration: 194340 loss: 0.0026 lr: 0.02\n",
      "iteration: 194350 loss: 0.0028 lr: 0.02\n",
      "iteration: 194360 loss: 0.0021 lr: 0.02\n",
      "iteration: 194370 loss: 0.0025 lr: 0.02\n",
      "iteration: 194380 loss: 0.0021 lr: 0.02\n",
      "iteration: 194390 loss: 0.0027 lr: 0.02\n",
      "iteration: 194400 loss: 0.0023 lr: 0.02\n",
      "iteration: 194410 loss: 0.0033 lr: 0.02\n",
      "iteration: 194420 loss: 0.0023 lr: 0.02\n",
      "iteration: 194430 loss: 0.0020 lr: 0.02\n",
      "iteration: 194440 loss: 0.0020 lr: 0.02\n",
      "iteration: 194450 loss: 0.0021 lr: 0.02\n",
      "iteration: 194460 loss: 0.0026 lr: 0.02\n",
      "iteration: 194470 loss: 0.0018 lr: 0.02\n",
      "iteration: 194480 loss: 0.0019 lr: 0.02\n",
      "iteration: 194490 loss: 0.0019 lr: 0.02\n",
      "iteration: 194500 loss: 0.0018 lr: 0.02\n",
      "iteration: 194510 loss: 0.0020 lr: 0.02\n",
      "iteration: 194520 loss: 0.0043 lr: 0.02\n",
      "iteration: 194530 loss: 0.0019 lr: 0.02\n",
      "iteration: 194540 loss: 0.0022 lr: 0.02\n",
      "iteration: 194550 loss: 0.0020 lr: 0.02\n",
      "iteration: 194560 loss: 0.0020 lr: 0.02\n",
      "iteration: 194570 loss: 0.0040 lr: 0.02\n",
      "iteration: 194580 loss: 0.0021 lr: 0.02\n",
      "iteration: 194590 loss: 0.0017 lr: 0.02\n",
      "iteration: 194600 loss: 0.0033 lr: 0.02\n",
      "iteration: 194610 loss: 0.0021 lr: 0.02\n",
      "iteration: 194620 loss: 0.0024 lr: 0.02\n",
      "iteration: 194630 loss: 0.0033 lr: 0.02\n",
      "iteration: 194640 loss: 0.0021 lr: 0.02\n",
      "iteration: 194650 loss: 0.0027 lr: 0.02\n",
      "iteration: 194660 loss: 0.0016 lr: 0.02\n",
      "iteration: 194670 loss: 0.0015 lr: 0.02\n",
      "iteration: 194680 loss: 0.0022 lr: 0.02\n",
      "iteration: 194690 loss: 0.0020 lr: 0.02\n",
      "iteration: 194700 loss: 0.0022 lr: 0.02\n",
      "iteration: 194710 loss: 0.0017 lr: 0.02\n",
      "iteration: 194720 loss: 0.0018 lr: 0.02\n",
      "iteration: 194730 loss: 0.0028 lr: 0.02\n",
      "iteration: 194740 loss: 0.0029 lr: 0.02\n",
      "iteration: 194750 loss: 0.0021 lr: 0.02\n",
      "iteration: 194760 loss: 0.0024 lr: 0.02\n",
      "iteration: 194770 loss: 0.0025 lr: 0.02\n",
      "iteration: 194780 loss: 0.0025 lr: 0.02\n",
      "iteration: 194790 loss: 0.0025 lr: 0.02\n",
      "iteration: 194800 loss: 0.0020 lr: 0.02\n",
      "iteration: 194810 loss: 0.0018 lr: 0.02\n",
      "iteration: 194820 loss: 0.0023 lr: 0.02\n",
      "iteration: 194830 loss: 0.0022 lr: 0.02\n",
      "iteration: 194840 loss: 0.0015 lr: 0.02\n",
      "iteration: 194850 loss: 0.0017 lr: 0.02\n",
      "iteration: 194860 loss: 0.0015 lr: 0.02\n",
      "iteration: 194870 loss: 0.0018 lr: 0.02\n",
      "iteration: 194880 loss: 0.0034 lr: 0.02\n",
      "iteration: 194890 loss: 0.0017 lr: 0.02\n",
      "iteration: 194900 loss: 0.0024 lr: 0.02\n",
      "iteration: 194910 loss: 0.0019 lr: 0.02\n",
      "iteration: 194920 loss: 0.0016 lr: 0.02\n",
      "iteration: 194930 loss: 0.0020 lr: 0.02\n",
      "iteration: 194940 loss: 0.0025 lr: 0.02\n",
      "iteration: 194950 loss: 0.0026 lr: 0.02\n",
      "iteration: 194960 loss: 0.0019 lr: 0.02\n",
      "iteration: 194970 loss: 0.0019 lr: 0.02\n",
      "iteration: 194980 loss: 0.0016 lr: 0.02\n",
      "iteration: 194990 loss: 0.0016 lr: 0.02\n",
      "iteration: 195000 loss: 0.0022 lr: 0.02\n",
      "iteration: 195010 loss: 0.0017 lr: 0.02\n",
      "iteration: 195020 loss: 0.0015 lr: 0.02\n",
      "iteration: 195030 loss: 0.0027 lr: 0.02\n",
      "iteration: 195040 loss: 0.0020 lr: 0.02\n",
      "iteration: 195050 loss: 0.0019 lr: 0.02\n",
      "iteration: 195060 loss: 0.0023 lr: 0.02\n",
      "iteration: 195070 loss: 0.0015 lr: 0.02\n",
      "iteration: 195080 loss: 0.0017 lr: 0.02\n",
      "iteration: 195090 loss: 0.0021 lr: 0.02\n",
      "iteration: 195100 loss: 0.0022 lr: 0.02\n",
      "iteration: 195110 loss: 0.0018 lr: 0.02\n",
      "iteration: 195120 loss: 0.0018 lr: 0.02\n",
      "iteration: 195130 loss: 0.0020 lr: 0.02\n",
      "iteration: 195140 loss: 0.0019 lr: 0.02\n",
      "iteration: 195150 loss: 0.0028 lr: 0.02\n",
      "iteration: 195160 loss: 0.0018 lr: 0.02\n",
      "iteration: 195170 loss: 0.0025 lr: 0.02\n",
      "iteration: 195180 loss: 0.0016 lr: 0.02\n",
      "iteration: 195190 loss: 0.0028 lr: 0.02\n",
      "iteration: 195200 loss: 0.0025 lr: 0.02\n",
      "iteration: 195210 loss: 0.0022 lr: 0.02\n",
      "iteration: 195220 loss: 0.0018 lr: 0.02\n",
      "iteration: 195230 loss: 0.0016 lr: 0.02\n",
      "iteration: 195240 loss: 0.0025 lr: 0.02\n",
      "iteration: 195250 loss: 0.0026 lr: 0.02\n",
      "iteration: 195260 loss: 0.0018 lr: 0.02\n",
      "iteration: 195270 loss: 0.0019 lr: 0.02\n",
      "iteration: 195280 loss: 0.0018 lr: 0.02\n",
      "iteration: 195290 loss: 0.0020 lr: 0.02\n",
      "iteration: 195300 loss: 0.0026 lr: 0.02\n",
      "iteration: 195310 loss: 0.0021 lr: 0.02\n",
      "iteration: 195320 loss: 0.0020 lr: 0.02\n",
      "iteration: 195330 loss: 0.0019 lr: 0.02\n",
      "iteration: 195340 loss: 0.0018 lr: 0.02\n",
      "iteration: 195350 loss: 0.0019 lr: 0.02\n",
      "iteration: 195360 loss: 0.0026 lr: 0.02\n",
      "iteration: 195370 loss: 0.0019 lr: 0.02\n",
      "iteration: 195380 loss: 0.0019 lr: 0.02\n",
      "iteration: 195390 loss: 0.0019 lr: 0.02\n",
      "iteration: 195400 loss: 0.0021 lr: 0.02\n",
      "iteration: 195410 loss: 0.0022 lr: 0.02\n",
      "iteration: 195420 loss: 0.0016 lr: 0.02\n",
      "iteration: 195430 loss: 0.0015 lr: 0.02\n",
      "iteration: 195440 loss: 0.0023 lr: 0.02\n",
      "iteration: 195450 loss: 0.0019 lr: 0.02\n",
      "iteration: 195460 loss: 0.0020 lr: 0.02\n",
      "iteration: 195470 loss: 0.0019 lr: 0.02\n",
      "iteration: 195480 loss: 0.0017 lr: 0.02\n",
      "iteration: 195490 loss: 0.0017 lr: 0.02\n",
      "iteration: 195500 loss: 0.0023 lr: 0.02\n",
      "iteration: 195510 loss: 0.0033 lr: 0.02\n",
      "iteration: 195520 loss: 0.0019 lr: 0.02\n",
      "iteration: 195530 loss: 0.0021 lr: 0.02\n",
      "iteration: 195540 loss: 0.0022 lr: 0.02\n",
      "iteration: 195550 loss: 0.0018 lr: 0.02\n",
      "iteration: 195560 loss: 0.0017 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 195570 loss: 0.0032 lr: 0.02\n",
      "iteration: 195580 loss: 0.0020 lr: 0.02\n",
      "iteration: 195590 loss: 0.0019 lr: 0.02\n",
      "iteration: 195600 loss: 0.0017 lr: 0.02\n",
      "iteration: 195610 loss: 0.0020 lr: 0.02\n",
      "iteration: 195620 loss: 0.0020 lr: 0.02\n",
      "iteration: 195630 loss: 0.0020 lr: 0.02\n",
      "iteration: 195640 loss: 0.0018 lr: 0.02\n",
      "iteration: 195650 loss: 0.0022 lr: 0.02\n",
      "iteration: 195660 loss: 0.0029 lr: 0.02\n",
      "iteration: 195670 loss: 0.0021 lr: 0.02\n",
      "iteration: 195680 loss: 0.0022 lr: 0.02\n",
      "iteration: 195690 loss: 0.0023 lr: 0.02\n",
      "iteration: 195700 loss: 0.0017 lr: 0.02\n",
      "iteration: 195710 loss: 0.0028 lr: 0.02\n",
      "iteration: 195720 loss: 0.0022 lr: 0.02\n",
      "iteration: 195730 loss: 0.0035 lr: 0.02\n",
      "iteration: 195740 loss: 0.0032 lr: 0.02\n",
      "iteration: 195750 loss: 0.0027 lr: 0.02\n",
      "iteration: 195760 loss: 0.0020 lr: 0.02\n",
      "iteration: 195770 loss: 0.0030 lr: 0.02\n",
      "iteration: 195780 loss: 0.0018 lr: 0.02\n",
      "iteration: 195790 loss: 0.0032 lr: 0.02\n",
      "iteration: 195800 loss: 0.0017 lr: 0.02\n",
      "iteration: 195810 loss: 0.0037 lr: 0.02\n",
      "iteration: 195820 loss: 0.0025 lr: 0.02\n",
      "iteration: 195830 loss: 0.0028 lr: 0.02\n",
      "iteration: 195840 loss: 0.0022 lr: 0.02\n",
      "iteration: 195850 loss: 0.0020 lr: 0.02\n",
      "iteration: 195860 loss: 0.0018 lr: 0.02\n",
      "iteration: 195870 loss: 0.0027 lr: 0.02\n",
      "iteration: 195880 loss: 0.0015 lr: 0.02\n",
      "iteration: 195890 loss: 0.0021 lr: 0.02\n",
      "iteration: 195900 loss: 0.0025 lr: 0.02\n",
      "iteration: 195910 loss: 0.0025 lr: 0.02\n",
      "iteration: 195920 loss: 0.0021 lr: 0.02\n",
      "iteration: 195930 loss: 0.0025 lr: 0.02\n",
      "iteration: 195940 loss: 0.0022 lr: 0.02\n",
      "iteration: 195950 loss: 0.0018 lr: 0.02\n",
      "iteration: 195960 loss: 0.0021 lr: 0.02\n",
      "iteration: 195970 loss: 0.0025 lr: 0.02\n",
      "iteration: 195980 loss: 0.0017 lr: 0.02\n",
      "iteration: 195990 loss: 0.0020 lr: 0.02\n",
      "iteration: 196000 loss: 0.0023 lr: 0.02\n",
      "iteration: 196010 loss: 0.0019 lr: 0.02\n",
      "iteration: 196020 loss: 0.0021 lr: 0.02\n",
      "iteration: 196030 loss: 0.0025 lr: 0.02\n",
      "iteration: 196040 loss: 0.0020 lr: 0.02\n",
      "iteration: 196050 loss: 0.0014 lr: 0.02\n",
      "iteration: 196060 loss: 0.0019 lr: 0.02\n",
      "iteration: 196070 loss: 0.0021 lr: 0.02\n",
      "iteration: 196080 loss: 0.0021 lr: 0.02\n",
      "iteration: 196090 loss: 0.0016 lr: 0.02\n",
      "iteration: 196100 loss: 0.0019 lr: 0.02\n",
      "iteration: 196110 loss: 0.0027 lr: 0.02\n",
      "iteration: 196120 loss: 0.0026 lr: 0.02\n",
      "iteration: 196130 loss: 0.0019 lr: 0.02\n",
      "iteration: 196140 loss: 0.0023 lr: 0.02\n",
      "iteration: 196150 loss: 0.0028 lr: 0.02\n",
      "iteration: 196160 loss: 0.0024 lr: 0.02\n",
      "iteration: 196170 loss: 0.0022 lr: 0.02\n",
      "iteration: 196180 loss: 0.0020 lr: 0.02\n",
      "iteration: 196190 loss: 0.0026 lr: 0.02\n",
      "iteration: 196200 loss: 0.0027 lr: 0.02\n",
      "iteration: 196210 loss: 0.0017 lr: 0.02\n",
      "iteration: 196220 loss: 0.0019 lr: 0.02\n",
      "iteration: 196230 loss: 0.0024 lr: 0.02\n",
      "iteration: 196240 loss: 0.0023 lr: 0.02\n",
      "iteration: 196250 loss: 0.0017 lr: 0.02\n",
      "iteration: 196260 loss: 0.0018 lr: 0.02\n",
      "iteration: 196270 loss: 0.0021 lr: 0.02\n",
      "iteration: 196280 loss: 0.0029 lr: 0.02\n",
      "iteration: 196290 loss: 0.0019 lr: 0.02\n",
      "iteration: 196300 loss: 0.0016 lr: 0.02\n",
      "iteration: 196310 loss: 0.0028 lr: 0.02\n",
      "iteration: 196320 loss: 0.0023 lr: 0.02\n",
      "iteration: 196330 loss: 0.0031 lr: 0.02\n",
      "iteration: 196340 loss: 0.0022 lr: 0.02\n",
      "iteration: 196350 loss: 0.0028 lr: 0.02\n",
      "iteration: 196360 loss: 0.0019 lr: 0.02\n",
      "iteration: 196370 loss: 0.0015 lr: 0.02\n",
      "iteration: 196380 loss: 0.0014 lr: 0.02\n",
      "iteration: 196390 loss: 0.0020 lr: 0.02\n",
      "iteration: 196400 loss: 0.0017 lr: 0.02\n",
      "iteration: 196410 loss: 0.0019 lr: 0.02\n",
      "iteration: 196420 loss: 0.0021 lr: 0.02\n",
      "iteration: 196430 loss: 0.0018 lr: 0.02\n",
      "iteration: 196440 loss: 0.0025 lr: 0.02\n",
      "iteration: 196450 loss: 0.0016 lr: 0.02\n",
      "iteration: 196460 loss: 0.0026 lr: 0.02\n",
      "iteration: 196470 loss: 0.0019 lr: 0.02\n",
      "iteration: 196480 loss: 0.0017 lr: 0.02\n",
      "iteration: 196490 loss: 0.0018 lr: 0.02\n",
      "iteration: 196500 loss: 0.0025 lr: 0.02\n",
      "iteration: 196510 loss: 0.0024 lr: 0.02\n",
      "iteration: 196520 loss: 0.0015 lr: 0.02\n",
      "iteration: 196530 loss: 0.0021 lr: 0.02\n",
      "iteration: 196540 loss: 0.0015 lr: 0.02\n",
      "iteration: 196550 loss: 0.0019 lr: 0.02\n",
      "iteration: 196560 loss: 0.0016 lr: 0.02\n",
      "iteration: 196570 loss: 0.0016 lr: 0.02\n",
      "iteration: 196580 loss: 0.0018 lr: 0.02\n",
      "iteration: 196590 loss: 0.0023 lr: 0.02\n",
      "iteration: 196600 loss: 0.0016 lr: 0.02\n",
      "iteration: 196610 loss: 0.0022 lr: 0.02\n",
      "iteration: 196620 loss: 0.0023 lr: 0.02\n",
      "iteration: 196630 loss: 0.0018 lr: 0.02\n",
      "iteration: 196640 loss: 0.0022 lr: 0.02\n",
      "iteration: 196650 loss: 0.0018 lr: 0.02\n",
      "iteration: 196660 loss: 0.0017 lr: 0.02\n",
      "iteration: 196670 loss: 0.0022 lr: 0.02\n",
      "iteration: 196680 loss: 0.0026 lr: 0.02\n",
      "iteration: 196690 loss: 0.0024 lr: 0.02\n",
      "iteration: 196700 loss: 0.0032 lr: 0.02\n",
      "iteration: 196710 loss: 0.0025 lr: 0.02\n",
      "iteration: 196720 loss: 0.0019 lr: 0.02\n",
      "iteration: 196730 loss: 0.0020 lr: 0.02\n",
      "iteration: 196740 loss: 0.0026 lr: 0.02\n",
      "iteration: 196750 loss: 0.0017 lr: 0.02\n",
      "iteration: 196760 loss: 0.0016 lr: 0.02\n",
      "iteration: 196770 loss: 0.0014 lr: 0.02\n",
      "iteration: 196780 loss: 0.0018 lr: 0.02\n",
      "iteration: 196790 loss: 0.0021 lr: 0.02\n",
      "iteration: 196800 loss: 0.0023 lr: 0.02\n",
      "iteration: 196810 loss: 0.0022 lr: 0.02\n",
      "iteration: 196820 loss: 0.0025 lr: 0.02\n",
      "iteration: 196830 loss: 0.0024 lr: 0.02\n",
      "iteration: 196840 loss: 0.0023 lr: 0.02\n",
      "iteration: 196850 loss: 0.0026 lr: 0.02\n",
      "iteration: 196860 loss: 0.0019 lr: 0.02\n",
      "iteration: 196870 loss: 0.0028 lr: 0.02\n",
      "iteration: 196880 loss: 0.0020 lr: 0.02\n",
      "iteration: 196890 loss: 0.0026 lr: 0.02\n",
      "iteration: 196900 loss: 0.0025 lr: 0.02\n",
      "iteration: 196910 loss: 0.0020 lr: 0.02\n",
      "iteration: 196920 loss: 0.0025 lr: 0.02\n",
      "iteration: 196930 loss: 0.0019 lr: 0.02\n",
      "iteration: 196940 loss: 0.0015 lr: 0.02\n",
      "iteration: 196950 loss: 0.0026 lr: 0.02\n",
      "iteration: 196960 loss: 0.0014 lr: 0.02\n",
      "iteration: 196970 loss: 0.0027 lr: 0.02\n",
      "iteration: 196980 loss: 0.0020 lr: 0.02\n",
      "iteration: 196990 loss: 0.0024 lr: 0.02\n",
      "iteration: 197000 loss: 0.0025 lr: 0.02\n",
      "iteration: 197010 loss: 0.0029 lr: 0.02\n",
      "iteration: 197020 loss: 0.0029 lr: 0.02\n",
      "iteration: 197030 loss: 0.0020 lr: 0.02\n",
      "iteration: 197040 loss: 0.0021 lr: 0.02\n",
      "iteration: 197050 loss: 0.0018 lr: 0.02\n",
      "iteration: 197060 loss: 0.0020 lr: 0.02\n",
      "iteration: 197070 loss: 0.0023 lr: 0.02\n",
      "iteration: 197080 loss: 0.0019 lr: 0.02\n",
      "iteration: 197090 loss: 0.0016 lr: 0.02\n",
      "iteration: 197100 loss: 0.0025 lr: 0.02\n",
      "iteration: 197110 loss: 0.0031 lr: 0.02\n",
      "iteration: 197120 loss: 0.0019 lr: 0.02\n",
      "iteration: 197130 loss: 0.0024 lr: 0.02\n",
      "iteration: 197140 loss: 0.0031 lr: 0.02\n",
      "iteration: 197150 loss: 0.0017 lr: 0.02\n",
      "iteration: 197160 loss: 0.0017 lr: 0.02\n",
      "iteration: 197170 loss: 0.0019 lr: 0.02\n",
      "iteration: 197180 loss: 0.0024 lr: 0.02\n",
      "iteration: 197190 loss: 0.0024 lr: 0.02\n",
      "iteration: 197200 loss: 0.0022 lr: 0.02\n",
      "iteration: 197210 loss: 0.0022 lr: 0.02\n",
      "iteration: 197220 loss: 0.0019 lr: 0.02\n",
      "iteration: 197230 loss: 0.0019 lr: 0.02\n",
      "iteration: 197240 loss: 0.0028 lr: 0.02\n",
      "iteration: 197250 loss: 0.0018 lr: 0.02\n",
      "iteration: 197260 loss: 0.0018 lr: 0.02\n",
      "iteration: 197270 loss: 0.0023 lr: 0.02\n",
      "iteration: 197280 loss: 0.0023 lr: 0.02\n",
      "iteration: 197290 loss: 0.0028 lr: 0.02\n",
      "iteration: 197300 loss: 0.0020 lr: 0.02\n",
      "iteration: 197310 loss: 0.0016 lr: 0.02\n",
      "iteration: 197320 loss: 0.0013 lr: 0.02\n",
      "iteration: 197330 loss: 0.0015 lr: 0.02\n",
      "iteration: 197340 loss: 0.0027 lr: 0.02\n",
      "iteration: 197350 loss: 0.0019 lr: 0.02\n",
      "iteration: 197360 loss: 0.0015 lr: 0.02\n",
      "iteration: 197370 loss: 0.0025 lr: 0.02\n",
      "iteration: 197380 loss: 0.0022 lr: 0.02\n",
      "iteration: 197390 loss: 0.0028 lr: 0.02\n",
      "iteration: 197400 loss: 0.0019 lr: 0.02\n",
      "iteration: 197410 loss: 0.0023 lr: 0.02\n",
      "iteration: 197420 loss: 0.0018 lr: 0.02\n",
      "iteration: 197430 loss: 0.0026 lr: 0.02\n",
      "iteration: 197440 loss: 0.0028 lr: 0.02\n",
      "iteration: 197450 loss: 0.0022 lr: 0.02\n",
      "iteration: 197460 loss: 0.0020 lr: 0.02\n",
      "iteration: 197470 loss: 0.0018 lr: 0.02\n",
      "iteration: 197480 loss: 0.0019 lr: 0.02\n",
      "iteration: 197490 loss: 0.0014 lr: 0.02\n",
      "iteration: 197500 loss: 0.0014 lr: 0.02\n",
      "iteration: 197510 loss: 0.0020 lr: 0.02\n",
      "iteration: 197520 loss: 0.0030 lr: 0.02\n",
      "iteration: 197530 loss: 0.0028 lr: 0.02\n",
      "iteration: 197540 loss: 0.0041 lr: 0.02\n",
      "iteration: 197550 loss: 0.0037 lr: 0.02\n",
      "iteration: 197560 loss: 0.0019 lr: 0.02\n",
      "iteration: 197570 loss: 0.0025 lr: 0.02\n",
      "iteration: 197580 loss: 0.0040 lr: 0.02\n",
      "iteration: 197590 loss: 0.0022 lr: 0.02\n",
      "iteration: 197600 loss: 0.0023 lr: 0.02\n",
      "iteration: 197610 loss: 0.0029 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 197620 loss: 0.0032 lr: 0.02\n",
      "iteration: 197630 loss: 0.0022 lr: 0.02\n",
      "iteration: 197640 loss: 0.0022 lr: 0.02\n",
      "iteration: 197650 loss: 0.0015 lr: 0.02\n",
      "iteration: 197660 loss: 0.0018 lr: 0.02\n",
      "iteration: 197670 loss: 0.0023 lr: 0.02\n",
      "iteration: 197680 loss: 0.0024 lr: 0.02\n",
      "iteration: 197690 loss: 0.0026 lr: 0.02\n",
      "iteration: 197700 loss: 0.0022 lr: 0.02\n",
      "iteration: 197710 loss: 0.0019 lr: 0.02\n",
      "iteration: 197720 loss: 0.0020 lr: 0.02\n",
      "iteration: 197730 loss: 0.0025 lr: 0.02\n",
      "iteration: 197740 loss: 0.0018 lr: 0.02\n",
      "iteration: 197750 loss: 0.0019 lr: 0.02\n",
      "iteration: 197760 loss: 0.0036 lr: 0.02\n",
      "iteration: 197770 loss: 0.0017 lr: 0.02\n",
      "iteration: 197780 loss: 0.0043 lr: 0.02\n",
      "iteration: 197790 loss: 0.0015 lr: 0.02\n",
      "iteration: 197800 loss: 0.0023 lr: 0.02\n",
      "iteration: 197810 loss: 0.0042 lr: 0.02\n",
      "iteration: 197820 loss: 0.0024 lr: 0.02\n",
      "iteration: 197830 loss: 0.0016 lr: 0.02\n",
      "iteration: 197840 loss: 0.0023 lr: 0.02\n",
      "iteration: 197850 loss: 0.0028 lr: 0.02\n",
      "iteration: 197860 loss: 0.0021 lr: 0.02\n",
      "iteration: 197870 loss: 0.0019 lr: 0.02\n",
      "iteration: 197880 loss: 0.0024 lr: 0.02\n",
      "iteration: 197890 loss: 0.0020 lr: 0.02\n",
      "iteration: 197900 loss: 0.0021 lr: 0.02\n",
      "iteration: 197910 loss: 0.0029 lr: 0.02\n",
      "iteration: 197920 loss: 0.0021 lr: 0.02\n",
      "iteration: 197930 loss: 0.0023 lr: 0.02\n",
      "iteration: 197940 loss: 0.0016 lr: 0.02\n",
      "iteration: 197950 loss: 0.0018 lr: 0.02\n",
      "iteration: 197960 loss: 0.0024 lr: 0.02\n",
      "iteration: 197970 loss: 0.0022 lr: 0.02\n",
      "iteration: 197980 loss: 0.0018 lr: 0.02\n",
      "iteration: 197990 loss: 0.0022 lr: 0.02\n",
      "iteration: 198000 loss: 0.0027 lr: 0.02\n",
      "iteration: 198010 loss: 0.0027 lr: 0.02\n",
      "iteration: 198020 loss: 0.0023 lr: 0.02\n",
      "iteration: 198030 loss: 0.0020 lr: 0.02\n",
      "iteration: 198040 loss: 0.0018 lr: 0.02\n",
      "iteration: 198050 loss: 0.0026 lr: 0.02\n",
      "iteration: 198060 loss: 0.0025 lr: 0.02\n",
      "iteration: 198070 loss: 0.0020 lr: 0.02\n",
      "iteration: 198080 loss: 0.0023 lr: 0.02\n",
      "iteration: 198090 loss: 0.0021 lr: 0.02\n",
      "iteration: 198100 loss: 0.0018 lr: 0.02\n",
      "iteration: 198110 loss: 0.0015 lr: 0.02\n",
      "iteration: 198120 loss: 0.0024 lr: 0.02\n",
      "iteration: 198130 loss: 0.0024 lr: 0.02\n",
      "iteration: 198140 loss: 0.0028 lr: 0.02\n",
      "iteration: 198150 loss: 0.0026 lr: 0.02\n",
      "iteration: 198160 loss: 0.0019 lr: 0.02\n",
      "iteration: 198170 loss: 0.0017 lr: 0.02\n",
      "iteration: 198180 loss: 0.0022 lr: 0.02\n",
      "iteration: 198190 loss: 0.0028 lr: 0.02\n",
      "iteration: 198200 loss: 0.0023 lr: 0.02\n",
      "iteration: 198210 loss: 0.0023 lr: 0.02\n",
      "iteration: 198220 loss: 0.0022 lr: 0.02\n",
      "iteration: 198230 loss: 0.0019 lr: 0.02\n",
      "iteration: 198240 loss: 0.0024 lr: 0.02\n",
      "iteration: 198250 loss: 0.0027 lr: 0.02\n",
      "iteration: 198260 loss: 0.0018 lr: 0.02\n",
      "iteration: 198270 loss: 0.0018 lr: 0.02\n",
      "iteration: 198280 loss: 0.0022 lr: 0.02\n",
      "iteration: 198290 loss: 0.0017 lr: 0.02\n",
      "iteration: 198300 loss: 0.0015 lr: 0.02\n",
      "iteration: 198310 loss: 0.0021 lr: 0.02\n",
      "iteration: 198320 loss: 0.0026 lr: 0.02\n",
      "iteration: 198330 loss: 0.0024 lr: 0.02\n",
      "iteration: 198340 loss: 0.0016 lr: 0.02\n",
      "iteration: 198350 loss: 0.0016 lr: 0.02\n",
      "iteration: 198360 loss: 0.0026 lr: 0.02\n",
      "iteration: 198370 loss: 0.0019 lr: 0.02\n",
      "iteration: 198380 loss: 0.0016 lr: 0.02\n",
      "iteration: 198390 loss: 0.0013 lr: 0.02\n",
      "iteration: 198400 loss: 0.0019 lr: 0.02\n",
      "iteration: 198410 loss: 0.0024 lr: 0.02\n",
      "iteration: 198420 loss: 0.0024 lr: 0.02\n",
      "iteration: 198430 loss: 0.0024 lr: 0.02\n",
      "iteration: 198440 loss: 0.0022 lr: 0.02\n",
      "iteration: 198450 loss: 0.0019 lr: 0.02\n",
      "iteration: 198460 loss: 0.0014 lr: 0.02\n",
      "iteration: 198470 loss: 0.0019 lr: 0.02\n",
      "iteration: 198480 loss: 0.0017 lr: 0.02\n",
      "iteration: 198490 loss: 0.0027 lr: 0.02\n",
      "iteration: 198500 loss: 0.0015 lr: 0.02\n",
      "iteration: 198510 loss: 0.0020 lr: 0.02\n",
      "iteration: 198520 loss: 0.0023 lr: 0.02\n",
      "iteration: 198530 loss: 0.0019 lr: 0.02\n",
      "iteration: 198540 loss: 0.0035 lr: 0.02\n",
      "iteration: 198550 loss: 0.0025 lr: 0.02\n",
      "iteration: 198560 loss: 0.0017 lr: 0.02\n",
      "iteration: 198570 loss: 0.0020 lr: 0.02\n",
      "iteration: 198580 loss: 0.0020 lr: 0.02\n",
      "iteration: 198590 loss: 0.0023 lr: 0.02\n",
      "iteration: 198600 loss: 0.0024 lr: 0.02\n",
      "iteration: 198610 loss: 0.0023 lr: 0.02\n",
      "iteration: 198620 loss: 0.0035 lr: 0.02\n",
      "iteration: 198630 loss: 0.0019 lr: 0.02\n",
      "iteration: 198640 loss: 0.0034 lr: 0.02\n",
      "iteration: 198650 loss: 0.0018 lr: 0.02\n",
      "iteration: 198660 loss: 0.0019 lr: 0.02\n",
      "iteration: 198670 loss: 0.0017 lr: 0.02\n",
      "iteration: 198680 loss: 0.0021 lr: 0.02\n",
      "iteration: 198690 loss: 0.0029 lr: 0.02\n",
      "iteration: 198700 loss: 0.0027 lr: 0.02\n",
      "iteration: 198710 loss: 0.0022 lr: 0.02\n",
      "iteration: 198720 loss: 0.0016 lr: 0.02\n",
      "iteration: 198730 loss: 0.0017 lr: 0.02\n",
      "iteration: 198740 loss: 0.0020 lr: 0.02\n",
      "iteration: 198750 loss: 0.0018 lr: 0.02\n",
      "iteration: 198760 loss: 0.0019 lr: 0.02\n",
      "iteration: 198770 loss: 0.0016 lr: 0.02\n",
      "iteration: 198780 loss: 0.0019 lr: 0.02\n",
      "iteration: 198790 loss: 0.0022 lr: 0.02\n",
      "iteration: 198800 loss: 0.0019 lr: 0.02\n",
      "iteration: 198810 loss: 0.0021 lr: 0.02\n",
      "iteration: 198820 loss: 0.0017 lr: 0.02\n",
      "iteration: 198830 loss: 0.0018 lr: 0.02\n",
      "iteration: 198840 loss: 0.0014 lr: 0.02\n",
      "iteration: 198850 loss: 0.0015 lr: 0.02\n",
      "iteration: 198860 loss: 0.0023 lr: 0.02\n",
      "iteration: 198870 loss: 0.0013 lr: 0.02\n",
      "iteration: 198880 loss: 0.0023 lr: 0.02\n",
      "iteration: 198890 loss: 0.0019 lr: 0.02\n",
      "iteration: 198900 loss: 0.0028 lr: 0.02\n",
      "iteration: 198910 loss: 0.0020 lr: 0.02\n",
      "iteration: 198920 loss: 0.0023 lr: 0.02\n",
      "iteration: 198930 loss: 0.0017 lr: 0.02\n",
      "iteration: 198940 loss: 0.0017 lr: 0.02\n",
      "iteration: 198950 loss: 0.0029 lr: 0.02\n",
      "iteration: 198960 loss: 0.0019 lr: 0.02\n",
      "iteration: 198970 loss: 0.0026 lr: 0.02\n",
      "iteration: 198980 loss: 0.0020 lr: 0.02\n",
      "iteration: 198990 loss: 0.0023 lr: 0.02\n",
      "iteration: 199000 loss: 0.0028 lr: 0.02\n",
      "iteration: 199010 loss: 0.0021 lr: 0.02\n",
      "iteration: 199020 loss: 0.0016 lr: 0.02\n",
      "iteration: 199030 loss: 0.0024 lr: 0.02\n",
      "iteration: 199040 loss: 0.0021 lr: 0.02\n",
      "iteration: 199050 loss: 0.0035 lr: 0.02\n",
      "iteration: 199060 loss: 0.0021 lr: 0.02\n",
      "iteration: 199070 loss: 0.0020 lr: 0.02\n",
      "iteration: 199080 loss: 0.0018 lr: 0.02\n",
      "iteration: 199090 loss: 0.0019 lr: 0.02\n",
      "iteration: 199100 loss: 0.0027 lr: 0.02\n",
      "iteration: 199110 loss: 0.0030 lr: 0.02\n",
      "iteration: 199120 loss: 0.0019 lr: 0.02\n",
      "iteration: 199130 loss: 0.0017 lr: 0.02\n",
      "iteration: 199140 loss: 0.0019 lr: 0.02\n",
      "iteration: 199150 loss: 0.0025 lr: 0.02\n",
      "iteration: 199160 loss: 0.0025 lr: 0.02\n",
      "iteration: 199170 loss: 0.0017 lr: 0.02\n",
      "iteration: 199180 loss: 0.0031 lr: 0.02\n",
      "iteration: 199190 loss: 0.0022 lr: 0.02\n",
      "iteration: 199200 loss: 0.0018 lr: 0.02\n",
      "iteration: 199210 loss: 0.0018 lr: 0.02\n",
      "iteration: 199220 loss: 0.0026 lr: 0.02\n",
      "iteration: 199230 loss: 0.0034 lr: 0.02\n",
      "iteration: 199240 loss: 0.0018 lr: 0.02\n",
      "iteration: 199250 loss: 0.0026 lr: 0.02\n",
      "iteration: 199260 loss: 0.0022 lr: 0.02\n",
      "iteration: 199270 loss: 0.0023 lr: 0.02\n",
      "iteration: 199280 loss: 0.0024 lr: 0.02\n",
      "iteration: 199290 loss: 0.0019 lr: 0.02\n",
      "iteration: 199300 loss: 0.0037 lr: 0.02\n",
      "iteration: 199310 loss: 0.0019 lr: 0.02\n",
      "iteration: 199320 loss: 0.0021 lr: 0.02\n",
      "iteration: 199330 loss: 0.0015 lr: 0.02\n",
      "iteration: 199340 loss: 0.0018 lr: 0.02\n",
      "iteration: 199350 loss: 0.0025 lr: 0.02\n",
      "iteration: 199360 loss: 0.0020 lr: 0.02\n",
      "iteration: 199370 loss: 0.0030 lr: 0.02\n",
      "iteration: 199380 loss: 0.0019 lr: 0.02\n",
      "iteration: 199390 loss: 0.0018 lr: 0.02\n",
      "iteration: 199400 loss: 0.0037 lr: 0.02\n",
      "iteration: 199410 loss: 0.0020 lr: 0.02\n",
      "iteration: 199420 loss: 0.0019 lr: 0.02\n",
      "iteration: 199430 loss: 0.0021 lr: 0.02\n",
      "iteration: 199440 loss: 0.0026 lr: 0.02\n",
      "iteration: 199450 loss: 0.0022 lr: 0.02\n",
      "iteration: 199460 loss: 0.0026 lr: 0.02\n",
      "iteration: 199470 loss: 0.0025 lr: 0.02\n",
      "iteration: 199480 loss: 0.0017 lr: 0.02\n",
      "iteration: 199490 loss: 0.0017 lr: 0.02\n",
      "iteration: 199500 loss: 0.0019 lr: 0.02\n",
      "iteration: 199510 loss: 0.0017 lr: 0.02\n",
      "iteration: 199520 loss: 0.0016 lr: 0.02\n",
      "iteration: 199530 loss: 0.0022 lr: 0.02\n",
      "iteration: 199540 loss: 0.0020 lr: 0.02\n",
      "iteration: 199550 loss: 0.0018 lr: 0.02\n",
      "iteration: 199560 loss: 0.0025 lr: 0.02\n",
      "iteration: 199570 loss: 0.0029 lr: 0.02\n",
      "iteration: 199580 loss: 0.0018 lr: 0.02\n",
      "iteration: 199590 loss: 0.0019 lr: 0.02\n",
      "iteration: 199600 loss: 0.0025 lr: 0.02\n",
      "iteration: 199610 loss: 0.0023 lr: 0.02\n",
      "iteration: 199620 loss: 0.0018 lr: 0.02\n",
      "iteration: 199630 loss: 0.0019 lr: 0.02\n",
      "iteration: 199640 loss: 0.0035 lr: 0.02\n",
      "iteration: 199650 loss: 0.0021 lr: 0.02\n",
      "iteration: 199660 loss: 0.0019 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 199670 loss: 0.0020 lr: 0.02\n",
      "iteration: 199680 loss: 0.0016 lr: 0.02\n",
      "iteration: 199690 loss: 0.0022 lr: 0.02\n",
      "iteration: 199700 loss: 0.0015 lr: 0.02\n",
      "iteration: 199710 loss: 0.0024 lr: 0.02\n",
      "iteration: 199720 loss: 0.0019 lr: 0.02\n",
      "iteration: 199730 loss: 0.0026 lr: 0.02\n",
      "iteration: 199740 loss: 0.0017 lr: 0.02\n",
      "iteration: 199750 loss: 0.0021 lr: 0.02\n",
      "iteration: 199760 loss: 0.0024 lr: 0.02\n",
      "iteration: 199770 loss: 0.0018 lr: 0.02\n",
      "iteration: 199780 loss: 0.0014 lr: 0.02\n",
      "iteration: 199790 loss: 0.0018 lr: 0.02\n",
      "iteration: 199800 loss: 0.0015 lr: 0.02\n",
      "iteration: 199810 loss: 0.0018 lr: 0.02\n",
      "iteration: 199820 loss: 0.0022 lr: 0.02\n",
      "iteration: 199830 loss: 0.0014 lr: 0.02\n",
      "iteration: 199840 loss: 0.0031 lr: 0.02\n",
      "iteration: 199850 loss: 0.0025 lr: 0.02\n",
      "iteration: 199860 loss: 0.0031 lr: 0.02\n",
      "iteration: 199870 loss: 0.0024 lr: 0.02\n",
      "iteration: 199880 loss: 0.0022 lr: 0.02\n",
      "iteration: 199890 loss: 0.0018 lr: 0.02\n",
      "iteration: 199900 loss: 0.0021 lr: 0.02\n",
      "iteration: 199910 loss: 0.0021 lr: 0.02\n",
      "iteration: 199920 loss: 0.0017 lr: 0.02\n",
      "iteration: 199930 loss: 0.0018 lr: 0.02\n",
      "iteration: 199940 loss: 0.0023 lr: 0.02\n",
      "iteration: 199950 loss: 0.0015 lr: 0.02\n",
      "iteration: 199960 loss: 0.0019 lr: 0.02\n",
      "iteration: 199970 loss: 0.0021 lr: 0.02\n",
      "iteration: 199980 loss: 0.0025 lr: 0.02\n",
      "iteration: 199990 loss: 0.0021 lr: 0.02\n",
      "iteration: 200000 loss: 0.0020 lr: 0.02\n",
      "iteration: 200010 loss: 0.0016 lr: 0.02\n",
      "iteration: 200020 loss: 0.0021 lr: 0.02\n",
      "iteration: 200030 loss: 0.0025 lr: 0.02\n",
      "iteration: 200040 loss: 0.0027 lr: 0.02\n",
      "iteration: 200050 loss: 0.0025 lr: 0.02\n",
      "iteration: 200060 loss: 0.0022 lr: 0.02\n",
      "iteration: 200070 loss: 0.0024 lr: 0.02\n",
      "iteration: 200080 loss: 0.0016 lr: 0.02\n",
      "iteration: 200090 loss: 0.0019 lr: 0.02\n",
      "iteration: 200100 loss: 0.0017 lr: 0.02\n",
      "iteration: 200110 loss: 0.0024 lr: 0.02\n",
      "iteration: 200120 loss: 0.0019 lr: 0.02\n",
      "iteration: 200130 loss: 0.0018 lr: 0.02\n",
      "iteration: 200140 loss: 0.0016 lr: 0.02\n",
      "iteration: 200150 loss: 0.0018 lr: 0.02\n",
      "iteration: 200160 loss: 0.0021 lr: 0.02\n",
      "iteration: 200170 loss: 0.0020 lr: 0.02\n",
      "iteration: 200180 loss: 0.0032 lr: 0.02\n",
      "iteration: 200190 loss: 0.0018 lr: 0.02\n",
      "iteration: 200200 loss: 0.0029 lr: 0.02\n",
      "iteration: 200210 loss: 0.0021 lr: 0.02\n",
      "iteration: 200220 loss: 0.0013 lr: 0.02\n",
      "iteration: 200230 loss: 0.0032 lr: 0.02\n",
      "iteration: 200240 loss: 0.0020 lr: 0.02\n",
      "iteration: 200250 loss: 0.0024 lr: 0.02\n",
      "iteration: 200260 loss: 0.0029 lr: 0.02\n",
      "iteration: 200270 loss: 0.0026 lr: 0.02\n",
      "iteration: 200280 loss: 0.0019 lr: 0.02\n",
      "iteration: 200290 loss: 0.0030 lr: 0.02\n",
      "iteration: 200300 loss: 0.0019 lr: 0.02\n",
      "iteration: 200310 loss: 0.0022 lr: 0.02\n",
      "iteration: 200320 loss: 0.0018 lr: 0.02\n",
      "iteration: 200330 loss: 0.0024 lr: 0.02\n",
      "iteration: 200340 loss: 0.0017 lr: 0.02\n",
      "iteration: 200350 loss: 0.0019 lr: 0.02\n",
      "iteration: 200360 loss: 0.0026 lr: 0.02\n",
      "iteration: 200370 loss: 0.0017 lr: 0.02\n",
      "iteration: 200380 loss: 0.0019 lr: 0.02\n",
      "iteration: 200390 loss: 0.0020 lr: 0.02\n",
      "iteration: 200400 loss: 0.0019 lr: 0.02\n",
      "iteration: 200410 loss: 0.0034 lr: 0.02\n",
      "iteration: 200420 loss: 0.0016 lr: 0.02\n",
      "iteration: 200430 loss: 0.0018 lr: 0.02\n",
      "iteration: 200440 loss: 0.0028 lr: 0.02\n",
      "iteration: 200450 loss: 0.0017 lr: 0.02\n",
      "iteration: 200460 loss: 0.0016 lr: 0.02\n",
      "iteration: 200470 loss: 0.0021 lr: 0.02\n",
      "iteration: 200480 loss: 0.0022 lr: 0.02\n",
      "iteration: 200490 loss: 0.0014 lr: 0.02\n",
      "iteration: 200500 loss: 0.0017 lr: 0.02\n",
      "iteration: 200510 loss: 0.0021 lr: 0.02\n",
      "iteration: 200520 loss: 0.0023 lr: 0.02\n",
      "iteration: 200530 loss: 0.0021 lr: 0.02\n",
      "iteration: 200540 loss: 0.0027 lr: 0.02\n",
      "iteration: 200550 loss: 0.0025 lr: 0.02\n",
      "iteration: 200560 loss: 0.0023 lr: 0.02\n",
      "iteration: 200570 loss: 0.0018 lr: 0.02\n",
      "iteration: 200580 loss: 0.0020 lr: 0.02\n",
      "iteration: 200590 loss: 0.0015 lr: 0.02\n",
      "iteration: 200600 loss: 0.0022 lr: 0.02\n",
      "iteration: 200610 loss: 0.0019 lr: 0.02\n",
      "iteration: 200620 loss: 0.0021 lr: 0.02\n",
      "iteration: 200630 loss: 0.0020 lr: 0.02\n",
      "iteration: 200640 loss: 0.0023 lr: 0.02\n",
      "iteration: 200650 loss: 0.0017 lr: 0.02\n",
      "iteration: 200660 loss: 0.0033 lr: 0.02\n",
      "iteration: 200670 loss: 0.0021 lr: 0.02\n",
      "iteration: 200680 loss: 0.0018 lr: 0.02\n",
      "iteration: 200690 loss: 0.0027 lr: 0.02\n",
      "iteration: 200700 loss: 0.0025 lr: 0.02\n",
      "iteration: 200710 loss: 0.0020 lr: 0.02\n",
      "iteration: 200720 loss: 0.0031 lr: 0.02\n",
      "iteration: 200730 loss: 0.0041 lr: 0.02\n",
      "iteration: 200740 loss: 0.0016 lr: 0.02\n",
      "iteration: 200750 loss: 0.0017 lr: 0.02\n",
      "iteration: 200760 loss: 0.0028 lr: 0.02\n",
      "iteration: 200770 loss: 0.0016 lr: 0.02\n",
      "iteration: 200780 loss: 0.0016 lr: 0.02\n",
      "iteration: 200790 loss: 0.0019 lr: 0.02\n",
      "iteration: 200800 loss: 0.0023 lr: 0.02\n",
      "iteration: 200810 loss: 0.0027 lr: 0.02\n",
      "iteration: 200820 loss: 0.0022 lr: 0.02\n",
      "iteration: 200830 loss: 0.0018 lr: 0.02\n",
      "iteration: 200840 loss: 0.0034 lr: 0.02\n",
      "iteration: 200850 loss: 0.0023 lr: 0.02\n",
      "iteration: 200860 loss: 0.0019 lr: 0.02\n",
      "iteration: 200870 loss: 0.0015 lr: 0.02\n",
      "iteration: 200880 loss: 0.0015 lr: 0.02\n",
      "iteration: 200890 loss: 0.0031 lr: 0.02\n",
      "iteration: 200900 loss: 0.0018 lr: 0.02\n",
      "iteration: 200910 loss: 0.0019 lr: 0.02\n",
      "iteration: 200920 loss: 0.0020 lr: 0.02\n",
      "iteration: 200930 loss: 0.0023 lr: 0.02\n",
      "iteration: 200940 loss: 0.0017 lr: 0.02\n",
      "iteration: 200950 loss: 0.0023 lr: 0.02\n",
      "iteration: 200960 loss: 0.0015 lr: 0.02\n",
      "iteration: 200970 loss: 0.0030 lr: 0.02\n",
      "iteration: 200980 loss: 0.0028 lr: 0.02\n",
      "iteration: 200990 loss: 0.0030 lr: 0.02\n",
      "iteration: 201000 loss: 0.0016 lr: 0.02\n",
      "iteration: 201010 loss: 0.0021 lr: 0.02\n",
      "iteration: 201020 loss: 0.0023 lr: 0.02\n",
      "iteration: 201030 loss: 0.0027 lr: 0.02\n",
      "iteration: 201040 loss: 0.0014 lr: 0.02\n",
      "iteration: 201050 loss: 0.0017 lr: 0.02\n",
      "iteration: 201060 loss: 0.0029 lr: 0.02\n",
      "iteration: 201070 loss: 0.0023 lr: 0.02\n",
      "iteration: 201080 loss: 0.0031 lr: 0.02\n",
      "iteration: 201090 loss: 0.0033 lr: 0.02\n",
      "iteration: 201100 loss: 0.0017 lr: 0.02\n",
      "iteration: 201110 loss: 0.0018 lr: 0.02\n",
      "iteration: 201120 loss: 0.0025 lr: 0.02\n",
      "iteration: 201130 loss: 0.0030 lr: 0.02\n",
      "iteration: 201140 loss: 0.0031 lr: 0.02\n",
      "iteration: 201150 loss: 0.0032 lr: 0.02\n",
      "iteration: 201160 loss: 0.0028 lr: 0.02\n",
      "iteration: 201170 loss: 0.0022 lr: 0.02\n",
      "iteration: 201180 loss: 0.0023 lr: 0.02\n",
      "iteration: 201190 loss: 0.0023 lr: 0.02\n",
      "iteration: 201200 loss: 0.0029 lr: 0.02\n",
      "iteration: 201210 loss: 0.0012 lr: 0.02\n",
      "iteration: 201220 loss: 0.0019 lr: 0.02\n",
      "iteration: 201230 loss: 0.0017 lr: 0.02\n",
      "iteration: 201240 loss: 0.0022 lr: 0.02\n",
      "iteration: 201250 loss: 0.0019 lr: 0.02\n",
      "iteration: 201260 loss: 0.0021 lr: 0.02\n",
      "iteration: 201270 loss: 0.0015 lr: 0.02\n",
      "iteration: 201280 loss: 0.0014 lr: 0.02\n",
      "iteration: 201290 loss: 0.0020 lr: 0.02\n",
      "iteration: 201300 loss: 0.0018 lr: 0.02\n",
      "iteration: 201310 loss: 0.0015 lr: 0.02\n",
      "iteration: 201320 loss: 0.0018 lr: 0.02\n",
      "iteration: 201330 loss: 0.0027 lr: 0.02\n",
      "iteration: 201340 loss: 0.0019 lr: 0.02\n",
      "iteration: 201350 loss: 0.0024 lr: 0.02\n",
      "iteration: 201360 loss: 0.0021 lr: 0.02\n",
      "iteration: 201370 loss: 0.0032 lr: 0.02\n",
      "iteration: 201380 loss: 0.0019 lr: 0.02\n",
      "iteration: 201390 loss: 0.0030 lr: 0.02\n",
      "iteration: 201400 loss: 0.0028 lr: 0.02\n",
      "iteration: 201410 loss: 0.0022 lr: 0.02\n",
      "iteration: 201420 loss: 0.0027 lr: 0.02\n",
      "iteration: 201430 loss: 0.0029 lr: 0.02\n",
      "iteration: 201440 loss: 0.0023 lr: 0.02\n",
      "iteration: 201450 loss: 0.0020 lr: 0.02\n",
      "iteration: 201460 loss: 0.0031 lr: 0.02\n",
      "iteration: 201470 loss: 0.0026 lr: 0.02\n",
      "iteration: 201480 loss: 0.0019 lr: 0.02\n",
      "iteration: 201490 loss: 0.0016 lr: 0.02\n",
      "iteration: 201500 loss: 0.0016 lr: 0.02\n",
      "iteration: 201510 loss: 0.0025 lr: 0.02\n",
      "iteration: 201520 loss: 0.0017 lr: 0.02\n",
      "iteration: 201530 loss: 0.0042 lr: 0.02\n",
      "iteration: 201540 loss: 0.0029 lr: 0.02\n",
      "iteration: 201550 loss: 0.0017 lr: 0.02\n",
      "iteration: 201560 loss: 0.0025 lr: 0.02\n",
      "iteration: 201570 loss: 0.0024 lr: 0.02\n",
      "iteration: 201580 loss: 0.0022 lr: 0.02\n",
      "iteration: 201590 loss: 0.0017 lr: 0.02\n",
      "iteration: 201600 loss: 0.0024 lr: 0.02\n",
      "iteration: 201610 loss: 0.0018 lr: 0.02\n",
      "iteration: 201620 loss: 0.0022 lr: 0.02\n",
      "iteration: 201630 loss: 0.0020 lr: 0.02\n",
      "iteration: 201640 loss: 0.0023 lr: 0.02\n",
      "iteration: 201650 loss: 0.0020 lr: 0.02\n",
      "iteration: 201660 loss: 0.0019 lr: 0.02\n",
      "iteration: 201670 loss: 0.0021 lr: 0.02\n",
      "iteration: 201680 loss: 0.0022 lr: 0.02\n",
      "iteration: 201690 loss: 0.0024 lr: 0.02\n",
      "iteration: 201700 loss: 0.0030 lr: 0.02\n",
      "iteration: 201710 loss: 0.0021 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 201720 loss: 0.0017 lr: 0.02\n",
      "iteration: 201730 loss: 0.0021 lr: 0.02\n",
      "iteration: 201740 loss: 0.0024 lr: 0.02\n",
      "iteration: 201750 loss: 0.0015 lr: 0.02\n",
      "iteration: 201760 loss: 0.0018 lr: 0.02\n",
      "iteration: 201770 loss: 0.0018 lr: 0.02\n",
      "iteration: 201780 loss: 0.0023 lr: 0.02\n",
      "iteration: 201790 loss: 0.0022 lr: 0.02\n",
      "iteration: 201800 loss: 0.0019 lr: 0.02\n",
      "iteration: 201810 loss: 0.0019 lr: 0.02\n",
      "iteration: 201820 loss: 0.0019 lr: 0.02\n",
      "iteration: 201830 loss: 0.0014 lr: 0.02\n",
      "iteration: 201840 loss: 0.0018 lr: 0.02\n",
      "iteration: 201850 loss: 0.0025 lr: 0.02\n",
      "iteration: 201860 loss: 0.0016 lr: 0.02\n",
      "iteration: 201870 loss: 0.0027 lr: 0.02\n",
      "iteration: 201880 loss: 0.0015 lr: 0.02\n",
      "iteration: 201890 loss: 0.0021 lr: 0.02\n",
      "iteration: 201900 loss: 0.0044 lr: 0.02\n",
      "iteration: 201910 loss: 0.0021 lr: 0.02\n",
      "iteration: 201920 loss: 0.0027 lr: 0.02\n",
      "iteration: 201930 loss: 0.0021 lr: 0.02\n",
      "iteration: 201940 loss: 0.0022 lr: 0.02\n",
      "iteration: 201950 loss: 0.0017 lr: 0.02\n",
      "iteration: 201960 loss: 0.0019 lr: 0.02\n",
      "iteration: 201970 loss: 0.0019 lr: 0.02\n",
      "iteration: 201980 loss: 0.0023 lr: 0.02\n",
      "iteration: 201990 loss: 0.0023 lr: 0.02\n",
      "iteration: 202000 loss: 0.0030 lr: 0.02\n",
      "iteration: 202010 loss: 0.0016 lr: 0.02\n",
      "iteration: 202020 loss: 0.0018 lr: 0.02\n",
      "iteration: 202030 loss: 0.0020 lr: 0.02\n",
      "iteration: 202040 loss: 0.0030 lr: 0.02\n",
      "iteration: 202050 loss: 0.0017 lr: 0.02\n",
      "iteration: 202060 loss: 0.0018 lr: 0.02\n",
      "iteration: 202070 loss: 0.0014 lr: 0.02\n",
      "iteration: 202080 loss: 0.0022 lr: 0.02\n",
      "iteration: 202090 loss: 0.0017 lr: 0.02\n",
      "iteration: 202100 loss: 0.0015 lr: 0.02\n",
      "iteration: 202110 loss: 0.0014 lr: 0.02\n",
      "iteration: 202120 loss: 0.0018 lr: 0.02\n",
      "iteration: 202130 loss: 0.0038 lr: 0.02\n",
      "iteration: 202140 loss: 0.0022 lr: 0.02\n",
      "iteration: 202150 loss: 0.0023 lr: 0.02\n",
      "iteration: 202160 loss: 0.0026 lr: 0.02\n",
      "iteration: 202170 loss: 0.0016 lr: 0.02\n",
      "iteration: 202180 loss: 0.0016 lr: 0.02\n",
      "iteration: 202190 loss: 0.0024 lr: 0.02\n",
      "iteration: 202200 loss: 0.0017 lr: 0.02\n",
      "iteration: 202210 loss: 0.0026 lr: 0.02\n",
      "iteration: 202220 loss: 0.0023 lr: 0.02\n",
      "iteration: 202230 loss: 0.0028 lr: 0.02\n",
      "iteration: 202240 loss: 0.0022 lr: 0.02\n",
      "iteration: 202250 loss: 0.0017 lr: 0.02\n",
      "iteration: 202260 loss: 0.0022 lr: 0.02\n",
      "iteration: 202270 loss: 0.0017 lr: 0.02\n",
      "iteration: 202280 loss: 0.0019 lr: 0.02\n",
      "iteration: 202290 loss: 0.0014 lr: 0.02\n",
      "iteration: 202300 loss: 0.0020 lr: 0.02\n",
      "iteration: 202310 loss: 0.0024 lr: 0.02\n",
      "iteration: 202320 loss: 0.0025 lr: 0.02\n",
      "iteration: 202330 loss: 0.0017 lr: 0.02\n",
      "iteration: 202340 loss: 0.0017 lr: 0.02\n",
      "iteration: 202350 loss: 0.0018 lr: 0.02\n",
      "iteration: 202360 loss: 0.0023 lr: 0.02\n",
      "iteration: 202370 loss: 0.0019 lr: 0.02\n",
      "iteration: 202380 loss: 0.0020 lr: 0.02\n",
      "iteration: 202390 loss: 0.0026 lr: 0.02\n",
      "iteration: 202400 loss: 0.0024 lr: 0.02\n",
      "iteration: 202410 loss: 0.0028 lr: 0.02\n",
      "iteration: 202420 loss: 0.0019 lr: 0.02\n",
      "iteration: 202430 loss: 0.0021 lr: 0.02\n",
      "iteration: 202440 loss: 0.0021 lr: 0.02\n",
      "iteration: 202450 loss: 0.0016 lr: 0.02\n",
      "iteration: 202460 loss: 0.0027 lr: 0.02\n",
      "iteration: 202470 loss: 0.0023 lr: 0.02\n",
      "iteration: 202480 loss: 0.0031 lr: 0.02\n",
      "iteration: 202490 loss: 0.0023 lr: 0.02\n",
      "iteration: 202500 loss: 0.0017 lr: 0.02\n",
      "iteration: 202510 loss: 0.0014 lr: 0.02\n",
      "iteration: 202520 loss: 0.0028 lr: 0.02\n",
      "iteration: 202530 loss: 0.0025 lr: 0.02\n",
      "iteration: 202540 loss: 0.0035 lr: 0.02\n",
      "iteration: 202550 loss: 0.0022 lr: 0.02\n",
      "iteration: 202560 loss: 0.0030 lr: 0.02\n",
      "iteration: 202570 loss: 0.0014 lr: 0.02\n",
      "iteration: 202580 loss: 0.0022 lr: 0.02\n",
      "iteration: 202590 loss: 0.0024 lr: 0.02\n",
      "iteration: 202600 loss: 0.0020 lr: 0.02\n",
      "iteration: 202610 loss: 0.0018 lr: 0.02\n",
      "iteration: 202620 loss: 0.0023 lr: 0.02\n",
      "iteration: 202630 loss: 0.0027 lr: 0.02\n",
      "iteration: 202640 loss: 0.0028 lr: 0.02\n",
      "iteration: 202650 loss: 0.0028 lr: 0.02\n",
      "iteration: 202660 loss: 0.0030 lr: 0.02\n",
      "iteration: 202670 loss: 0.0016 lr: 0.02\n",
      "iteration: 202680 loss: 0.0020 lr: 0.02\n",
      "iteration: 202690 loss: 0.0021 lr: 0.02\n",
      "iteration: 202700 loss: 0.0035 lr: 0.02\n",
      "iteration: 202710 loss: 0.0023 lr: 0.02\n",
      "iteration: 202720 loss: 0.0038 lr: 0.02\n",
      "iteration: 202730 loss: 0.0016 lr: 0.02\n",
      "iteration: 202740 loss: 0.0022 lr: 0.02\n",
      "iteration: 202750 loss: 0.0018 lr: 0.02\n",
      "iteration: 202760 loss: 0.0025 lr: 0.02\n",
      "iteration: 202770 loss: 0.0019 lr: 0.02\n",
      "iteration: 202780 loss: 0.0014 lr: 0.02\n",
      "iteration: 202790 loss: 0.0018 lr: 0.02\n",
      "iteration: 202800 loss: 0.0021 lr: 0.02\n",
      "iteration: 202810 loss: 0.0030 lr: 0.02\n",
      "iteration: 202820 loss: 0.0016 lr: 0.02\n",
      "iteration: 202830 loss: 0.0021 lr: 0.02\n",
      "iteration: 202840 loss: 0.0021 lr: 0.02\n",
      "iteration: 202850 loss: 0.0017 lr: 0.02\n",
      "iteration: 202860 loss: 0.0023 lr: 0.02\n",
      "iteration: 202870 loss: 0.0024 lr: 0.02\n",
      "iteration: 202880 loss: 0.0020 lr: 0.02\n",
      "iteration: 202890 loss: 0.0024 lr: 0.02\n",
      "iteration: 202900 loss: 0.0031 lr: 0.02\n",
      "iteration: 202910 loss: 0.0020 lr: 0.02\n",
      "iteration: 202920 loss: 0.0024 lr: 0.02\n",
      "iteration: 202930 loss: 0.0027 lr: 0.02\n",
      "iteration: 202940 loss: 0.0022 lr: 0.02\n",
      "iteration: 202950 loss: 0.0022 lr: 0.02\n",
      "iteration: 202960 loss: 0.0017 lr: 0.02\n",
      "iteration: 202970 loss: 0.0031 lr: 0.02\n",
      "iteration: 202980 loss: 0.0015 lr: 0.02\n",
      "iteration: 202990 loss: 0.0020 lr: 0.02\n",
      "iteration: 203000 loss: 0.0019 lr: 0.02\n",
      "iteration: 203010 loss: 0.0029 lr: 0.02\n",
      "iteration: 203020 loss: 0.0019 lr: 0.02\n",
      "iteration: 203030 loss: 0.0028 lr: 0.02\n",
      "iteration: 203040 loss: 0.0026 lr: 0.02\n",
      "iteration: 203050 loss: 0.0018 lr: 0.02\n",
      "iteration: 203060 loss: 0.0014 lr: 0.02\n",
      "iteration: 203070 loss: 0.0040 lr: 0.02\n",
      "iteration: 203080 loss: 0.0025 lr: 0.02\n",
      "iteration: 203090 loss: 0.0023 lr: 0.02\n",
      "iteration: 203100 loss: 0.0029 lr: 0.02\n",
      "iteration: 203110 loss: 0.0025 lr: 0.02\n",
      "iteration: 203120 loss: 0.0017 lr: 0.02\n",
      "iteration: 203130 loss: 0.0018 lr: 0.02\n",
      "iteration: 203140 loss: 0.0025 lr: 0.02\n",
      "iteration: 203150 loss: 0.0022 lr: 0.02\n",
      "iteration: 203160 loss: 0.0028 lr: 0.02\n",
      "iteration: 203170 loss: 0.0023 lr: 0.02\n",
      "iteration: 203180 loss: 0.0017 lr: 0.02\n",
      "iteration: 203190 loss: 0.0018 lr: 0.02\n",
      "iteration: 203200 loss: 0.0019 lr: 0.02\n",
      "iteration: 203210 loss: 0.0022 lr: 0.02\n",
      "iteration: 203220 loss: 0.0022 lr: 0.02\n",
      "iteration: 203230 loss: 0.0021 lr: 0.02\n",
      "iteration: 203240 loss: 0.0020 lr: 0.02\n",
      "iteration: 203250 loss: 0.0031 lr: 0.02\n",
      "iteration: 203260 loss: 0.0022 lr: 0.02\n",
      "iteration: 203270 loss: 0.0027 lr: 0.02\n",
      "iteration: 203280 loss: 0.0019 lr: 0.02\n",
      "iteration: 203290 loss: 0.0023 lr: 0.02\n",
      "iteration: 203300 loss: 0.0015 lr: 0.02\n",
      "iteration: 203310 loss: 0.0027 lr: 0.02\n",
      "iteration: 203320 loss: 0.0019 lr: 0.02\n",
      "iteration: 203330 loss: 0.0015 lr: 0.02\n",
      "iteration: 203340 loss: 0.0022 lr: 0.02\n",
      "iteration: 203350 loss: 0.0025 lr: 0.02\n",
      "iteration: 203360 loss: 0.0020 lr: 0.02\n",
      "iteration: 203370 loss: 0.0019 lr: 0.02\n",
      "iteration: 203380 loss: 0.0015 lr: 0.02\n",
      "iteration: 203390 loss: 0.0029 lr: 0.02\n",
      "iteration: 203400 loss: 0.0019 lr: 0.02\n",
      "iteration: 203410 loss: 0.0016 lr: 0.02\n",
      "iteration: 203420 loss: 0.0028 lr: 0.02\n",
      "iteration: 203430 loss: 0.0034 lr: 0.02\n",
      "iteration: 203440 loss: 0.0018 lr: 0.02\n",
      "iteration: 203450 loss: 0.0016 lr: 0.02\n",
      "iteration: 203460 loss: 0.0025 lr: 0.02\n",
      "iteration: 203470 loss: 0.0024 lr: 0.02\n",
      "iteration: 203480 loss: 0.0020 lr: 0.02\n",
      "iteration: 203490 loss: 0.0017 lr: 0.02\n",
      "iteration: 203500 loss: 0.0019 lr: 0.02\n",
      "iteration: 203510 loss: 0.0017 lr: 0.02\n",
      "iteration: 203520 loss: 0.0021 lr: 0.02\n",
      "iteration: 203530 loss: 0.0025 lr: 0.02\n",
      "iteration: 203540 loss: 0.0019 lr: 0.02\n",
      "iteration: 203550 loss: 0.0018 lr: 0.02\n",
      "iteration: 203560 loss: 0.0017 lr: 0.02\n",
      "iteration: 203570 loss: 0.0018 lr: 0.02\n",
      "iteration: 203580 loss: 0.0032 lr: 0.02\n",
      "iteration: 203590 loss: 0.0020 lr: 0.02\n",
      "iteration: 203600 loss: 0.0024 lr: 0.02\n",
      "iteration: 203610 loss: 0.0025 lr: 0.02\n",
      "iteration: 203620 loss: 0.0026 lr: 0.02\n",
      "iteration: 203630 loss: 0.0017 lr: 0.02\n",
      "iteration: 203640 loss: 0.0020 lr: 0.02\n",
      "iteration: 203650 loss: 0.0025 lr: 0.02\n",
      "iteration: 203660 loss: 0.0019 lr: 0.02\n",
      "iteration: 203670 loss: 0.0024 lr: 0.02\n",
      "iteration: 203680 loss: 0.0021 lr: 0.02\n",
      "iteration: 203690 loss: 0.0017 lr: 0.02\n",
      "iteration: 203700 loss: 0.0025 lr: 0.02\n",
      "iteration: 203710 loss: 0.0020 lr: 0.02\n",
      "iteration: 203720 loss: 0.0018 lr: 0.02\n",
      "iteration: 203730 loss: 0.0023 lr: 0.02\n",
      "iteration: 203740 loss: 0.0022 lr: 0.02\n",
      "iteration: 203750 loss: 0.0024 lr: 0.02\n",
      "iteration: 203760 loss: 0.0023 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 203770 loss: 0.0034 lr: 0.02\n",
      "iteration: 203780 loss: 0.0027 lr: 0.02\n",
      "iteration: 203790 loss: 0.0016 lr: 0.02\n",
      "iteration: 203800 loss: 0.0017 lr: 0.02\n",
      "iteration: 203810 loss: 0.0012 lr: 0.02\n",
      "iteration: 203820 loss: 0.0016 lr: 0.02\n",
      "iteration: 203830 loss: 0.0013 lr: 0.02\n",
      "iteration: 203840 loss: 0.0020 lr: 0.02\n",
      "iteration: 203850 loss: 0.0016 lr: 0.02\n",
      "iteration: 203860 loss: 0.0021 lr: 0.02\n",
      "iteration: 203870 loss: 0.0019 lr: 0.02\n",
      "iteration: 203880 loss: 0.0024 lr: 0.02\n",
      "iteration: 203890 loss: 0.0026 lr: 0.02\n",
      "iteration: 203900 loss: 0.0022 lr: 0.02\n",
      "iteration: 203910 loss: 0.0029 lr: 0.02\n",
      "iteration: 203920 loss: 0.0035 lr: 0.02\n",
      "iteration: 203930 loss: 0.0021 lr: 0.02\n",
      "iteration: 203940 loss: 0.0023 lr: 0.02\n",
      "iteration: 203950 loss: 0.0023 lr: 0.02\n",
      "iteration: 203960 loss: 0.0016 lr: 0.02\n",
      "iteration: 203970 loss: 0.0022 lr: 0.02\n",
      "iteration: 203980 loss: 0.0019 lr: 0.02\n",
      "iteration: 203990 loss: 0.0015 lr: 0.02\n",
      "iteration: 204000 loss: 0.0020 lr: 0.02\n",
      "iteration: 204010 loss: 0.0015 lr: 0.02\n",
      "iteration: 204020 loss: 0.0022 lr: 0.02\n",
      "iteration: 204030 loss: 0.0020 lr: 0.02\n",
      "iteration: 204040 loss: 0.0011 lr: 0.02\n",
      "iteration: 204050 loss: 0.0019 lr: 0.02\n",
      "iteration: 204060 loss: 0.0024 lr: 0.02\n",
      "iteration: 204070 loss: 0.0020 lr: 0.02\n",
      "iteration: 204080 loss: 0.0022 lr: 0.02\n",
      "iteration: 204090 loss: 0.0015 lr: 0.02\n",
      "iteration: 204100 loss: 0.0017 lr: 0.02\n",
      "iteration: 204110 loss: 0.0019 lr: 0.02\n",
      "iteration: 204120 loss: 0.0018 lr: 0.02\n",
      "iteration: 204130 loss: 0.0019 lr: 0.02\n",
      "iteration: 204140 loss: 0.0040 lr: 0.02\n",
      "iteration: 204150 loss: 0.0023 lr: 0.02\n",
      "iteration: 204160 loss: 0.0030 lr: 0.02\n",
      "iteration: 204170 loss: 0.0023 lr: 0.02\n",
      "iteration: 204180 loss: 0.0038 lr: 0.02\n",
      "iteration: 204190 loss: 0.0018 lr: 0.02\n",
      "iteration: 204200 loss: 0.0023 lr: 0.02\n",
      "iteration: 204210 loss: 0.0024 lr: 0.02\n",
      "iteration: 204220 loss: 0.0015 lr: 0.02\n",
      "iteration: 204230 loss: 0.0027 lr: 0.02\n",
      "iteration: 204240 loss: 0.0022 lr: 0.02\n",
      "iteration: 204250 loss: 0.0021 lr: 0.02\n",
      "iteration: 204260 loss: 0.0025 lr: 0.02\n",
      "iteration: 204270 loss: 0.0020 lr: 0.02\n",
      "iteration: 204280 loss: 0.0020 lr: 0.02\n",
      "iteration: 204290 loss: 0.0017 lr: 0.02\n",
      "iteration: 204300 loss: 0.0017 lr: 0.02\n",
      "iteration: 204310 loss: 0.0016 lr: 0.02\n",
      "iteration: 204320 loss: 0.0020 lr: 0.02\n",
      "iteration: 204330 loss: 0.0017 lr: 0.02\n",
      "iteration: 204340 loss: 0.0032 lr: 0.02\n",
      "iteration: 204350 loss: 0.0020 lr: 0.02\n",
      "iteration: 204360 loss: 0.0023 lr: 0.02\n",
      "iteration: 204370 loss: 0.0023 lr: 0.02\n",
      "iteration: 204380 loss: 0.0019 lr: 0.02\n",
      "iteration: 204390 loss: 0.0022 lr: 0.02\n",
      "iteration: 204400 loss: 0.0024 lr: 0.02\n",
      "iteration: 204410 loss: 0.0025 lr: 0.02\n",
      "iteration: 204420 loss: 0.0024 lr: 0.02\n",
      "iteration: 204430 loss: 0.0030 lr: 0.02\n",
      "iteration: 204440 loss: 0.0019 lr: 0.02\n",
      "iteration: 204450 loss: 0.0025 lr: 0.02\n",
      "iteration: 204460 loss: 0.0022 lr: 0.02\n",
      "iteration: 204470 loss: 0.0019 lr: 0.02\n",
      "iteration: 204480 loss: 0.0017 lr: 0.02\n",
      "iteration: 204490 loss: 0.0037 lr: 0.02\n",
      "iteration: 204500 loss: 0.0028 lr: 0.02\n",
      "iteration: 204510 loss: 0.0033 lr: 0.02\n",
      "iteration: 204520 loss: 0.0028 lr: 0.02\n",
      "iteration: 204530 loss: 0.0015 lr: 0.02\n",
      "iteration: 204540 loss: 0.0026 lr: 0.02\n",
      "iteration: 204550 loss: 0.0024 lr: 0.02\n",
      "iteration: 204560 loss: 0.0025 lr: 0.02\n",
      "iteration: 204570 loss: 0.0034 lr: 0.02\n",
      "iteration: 204580 loss: 0.0018 lr: 0.02\n",
      "iteration: 204590 loss: 0.0021 lr: 0.02\n",
      "iteration: 204600 loss: 0.0031 lr: 0.02\n",
      "iteration: 204610 loss: 0.0021 lr: 0.02\n",
      "iteration: 204620 loss: 0.0016 lr: 0.02\n",
      "iteration: 204630 loss: 0.0016 lr: 0.02\n",
      "iteration: 204640 loss: 0.0020 lr: 0.02\n",
      "iteration: 204650 loss: 0.0017 lr: 0.02\n",
      "iteration: 204660 loss: 0.0018 lr: 0.02\n",
      "iteration: 204670 loss: 0.0020 lr: 0.02\n",
      "iteration: 204680 loss: 0.0026 lr: 0.02\n",
      "iteration: 204690 loss: 0.0024 lr: 0.02\n",
      "iteration: 204700 loss: 0.0017 lr: 0.02\n",
      "iteration: 204710 loss: 0.0027 lr: 0.02\n",
      "iteration: 204720 loss: 0.0034 lr: 0.02\n",
      "iteration: 204730 loss: 0.0019 lr: 0.02\n",
      "iteration: 204740 loss: 0.0026 lr: 0.02\n",
      "iteration: 204750 loss: 0.0031 lr: 0.02\n",
      "iteration: 204760 loss: 0.0028 lr: 0.02\n",
      "iteration: 204770 loss: 0.0021 lr: 0.02\n",
      "iteration: 204780 loss: 0.0025 lr: 0.02\n",
      "iteration: 204790 loss: 0.0018 lr: 0.02\n",
      "iteration: 204800 loss: 0.0017 lr: 0.02\n",
      "iteration: 204810 loss: 0.0019 lr: 0.02\n",
      "iteration: 204820 loss: 0.0025 lr: 0.02\n",
      "iteration: 204830 loss: 0.0019 lr: 0.02\n",
      "iteration: 204840 loss: 0.0016 lr: 0.02\n",
      "iteration: 204850 loss: 0.0017 lr: 0.02\n",
      "iteration: 204860 loss: 0.0017 lr: 0.02\n",
      "iteration: 204870 loss: 0.0027 lr: 0.02\n",
      "iteration: 204880 loss: 0.0031 lr: 0.02\n",
      "iteration: 204890 loss: 0.0018 lr: 0.02\n",
      "iteration: 204900 loss: 0.0016 lr: 0.02\n",
      "iteration: 204910 loss: 0.0037 lr: 0.02\n",
      "iteration: 204920 loss: 0.0017 lr: 0.02\n",
      "iteration: 204930 loss: 0.0024 lr: 0.02\n",
      "iteration: 204940 loss: 0.0016 lr: 0.02\n",
      "iteration: 204950 loss: 0.0020 lr: 0.02\n",
      "iteration: 204960 loss: 0.0031 lr: 0.02\n",
      "iteration: 204970 loss: 0.0021 lr: 0.02\n",
      "iteration: 204980 loss: 0.0027 lr: 0.02\n",
      "iteration: 204990 loss: 0.0016 lr: 0.02\n",
      "iteration: 205000 loss: 0.0018 lr: 0.02\n",
      "iteration: 205010 loss: 0.0027 lr: 0.02\n",
      "iteration: 205020 loss: 0.0025 lr: 0.02\n",
      "iteration: 205030 loss: 0.0020 lr: 0.02\n",
      "iteration: 205040 loss: 0.0018 lr: 0.02\n",
      "iteration: 205050 loss: 0.0025 lr: 0.02\n",
      "iteration: 205060 loss: 0.0023 lr: 0.02\n",
      "iteration: 205070 loss: 0.0036 lr: 0.02\n",
      "iteration: 205080 loss: 0.0026 lr: 0.02\n",
      "iteration: 205090 loss: 0.0027 lr: 0.02\n",
      "iteration: 205100 loss: 0.0027 lr: 0.02\n",
      "iteration: 205110 loss: 0.0019 lr: 0.02\n",
      "iteration: 205120 loss: 0.0021 lr: 0.02\n",
      "iteration: 205130 loss: 0.0019 lr: 0.02\n",
      "iteration: 205140 loss: 0.0030 lr: 0.02\n",
      "iteration: 205150 loss: 0.0018 lr: 0.02\n",
      "iteration: 205160 loss: 0.0032 lr: 0.02\n",
      "iteration: 205170 loss: 0.0020 lr: 0.02\n",
      "iteration: 205180 loss: 0.0023 lr: 0.02\n",
      "iteration: 205190 loss: 0.0019 lr: 0.02\n",
      "iteration: 205200 loss: 0.0016 lr: 0.02\n",
      "iteration: 205210 loss: 0.0024 lr: 0.02\n",
      "iteration: 205220 loss: 0.0017 lr: 0.02\n",
      "iteration: 205230 loss: 0.0022 lr: 0.02\n",
      "iteration: 205240 loss: 0.0022 lr: 0.02\n",
      "iteration: 205250 loss: 0.0017 lr: 0.02\n",
      "iteration: 205260 loss: 0.0019 lr: 0.02\n",
      "iteration: 205270 loss: 0.0028 lr: 0.02\n",
      "iteration: 205280 loss: 0.0019 lr: 0.02\n",
      "iteration: 205290 loss: 0.0015 lr: 0.02\n",
      "iteration: 205300 loss: 0.0018 lr: 0.02\n",
      "iteration: 205310 loss: 0.0012 lr: 0.02\n",
      "iteration: 205320 loss: 0.0020 lr: 0.02\n",
      "iteration: 205330 loss: 0.0019 lr: 0.02\n",
      "iteration: 205340 loss: 0.0019 lr: 0.02\n",
      "iteration: 205350 loss: 0.0019 lr: 0.02\n",
      "iteration: 205360 loss: 0.0022 lr: 0.02\n",
      "iteration: 205370 loss: 0.0018 lr: 0.02\n",
      "iteration: 205380 loss: 0.0022 lr: 0.02\n",
      "iteration: 205390 loss: 0.0023 lr: 0.02\n",
      "iteration: 205400 loss: 0.0021 lr: 0.02\n",
      "iteration: 205410 loss: 0.0020 lr: 0.02\n",
      "iteration: 205420 loss: 0.0036 lr: 0.02\n",
      "iteration: 205430 loss: 0.0018 lr: 0.02\n",
      "iteration: 205440 loss: 0.0018 lr: 0.02\n",
      "iteration: 205450 loss: 0.0019 lr: 0.02\n",
      "iteration: 205460 loss: 0.0018 lr: 0.02\n",
      "iteration: 205470 loss: 0.0017 lr: 0.02\n",
      "iteration: 205480 loss: 0.0015 lr: 0.02\n",
      "iteration: 205490 loss: 0.0016 lr: 0.02\n",
      "iteration: 205500 loss: 0.0021 lr: 0.02\n",
      "iteration: 205510 loss: 0.0017 lr: 0.02\n",
      "iteration: 205520 loss: 0.0021 lr: 0.02\n",
      "iteration: 205530 loss: 0.0026 lr: 0.02\n",
      "iteration: 205540 loss: 0.0022 lr: 0.02\n",
      "iteration: 205550 loss: 0.0027 lr: 0.02\n",
      "iteration: 205560 loss: 0.0020 lr: 0.02\n",
      "iteration: 205570 loss: 0.0021 lr: 0.02\n",
      "iteration: 205580 loss: 0.0026 lr: 0.02\n",
      "iteration: 205590 loss: 0.0016 lr: 0.02\n",
      "iteration: 205600 loss: 0.0022 lr: 0.02\n",
      "iteration: 205610 loss: 0.0025 lr: 0.02\n",
      "iteration: 205620 loss: 0.0024 lr: 0.02\n",
      "iteration: 205630 loss: 0.0018 lr: 0.02\n",
      "iteration: 205640 loss: 0.0022 lr: 0.02\n",
      "iteration: 205650 loss: 0.0017 lr: 0.02\n",
      "iteration: 205660 loss: 0.0023 lr: 0.02\n",
      "iteration: 205670 loss: 0.0017 lr: 0.02\n",
      "iteration: 205680 loss: 0.0017 lr: 0.02\n",
      "iteration: 205690 loss: 0.0025 lr: 0.02\n",
      "iteration: 205700 loss: 0.0019 lr: 0.02\n",
      "iteration: 205710 loss: 0.0021 lr: 0.02\n",
      "iteration: 205720 loss: 0.0025 lr: 0.02\n",
      "iteration: 205730 loss: 0.0020 lr: 0.02\n",
      "iteration: 205740 loss: 0.0029 lr: 0.02\n",
      "iteration: 205750 loss: 0.0026 lr: 0.02\n",
      "iteration: 205760 loss: 0.0026 lr: 0.02\n",
      "iteration: 205770 loss: 0.0023 lr: 0.02\n",
      "iteration: 205780 loss: 0.0022 lr: 0.02\n",
      "iteration: 205790 loss: 0.0028 lr: 0.02\n",
      "iteration: 205800 loss: 0.0021 lr: 0.02\n",
      "iteration: 205810 loss: 0.0021 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 205820 loss: 0.0020 lr: 0.02\n",
      "iteration: 205830 loss: 0.0023 lr: 0.02\n",
      "iteration: 205840 loss: 0.0015 lr: 0.02\n",
      "iteration: 205850 loss: 0.0024 lr: 0.02\n",
      "iteration: 205860 loss: 0.0018 lr: 0.02\n",
      "iteration: 205870 loss: 0.0018 lr: 0.02\n",
      "iteration: 205880 loss: 0.0019 lr: 0.02\n",
      "iteration: 205890 loss: 0.0020 lr: 0.02\n",
      "iteration: 205900 loss: 0.0018 lr: 0.02\n",
      "iteration: 205910 loss: 0.0017 lr: 0.02\n",
      "iteration: 205920 loss: 0.0014 lr: 0.02\n",
      "iteration: 205930 loss: 0.0023 lr: 0.02\n",
      "iteration: 205940 loss: 0.0020 lr: 0.02\n",
      "iteration: 205950 loss: 0.0019 lr: 0.02\n",
      "iteration: 205960 loss: 0.0021 lr: 0.02\n",
      "iteration: 205970 loss: 0.0018 lr: 0.02\n",
      "iteration: 205980 loss: 0.0017 lr: 0.02\n",
      "iteration: 205990 loss: 0.0017 lr: 0.02\n",
      "iteration: 206000 loss: 0.0019 lr: 0.02\n",
      "iteration: 206010 loss: 0.0022 lr: 0.02\n",
      "iteration: 206020 loss: 0.0020 lr: 0.02\n",
      "iteration: 206030 loss: 0.0025 lr: 0.02\n",
      "iteration: 206040 loss: 0.0018 lr: 0.02\n",
      "iteration: 206050 loss: 0.0018 lr: 0.02\n",
      "iteration: 206060 loss: 0.0020 lr: 0.02\n",
      "iteration: 206070 loss: 0.0018 lr: 0.02\n",
      "iteration: 206080 loss: 0.0020 lr: 0.02\n",
      "iteration: 206090 loss: 0.0019 lr: 0.02\n",
      "iteration: 206100 loss: 0.0019 lr: 0.02\n",
      "iteration: 206110 loss: 0.0022 lr: 0.02\n",
      "iteration: 206120 loss: 0.0017 lr: 0.02\n",
      "iteration: 206130 loss: 0.0026 lr: 0.02\n",
      "iteration: 206140 loss: 0.0016 lr: 0.02\n",
      "iteration: 206150 loss: 0.0019 lr: 0.02\n",
      "iteration: 206160 loss: 0.0018 lr: 0.02\n",
      "iteration: 206170 loss: 0.0024 lr: 0.02\n",
      "iteration: 206180 loss: 0.0014 lr: 0.02\n",
      "iteration: 206190 loss: 0.0024 lr: 0.02\n",
      "iteration: 206200 loss: 0.0021 lr: 0.02\n",
      "iteration: 206210 loss: 0.0023 lr: 0.02\n",
      "iteration: 206220 loss: 0.0021 lr: 0.02\n",
      "iteration: 206230 loss: 0.0025 lr: 0.02\n",
      "iteration: 206240 loss: 0.0020 lr: 0.02\n",
      "iteration: 206250 loss: 0.0020 lr: 0.02\n",
      "iteration: 206260 loss: 0.0022 lr: 0.02\n",
      "iteration: 206270 loss: 0.0021 lr: 0.02\n",
      "iteration: 206280 loss: 0.0028 lr: 0.02\n",
      "iteration: 206290 loss: 0.0030 lr: 0.02\n",
      "iteration: 206300 loss: 0.0025 lr: 0.02\n",
      "iteration: 206310 loss: 0.0033 lr: 0.02\n",
      "iteration: 206320 loss: 0.0023 lr: 0.02\n",
      "iteration: 206330 loss: 0.0018 lr: 0.02\n",
      "iteration: 206340 loss: 0.0024 lr: 0.02\n",
      "iteration: 206350 loss: 0.0024 lr: 0.02\n",
      "iteration: 206360 loss: 0.0022 lr: 0.02\n",
      "iteration: 206370 loss: 0.0020 lr: 0.02\n",
      "iteration: 206380 loss: 0.0020 lr: 0.02\n",
      "iteration: 206390 loss: 0.0028 lr: 0.02\n",
      "iteration: 206400 loss: 0.0034 lr: 0.02\n",
      "iteration: 206410 loss: 0.0029 lr: 0.02\n",
      "iteration: 206420 loss: 0.0022 lr: 0.02\n",
      "iteration: 206430 loss: 0.0025 lr: 0.02\n",
      "iteration: 206440 loss: 0.0025 lr: 0.02\n",
      "iteration: 206450 loss: 0.0014 lr: 0.02\n",
      "iteration: 206460 loss: 0.0042 lr: 0.02\n",
      "iteration: 206470 loss: 0.0028 lr: 0.02\n",
      "iteration: 206480 loss: 0.0034 lr: 0.02\n",
      "iteration: 206490 loss: 0.0024 lr: 0.02\n",
      "iteration: 206500 loss: 0.0030 lr: 0.02\n",
      "iteration: 206510 loss: 0.0024 lr: 0.02\n",
      "iteration: 206520 loss: 0.0023 lr: 0.02\n",
      "iteration: 206530 loss: 0.0023 lr: 0.02\n",
      "iteration: 206540 loss: 0.0025 lr: 0.02\n",
      "iteration: 206550 loss: 0.0017 lr: 0.02\n",
      "iteration: 206560 loss: 0.0021 lr: 0.02\n",
      "iteration: 206570 loss: 0.0017 lr: 0.02\n",
      "iteration: 206580 loss: 0.0020 lr: 0.02\n",
      "iteration: 206590 loss: 0.0022 lr: 0.02\n",
      "iteration: 206600 loss: 0.0015 lr: 0.02\n",
      "iteration: 206610 loss: 0.0019 lr: 0.02\n",
      "iteration: 206620 loss: 0.0021 lr: 0.02\n",
      "iteration: 206630 loss: 0.0017 lr: 0.02\n",
      "iteration: 206640 loss: 0.0021 lr: 0.02\n",
      "iteration: 206650 loss: 0.0033 lr: 0.02\n",
      "iteration: 206660 loss: 0.0018 lr: 0.02\n",
      "iteration: 206670 loss: 0.0023 lr: 0.02\n",
      "iteration: 206680 loss: 0.0016 lr: 0.02\n",
      "iteration: 206690 loss: 0.0024 lr: 0.02\n",
      "iteration: 206700 loss: 0.0024 lr: 0.02\n",
      "iteration: 206710 loss: 0.0020 lr: 0.02\n",
      "iteration: 206720 loss: 0.0026 lr: 0.02\n",
      "iteration: 206730 loss: 0.0016 lr: 0.02\n",
      "iteration: 206740 loss: 0.0019 lr: 0.02\n",
      "iteration: 206750 loss: 0.0024 lr: 0.02\n",
      "iteration: 206760 loss: 0.0028 lr: 0.02\n",
      "iteration: 206770 loss: 0.0025 lr: 0.02\n",
      "iteration: 206780 loss: 0.0017 lr: 0.02\n",
      "iteration: 206790 loss: 0.0024 lr: 0.02\n",
      "iteration: 206800 loss: 0.0024 lr: 0.02\n",
      "iteration: 206810 loss: 0.0020 lr: 0.02\n",
      "iteration: 206820 loss: 0.0018 lr: 0.02\n",
      "iteration: 206830 loss: 0.0013 lr: 0.02\n",
      "iteration: 206840 loss: 0.0018 lr: 0.02\n",
      "iteration: 206850 loss: 0.0023 lr: 0.02\n",
      "iteration: 206860 loss: 0.0016 lr: 0.02\n",
      "iteration: 206870 loss: 0.0018 lr: 0.02\n",
      "iteration: 206880 loss: 0.0014 lr: 0.02\n",
      "iteration: 206890 loss: 0.0017 lr: 0.02\n",
      "iteration: 206900 loss: 0.0016 lr: 0.02\n",
      "iteration: 206910 loss: 0.0019 lr: 0.02\n",
      "iteration: 206920 loss: 0.0015 lr: 0.02\n",
      "iteration: 206930 loss: 0.0017 lr: 0.02\n",
      "iteration: 206940 loss: 0.0016 lr: 0.02\n",
      "iteration: 206950 loss: 0.0020 lr: 0.02\n",
      "iteration: 206960 loss: 0.0018 lr: 0.02\n",
      "iteration: 206970 loss: 0.0018 lr: 0.02\n",
      "iteration: 206980 loss: 0.0013 lr: 0.02\n",
      "iteration: 206990 loss: 0.0018 lr: 0.02\n",
      "iteration: 207000 loss: 0.0017 lr: 0.02\n",
      "iteration: 207010 loss: 0.0018 lr: 0.02\n",
      "iteration: 207020 loss: 0.0019 lr: 0.02\n",
      "iteration: 207030 loss: 0.0023 lr: 0.02\n",
      "iteration: 207040 loss: 0.0029 lr: 0.02\n",
      "iteration: 207050 loss: 0.0024 lr: 0.02\n",
      "iteration: 207060 loss: 0.0020 lr: 0.02\n",
      "iteration: 207070 loss: 0.0019 lr: 0.02\n",
      "iteration: 207080 loss: 0.0019 lr: 0.02\n",
      "iteration: 207090 loss: 0.0026 lr: 0.02\n",
      "iteration: 207100 loss: 0.0028 lr: 0.02\n",
      "iteration: 207110 loss: 0.0024 lr: 0.02\n",
      "iteration: 207120 loss: 0.0017 lr: 0.02\n",
      "iteration: 207130 loss: 0.0018 lr: 0.02\n",
      "iteration: 207140 loss: 0.0016 lr: 0.02\n",
      "iteration: 207150 loss: 0.0019 lr: 0.02\n",
      "iteration: 207160 loss: 0.0023 lr: 0.02\n",
      "iteration: 207170 loss: 0.0019 lr: 0.02\n",
      "iteration: 207180 loss: 0.0017 lr: 0.02\n",
      "iteration: 207190 loss: 0.0018 lr: 0.02\n",
      "iteration: 207200 loss: 0.0015 lr: 0.02\n",
      "iteration: 207210 loss: 0.0019 lr: 0.02\n",
      "iteration: 207220 loss: 0.0015 lr: 0.02\n",
      "iteration: 207230 loss: 0.0015 lr: 0.02\n",
      "iteration: 207240 loss: 0.0012 lr: 0.02\n",
      "iteration: 207250 loss: 0.0020 lr: 0.02\n",
      "iteration: 207260 loss: 0.0020 lr: 0.02\n",
      "iteration: 207270 loss: 0.0022 lr: 0.02\n",
      "iteration: 207280 loss: 0.0017 lr: 0.02\n",
      "iteration: 207290 loss: 0.0029 lr: 0.02\n",
      "iteration: 207300 loss: 0.0020 lr: 0.02\n",
      "iteration: 207310 loss: 0.0023 lr: 0.02\n",
      "iteration: 207320 loss: 0.0017 lr: 0.02\n",
      "iteration: 207330 loss: 0.0032 lr: 0.02\n",
      "iteration: 207340 loss: 0.0024 lr: 0.02\n",
      "iteration: 207350 loss: 0.0019 lr: 0.02\n",
      "iteration: 207360 loss: 0.0022 lr: 0.02\n",
      "iteration: 207370 loss: 0.0019 lr: 0.02\n",
      "iteration: 207380 loss: 0.0016 lr: 0.02\n",
      "iteration: 207390 loss: 0.0027 lr: 0.02\n",
      "iteration: 207400 loss: 0.0031 lr: 0.02\n",
      "iteration: 207410 loss: 0.0042 lr: 0.02\n",
      "iteration: 207420 loss: 0.0020 lr: 0.02\n",
      "iteration: 207430 loss: 0.0020 lr: 0.02\n",
      "iteration: 207440 loss: 0.0020 lr: 0.02\n",
      "iteration: 207450 loss: 0.0023 lr: 0.02\n",
      "iteration: 207460 loss: 0.0021 lr: 0.02\n",
      "iteration: 207470 loss: 0.0014 lr: 0.02\n",
      "iteration: 207480 loss: 0.0015 lr: 0.02\n",
      "iteration: 207490 loss: 0.0017 lr: 0.02\n",
      "iteration: 207500 loss: 0.0017 lr: 0.02\n",
      "iteration: 207510 loss: 0.0024 lr: 0.02\n",
      "iteration: 207520 loss: 0.0024 lr: 0.02\n",
      "iteration: 207530 loss: 0.0027 lr: 0.02\n",
      "iteration: 207540 loss: 0.0018 lr: 0.02\n",
      "iteration: 207550 loss: 0.0020 lr: 0.02\n",
      "iteration: 207560 loss: 0.0015 lr: 0.02\n",
      "iteration: 207570 loss: 0.0015 lr: 0.02\n",
      "iteration: 207580 loss: 0.0018 lr: 0.02\n",
      "iteration: 207590 loss: 0.0021 lr: 0.02\n",
      "iteration: 207600 loss: 0.0020 lr: 0.02\n",
      "iteration: 207610 loss: 0.0030 lr: 0.02\n",
      "iteration: 207620 loss: 0.0036 lr: 0.02\n",
      "iteration: 207630 loss: 0.0020 lr: 0.02\n",
      "iteration: 207640 loss: 0.0025 lr: 0.02\n",
      "iteration: 207650 loss: 0.0019 lr: 0.02\n",
      "iteration: 207660 loss: 0.0019 lr: 0.02\n",
      "iteration: 207670 loss: 0.0023 lr: 0.02\n",
      "iteration: 207680 loss: 0.0020 lr: 0.02\n",
      "iteration: 207690 loss: 0.0028 lr: 0.02\n",
      "iteration: 207700 loss: 0.0023 lr: 0.02\n",
      "iteration: 207710 loss: 0.0017 lr: 0.02\n",
      "iteration: 207720 loss: 0.0027 lr: 0.02\n",
      "iteration: 207730 loss: 0.0020 lr: 0.02\n",
      "iteration: 207740 loss: 0.0021 lr: 0.02\n",
      "iteration: 207750 loss: 0.0026 lr: 0.02\n",
      "iteration: 207760 loss: 0.0018 lr: 0.02\n",
      "iteration: 207770 loss: 0.0024 lr: 0.02\n",
      "iteration: 207780 loss: 0.0024 lr: 0.02\n",
      "iteration: 207790 loss: 0.0023 lr: 0.02\n",
      "iteration: 207800 loss: 0.0019 lr: 0.02\n",
      "iteration: 207810 loss: 0.0020 lr: 0.02\n",
      "iteration: 207820 loss: 0.0023 lr: 0.02\n",
      "iteration: 207830 loss: 0.0022 lr: 0.02\n",
      "iteration: 207840 loss: 0.0021 lr: 0.02\n",
      "iteration: 207850 loss: 0.0027 lr: 0.02\n",
      "iteration: 207860 loss: 0.0019 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 207870 loss: 0.0016 lr: 0.02\n",
      "iteration: 207880 loss: 0.0028 lr: 0.02\n",
      "iteration: 207890 loss: 0.0025 lr: 0.02\n",
      "iteration: 207900 loss: 0.0022 lr: 0.02\n",
      "iteration: 207910 loss: 0.0022 lr: 0.02\n",
      "iteration: 207920 loss: 0.0019 lr: 0.02\n",
      "iteration: 207930 loss: 0.0024 lr: 0.02\n",
      "iteration: 207940 loss: 0.0020 lr: 0.02\n",
      "iteration: 207950 loss: 0.0019 lr: 0.02\n",
      "iteration: 207960 loss: 0.0022 lr: 0.02\n",
      "iteration: 207970 loss: 0.0024 lr: 0.02\n",
      "iteration: 207980 loss: 0.0020 lr: 0.02\n",
      "iteration: 207990 loss: 0.0021 lr: 0.02\n",
      "iteration: 208000 loss: 0.0020 lr: 0.02\n",
      "iteration: 208010 loss: 0.0021 lr: 0.02\n",
      "iteration: 208020 loss: 0.0011 lr: 0.02\n",
      "iteration: 208030 loss: 0.0027 lr: 0.02\n",
      "iteration: 208040 loss: 0.0020 lr: 0.02\n",
      "iteration: 208050 loss: 0.0017 lr: 0.02\n",
      "iteration: 208060 loss: 0.0017 lr: 0.02\n",
      "iteration: 208070 loss: 0.0020 lr: 0.02\n",
      "iteration: 208080 loss: 0.0032 lr: 0.02\n",
      "iteration: 208090 loss: 0.0030 lr: 0.02\n",
      "iteration: 208100 loss: 0.0014 lr: 0.02\n",
      "iteration: 208110 loss: 0.0024 lr: 0.02\n",
      "iteration: 208120 loss: 0.0022 lr: 0.02\n",
      "iteration: 208130 loss: 0.0032 lr: 0.02\n",
      "iteration: 208140 loss: 0.0020 lr: 0.02\n",
      "iteration: 208150 loss: 0.0020 lr: 0.02\n",
      "iteration: 208160 loss: 0.0022 lr: 0.02\n",
      "iteration: 208170 loss: 0.0026 lr: 0.02\n",
      "iteration: 208180 loss: 0.0021 lr: 0.02\n",
      "iteration: 208190 loss: 0.0021 lr: 0.02\n",
      "iteration: 208200 loss: 0.0017 lr: 0.02\n",
      "iteration: 208210 loss: 0.0028 lr: 0.02\n",
      "iteration: 208220 loss: 0.0014 lr: 0.02\n",
      "iteration: 208230 loss: 0.0014 lr: 0.02\n",
      "iteration: 208240 loss: 0.0018 lr: 0.02\n",
      "iteration: 208250 loss: 0.0020 lr: 0.02\n",
      "iteration: 208260 loss: 0.0018 lr: 0.02\n",
      "iteration: 208270 loss: 0.0016 lr: 0.02\n",
      "iteration: 208280 loss: 0.0016 lr: 0.02\n",
      "iteration: 208290 loss: 0.0019 lr: 0.02\n",
      "iteration: 208300 loss: 0.0026 lr: 0.02\n",
      "iteration: 208310 loss: 0.0019 lr: 0.02\n",
      "iteration: 208320 loss: 0.0020 lr: 0.02\n",
      "iteration: 208330 loss: 0.0022 lr: 0.02\n",
      "iteration: 208340 loss: 0.0022 lr: 0.02\n",
      "iteration: 208350 loss: 0.0025 lr: 0.02\n",
      "iteration: 208360 loss: 0.0017 lr: 0.02\n",
      "iteration: 208370 loss: 0.0020 lr: 0.02\n",
      "iteration: 208380 loss: 0.0028 lr: 0.02\n",
      "iteration: 208390 loss: 0.0021 lr: 0.02\n",
      "iteration: 208400 loss: 0.0022 lr: 0.02\n",
      "iteration: 208410 loss: 0.0019 lr: 0.02\n",
      "iteration: 208420 loss: 0.0022 lr: 0.02\n",
      "iteration: 208430 loss: 0.0024 lr: 0.02\n",
      "iteration: 208440 loss: 0.0019 lr: 0.02\n",
      "iteration: 208450 loss: 0.0032 lr: 0.02\n",
      "iteration: 208460 loss: 0.0025 lr: 0.02\n",
      "iteration: 208470 loss: 0.0023 lr: 0.02\n",
      "iteration: 208480 loss: 0.0027 lr: 0.02\n",
      "iteration: 208490 loss: 0.0022 lr: 0.02\n",
      "iteration: 208500 loss: 0.0017 lr: 0.02\n",
      "iteration: 208510 loss: 0.0022 lr: 0.02\n",
      "iteration: 208520 loss: 0.0026 lr: 0.02\n",
      "iteration: 208530 loss: 0.0020 lr: 0.02\n",
      "iteration: 208540 loss: 0.0027 lr: 0.02\n",
      "iteration: 208550 loss: 0.0018 lr: 0.02\n",
      "iteration: 208560 loss: 0.0017 lr: 0.02\n",
      "iteration: 208570 loss: 0.0020 lr: 0.02\n",
      "iteration: 208580 loss: 0.0025 lr: 0.02\n",
      "iteration: 208590 loss: 0.0022 lr: 0.02\n",
      "iteration: 208600 loss: 0.0026 lr: 0.02\n",
      "iteration: 208610 loss: 0.0027 lr: 0.02\n",
      "iteration: 208620 loss: 0.0021 lr: 0.02\n",
      "iteration: 208630 loss: 0.0020 lr: 0.02\n",
      "iteration: 208640 loss: 0.0018 lr: 0.02\n",
      "iteration: 208650 loss: 0.0020 lr: 0.02\n",
      "iteration: 208660 loss: 0.0018 lr: 0.02\n",
      "iteration: 208670 loss: 0.0021 lr: 0.02\n",
      "iteration: 208680 loss: 0.0045 lr: 0.02\n",
      "iteration: 208690 loss: 0.0027 lr: 0.02\n",
      "iteration: 208700 loss: 0.0028 lr: 0.02\n",
      "iteration: 208710 loss: 0.0028 lr: 0.02\n",
      "iteration: 208720 loss: 0.0016 lr: 0.02\n",
      "iteration: 208730 loss: 0.0017 lr: 0.02\n",
      "iteration: 208740 loss: 0.0020 lr: 0.02\n",
      "iteration: 208750 loss: 0.0037 lr: 0.02\n",
      "iteration: 208760 loss: 0.0016 lr: 0.02\n",
      "iteration: 208770 loss: 0.0043 lr: 0.02\n",
      "iteration: 208780 loss: 0.0014 lr: 0.02\n",
      "iteration: 208790 loss: 0.0015 lr: 0.02\n",
      "iteration: 208800 loss: 0.0020 lr: 0.02\n",
      "iteration: 208810 loss: 0.0021 lr: 0.02\n",
      "iteration: 208820 loss: 0.0020 lr: 0.02\n",
      "iteration: 208830 loss: 0.0013 lr: 0.02\n",
      "iteration: 208840 loss: 0.0017 lr: 0.02\n",
      "iteration: 208850 loss: 0.0015 lr: 0.02\n",
      "iteration: 208860 loss: 0.0023 lr: 0.02\n",
      "iteration: 208870 loss: 0.0019 lr: 0.02\n",
      "iteration: 208880 loss: 0.0020 lr: 0.02\n",
      "iteration: 208890 loss: 0.0024 lr: 0.02\n",
      "iteration: 208900 loss: 0.0025 lr: 0.02\n",
      "iteration: 208910 loss: 0.0019 lr: 0.02\n",
      "iteration: 208920 loss: 0.0015 lr: 0.02\n",
      "iteration: 208930 loss: 0.0017 lr: 0.02\n",
      "iteration: 208940 loss: 0.0017 lr: 0.02\n",
      "iteration: 208950 loss: 0.0012 lr: 0.02\n",
      "iteration: 208960 loss: 0.0014 lr: 0.02\n",
      "iteration: 208970 loss: 0.0020 lr: 0.02\n",
      "iteration: 208980 loss: 0.0025 lr: 0.02\n",
      "iteration: 208990 loss: 0.0019 lr: 0.02\n",
      "iteration: 209000 loss: 0.0028 lr: 0.02\n",
      "iteration: 209010 loss: 0.0017 lr: 0.02\n",
      "iteration: 209020 loss: 0.0030 lr: 0.02\n",
      "iteration: 209030 loss: 0.0018 lr: 0.02\n",
      "iteration: 209040 loss: 0.0023 lr: 0.02\n",
      "iteration: 209050 loss: 0.0018 lr: 0.02\n",
      "iteration: 209060 loss: 0.0019 lr: 0.02\n",
      "iteration: 209070 loss: 0.0018 lr: 0.02\n",
      "iteration: 209080 loss: 0.0017 lr: 0.02\n",
      "iteration: 209090 loss: 0.0024 lr: 0.02\n",
      "iteration: 209100 loss: 0.0017 lr: 0.02\n",
      "iteration: 209110 loss: 0.0021 lr: 0.02\n",
      "iteration: 209120 loss: 0.0020 lr: 0.02\n",
      "iteration: 209130 loss: 0.0022 lr: 0.02\n",
      "iteration: 209140 loss: 0.0021 lr: 0.02\n",
      "iteration: 209150 loss: 0.0026 lr: 0.02\n",
      "iteration: 209160 loss: 0.0027 lr: 0.02\n",
      "iteration: 209170 loss: 0.0019 lr: 0.02\n",
      "iteration: 209180 loss: 0.0031 lr: 0.02\n",
      "iteration: 209190 loss: 0.0018 lr: 0.02\n",
      "iteration: 209200 loss: 0.0023 lr: 0.02\n",
      "iteration: 209210 loss: 0.0023 lr: 0.02\n",
      "iteration: 209220 loss: 0.0020 lr: 0.02\n",
      "iteration: 209230 loss: 0.0015 lr: 0.02\n",
      "iteration: 209240 loss: 0.0027 lr: 0.02\n",
      "iteration: 209250 loss: 0.0028 lr: 0.02\n",
      "iteration: 209260 loss: 0.0023 lr: 0.02\n",
      "iteration: 209270 loss: 0.0022 lr: 0.02\n",
      "iteration: 209280 loss: 0.0026 lr: 0.02\n",
      "iteration: 209290 loss: 0.0013 lr: 0.02\n",
      "iteration: 209300 loss: 0.0023 lr: 0.02\n",
      "iteration: 209310 loss: 0.0016 lr: 0.02\n",
      "iteration: 209320 loss: 0.0018 lr: 0.02\n",
      "iteration: 209330 loss: 0.0019 lr: 0.02\n",
      "iteration: 209340 loss: 0.0014 lr: 0.02\n",
      "iteration: 209350 loss: 0.0024 lr: 0.02\n",
      "iteration: 209360 loss: 0.0015 lr: 0.02\n",
      "iteration: 209370 loss: 0.0015 lr: 0.02\n",
      "iteration: 209380 loss: 0.0026 lr: 0.02\n",
      "iteration: 209390 loss: 0.0022 lr: 0.02\n",
      "iteration: 209400 loss: 0.0024 lr: 0.02\n",
      "iteration: 209410 loss: 0.0017 lr: 0.02\n",
      "iteration: 209420 loss: 0.0022 lr: 0.02\n",
      "iteration: 209430 loss: 0.0014 lr: 0.02\n",
      "iteration: 209440 loss: 0.0018 lr: 0.02\n",
      "iteration: 209450 loss: 0.0021 lr: 0.02\n",
      "iteration: 209460 loss: 0.0023 lr: 0.02\n",
      "iteration: 209470 loss: 0.0018 lr: 0.02\n",
      "iteration: 209480 loss: 0.0028 lr: 0.02\n",
      "iteration: 209490 loss: 0.0020 lr: 0.02\n",
      "iteration: 209500 loss: 0.0021 lr: 0.02\n",
      "iteration: 209510 loss: 0.0028 lr: 0.02\n",
      "iteration: 209520 loss: 0.0042 lr: 0.02\n",
      "iteration: 209530 loss: 0.0053 lr: 0.02\n",
      "iteration: 209540 loss: 0.0020 lr: 0.02\n",
      "iteration: 209550 loss: 0.0022 lr: 0.02\n",
      "iteration: 209560 loss: 0.0028 lr: 0.02\n",
      "iteration: 209570 loss: 0.0026 lr: 0.02\n",
      "iteration: 209580 loss: 0.0024 lr: 0.02\n",
      "iteration: 209590 loss: 0.0018 lr: 0.02\n",
      "iteration: 209600 loss: 0.0028 lr: 0.02\n",
      "iteration: 209610 loss: 0.0018 lr: 0.02\n",
      "iteration: 209620 loss: 0.0019 lr: 0.02\n",
      "iteration: 209630 loss: 0.0019 lr: 0.02\n",
      "iteration: 209640 loss: 0.0020 lr: 0.02\n",
      "iteration: 209650 loss: 0.0016 lr: 0.02\n",
      "iteration: 209660 loss: 0.0028 lr: 0.02\n",
      "iteration: 209670 loss: 0.0021 lr: 0.02\n",
      "iteration: 209680 loss: 0.0023 lr: 0.02\n",
      "iteration: 209690 loss: 0.0016 lr: 0.02\n",
      "iteration: 209700 loss: 0.0023 lr: 0.02\n",
      "iteration: 209710 loss: 0.0018 lr: 0.02\n",
      "iteration: 209720 loss: 0.0020 lr: 0.02\n",
      "iteration: 209730 loss: 0.0018 lr: 0.02\n",
      "iteration: 209740 loss: 0.0020 lr: 0.02\n",
      "iteration: 209750 loss: 0.0019 lr: 0.02\n",
      "iteration: 209760 loss: 0.0021 lr: 0.02\n",
      "iteration: 209770 loss: 0.0027 lr: 0.02\n",
      "iteration: 209780 loss: 0.0020 lr: 0.02\n",
      "iteration: 209790 loss: 0.0020 lr: 0.02\n",
      "iteration: 209800 loss: 0.0016 lr: 0.02\n",
      "iteration: 209810 loss: 0.0028 lr: 0.02\n",
      "iteration: 209820 loss: 0.0018 lr: 0.02\n",
      "iteration: 209830 loss: 0.0029 lr: 0.02\n",
      "iteration: 209840 loss: 0.0027 lr: 0.02\n",
      "iteration: 209850 loss: 0.0013 lr: 0.02\n",
      "iteration: 209860 loss: 0.0021 lr: 0.02\n",
      "iteration: 209870 loss: 0.0015 lr: 0.02\n",
      "iteration: 209880 loss: 0.0024 lr: 0.02\n",
      "iteration: 209890 loss: 0.0018 lr: 0.02\n",
      "iteration: 209900 loss: 0.0021 lr: 0.02\n",
      "iteration: 209910 loss: 0.0026 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 209920 loss: 0.0024 lr: 0.02\n",
      "iteration: 209930 loss: 0.0019 lr: 0.02\n",
      "iteration: 209940 loss: 0.0020 lr: 0.02\n",
      "iteration: 209950 loss: 0.0020 lr: 0.02\n",
      "iteration: 209960 loss: 0.0022 lr: 0.02\n",
      "iteration: 209970 loss: 0.0018 lr: 0.02\n",
      "iteration: 209980 loss: 0.0018 lr: 0.02\n",
      "iteration: 209990 loss: 0.0021 lr: 0.02\n",
      "iteration: 210000 loss: 0.0020 lr: 0.02\n",
      "iteration: 210010 loss: 0.0018 lr: 0.02\n",
      "iteration: 210020 loss: 0.0016 lr: 0.02\n",
      "iteration: 210030 loss: 0.0021 lr: 0.02\n",
      "iteration: 210040 loss: 0.0019 lr: 0.02\n",
      "iteration: 210050 loss: 0.0020 lr: 0.02\n",
      "iteration: 210060 loss: 0.0027 lr: 0.02\n",
      "iteration: 210070 loss: 0.0023 lr: 0.02\n",
      "iteration: 210080 loss: 0.0021 lr: 0.02\n",
      "iteration: 210090 loss: 0.0028 lr: 0.02\n",
      "iteration: 210100 loss: 0.0015 lr: 0.02\n",
      "iteration: 210110 loss: 0.0023 lr: 0.02\n",
      "iteration: 210120 loss: 0.0024 lr: 0.02\n",
      "iteration: 210130 loss: 0.0017 lr: 0.02\n",
      "iteration: 210140 loss: 0.0020 lr: 0.02\n",
      "iteration: 210150 loss: 0.0027 lr: 0.02\n",
      "iteration: 210160 loss: 0.0019 lr: 0.02\n",
      "iteration: 210170 loss: 0.0020 lr: 0.02\n",
      "iteration: 210180 loss: 0.0015 lr: 0.02\n",
      "iteration: 210190 loss: 0.0013 lr: 0.02\n",
      "iteration: 210200 loss: 0.0019 lr: 0.02\n",
      "iteration: 210210 loss: 0.0028 lr: 0.02\n",
      "iteration: 210220 loss: 0.0020 lr: 0.02\n",
      "iteration: 210230 loss: 0.0026 lr: 0.02\n",
      "iteration: 210240 loss: 0.0017 lr: 0.02\n",
      "iteration: 210250 loss: 0.0020 lr: 0.02\n",
      "iteration: 210260 loss: 0.0020 lr: 0.02\n",
      "iteration: 210270 loss: 0.0025 lr: 0.02\n",
      "iteration: 210280 loss: 0.0024 lr: 0.02\n",
      "iteration: 210290 loss: 0.0019 lr: 0.02\n",
      "iteration: 210300 loss: 0.0023 lr: 0.02\n",
      "iteration: 210310 loss: 0.0020 lr: 0.02\n",
      "iteration: 210320 loss: 0.0017 lr: 0.02\n",
      "iteration: 210330 loss: 0.0025 lr: 0.02\n",
      "iteration: 210340 loss: 0.0025 lr: 0.02\n",
      "iteration: 210350 loss: 0.0018 lr: 0.02\n",
      "iteration: 210360 loss: 0.0024 lr: 0.02\n",
      "iteration: 210370 loss: 0.0016 lr: 0.02\n",
      "iteration: 210380 loss: 0.0013 lr: 0.02\n",
      "iteration: 210390 loss: 0.0015 lr: 0.02\n",
      "iteration: 210400 loss: 0.0019 lr: 0.02\n",
      "iteration: 210410 loss: 0.0024 lr: 0.02\n",
      "iteration: 210420 loss: 0.0020 lr: 0.02\n",
      "iteration: 210430 loss: 0.0024 lr: 0.02\n",
      "iteration: 210440 loss: 0.0021 lr: 0.02\n",
      "iteration: 210450 loss: 0.0023 lr: 0.02\n",
      "iteration: 210460 loss: 0.0022 lr: 0.02\n",
      "iteration: 210470 loss: 0.0020 lr: 0.02\n",
      "iteration: 210480 loss: 0.0017 lr: 0.02\n",
      "iteration: 210490 loss: 0.0018 lr: 0.02\n",
      "iteration: 210500 loss: 0.0023 lr: 0.02\n",
      "iteration: 210510 loss: 0.0016 lr: 0.02\n",
      "iteration: 210520 loss: 0.0015 lr: 0.02\n",
      "iteration: 210530 loss: 0.0027 lr: 0.02\n",
      "iteration: 210540 loss: 0.0024 lr: 0.02\n",
      "iteration: 210550 loss: 0.0021 lr: 0.02\n",
      "iteration: 210560 loss: 0.0016 lr: 0.02\n",
      "iteration: 210570 loss: 0.0022 lr: 0.02\n",
      "iteration: 210580 loss: 0.0024 lr: 0.02\n",
      "iteration: 210590 loss: 0.0016 lr: 0.02\n",
      "iteration: 210600 loss: 0.0016 lr: 0.02\n",
      "iteration: 210610 loss: 0.0021 lr: 0.02\n",
      "iteration: 210620 loss: 0.0020 lr: 0.02\n",
      "iteration: 210630 loss: 0.0027 lr: 0.02\n",
      "iteration: 210640 loss: 0.0018 lr: 0.02\n",
      "iteration: 210650 loss: 0.0016 lr: 0.02\n",
      "iteration: 210660 loss: 0.0022 lr: 0.02\n",
      "iteration: 210670 loss: 0.0020 lr: 0.02\n",
      "iteration: 210680 loss: 0.0022 lr: 0.02\n",
      "iteration: 210690 loss: 0.0020 lr: 0.02\n",
      "iteration: 210700 loss: 0.0022 lr: 0.02\n",
      "iteration: 210710 loss: 0.0017 lr: 0.02\n",
      "iteration: 210720 loss: 0.0022 lr: 0.02\n",
      "iteration: 210730 loss: 0.0022 lr: 0.02\n",
      "iteration: 210740 loss: 0.0019 lr: 0.02\n",
      "iteration: 210750 loss: 0.0025 lr: 0.02\n",
      "iteration: 210760 loss: 0.0016 lr: 0.02\n",
      "iteration: 210770 loss: 0.0015 lr: 0.02\n",
      "iteration: 210780 loss: 0.0015 lr: 0.02\n",
      "iteration: 210790 loss: 0.0020 lr: 0.02\n",
      "iteration: 210800 loss: 0.0024 lr: 0.02\n",
      "iteration: 210810 loss: 0.0016 lr: 0.02\n",
      "iteration: 210820 loss: 0.0014 lr: 0.02\n",
      "iteration: 210830 loss: 0.0015 lr: 0.02\n",
      "iteration: 210840 loss: 0.0017 lr: 0.02\n",
      "iteration: 210850 loss: 0.0011 lr: 0.02\n",
      "iteration: 210860 loss: 0.0017 lr: 0.02\n",
      "iteration: 210870 loss: 0.0016 lr: 0.02\n",
      "iteration: 210880 loss: 0.0015 lr: 0.02\n",
      "iteration: 210890 loss: 0.0016 lr: 0.02\n",
      "iteration: 210900 loss: 0.0024 lr: 0.02\n",
      "iteration: 210910 loss: 0.0030 lr: 0.02\n",
      "iteration: 210920 loss: 0.0033 lr: 0.02\n",
      "iteration: 210930 loss: 0.0028 lr: 0.02\n",
      "iteration: 210940 loss: 0.0020 lr: 0.02\n",
      "iteration: 210950 loss: 0.0024 lr: 0.02\n",
      "iteration: 210960 loss: 0.0025 lr: 0.02\n",
      "iteration: 210970 loss: 0.0022 lr: 0.02\n",
      "iteration: 210980 loss: 0.0019 lr: 0.02\n",
      "iteration: 210990 loss: 0.0030 lr: 0.02\n",
      "iteration: 211000 loss: 0.0023 lr: 0.02\n",
      "iteration: 211010 loss: 0.0029 lr: 0.02\n",
      "iteration: 211020 loss: 0.0019 lr: 0.02\n",
      "iteration: 211030 loss: 0.0028 lr: 0.02\n",
      "iteration: 211040 loss: 0.0013 lr: 0.02\n",
      "iteration: 211050 loss: 0.0025 lr: 0.02\n",
      "iteration: 211060 loss: 0.0020 lr: 0.02\n",
      "iteration: 211070 loss: 0.0021 lr: 0.02\n",
      "iteration: 211080 loss: 0.0019 lr: 0.02\n",
      "iteration: 211090 loss: 0.0023 lr: 0.02\n",
      "iteration: 211100 loss: 0.0029 lr: 0.02\n",
      "iteration: 211110 loss: 0.0026 lr: 0.02\n",
      "iteration: 211120 loss: 0.0016 lr: 0.02\n",
      "iteration: 211130 loss: 0.0017 lr: 0.02\n",
      "iteration: 211140 loss: 0.0034 lr: 0.02\n",
      "iteration: 211150 loss: 0.0024 lr: 0.02\n",
      "iteration: 211160 loss: 0.0023 lr: 0.02\n",
      "iteration: 211170 loss: 0.0023 lr: 0.02\n",
      "iteration: 211180 loss: 0.0029 lr: 0.02\n",
      "iteration: 211190 loss: 0.0017 lr: 0.02\n",
      "iteration: 211200 loss: 0.0023 lr: 0.02\n",
      "iteration: 211210 loss: 0.0022 lr: 0.02\n",
      "iteration: 211220 loss: 0.0028 lr: 0.02\n",
      "iteration: 211230 loss: 0.0025 lr: 0.02\n",
      "iteration: 211240 loss: 0.0030 lr: 0.02\n",
      "iteration: 211250 loss: 0.0018 lr: 0.02\n",
      "iteration: 211260 loss: 0.0028 lr: 0.02\n",
      "iteration: 211270 loss: 0.0019 lr: 0.02\n",
      "iteration: 211280 loss: 0.0020 lr: 0.02\n",
      "iteration: 211290 loss: 0.0021 lr: 0.02\n",
      "iteration: 211300 loss: 0.0020 lr: 0.02\n",
      "iteration: 211310 loss: 0.0029 lr: 0.02\n",
      "iteration: 211320 loss: 0.0024 lr: 0.02\n",
      "iteration: 211330 loss: 0.0022 lr: 0.02\n",
      "iteration: 211340 loss: 0.0027 lr: 0.02\n",
      "iteration: 211350 loss: 0.0024 lr: 0.02\n",
      "iteration: 211360 loss: 0.0022 lr: 0.02\n",
      "iteration: 211370 loss: 0.0018 lr: 0.02\n",
      "iteration: 211380 loss: 0.0032 lr: 0.02\n",
      "iteration: 211390 loss: 0.0026 lr: 0.02\n",
      "iteration: 211400 loss: 0.0017 lr: 0.02\n",
      "iteration: 211410 loss: 0.0018 lr: 0.02\n",
      "iteration: 211420 loss: 0.0026 lr: 0.02\n",
      "iteration: 211430 loss: 0.0018 lr: 0.02\n",
      "iteration: 211440 loss: 0.0024 lr: 0.02\n",
      "iteration: 211450 loss: 0.0019 lr: 0.02\n",
      "iteration: 211460 loss: 0.0021 lr: 0.02\n",
      "iteration: 211470 loss: 0.0032 lr: 0.02\n",
      "iteration: 211480 loss: 0.0016 lr: 0.02\n",
      "iteration: 211490 loss: 0.0022 lr: 0.02\n",
      "iteration: 211500 loss: 0.0021 lr: 0.02\n",
      "iteration: 211510 loss: 0.0019 lr: 0.02\n",
      "iteration: 211520 loss: 0.0019 lr: 0.02\n",
      "iteration: 211530 loss: 0.0018 lr: 0.02\n",
      "iteration: 211540 loss: 0.0022 lr: 0.02\n",
      "iteration: 211550 loss: 0.0014 lr: 0.02\n",
      "iteration: 211560 loss: 0.0019 lr: 0.02\n",
      "iteration: 211570 loss: 0.0017 lr: 0.02\n",
      "iteration: 211580 loss: 0.0021 lr: 0.02\n",
      "iteration: 211590 loss: 0.0016 lr: 0.02\n",
      "iteration: 211600 loss: 0.0015 lr: 0.02\n",
      "iteration: 211610 loss: 0.0025 lr: 0.02\n",
      "iteration: 211620 loss: 0.0017 lr: 0.02\n",
      "iteration: 211630 loss: 0.0019 lr: 0.02\n",
      "iteration: 211640 loss: 0.0015 lr: 0.02\n",
      "iteration: 211650 loss: 0.0021 lr: 0.02\n",
      "iteration: 211660 loss: 0.0021 lr: 0.02\n",
      "iteration: 211670 loss: 0.0027 lr: 0.02\n",
      "iteration: 211680 loss: 0.0013 lr: 0.02\n",
      "iteration: 211690 loss: 0.0017 lr: 0.02\n",
      "iteration: 211700 loss: 0.0020 lr: 0.02\n",
      "iteration: 211710 loss: 0.0020 lr: 0.02\n",
      "iteration: 211720 loss: 0.0019 lr: 0.02\n",
      "iteration: 211730 loss: 0.0018 lr: 0.02\n",
      "iteration: 211740 loss: 0.0015 lr: 0.02\n",
      "iteration: 211750 loss: 0.0016 lr: 0.02\n",
      "iteration: 211760 loss: 0.0017 lr: 0.02\n",
      "iteration: 211770 loss: 0.0019 lr: 0.02\n",
      "iteration: 211780 loss: 0.0023 lr: 0.02\n",
      "iteration: 211790 loss: 0.0026 lr: 0.02\n",
      "iteration: 211800 loss: 0.0021 lr: 0.02\n",
      "iteration: 211810 loss: 0.0021 lr: 0.02\n",
      "iteration: 211820 loss: 0.0028 lr: 0.02\n",
      "iteration: 211830 loss: 0.0016 lr: 0.02\n",
      "iteration: 211840 loss: 0.0023 lr: 0.02\n",
      "iteration: 211850 loss: 0.0022 lr: 0.02\n",
      "iteration: 211860 loss: 0.0029 lr: 0.02\n",
      "iteration: 211870 loss: 0.0026 lr: 0.02\n",
      "iteration: 211880 loss: 0.0028 lr: 0.02\n",
      "iteration: 211890 loss: 0.0020 lr: 0.02\n",
      "iteration: 211900 loss: 0.0021 lr: 0.02\n",
      "iteration: 211910 loss: 0.0019 lr: 0.02\n",
      "iteration: 211920 loss: 0.0021 lr: 0.02\n",
      "iteration: 211930 loss: 0.0018 lr: 0.02\n",
      "iteration: 211940 loss: 0.0014 lr: 0.02\n",
      "iteration: 211950 loss: 0.0014 lr: 0.02\n",
      "iteration: 211960 loss: 0.0023 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 211970 loss: 0.0019 lr: 0.02\n",
      "iteration: 211980 loss: 0.0023 lr: 0.02\n",
      "iteration: 211990 loss: 0.0016 lr: 0.02\n",
      "iteration: 212000 loss: 0.0016 lr: 0.02\n",
      "iteration: 212010 loss: 0.0018 lr: 0.02\n",
      "iteration: 212020 loss: 0.0018 lr: 0.02\n",
      "iteration: 212030 loss: 0.0026 lr: 0.02\n",
      "iteration: 212040 loss: 0.0025 lr: 0.02\n",
      "iteration: 212050 loss: 0.0028 lr: 0.02\n",
      "iteration: 212060 loss: 0.0024 lr: 0.02\n",
      "iteration: 212070 loss: 0.0022 lr: 0.02\n",
      "iteration: 212080 loss: 0.0019 lr: 0.02\n",
      "iteration: 212090 loss: 0.0020 lr: 0.02\n",
      "iteration: 212100 loss: 0.0022 lr: 0.02\n",
      "iteration: 212110 loss: 0.0027 lr: 0.02\n",
      "iteration: 212120 loss: 0.0021 lr: 0.02\n",
      "iteration: 212130 loss: 0.0020 lr: 0.02\n",
      "iteration: 212140 loss: 0.0021 lr: 0.02\n",
      "iteration: 212150 loss: 0.0020 lr: 0.02\n",
      "iteration: 212160 loss: 0.0015 lr: 0.02\n",
      "iteration: 212170 loss: 0.0020 lr: 0.02\n",
      "iteration: 212180 loss: 0.0021 lr: 0.02\n",
      "iteration: 212190 loss: 0.0015 lr: 0.02\n",
      "iteration: 212200 loss: 0.0017 lr: 0.02\n",
      "iteration: 212210 loss: 0.0021 lr: 0.02\n",
      "iteration: 212220 loss: 0.0022 lr: 0.02\n",
      "iteration: 212230 loss: 0.0015 lr: 0.02\n",
      "iteration: 212240 loss: 0.0017 lr: 0.02\n",
      "iteration: 212250 loss: 0.0029 lr: 0.02\n",
      "iteration: 212260 loss: 0.0019 lr: 0.02\n",
      "iteration: 212270 loss: 0.0027 lr: 0.02\n",
      "iteration: 212280 loss: 0.0024 lr: 0.02\n",
      "iteration: 212290 loss: 0.0022 lr: 0.02\n",
      "iteration: 212300 loss: 0.0024 lr: 0.02\n",
      "iteration: 212310 loss: 0.0017 lr: 0.02\n",
      "iteration: 212320 loss: 0.0019 lr: 0.02\n",
      "iteration: 212330 loss: 0.0019 lr: 0.02\n",
      "iteration: 212340 loss: 0.0018 lr: 0.02\n",
      "iteration: 212350 loss: 0.0019 lr: 0.02\n",
      "iteration: 212360 loss: 0.0041 lr: 0.02\n",
      "iteration: 212370 loss: 0.0026 lr: 0.02\n",
      "iteration: 212380 loss: 0.0018 lr: 0.02\n",
      "iteration: 212390 loss: 0.0021 lr: 0.02\n",
      "iteration: 212400 loss: 0.0020 lr: 0.02\n",
      "iteration: 212410 loss: 0.0025 lr: 0.02\n",
      "iteration: 212420 loss: 0.0020 lr: 0.02\n",
      "iteration: 212430 loss: 0.0024 lr: 0.02\n",
      "iteration: 212440 loss: 0.0022 lr: 0.02\n",
      "iteration: 212450 loss: 0.0022 lr: 0.02\n",
      "iteration: 212460 loss: 0.0029 lr: 0.02\n",
      "iteration: 212470 loss: 0.0028 lr: 0.02\n",
      "iteration: 212480 loss: 0.0025 lr: 0.02\n",
      "iteration: 212490 loss: 0.0031 lr: 0.02\n",
      "iteration: 212500 loss: 0.0014 lr: 0.02\n",
      "iteration: 212510 loss: 0.0019 lr: 0.02\n",
      "iteration: 212520 loss: 0.0019 lr: 0.02\n",
      "iteration: 212530 loss: 0.0015 lr: 0.02\n",
      "iteration: 212540 loss: 0.0020 lr: 0.02\n",
      "iteration: 212550 loss: 0.0017 lr: 0.02\n",
      "iteration: 212560 loss: 0.0020 lr: 0.02\n",
      "iteration: 212570 loss: 0.0016 lr: 0.02\n",
      "iteration: 212580 loss: 0.0019 lr: 0.02\n",
      "iteration: 212590 loss: 0.0016 lr: 0.02\n",
      "iteration: 212600 loss: 0.0012 lr: 0.02\n",
      "iteration: 212610 loss: 0.0017 lr: 0.02\n",
      "iteration: 212620 loss: 0.0021 lr: 0.02\n",
      "iteration: 212630 loss: 0.0024 lr: 0.02\n",
      "iteration: 212640 loss: 0.0027 lr: 0.02\n",
      "iteration: 212650 loss: 0.0027 lr: 0.02\n",
      "iteration: 212660 loss: 0.0019 lr: 0.02\n",
      "iteration: 212670 loss: 0.0022 lr: 0.02\n",
      "iteration: 212680 loss: 0.0017 lr: 0.02\n",
      "iteration: 212690 loss: 0.0024 lr: 0.02\n",
      "iteration: 212700 loss: 0.0025 lr: 0.02\n",
      "iteration: 212710 loss: 0.0018 lr: 0.02\n",
      "iteration: 212720 loss: 0.0022 lr: 0.02\n",
      "iteration: 212730 loss: 0.0023 lr: 0.02\n",
      "iteration: 212740 loss: 0.0022 lr: 0.02\n",
      "iteration: 212750 loss: 0.0015 lr: 0.02\n",
      "iteration: 212760 loss: 0.0018 lr: 0.02\n",
      "iteration: 212770 loss: 0.0021 lr: 0.02\n",
      "iteration: 212780 loss: 0.0030 lr: 0.02\n",
      "iteration: 212790 loss: 0.0025 lr: 0.02\n",
      "iteration: 212800 loss: 0.0018 lr: 0.02\n",
      "iteration: 212810 loss: 0.0033 lr: 0.02\n",
      "iteration: 212820 loss: 0.0024 lr: 0.02\n",
      "iteration: 212830 loss: 0.0023 lr: 0.02\n",
      "iteration: 212840 loss: 0.0028 lr: 0.02\n",
      "iteration: 212850 loss: 0.0016 lr: 0.02\n",
      "iteration: 212860 loss: 0.0020 lr: 0.02\n",
      "iteration: 212870 loss: 0.0024 lr: 0.02\n",
      "iteration: 212880 loss: 0.0024 lr: 0.02\n",
      "iteration: 212890 loss: 0.0025 lr: 0.02\n",
      "iteration: 212900 loss: 0.0026 lr: 0.02\n",
      "iteration: 212910 loss: 0.0019 lr: 0.02\n",
      "iteration: 212920 loss: 0.0020 lr: 0.02\n",
      "iteration: 212930 loss: 0.0025 lr: 0.02\n",
      "iteration: 212940 loss: 0.0017 lr: 0.02\n",
      "iteration: 212950 loss: 0.0029 lr: 0.02\n",
      "iteration: 212960 loss: 0.0017 lr: 0.02\n",
      "iteration: 212970 loss: 0.0024 lr: 0.02\n",
      "iteration: 212980 loss: 0.0024 lr: 0.02\n",
      "iteration: 212990 loss: 0.0024 lr: 0.02\n",
      "iteration: 213000 loss: 0.0022 lr: 0.02\n",
      "iteration: 213010 loss: 0.0023 lr: 0.02\n",
      "iteration: 213020 loss: 0.0025 lr: 0.02\n",
      "iteration: 213030 loss: 0.0029 lr: 0.02\n",
      "iteration: 213040 loss: 0.0014 lr: 0.02\n",
      "iteration: 213050 loss: 0.0023 lr: 0.02\n",
      "iteration: 213060 loss: 0.0016 lr: 0.02\n",
      "iteration: 213070 loss: 0.0027 lr: 0.02\n",
      "iteration: 213080 loss: 0.0020 lr: 0.02\n",
      "iteration: 213090 loss: 0.0023 lr: 0.02\n",
      "iteration: 213100 loss: 0.0021 lr: 0.02\n",
      "iteration: 213110 loss: 0.0029 lr: 0.02\n",
      "iteration: 213120 loss: 0.0020 lr: 0.02\n",
      "iteration: 213130 loss: 0.0022 lr: 0.02\n",
      "iteration: 213140 loss: 0.0014 lr: 0.02\n",
      "iteration: 213150 loss: 0.0026 lr: 0.02\n",
      "iteration: 213160 loss: 0.0020 lr: 0.02\n",
      "iteration: 213170 loss: 0.0028 lr: 0.02\n",
      "iteration: 213180 loss: 0.0016 lr: 0.02\n",
      "iteration: 213190 loss: 0.0024 lr: 0.02\n",
      "iteration: 213200 loss: 0.0022 lr: 0.02\n",
      "iteration: 213210 loss: 0.0023 lr: 0.02\n",
      "iteration: 213220 loss: 0.0035 lr: 0.02\n",
      "iteration: 213230 loss: 0.0024 lr: 0.02\n",
      "iteration: 213240 loss: 0.0023 lr: 0.02\n",
      "iteration: 213250 loss: 0.0022 lr: 0.02\n",
      "iteration: 213260 loss: 0.0023 lr: 0.02\n",
      "iteration: 213270 loss: 0.0028 lr: 0.02\n",
      "iteration: 213280 loss: 0.0025 lr: 0.02\n",
      "iteration: 213290 loss: 0.0018 lr: 0.02\n",
      "iteration: 213300 loss: 0.0025 lr: 0.02\n",
      "iteration: 213310 loss: 0.0021 lr: 0.02\n",
      "iteration: 213320 loss: 0.0020 lr: 0.02\n",
      "iteration: 213330 loss: 0.0023 lr: 0.02\n",
      "iteration: 213340 loss: 0.0018 lr: 0.02\n",
      "iteration: 213350 loss: 0.0020 lr: 0.02\n",
      "iteration: 213360 loss: 0.0017 lr: 0.02\n",
      "iteration: 213370 loss: 0.0018 lr: 0.02\n",
      "iteration: 213380 loss: 0.0020 lr: 0.02\n",
      "iteration: 213390 loss: 0.0014 lr: 0.02\n",
      "iteration: 213400 loss: 0.0022 lr: 0.02\n",
      "iteration: 213410 loss: 0.0025 lr: 0.02\n",
      "iteration: 213420 loss: 0.0013 lr: 0.02\n",
      "iteration: 213430 loss: 0.0013 lr: 0.02\n",
      "iteration: 213440 loss: 0.0019 lr: 0.02\n",
      "iteration: 213450 loss: 0.0017 lr: 0.02\n",
      "iteration: 213460 loss: 0.0016 lr: 0.02\n",
      "iteration: 213470 loss: 0.0023 lr: 0.02\n",
      "iteration: 213480 loss: 0.0017 lr: 0.02\n",
      "iteration: 213490 loss: 0.0023 lr: 0.02\n",
      "iteration: 213500 loss: 0.0017 lr: 0.02\n",
      "iteration: 213510 loss: 0.0019 lr: 0.02\n",
      "iteration: 213520 loss: 0.0028 lr: 0.02\n",
      "iteration: 213530 loss: 0.0025 lr: 0.02\n",
      "iteration: 213540 loss: 0.0022 lr: 0.02\n",
      "iteration: 213550 loss: 0.0023 lr: 0.02\n",
      "iteration: 213560 loss: 0.0027 lr: 0.02\n",
      "iteration: 213570 loss: 0.0016 lr: 0.02\n",
      "iteration: 213580 loss: 0.0034 lr: 0.02\n",
      "iteration: 213590 loss: 0.0030 lr: 0.02\n",
      "iteration: 213600 loss: 0.0018 lr: 0.02\n",
      "iteration: 213610 loss: 0.0034 lr: 0.02\n",
      "iteration: 213620 loss: 0.0027 lr: 0.02\n",
      "iteration: 213630 loss: 0.0016 lr: 0.02\n",
      "iteration: 213640 loss: 0.0020 lr: 0.02\n",
      "iteration: 213650 loss: 0.0021 lr: 0.02\n",
      "iteration: 213660 loss: 0.0020 lr: 0.02\n",
      "iteration: 213670 loss: 0.0020 lr: 0.02\n",
      "iteration: 213680 loss: 0.0015 lr: 0.02\n",
      "iteration: 213690 loss: 0.0020 lr: 0.02\n",
      "iteration: 213700 loss: 0.0015 lr: 0.02\n",
      "iteration: 213710 loss: 0.0015 lr: 0.02\n",
      "iteration: 213720 loss: 0.0018 lr: 0.02\n",
      "iteration: 213730 loss: 0.0025 lr: 0.02\n",
      "iteration: 213740 loss: 0.0012 lr: 0.02\n",
      "iteration: 213750 loss: 0.0024 lr: 0.02\n",
      "iteration: 213760 loss: 0.0019 lr: 0.02\n",
      "iteration: 213770 loss: 0.0015 lr: 0.02\n",
      "iteration: 213780 loss: 0.0018 lr: 0.02\n",
      "iteration: 213790 loss: 0.0017 lr: 0.02\n",
      "iteration: 213800 loss: 0.0013 lr: 0.02\n",
      "iteration: 213810 loss: 0.0029 lr: 0.02\n",
      "iteration: 213820 loss: 0.0019 lr: 0.02\n",
      "iteration: 213830 loss: 0.0020 lr: 0.02\n",
      "iteration: 213840 loss: 0.0016 lr: 0.02\n",
      "iteration: 213850 loss: 0.0017 lr: 0.02\n",
      "iteration: 213860 loss: 0.0022 lr: 0.02\n",
      "iteration: 213870 loss: 0.0017 lr: 0.02\n",
      "iteration: 213880 loss: 0.0020 lr: 0.02\n",
      "iteration: 213890 loss: 0.0019 lr: 0.02\n",
      "iteration: 213900 loss: 0.0026 lr: 0.02\n",
      "iteration: 213910 loss: 0.0016 lr: 0.02\n",
      "iteration: 213920 loss: 0.0024 lr: 0.02\n",
      "iteration: 213930 loss: 0.0021 lr: 0.02\n",
      "iteration: 213940 loss: 0.0024 lr: 0.02\n",
      "iteration: 213950 loss: 0.0026 lr: 0.02\n",
      "iteration: 213960 loss: 0.0020 lr: 0.02\n",
      "iteration: 213970 loss: 0.0030 lr: 0.02\n",
      "iteration: 213980 loss: 0.0015 lr: 0.02\n",
      "iteration: 213990 loss: 0.0019 lr: 0.02\n",
      "iteration: 214000 loss: 0.0037 lr: 0.02\n",
      "iteration: 214010 loss: 0.0017 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 214020 loss: 0.0012 lr: 0.02\n",
      "iteration: 214030 loss: 0.0017 lr: 0.02\n",
      "iteration: 214040 loss: 0.0021 lr: 0.02\n",
      "iteration: 214050 loss: 0.0018 lr: 0.02\n",
      "iteration: 214060 loss: 0.0026 lr: 0.02\n",
      "iteration: 214070 loss: 0.0017 lr: 0.02\n",
      "iteration: 214080 loss: 0.0021 lr: 0.02\n",
      "iteration: 214090 loss: 0.0017 lr: 0.02\n",
      "iteration: 214100 loss: 0.0018 lr: 0.02\n",
      "iteration: 214110 loss: 0.0020 lr: 0.02\n",
      "iteration: 214120 loss: 0.0017 lr: 0.02\n",
      "iteration: 214130 loss: 0.0017 lr: 0.02\n",
      "iteration: 214140 loss: 0.0018 lr: 0.02\n",
      "iteration: 214150 loss: 0.0014 lr: 0.02\n",
      "iteration: 214160 loss: 0.0024 lr: 0.02\n",
      "iteration: 214170 loss: 0.0022 lr: 0.02\n",
      "iteration: 214180 loss: 0.0019 lr: 0.02\n",
      "iteration: 214190 loss: 0.0016 lr: 0.02\n",
      "iteration: 214200 loss: 0.0016 lr: 0.02\n",
      "iteration: 214210 loss: 0.0035 lr: 0.02\n",
      "iteration: 214220 loss: 0.0021 lr: 0.02\n",
      "iteration: 214230 loss: 0.0026 lr: 0.02\n",
      "iteration: 214240 loss: 0.0021 lr: 0.02\n",
      "iteration: 214250 loss: 0.0022 lr: 0.02\n",
      "iteration: 214260 loss: 0.0020 lr: 0.02\n",
      "iteration: 214270 loss: 0.0021 lr: 0.02\n",
      "iteration: 214280 loss: 0.0017 lr: 0.02\n",
      "iteration: 214290 loss: 0.0017 lr: 0.02\n",
      "iteration: 214300 loss: 0.0022 lr: 0.02\n",
      "iteration: 214310 loss: 0.0024 lr: 0.02\n",
      "iteration: 214320 loss: 0.0032 lr: 0.02\n",
      "iteration: 214330 loss: 0.0022 lr: 0.02\n",
      "iteration: 214340 loss: 0.0014 lr: 0.02\n",
      "iteration: 214350 loss: 0.0017 lr: 0.02\n",
      "iteration: 214360 loss: 0.0019 lr: 0.02\n",
      "iteration: 214370 loss: 0.0018 lr: 0.02\n",
      "iteration: 214380 loss: 0.0013 lr: 0.02\n",
      "iteration: 214390 loss: 0.0019 lr: 0.02\n",
      "iteration: 214400 loss: 0.0018 lr: 0.02\n",
      "iteration: 214410 loss: 0.0016 lr: 0.02\n",
      "iteration: 214420 loss: 0.0012 lr: 0.02\n",
      "iteration: 214430 loss: 0.0025 lr: 0.02\n",
      "iteration: 214440 loss: 0.0019 lr: 0.02\n",
      "iteration: 214450 loss: 0.0016 lr: 0.02\n",
      "iteration: 214460 loss: 0.0021 lr: 0.02\n",
      "iteration: 214470 loss: 0.0024 lr: 0.02\n",
      "iteration: 214480 loss: 0.0018 lr: 0.02\n",
      "iteration: 214490 loss: 0.0016 lr: 0.02\n",
      "iteration: 214500 loss: 0.0022 lr: 0.02\n",
      "iteration: 214510 loss: 0.0019 lr: 0.02\n",
      "iteration: 214520 loss: 0.0016 lr: 0.02\n",
      "iteration: 214530 loss: 0.0017 lr: 0.02\n",
      "iteration: 214540 loss: 0.0018 lr: 0.02\n",
      "iteration: 214550 loss: 0.0022 lr: 0.02\n",
      "iteration: 214560 loss: 0.0028 lr: 0.02\n",
      "iteration: 214570 loss: 0.0018 lr: 0.02\n",
      "iteration: 214580 loss: 0.0025 lr: 0.02\n",
      "iteration: 214590 loss: 0.0019 lr: 0.02\n",
      "iteration: 214600 loss: 0.0027 lr: 0.02\n",
      "iteration: 214610 loss: 0.0018 lr: 0.02\n",
      "iteration: 214620 loss: 0.0016 lr: 0.02\n",
      "iteration: 214630 loss: 0.0017 lr: 0.02\n",
      "iteration: 214640 loss: 0.0019 lr: 0.02\n",
      "iteration: 214650 loss: 0.0019 lr: 0.02\n",
      "iteration: 214660 loss: 0.0019 lr: 0.02\n",
      "iteration: 214670 loss: 0.0014 lr: 0.02\n",
      "iteration: 214680 loss: 0.0022 lr: 0.02\n",
      "iteration: 214690 loss: 0.0016 lr: 0.02\n",
      "iteration: 214700 loss: 0.0022 lr: 0.02\n",
      "iteration: 214710 loss: 0.0032 lr: 0.02\n",
      "iteration: 214720 loss: 0.0027 lr: 0.02\n",
      "iteration: 214730 loss: 0.0026 lr: 0.02\n",
      "iteration: 214740 loss: 0.0022 lr: 0.02\n",
      "iteration: 214750 loss: 0.0020 lr: 0.02\n",
      "iteration: 214760 loss: 0.0016 lr: 0.02\n",
      "iteration: 214770 loss: 0.0033 lr: 0.02\n",
      "iteration: 214780 loss: 0.0025 lr: 0.02\n",
      "iteration: 214790 loss: 0.0032 lr: 0.02\n",
      "iteration: 214800 loss: 0.0025 lr: 0.02\n",
      "iteration: 214810 loss: 0.0028 lr: 0.02\n",
      "iteration: 214820 loss: 0.0019 lr: 0.02\n",
      "iteration: 214830 loss: 0.0014 lr: 0.02\n",
      "iteration: 214840 loss: 0.0020 lr: 0.02\n",
      "iteration: 214850 loss: 0.0016 lr: 0.02\n",
      "iteration: 214860 loss: 0.0017 lr: 0.02\n",
      "iteration: 214870 loss: 0.0036 lr: 0.02\n",
      "iteration: 214880 loss: 0.0026 lr: 0.02\n",
      "iteration: 214890 loss: 0.0027 lr: 0.02\n",
      "iteration: 214900 loss: 0.0024 lr: 0.02\n",
      "iteration: 214910 loss: 0.0016 lr: 0.02\n",
      "iteration: 214920 loss: 0.0015 lr: 0.02\n",
      "iteration: 214930 loss: 0.0021 lr: 0.02\n",
      "iteration: 214940 loss: 0.0020 lr: 0.02\n",
      "iteration: 214950 loss: 0.0020 lr: 0.02\n",
      "iteration: 214960 loss: 0.0019 lr: 0.02\n",
      "iteration: 214970 loss: 0.0030 lr: 0.02\n",
      "iteration: 214980 loss: 0.0017 lr: 0.02\n",
      "iteration: 214990 loss: 0.0014 lr: 0.02\n",
      "iteration: 215000 loss: 0.0027 lr: 0.02\n",
      "iteration: 215010 loss: 0.0021 lr: 0.02\n",
      "iteration: 215020 loss: 0.0028 lr: 0.02\n",
      "iteration: 215030 loss: 0.0024 lr: 0.02\n",
      "iteration: 215040 loss: 0.0018 lr: 0.02\n",
      "iteration: 215050 loss: 0.0015 lr: 0.02\n",
      "iteration: 215060 loss: 0.0022 lr: 0.02\n",
      "iteration: 215070 loss: 0.0023 lr: 0.02\n",
      "iteration: 215080 loss: 0.0017 lr: 0.02\n",
      "iteration: 215090 loss: 0.0036 lr: 0.02\n",
      "iteration: 215100 loss: 0.0031 lr: 0.02\n",
      "iteration: 215110 loss: 0.0018 lr: 0.02\n",
      "iteration: 215120 loss: 0.0025 lr: 0.02\n",
      "iteration: 215130 loss: 0.0031 lr: 0.02\n",
      "iteration: 215140 loss: 0.0016 lr: 0.02\n",
      "iteration: 215150 loss: 0.0019 lr: 0.02\n",
      "iteration: 215160 loss: 0.0017 lr: 0.02\n",
      "iteration: 215170 loss: 0.0022 lr: 0.02\n",
      "iteration: 215180 loss: 0.0021 lr: 0.02\n",
      "iteration: 215190 loss: 0.0016 lr: 0.02\n",
      "iteration: 215200 loss: 0.0028 lr: 0.02\n",
      "iteration: 215210 loss: 0.0023 lr: 0.02\n",
      "iteration: 215220 loss: 0.0026 lr: 0.02\n",
      "iteration: 215230 loss: 0.0017 lr: 0.02\n",
      "iteration: 215240 loss: 0.0022 lr: 0.02\n",
      "iteration: 215250 loss: 0.0017 lr: 0.02\n",
      "iteration: 215260 loss: 0.0016 lr: 0.02\n",
      "iteration: 215270 loss: 0.0039 lr: 0.02\n",
      "iteration: 215280 loss: 0.0020 lr: 0.02\n",
      "iteration: 215290 loss: 0.0021 lr: 0.02\n",
      "iteration: 215300 loss: 0.0017 lr: 0.02\n",
      "iteration: 215310 loss: 0.0024 lr: 0.02\n",
      "iteration: 215320 loss: 0.0029 lr: 0.02\n",
      "iteration: 215330 loss: 0.0015 lr: 0.02\n",
      "iteration: 215340 loss: 0.0019 lr: 0.02\n",
      "iteration: 215350 loss: 0.0020 lr: 0.02\n",
      "iteration: 215360 loss: 0.0019 lr: 0.02\n",
      "iteration: 215370 loss: 0.0017 lr: 0.02\n",
      "iteration: 215380 loss: 0.0020 lr: 0.02\n",
      "iteration: 215390 loss: 0.0021 lr: 0.02\n",
      "iteration: 215400 loss: 0.0021 lr: 0.02\n",
      "iteration: 215410 loss: 0.0018 lr: 0.02\n",
      "iteration: 215420 loss: 0.0019 lr: 0.02\n",
      "iteration: 215430 loss: 0.0018 lr: 0.02\n",
      "iteration: 215440 loss: 0.0018 lr: 0.02\n",
      "iteration: 215450 loss: 0.0013 lr: 0.02\n",
      "iteration: 215460 loss: 0.0022 lr: 0.02\n",
      "iteration: 215470 loss: 0.0021 lr: 0.02\n",
      "iteration: 215480 loss: 0.0024 lr: 0.02\n",
      "iteration: 215490 loss: 0.0022 lr: 0.02\n",
      "iteration: 215500 loss: 0.0019 lr: 0.02\n",
      "iteration: 215510 loss: 0.0020 lr: 0.02\n",
      "iteration: 215520 loss: 0.0024 lr: 0.02\n",
      "iteration: 215530 loss: 0.0018 lr: 0.02\n",
      "iteration: 215540 loss: 0.0015 lr: 0.02\n",
      "iteration: 215550 loss: 0.0022 lr: 0.02\n",
      "iteration: 215560 loss: 0.0021 lr: 0.02\n",
      "iteration: 215570 loss: 0.0016 lr: 0.02\n",
      "iteration: 215580 loss: 0.0016 lr: 0.02\n",
      "iteration: 215590 loss: 0.0017 lr: 0.02\n",
      "iteration: 215600 loss: 0.0023 lr: 0.02\n",
      "iteration: 215610 loss: 0.0019 lr: 0.02\n",
      "iteration: 215620 loss: 0.0019 lr: 0.02\n",
      "iteration: 215630 loss: 0.0016 lr: 0.02\n",
      "iteration: 215640 loss: 0.0022 lr: 0.02\n",
      "iteration: 215650 loss: 0.0027 lr: 0.02\n",
      "iteration: 215660 loss: 0.0027 lr: 0.02\n",
      "iteration: 215670 loss: 0.0020 lr: 0.02\n",
      "iteration: 215680 loss: 0.0021 lr: 0.02\n",
      "iteration: 215690 loss: 0.0020 lr: 0.02\n",
      "iteration: 215700 loss: 0.0021 lr: 0.02\n",
      "iteration: 215710 loss: 0.0034 lr: 0.02\n",
      "iteration: 215720 loss: 0.0019 lr: 0.02\n",
      "iteration: 215730 loss: 0.0022 lr: 0.02\n",
      "iteration: 215740 loss: 0.0019 lr: 0.02\n",
      "iteration: 215750 loss: 0.0023 lr: 0.02\n",
      "iteration: 215760 loss: 0.0022 lr: 0.02\n",
      "iteration: 215770 loss: 0.0014 lr: 0.02\n",
      "iteration: 215780 loss: 0.0016 lr: 0.02\n",
      "iteration: 215790 loss: 0.0020 lr: 0.02\n",
      "iteration: 215800 loss: 0.0018 lr: 0.02\n",
      "iteration: 215810 loss: 0.0016 lr: 0.02\n",
      "iteration: 215820 loss: 0.0023 lr: 0.02\n",
      "iteration: 215830 loss: 0.0013 lr: 0.02\n",
      "iteration: 215840 loss: 0.0017 lr: 0.02\n",
      "iteration: 215850 loss: 0.0020 lr: 0.02\n",
      "iteration: 215860 loss: 0.0018 lr: 0.02\n",
      "iteration: 215870 loss: 0.0022 lr: 0.02\n",
      "iteration: 215880 loss: 0.0017 lr: 0.02\n",
      "iteration: 215890 loss: 0.0022 lr: 0.02\n",
      "iteration: 215900 loss: 0.0023 lr: 0.02\n",
      "iteration: 215910 loss: 0.0016 lr: 0.02\n",
      "iteration: 215920 loss: 0.0016 lr: 0.02\n",
      "iteration: 215930 loss: 0.0021 lr: 0.02\n",
      "iteration: 215940 loss: 0.0014 lr: 0.02\n",
      "iteration: 215950 loss: 0.0015 lr: 0.02\n",
      "iteration: 215960 loss: 0.0026 lr: 0.02\n",
      "iteration: 215970 loss: 0.0020 lr: 0.02\n",
      "iteration: 215980 loss: 0.0024 lr: 0.02\n",
      "iteration: 215990 loss: 0.0023 lr: 0.02\n",
      "iteration: 216000 loss: 0.0018 lr: 0.02\n",
      "iteration: 216010 loss: 0.0034 lr: 0.02\n",
      "iteration: 216020 loss: 0.0019 lr: 0.02\n",
      "iteration: 216030 loss: 0.0023 lr: 0.02\n",
      "iteration: 216040 loss: 0.0019 lr: 0.02\n",
      "iteration: 216050 loss: 0.0017 lr: 0.02\n",
      "iteration: 216060 loss: 0.0015 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 216070 loss: 0.0015 lr: 0.02\n",
      "iteration: 216080 loss: 0.0020 lr: 0.02\n",
      "iteration: 216090 loss: 0.0017 lr: 0.02\n",
      "iteration: 216100 loss: 0.0025 lr: 0.02\n",
      "iteration: 216110 loss: 0.0019 lr: 0.02\n",
      "iteration: 216120 loss: 0.0015 lr: 0.02\n",
      "iteration: 216130 loss: 0.0017 lr: 0.02\n",
      "iteration: 216140 loss: 0.0021 lr: 0.02\n",
      "iteration: 216150 loss: 0.0017 lr: 0.02\n",
      "iteration: 216160 loss: 0.0017 lr: 0.02\n",
      "iteration: 216170 loss: 0.0029 lr: 0.02\n",
      "iteration: 216180 loss: 0.0024 lr: 0.02\n",
      "iteration: 216190 loss: 0.0017 lr: 0.02\n",
      "iteration: 216200 loss: 0.0020 lr: 0.02\n",
      "iteration: 216210 loss: 0.0024 lr: 0.02\n",
      "iteration: 216220 loss: 0.0022 lr: 0.02\n",
      "iteration: 216230 loss: 0.0032 lr: 0.02\n",
      "iteration: 216240 loss: 0.0020 lr: 0.02\n",
      "iteration: 216250 loss: 0.0019 lr: 0.02\n",
      "iteration: 216260 loss: 0.0018 lr: 0.02\n",
      "iteration: 216270 loss: 0.0018 lr: 0.02\n",
      "iteration: 216280 loss: 0.0021 lr: 0.02\n",
      "iteration: 216290 loss: 0.0017 lr: 0.02\n",
      "iteration: 216300 loss: 0.0027 lr: 0.02\n",
      "iteration: 216310 loss: 0.0022 lr: 0.02\n",
      "iteration: 216320 loss: 0.0025 lr: 0.02\n",
      "iteration: 216330 loss: 0.0018 lr: 0.02\n",
      "iteration: 216340 loss: 0.0016 lr: 0.02\n",
      "iteration: 216350 loss: 0.0032 lr: 0.02\n",
      "iteration: 216360 loss: 0.0025 lr: 0.02\n",
      "iteration: 216370 loss: 0.0026 lr: 0.02\n",
      "iteration: 216380 loss: 0.0021 lr: 0.02\n",
      "iteration: 216390 loss: 0.0019 lr: 0.02\n",
      "iteration: 216400 loss: 0.0020 lr: 0.02\n",
      "iteration: 216410 loss: 0.0020 lr: 0.02\n",
      "iteration: 216420 loss: 0.0022 lr: 0.02\n",
      "iteration: 216430 loss: 0.0019 lr: 0.02\n",
      "iteration: 216440 loss: 0.0016 lr: 0.02\n",
      "iteration: 216450 loss: 0.0020 lr: 0.02\n",
      "iteration: 216460 loss: 0.0016 lr: 0.02\n",
      "iteration: 216470 loss: 0.0017 lr: 0.02\n",
      "iteration: 216480 loss: 0.0019 lr: 0.02\n",
      "iteration: 216490 loss: 0.0020 lr: 0.02\n",
      "iteration: 216500 loss: 0.0017 lr: 0.02\n",
      "iteration: 216510 loss: 0.0024 lr: 0.02\n",
      "iteration: 216520 loss: 0.0020 lr: 0.02\n",
      "iteration: 216530 loss: 0.0017 lr: 0.02\n",
      "iteration: 216540 loss: 0.0018 lr: 0.02\n",
      "iteration: 216550 loss: 0.0025 lr: 0.02\n",
      "iteration: 216560 loss: 0.0019 lr: 0.02\n",
      "iteration: 216570 loss: 0.0016 lr: 0.02\n",
      "iteration: 216580 loss: 0.0019 lr: 0.02\n",
      "iteration: 216590 loss: 0.0019 lr: 0.02\n",
      "iteration: 216600 loss: 0.0019 lr: 0.02\n",
      "iteration: 216610 loss: 0.0019 lr: 0.02\n",
      "iteration: 216620 loss: 0.0017 lr: 0.02\n",
      "iteration: 216630 loss: 0.0020 lr: 0.02\n",
      "iteration: 216640 loss: 0.0016 lr: 0.02\n",
      "iteration: 216650 loss: 0.0019 lr: 0.02\n",
      "iteration: 216660 loss: 0.0014 lr: 0.02\n",
      "iteration: 216670 loss: 0.0022 lr: 0.02\n",
      "iteration: 216680 loss: 0.0016 lr: 0.02\n",
      "iteration: 216690 loss: 0.0020 lr: 0.02\n",
      "iteration: 216700 loss: 0.0017 lr: 0.02\n",
      "iteration: 216710 loss: 0.0019 lr: 0.02\n",
      "iteration: 216720 loss: 0.0011 lr: 0.02\n",
      "iteration: 216730 loss: 0.0011 lr: 0.02\n",
      "iteration: 216740 loss: 0.0018 lr: 0.02\n",
      "iteration: 216750 loss: 0.0022 lr: 0.02\n",
      "iteration: 216760 loss: 0.0016 lr: 0.02\n",
      "iteration: 216770 loss: 0.0022 lr: 0.02\n",
      "iteration: 216780 loss: 0.0031 lr: 0.02\n",
      "iteration: 216790 loss: 0.0022 lr: 0.02\n",
      "iteration: 216800 loss: 0.0015 lr: 0.02\n",
      "iteration: 216810 loss: 0.0047 lr: 0.02\n",
      "iteration: 216820 loss: 0.0028 lr: 0.02\n",
      "iteration: 216830 loss: 0.0017 lr: 0.02\n",
      "iteration: 216840 loss: 0.0028 lr: 0.02\n",
      "iteration: 216850 loss: 0.0025 lr: 0.02\n",
      "iteration: 216860 loss: 0.0025 lr: 0.02\n",
      "iteration: 216870 loss: 0.0024 lr: 0.02\n",
      "iteration: 216880 loss: 0.0014 lr: 0.02\n",
      "iteration: 216890 loss: 0.0024 lr: 0.02\n",
      "iteration: 216900 loss: 0.0020 lr: 0.02\n",
      "iteration: 216910 loss: 0.0019 lr: 0.02\n",
      "iteration: 216920 loss: 0.0031 lr: 0.02\n",
      "iteration: 216930 loss: 0.0019 lr: 0.02\n",
      "iteration: 216940 loss: 0.0019 lr: 0.02\n",
      "iteration: 216950 loss: 0.0018 lr: 0.02\n",
      "iteration: 216960 loss: 0.0023 lr: 0.02\n",
      "iteration: 216970 loss: 0.0021 lr: 0.02\n",
      "iteration: 216980 loss: 0.0020 lr: 0.02\n",
      "iteration: 216990 loss: 0.0015 lr: 0.02\n",
      "iteration: 217000 loss: 0.0015 lr: 0.02\n",
      "iteration: 217010 loss: 0.0014 lr: 0.02\n",
      "iteration: 217020 loss: 0.0015 lr: 0.02\n",
      "iteration: 217030 loss: 0.0017 lr: 0.02\n",
      "iteration: 217040 loss: 0.0021 lr: 0.02\n",
      "iteration: 217050 loss: 0.0026 lr: 0.02\n",
      "iteration: 217060 loss: 0.0020 lr: 0.02\n",
      "iteration: 217070 loss: 0.0023 lr: 0.02\n",
      "iteration: 217080 loss: 0.0022 lr: 0.02\n",
      "iteration: 217090 loss: 0.0024 lr: 0.02\n",
      "iteration: 217100 loss: 0.0018 lr: 0.02\n",
      "iteration: 217110 loss: 0.0019 lr: 0.02\n",
      "iteration: 217120 loss: 0.0018 lr: 0.02\n",
      "iteration: 217130 loss: 0.0021 lr: 0.02\n",
      "iteration: 217140 loss: 0.0032 lr: 0.02\n",
      "iteration: 217150 loss: 0.0028 lr: 0.02\n",
      "iteration: 217160 loss: 0.0015 lr: 0.02\n",
      "iteration: 217170 loss: 0.0033 lr: 0.02\n",
      "iteration: 217180 loss: 0.0020 lr: 0.02\n",
      "iteration: 217190 loss: 0.0017 lr: 0.02\n",
      "iteration: 217200 loss: 0.0018 lr: 0.02\n",
      "iteration: 217210 loss: 0.0024 lr: 0.02\n",
      "iteration: 217220 loss: 0.0017 lr: 0.02\n",
      "iteration: 217230 loss: 0.0025 lr: 0.02\n",
      "iteration: 217240 loss: 0.0023 lr: 0.02\n",
      "iteration: 217250 loss: 0.0022 lr: 0.02\n",
      "iteration: 217260 loss: 0.0020 lr: 0.02\n",
      "iteration: 217270 loss: 0.0025 lr: 0.02\n",
      "iteration: 217280 loss: 0.0023 lr: 0.02\n",
      "iteration: 217290 loss: 0.0027 lr: 0.02\n",
      "iteration: 217300 loss: 0.0031 lr: 0.02\n",
      "iteration: 217310 loss: 0.0025 lr: 0.02\n",
      "iteration: 217320 loss: 0.0026 lr: 0.02\n",
      "iteration: 217330 loss: 0.0015 lr: 0.02\n",
      "iteration: 217340 loss: 0.0026 lr: 0.02\n",
      "iteration: 217350 loss: 0.0028 lr: 0.02\n",
      "iteration: 217360 loss: 0.0034 lr: 0.02\n",
      "iteration: 217370 loss: 0.0018 lr: 0.02\n",
      "iteration: 217380 loss: 0.0018 lr: 0.02\n",
      "iteration: 217390 loss: 0.0017 lr: 0.02\n",
      "iteration: 217400 loss: 0.0020 lr: 0.02\n",
      "iteration: 217410 loss: 0.0027 lr: 0.02\n",
      "iteration: 217420 loss: 0.0023 lr: 0.02\n",
      "iteration: 217430 loss: 0.0034 lr: 0.02\n",
      "iteration: 217440 loss: 0.0017 lr: 0.02\n",
      "iteration: 217450 loss: 0.0022 lr: 0.02\n",
      "iteration: 217460 loss: 0.0017 lr: 0.02\n",
      "iteration: 217470 loss: 0.0016 lr: 0.02\n",
      "iteration: 217480 loss: 0.0015 lr: 0.02\n",
      "iteration: 217490 loss: 0.0015 lr: 0.02\n",
      "iteration: 217500 loss: 0.0019 lr: 0.02\n",
      "iteration: 217510 loss: 0.0017 lr: 0.02\n",
      "iteration: 217520 loss: 0.0017 lr: 0.02\n",
      "iteration: 217530 loss: 0.0016 lr: 0.02\n",
      "iteration: 217540 loss: 0.0016 lr: 0.02\n",
      "iteration: 217550 loss: 0.0017 lr: 0.02\n",
      "iteration: 217560 loss: 0.0023 lr: 0.02\n",
      "iteration: 217570 loss: 0.0024 lr: 0.02\n",
      "iteration: 217580 loss: 0.0023 lr: 0.02\n",
      "iteration: 217590 loss: 0.0025 lr: 0.02\n",
      "iteration: 217600 loss: 0.0017 lr: 0.02\n",
      "iteration: 217610 loss: 0.0021 lr: 0.02\n",
      "iteration: 217620 loss: 0.0018 lr: 0.02\n",
      "iteration: 217630 loss: 0.0012 lr: 0.02\n",
      "iteration: 217640 loss: 0.0028 lr: 0.02\n",
      "iteration: 217650 loss: 0.0022 lr: 0.02\n",
      "iteration: 217660 loss: 0.0016 lr: 0.02\n",
      "iteration: 217670 loss: 0.0018 lr: 0.02\n",
      "iteration: 217680 loss: 0.0015 lr: 0.02\n",
      "iteration: 217690 loss: 0.0022 lr: 0.02\n",
      "iteration: 217700 loss: 0.0019 lr: 0.02\n",
      "iteration: 217710 loss: 0.0020 lr: 0.02\n",
      "iteration: 217720 loss: 0.0026 lr: 0.02\n",
      "iteration: 217730 loss: 0.0022 lr: 0.02\n",
      "iteration: 217740 loss: 0.0021 lr: 0.02\n",
      "iteration: 217750 loss: 0.0016 lr: 0.02\n",
      "iteration: 217760 loss: 0.0021 lr: 0.02\n",
      "iteration: 217770 loss: 0.0016 lr: 0.02\n",
      "iteration: 217780 loss: 0.0017 lr: 0.02\n",
      "iteration: 217790 loss: 0.0013 lr: 0.02\n",
      "iteration: 217800 loss: 0.0019 lr: 0.02\n",
      "iteration: 217810 loss: 0.0019 lr: 0.02\n",
      "iteration: 217820 loss: 0.0015 lr: 0.02\n",
      "iteration: 217830 loss: 0.0022 lr: 0.02\n",
      "iteration: 217840 loss: 0.0016 lr: 0.02\n",
      "iteration: 217850 loss: 0.0012 lr: 0.02\n",
      "iteration: 217860 loss: 0.0013 lr: 0.02\n",
      "iteration: 217870 loss: 0.0021 lr: 0.02\n",
      "iteration: 217880 loss: 0.0020 lr: 0.02\n",
      "iteration: 217890 loss: 0.0018 lr: 0.02\n",
      "iteration: 217900 loss: 0.0027 lr: 0.02\n",
      "iteration: 217910 loss: 0.0029 lr: 0.02\n",
      "iteration: 217920 loss: 0.0017 lr: 0.02\n",
      "iteration: 217930 loss: 0.0019 lr: 0.02\n",
      "iteration: 217940 loss: 0.0019 lr: 0.02\n",
      "iteration: 217950 loss: 0.0024 lr: 0.02\n",
      "iteration: 217960 loss: 0.0026 lr: 0.02\n",
      "iteration: 217970 loss: 0.0018 lr: 0.02\n",
      "iteration: 217980 loss: 0.0024 lr: 0.02\n",
      "iteration: 217990 loss: 0.0019 lr: 0.02\n",
      "iteration: 218000 loss: 0.0015 lr: 0.02\n",
      "iteration: 218010 loss: 0.0017 lr: 0.02\n",
      "iteration: 218020 loss: 0.0022 lr: 0.02\n",
      "iteration: 218030 loss: 0.0036 lr: 0.02\n",
      "iteration: 218040 loss: 0.0023 lr: 0.02\n",
      "iteration: 218050 loss: 0.0020 lr: 0.02\n",
      "iteration: 218060 loss: 0.0016 lr: 0.02\n",
      "iteration: 218070 loss: 0.0020 lr: 0.02\n",
      "iteration: 218080 loss: 0.0017 lr: 0.02\n",
      "iteration: 218090 loss: 0.0025 lr: 0.02\n",
      "iteration: 218100 loss: 0.0035 lr: 0.02\n",
      "iteration: 218110 loss: 0.0022 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 218120 loss: 0.0019 lr: 0.02\n",
      "iteration: 218130 loss: 0.0031 lr: 0.02\n",
      "iteration: 218140 loss: 0.0017 lr: 0.02\n",
      "iteration: 218150 loss: 0.0027 lr: 0.02\n",
      "iteration: 218160 loss: 0.0026 lr: 0.02\n",
      "iteration: 218170 loss: 0.0021 lr: 0.02\n",
      "iteration: 218180 loss: 0.0026 lr: 0.02\n",
      "iteration: 218190 loss: 0.0027 lr: 0.02\n",
      "iteration: 218200 loss: 0.0024 lr: 0.02\n",
      "iteration: 218210 loss: 0.0014 lr: 0.02\n",
      "iteration: 218220 loss: 0.0021 lr: 0.02\n",
      "iteration: 218230 loss: 0.0022 lr: 0.02\n",
      "iteration: 218240 loss: 0.0020 lr: 0.02\n",
      "iteration: 218250 loss: 0.0018 lr: 0.02\n",
      "iteration: 218260 loss: 0.0018 lr: 0.02\n",
      "iteration: 218270 loss: 0.0018 lr: 0.02\n",
      "iteration: 218280 loss: 0.0017 lr: 0.02\n",
      "iteration: 218290 loss: 0.0015 lr: 0.02\n",
      "iteration: 218300 loss: 0.0019 lr: 0.02\n",
      "iteration: 218310 loss: 0.0015 lr: 0.02\n",
      "iteration: 218320 loss: 0.0025 lr: 0.02\n",
      "iteration: 218330 loss: 0.0018 lr: 0.02\n",
      "iteration: 218340 loss: 0.0024 lr: 0.02\n",
      "iteration: 218350 loss: 0.0021 lr: 0.02\n",
      "iteration: 218360 loss: 0.0018 lr: 0.02\n",
      "iteration: 218370 loss: 0.0020 lr: 0.02\n",
      "iteration: 218380 loss: 0.0019 lr: 0.02\n",
      "iteration: 218390 loss: 0.0022 lr: 0.02\n",
      "iteration: 218400 loss: 0.0019 lr: 0.02\n",
      "iteration: 218410 loss: 0.0017 lr: 0.02\n",
      "iteration: 218420 loss: 0.0019 lr: 0.02\n",
      "iteration: 218430 loss: 0.0017 lr: 0.02\n",
      "iteration: 218440 loss: 0.0014 lr: 0.02\n",
      "iteration: 218450 loss: 0.0021 lr: 0.02\n",
      "iteration: 218460 loss: 0.0019 lr: 0.02\n",
      "iteration: 218470 loss: 0.0016 lr: 0.02\n",
      "iteration: 218480 loss: 0.0022 lr: 0.02\n",
      "iteration: 218490 loss: 0.0018 lr: 0.02\n",
      "iteration: 218500 loss: 0.0017 lr: 0.02\n",
      "iteration: 218510 loss: 0.0019 lr: 0.02\n",
      "iteration: 218520 loss: 0.0028 lr: 0.02\n",
      "iteration: 218530 loss: 0.0020 lr: 0.02\n",
      "iteration: 218540 loss: 0.0024 lr: 0.02\n",
      "iteration: 218550 loss: 0.0019 lr: 0.02\n",
      "iteration: 218560 loss: 0.0017 lr: 0.02\n",
      "iteration: 218570 loss: 0.0015 lr: 0.02\n",
      "iteration: 218580 loss: 0.0017 lr: 0.02\n",
      "iteration: 218590 loss: 0.0035 lr: 0.02\n",
      "iteration: 218600 loss: 0.0031 lr: 0.02\n",
      "iteration: 218610 loss: 0.0015 lr: 0.02\n",
      "iteration: 218620 loss: 0.0019 lr: 0.02\n",
      "iteration: 218630 loss: 0.0021 lr: 0.02\n",
      "iteration: 218640 loss: 0.0022 lr: 0.02\n",
      "iteration: 218650 loss: 0.0025 lr: 0.02\n",
      "iteration: 218660 loss: 0.0018 lr: 0.02\n",
      "iteration: 218670 loss: 0.0029 lr: 0.02\n",
      "iteration: 218680 loss: 0.0020 lr: 0.02\n",
      "iteration: 218690 loss: 0.0018 lr: 0.02\n",
      "iteration: 218700 loss: 0.0023 lr: 0.02\n",
      "iteration: 218710 loss: 0.0021 lr: 0.02\n",
      "iteration: 218720 loss: 0.0023 lr: 0.02\n",
      "iteration: 218730 loss: 0.0020 lr: 0.02\n",
      "iteration: 218740 loss: 0.0016 lr: 0.02\n",
      "iteration: 218750 loss: 0.0023 lr: 0.02\n",
      "iteration: 218760 loss: 0.0019 lr: 0.02\n",
      "iteration: 218770 loss: 0.0022 lr: 0.02\n",
      "iteration: 218780 loss: 0.0026 lr: 0.02\n",
      "iteration: 218790 loss: 0.0026 lr: 0.02\n",
      "iteration: 218800 loss: 0.0020 lr: 0.02\n",
      "iteration: 218810 loss: 0.0021 lr: 0.02\n",
      "iteration: 218820 loss: 0.0013 lr: 0.02\n",
      "iteration: 218830 loss: 0.0018 lr: 0.02\n",
      "iteration: 218840 loss: 0.0012 lr: 0.02\n",
      "iteration: 218850 loss: 0.0025 lr: 0.02\n",
      "iteration: 218860 loss: 0.0022 lr: 0.02\n",
      "iteration: 218870 loss: 0.0020 lr: 0.02\n",
      "iteration: 218880 loss: 0.0022 lr: 0.02\n",
      "iteration: 218890 loss: 0.0022 lr: 0.02\n",
      "iteration: 218900 loss: 0.0020 lr: 0.02\n",
      "iteration: 218910 loss: 0.0022 lr: 0.02\n",
      "iteration: 218920 loss: 0.0020 lr: 0.02\n",
      "iteration: 218930 loss: 0.0020 lr: 0.02\n",
      "iteration: 218940 loss: 0.0026 lr: 0.02\n",
      "iteration: 218950 loss: 0.0021 lr: 0.02\n",
      "iteration: 218960 loss: 0.0025 lr: 0.02\n",
      "iteration: 218970 loss: 0.0019 lr: 0.02\n",
      "iteration: 218980 loss: 0.0018 lr: 0.02\n",
      "iteration: 218990 loss: 0.0019 lr: 0.02\n",
      "iteration: 219000 loss: 0.0028 lr: 0.02\n",
      "iteration: 219010 loss: 0.0033 lr: 0.02\n",
      "iteration: 219020 loss: 0.0021 lr: 0.02\n",
      "iteration: 219030 loss: 0.0022 lr: 0.02\n",
      "iteration: 219040 loss: 0.0029 lr: 0.02\n",
      "iteration: 219050 loss: 0.0017 lr: 0.02\n",
      "iteration: 219060 loss: 0.0015 lr: 0.02\n",
      "iteration: 219070 loss: 0.0039 lr: 0.02\n",
      "iteration: 219080 loss: 0.0022 lr: 0.02\n",
      "iteration: 219090 loss: 0.0020 lr: 0.02\n",
      "iteration: 219100 loss: 0.0015 lr: 0.02\n",
      "iteration: 219110 loss: 0.0025 lr: 0.02\n",
      "iteration: 219120 loss: 0.0021 lr: 0.02\n",
      "iteration: 219130 loss: 0.0016 lr: 0.02\n",
      "iteration: 219140 loss: 0.0017 lr: 0.02\n",
      "iteration: 219150 loss: 0.0028 lr: 0.02\n",
      "iteration: 219160 loss: 0.0044 lr: 0.02\n",
      "iteration: 219170 loss: 0.0020 lr: 0.02\n",
      "iteration: 219180 loss: 0.0021 lr: 0.02\n",
      "iteration: 219190 loss: 0.0015 lr: 0.02\n",
      "iteration: 219200 loss: 0.0024 lr: 0.02\n",
      "iteration: 219210 loss: 0.0027 lr: 0.02\n",
      "iteration: 219220 loss: 0.0017 lr: 0.02\n",
      "iteration: 219230 loss: 0.0016 lr: 0.02\n",
      "iteration: 219240 loss: 0.0016 lr: 0.02\n",
      "iteration: 219250 loss: 0.0017 lr: 0.02\n",
      "iteration: 219260 loss: 0.0016 lr: 0.02\n",
      "iteration: 219270 loss: 0.0019 lr: 0.02\n",
      "iteration: 219280 loss: 0.0021 lr: 0.02\n",
      "iteration: 219290 loss: 0.0020 lr: 0.02\n",
      "iteration: 219300 loss: 0.0024 lr: 0.02\n",
      "iteration: 219310 loss: 0.0016 lr: 0.02\n",
      "iteration: 219320 loss: 0.0017 lr: 0.02\n",
      "iteration: 219330 loss: 0.0028 lr: 0.02\n",
      "iteration: 219340 loss: 0.0015 lr: 0.02\n",
      "iteration: 219350 loss: 0.0024 lr: 0.02\n",
      "iteration: 219360 loss: 0.0014 lr: 0.02\n",
      "iteration: 219370 loss: 0.0019 lr: 0.02\n",
      "iteration: 219380 loss: 0.0020 lr: 0.02\n",
      "iteration: 219390 loss: 0.0015 lr: 0.02\n",
      "iteration: 219400 loss: 0.0026 lr: 0.02\n",
      "iteration: 219410 loss: 0.0015 lr: 0.02\n",
      "iteration: 219420 loss: 0.0015 lr: 0.02\n",
      "iteration: 219430 loss: 0.0030 lr: 0.02\n",
      "iteration: 219440 loss: 0.0016 lr: 0.02\n",
      "iteration: 219450 loss: 0.0022 lr: 0.02\n",
      "iteration: 219460 loss: 0.0016 lr: 0.02\n",
      "iteration: 219470 loss: 0.0018 lr: 0.02\n",
      "iteration: 219480 loss: 0.0018 lr: 0.02\n",
      "iteration: 219490 loss: 0.0022 lr: 0.02\n",
      "iteration: 219500 loss: 0.0024 lr: 0.02\n",
      "iteration: 219510 loss: 0.0018 lr: 0.02\n",
      "iteration: 219520 loss: 0.0023 lr: 0.02\n",
      "iteration: 219530 loss: 0.0015 lr: 0.02\n",
      "iteration: 219540 loss: 0.0019 lr: 0.02\n",
      "iteration: 219550 loss: 0.0017 lr: 0.02\n",
      "iteration: 219560 loss: 0.0019 lr: 0.02\n",
      "iteration: 219570 loss: 0.0026 lr: 0.02\n",
      "iteration: 219580 loss: 0.0020 lr: 0.02\n",
      "iteration: 219590 loss: 0.0026 lr: 0.02\n",
      "iteration: 219600 loss: 0.0020 lr: 0.02\n",
      "iteration: 219610 loss: 0.0016 lr: 0.02\n",
      "iteration: 219620 loss: 0.0019 lr: 0.02\n",
      "iteration: 219630 loss: 0.0019 lr: 0.02\n",
      "iteration: 219640 loss: 0.0015 lr: 0.02\n",
      "iteration: 219650 loss: 0.0016 lr: 0.02\n",
      "iteration: 219660 loss: 0.0017 lr: 0.02\n",
      "iteration: 219670 loss: 0.0018 lr: 0.02\n",
      "iteration: 219680 loss: 0.0017 lr: 0.02\n",
      "iteration: 219690 loss: 0.0017 lr: 0.02\n",
      "iteration: 219700 loss: 0.0016 lr: 0.02\n",
      "iteration: 219710 loss: 0.0016 lr: 0.02\n",
      "iteration: 219720 loss: 0.0017 lr: 0.02\n",
      "iteration: 219730 loss: 0.0021 lr: 0.02\n",
      "iteration: 219740 loss: 0.0023 lr: 0.02\n",
      "iteration: 219750 loss: 0.0018 lr: 0.02\n",
      "iteration: 219760 loss: 0.0023 lr: 0.02\n",
      "iteration: 219770 loss: 0.0020 lr: 0.02\n",
      "iteration: 219780 loss: 0.0018 lr: 0.02\n",
      "iteration: 219790 loss: 0.0022 lr: 0.02\n",
      "iteration: 219800 loss: 0.0015 lr: 0.02\n",
      "iteration: 219810 loss: 0.0017 lr: 0.02\n",
      "iteration: 219820 loss: 0.0021 lr: 0.02\n",
      "iteration: 219830 loss: 0.0025 lr: 0.02\n",
      "iteration: 219840 loss: 0.0022 lr: 0.02\n",
      "iteration: 219850 loss: 0.0024 lr: 0.02\n",
      "iteration: 219860 loss: 0.0013 lr: 0.02\n",
      "iteration: 219870 loss: 0.0027 lr: 0.02\n",
      "iteration: 219880 loss: 0.0028 lr: 0.02\n",
      "iteration: 219890 loss: 0.0017 lr: 0.02\n",
      "iteration: 219900 loss: 0.0021 lr: 0.02\n",
      "iteration: 219910 loss: 0.0019 lr: 0.02\n",
      "iteration: 219920 loss: 0.0020 lr: 0.02\n",
      "iteration: 219930 loss: 0.0015 lr: 0.02\n",
      "iteration: 219940 loss: 0.0020 lr: 0.02\n",
      "iteration: 219950 loss: 0.0025 lr: 0.02\n",
      "iteration: 219960 loss: 0.0019 lr: 0.02\n",
      "iteration: 219970 loss: 0.0024 lr: 0.02\n",
      "iteration: 219980 loss: 0.0017 lr: 0.02\n",
      "iteration: 219990 loss: 0.0021 lr: 0.02\n",
      "iteration: 220000 loss: 0.0021 lr: 0.02\n",
      "iteration: 220010 loss: 0.0068 lr: 0.02\n",
      "iteration: 220020 loss: 0.0025 lr: 0.02\n",
      "iteration: 220030 loss: 0.0026 lr: 0.02\n",
      "iteration: 220040 loss: 0.0021 lr: 0.02\n",
      "iteration: 220050 loss: 0.0018 lr: 0.02\n",
      "iteration: 220060 loss: 0.0014 lr: 0.02\n",
      "iteration: 220070 loss: 0.0018 lr: 0.02\n",
      "iteration: 220080 loss: 0.0027 lr: 0.02\n",
      "iteration: 220090 loss: 0.0015 lr: 0.02\n",
      "iteration: 220100 loss: 0.0016 lr: 0.02\n",
      "iteration: 220110 loss: 0.0024 lr: 0.02\n",
      "iteration: 220120 loss: 0.0017 lr: 0.02\n",
      "iteration: 220130 loss: 0.0030 lr: 0.02\n",
      "iteration: 220140 loss: 0.0017 lr: 0.02\n",
      "iteration: 220150 loss: 0.0027 lr: 0.02\n",
      "iteration: 220160 loss: 0.0026 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 220170 loss: 0.0020 lr: 0.02\n",
      "iteration: 220180 loss: 0.0017 lr: 0.02\n",
      "iteration: 220190 loss: 0.0029 lr: 0.02\n",
      "iteration: 220200 loss: 0.0015 lr: 0.02\n",
      "iteration: 220210 loss: 0.0022 lr: 0.02\n",
      "iteration: 220220 loss: 0.0018 lr: 0.02\n",
      "iteration: 220230 loss: 0.0015 lr: 0.02\n",
      "iteration: 220240 loss: 0.0017 lr: 0.02\n",
      "iteration: 220250 loss: 0.0018 lr: 0.02\n",
      "iteration: 220260 loss: 0.0023 lr: 0.02\n",
      "iteration: 220270 loss: 0.0015 lr: 0.02\n",
      "iteration: 220280 loss: 0.0019 lr: 0.02\n",
      "iteration: 220290 loss: 0.0016 lr: 0.02\n",
      "iteration: 220300 loss: 0.0016 lr: 0.02\n",
      "iteration: 220310 loss: 0.0035 lr: 0.02\n",
      "iteration: 220320 loss: 0.0021 lr: 0.02\n",
      "iteration: 220330 loss: 0.0028 lr: 0.02\n",
      "iteration: 220340 loss: 0.0016 lr: 0.02\n",
      "iteration: 220350 loss: 0.0019 lr: 0.02\n",
      "iteration: 220360 loss: 0.0025 lr: 0.02\n",
      "iteration: 220370 loss: 0.0022 lr: 0.02\n",
      "iteration: 220380 loss: 0.0017 lr: 0.02\n",
      "iteration: 220390 loss: 0.0022 lr: 0.02\n",
      "iteration: 220400 loss: 0.0022 lr: 0.02\n",
      "iteration: 220410 loss: 0.0015 lr: 0.02\n",
      "iteration: 220420 loss: 0.0031 lr: 0.02\n",
      "iteration: 220430 loss: 0.0020 lr: 0.02\n",
      "iteration: 220440 loss: 0.0021 lr: 0.02\n",
      "iteration: 220450 loss: 0.0015 lr: 0.02\n",
      "iteration: 220460 loss: 0.0027 lr: 0.02\n",
      "iteration: 220470 loss: 0.0027 lr: 0.02\n",
      "iteration: 220480 loss: 0.0022 lr: 0.02\n",
      "iteration: 220490 loss: 0.0021 lr: 0.02\n",
      "iteration: 220500 loss: 0.0025 lr: 0.02\n",
      "iteration: 220510 loss: 0.0014 lr: 0.02\n",
      "iteration: 220520 loss: 0.0018 lr: 0.02\n",
      "iteration: 220530 loss: 0.0023 lr: 0.02\n",
      "iteration: 220540 loss: 0.0017 lr: 0.02\n",
      "iteration: 220550 loss: 0.0013 lr: 0.02\n",
      "iteration: 220560 loss: 0.0020 lr: 0.02\n",
      "iteration: 220570 loss: 0.0020 lr: 0.02\n",
      "iteration: 220580 loss: 0.0018 lr: 0.02\n",
      "iteration: 220590 loss: 0.0027 lr: 0.02\n",
      "iteration: 220600 loss: 0.0016 lr: 0.02\n",
      "iteration: 220610 loss: 0.0023 lr: 0.02\n",
      "iteration: 220620 loss: 0.0024 lr: 0.02\n",
      "iteration: 220630 loss: 0.0015 lr: 0.02\n",
      "iteration: 220640 loss: 0.0017 lr: 0.02\n",
      "iteration: 220650 loss: 0.0024 lr: 0.02\n",
      "iteration: 220660 loss: 0.0040 lr: 0.02\n",
      "iteration: 220670 loss: 0.0026 lr: 0.02\n",
      "iteration: 220680 loss: 0.0031 lr: 0.02\n",
      "iteration: 220690 loss: 0.0027 lr: 0.02\n",
      "iteration: 220700 loss: 0.0011 lr: 0.02\n",
      "iteration: 220710 loss: 0.0017 lr: 0.02\n",
      "iteration: 220720 loss: 0.0026 lr: 0.02\n",
      "iteration: 220730 loss: 0.0016 lr: 0.02\n",
      "iteration: 220740 loss: 0.0018 lr: 0.02\n",
      "iteration: 220750 loss: 0.0015 lr: 0.02\n",
      "iteration: 220760 loss: 0.0017 lr: 0.02\n",
      "iteration: 220770 loss: 0.0024 lr: 0.02\n",
      "iteration: 220780 loss: 0.0017 lr: 0.02\n",
      "iteration: 220790 loss: 0.0022 lr: 0.02\n",
      "iteration: 220800 loss: 0.0029 lr: 0.02\n",
      "iteration: 220810 loss: 0.0015 lr: 0.02\n",
      "iteration: 220820 loss: 0.0018 lr: 0.02\n",
      "iteration: 220830 loss: 0.0016 lr: 0.02\n",
      "iteration: 220840 loss: 0.0018 lr: 0.02\n",
      "iteration: 220850 loss: 0.0020 lr: 0.02\n",
      "iteration: 220860 loss: 0.0021 lr: 0.02\n",
      "iteration: 220870 loss: 0.0018 lr: 0.02\n",
      "iteration: 220880 loss: 0.0019 lr: 0.02\n",
      "iteration: 220890 loss: 0.0019 lr: 0.02\n",
      "iteration: 220900 loss: 0.0025 lr: 0.02\n",
      "iteration: 220910 loss: 0.0018 lr: 0.02\n",
      "iteration: 220920 loss: 0.0026 lr: 0.02\n",
      "iteration: 220930 loss: 0.0021 lr: 0.02\n",
      "iteration: 220940 loss: 0.0018 lr: 0.02\n",
      "iteration: 220950 loss: 0.0016 lr: 0.02\n",
      "iteration: 220960 loss: 0.0024 lr: 0.02\n",
      "iteration: 220970 loss: 0.0018 lr: 0.02\n",
      "iteration: 220980 loss: 0.0016 lr: 0.02\n",
      "iteration: 220990 loss: 0.0021 lr: 0.02\n",
      "iteration: 221000 loss: 0.0021 lr: 0.02\n",
      "iteration: 221010 loss: 0.0016 lr: 0.02\n",
      "iteration: 221020 loss: 0.0018 lr: 0.02\n",
      "iteration: 221030 loss: 0.0017 lr: 0.02\n",
      "iteration: 221040 loss: 0.0015 lr: 0.02\n",
      "iteration: 221050 loss: 0.0023 lr: 0.02\n",
      "iteration: 221060 loss: 0.0016 lr: 0.02\n",
      "iteration: 221070 loss: 0.0014 lr: 0.02\n",
      "iteration: 221080 loss: 0.0015 lr: 0.02\n",
      "iteration: 221090 loss: 0.0015 lr: 0.02\n",
      "iteration: 221100 loss: 0.0017 lr: 0.02\n",
      "iteration: 221110 loss: 0.0020 lr: 0.02\n",
      "iteration: 221120 loss: 0.0025 lr: 0.02\n",
      "iteration: 221130 loss: 0.0020 lr: 0.02\n",
      "iteration: 221140 loss: 0.0012 lr: 0.02\n",
      "iteration: 221150 loss: 0.0017 lr: 0.02\n",
      "iteration: 221160 loss: 0.0018 lr: 0.02\n",
      "iteration: 221170 loss: 0.0021 lr: 0.02\n",
      "iteration: 221180 loss: 0.0012 lr: 0.02\n",
      "iteration: 221190 loss: 0.0015 lr: 0.02\n",
      "iteration: 221200 loss: 0.0027 lr: 0.02\n",
      "iteration: 221210 loss: 0.0017 lr: 0.02\n",
      "iteration: 221220 loss: 0.0017 lr: 0.02\n",
      "iteration: 221230 loss: 0.0026 lr: 0.02\n",
      "iteration: 221240 loss: 0.0018 lr: 0.02\n",
      "iteration: 221250 loss: 0.0019 lr: 0.02\n",
      "iteration: 221260 loss: 0.0024 lr: 0.02\n",
      "iteration: 221270 loss: 0.0016 lr: 0.02\n",
      "iteration: 221280 loss: 0.0018 lr: 0.02\n",
      "iteration: 221290 loss: 0.0016 lr: 0.02\n",
      "iteration: 221300 loss: 0.0018 lr: 0.02\n",
      "iteration: 221310 loss: 0.0014 lr: 0.02\n",
      "iteration: 221320 loss: 0.0025 lr: 0.02\n",
      "iteration: 221330 loss: 0.0014 lr: 0.02\n",
      "iteration: 221340 loss: 0.0018 lr: 0.02\n",
      "iteration: 221350 loss: 0.0020 lr: 0.02\n",
      "iteration: 221360 loss: 0.0031 lr: 0.02\n",
      "iteration: 221370 loss: 0.0018 lr: 0.02\n",
      "iteration: 221380 loss: 0.0014 lr: 0.02\n",
      "iteration: 221390 loss: 0.0014 lr: 0.02\n",
      "iteration: 221400 loss: 0.0022 lr: 0.02\n",
      "iteration: 221410 loss: 0.0013 lr: 0.02\n",
      "iteration: 221420 loss: 0.0024 lr: 0.02\n",
      "iteration: 221430 loss: 0.0015 lr: 0.02\n",
      "iteration: 221440 loss: 0.0021 lr: 0.02\n",
      "iteration: 221450 loss: 0.0016 lr: 0.02\n",
      "iteration: 221460 loss: 0.0017 lr: 0.02\n",
      "iteration: 221470 loss: 0.0020 lr: 0.02\n",
      "iteration: 221480 loss: 0.0016 lr: 0.02\n",
      "iteration: 221490 loss: 0.0021 lr: 0.02\n",
      "iteration: 221500 loss: 0.0013 lr: 0.02\n",
      "iteration: 221510 loss: 0.0017 lr: 0.02\n",
      "iteration: 221520 loss: 0.0017 lr: 0.02\n",
      "iteration: 221530 loss: 0.0021 lr: 0.02\n",
      "iteration: 221540 loss: 0.0021 lr: 0.02\n",
      "iteration: 221550 loss: 0.0018 lr: 0.02\n",
      "iteration: 221560 loss: 0.0017 lr: 0.02\n",
      "iteration: 221570 loss: 0.0023 lr: 0.02\n",
      "iteration: 221580 loss: 0.0027 lr: 0.02\n",
      "iteration: 221590 loss: 0.0019 lr: 0.02\n",
      "iteration: 221600 loss: 0.0017 lr: 0.02\n",
      "iteration: 221610 loss: 0.0019 lr: 0.02\n",
      "iteration: 221620 loss: 0.0018 lr: 0.02\n",
      "iteration: 221630 loss: 0.0013 lr: 0.02\n",
      "iteration: 221640 loss: 0.0014 lr: 0.02\n",
      "iteration: 221650 loss: 0.0013 lr: 0.02\n",
      "iteration: 221660 loss: 0.0015 lr: 0.02\n",
      "iteration: 221670 loss: 0.0023 lr: 0.02\n",
      "iteration: 221680 loss: 0.0020 lr: 0.02\n",
      "iteration: 221690 loss: 0.0017 lr: 0.02\n",
      "iteration: 221700 loss: 0.0017 lr: 0.02\n",
      "iteration: 221710 loss: 0.0018 lr: 0.02\n",
      "iteration: 221720 loss: 0.0014 lr: 0.02\n",
      "iteration: 221730 loss: 0.0021 lr: 0.02\n",
      "iteration: 221740 loss: 0.0017 lr: 0.02\n",
      "iteration: 221750 loss: 0.0029 lr: 0.02\n",
      "iteration: 221760 loss: 0.0019 lr: 0.02\n",
      "iteration: 221770 loss: 0.0026 lr: 0.02\n",
      "iteration: 221780 loss: 0.0022 lr: 0.02\n",
      "iteration: 221790 loss: 0.0023 lr: 0.02\n",
      "iteration: 221800 loss: 0.0019 lr: 0.02\n",
      "iteration: 221810 loss: 0.0021 lr: 0.02\n",
      "iteration: 221820 loss: 0.0025 lr: 0.02\n",
      "iteration: 221830 loss: 0.0024 lr: 0.02\n",
      "iteration: 221840 loss: 0.0018 lr: 0.02\n",
      "iteration: 221850 loss: 0.0022 lr: 0.02\n",
      "iteration: 221860 loss: 0.0023 lr: 0.02\n",
      "iteration: 221870 loss: 0.0022 lr: 0.02\n",
      "iteration: 221880 loss: 0.0015 lr: 0.02\n",
      "iteration: 221890 loss: 0.0028 lr: 0.02\n",
      "iteration: 221900 loss: 0.0026 lr: 0.02\n",
      "iteration: 221910 loss: 0.0019 lr: 0.02\n",
      "iteration: 221920 loss: 0.0015 lr: 0.02\n",
      "iteration: 221930 loss: 0.0021 lr: 0.02\n",
      "iteration: 221940 loss: 0.0023 lr: 0.02\n",
      "iteration: 221950 loss: 0.0023 lr: 0.02\n",
      "iteration: 221960 loss: 0.0025 lr: 0.02\n",
      "iteration: 221970 loss: 0.0019 lr: 0.02\n",
      "iteration: 221980 loss: 0.0024 lr: 0.02\n",
      "iteration: 221990 loss: 0.0015 lr: 0.02\n",
      "iteration: 222000 loss: 0.0026 lr: 0.02\n",
      "iteration: 222010 loss: 0.0018 lr: 0.02\n",
      "iteration: 222020 loss: 0.0024 lr: 0.02\n",
      "iteration: 222030 loss: 0.0017 lr: 0.02\n",
      "iteration: 222040 loss: 0.0019 lr: 0.02\n",
      "iteration: 222050 loss: 0.0019 lr: 0.02\n",
      "iteration: 222060 loss: 0.0020 lr: 0.02\n",
      "iteration: 222070 loss: 0.0030 lr: 0.02\n",
      "iteration: 222080 loss: 0.0019 lr: 0.02\n",
      "iteration: 222090 loss: 0.0024 lr: 0.02\n",
      "iteration: 222100 loss: 0.0021 lr: 0.02\n",
      "iteration: 222110 loss: 0.0021 lr: 0.02\n",
      "iteration: 222120 loss: 0.0020 lr: 0.02\n",
      "iteration: 222130 loss: 0.0014 lr: 0.02\n",
      "iteration: 222140 loss: 0.0021 lr: 0.02\n",
      "iteration: 222150 loss: 0.0025 lr: 0.02\n",
      "iteration: 222160 loss: 0.0025 lr: 0.02\n",
      "iteration: 222170 loss: 0.0022 lr: 0.02\n",
      "iteration: 222180 loss: 0.0023 lr: 0.02\n",
      "iteration: 222190 loss: 0.0035 lr: 0.02\n",
      "iteration: 222200 loss: 0.0018 lr: 0.02\n",
      "iteration: 222210 loss: 0.0018 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 222220 loss: 0.0019 lr: 0.02\n",
      "iteration: 222230 loss: 0.0014 lr: 0.02\n",
      "iteration: 222240 loss: 0.0028 lr: 0.02\n",
      "iteration: 222250 loss: 0.0025 lr: 0.02\n",
      "iteration: 222260 loss: 0.0017 lr: 0.02\n",
      "iteration: 222270 loss: 0.0015 lr: 0.02\n",
      "iteration: 222280 loss: 0.0016 lr: 0.02\n",
      "iteration: 222290 loss: 0.0032 lr: 0.02\n",
      "iteration: 222300 loss: 0.0015 lr: 0.02\n",
      "iteration: 222310 loss: 0.0018 lr: 0.02\n",
      "iteration: 222320 loss: 0.0017 lr: 0.02\n",
      "iteration: 222330 loss: 0.0021 lr: 0.02\n",
      "iteration: 222340 loss: 0.0018 lr: 0.02\n",
      "iteration: 222350 loss: 0.0025 lr: 0.02\n",
      "iteration: 222360 loss: 0.0022 lr: 0.02\n",
      "iteration: 222370 loss: 0.0021 lr: 0.02\n",
      "iteration: 222380 loss: 0.0024 lr: 0.02\n",
      "iteration: 222390 loss: 0.0015 lr: 0.02\n",
      "iteration: 222400 loss: 0.0019 lr: 0.02\n",
      "iteration: 222410 loss: 0.0016 lr: 0.02\n",
      "iteration: 222420 loss: 0.0018 lr: 0.02\n",
      "iteration: 222430 loss: 0.0016 lr: 0.02\n",
      "iteration: 222440 loss: 0.0030 lr: 0.02\n",
      "iteration: 222450 loss: 0.0016 lr: 0.02\n",
      "iteration: 222460 loss: 0.0021 lr: 0.02\n",
      "iteration: 222470 loss: 0.0024 lr: 0.02\n",
      "iteration: 222480 loss: 0.0017 lr: 0.02\n",
      "iteration: 222490 loss: 0.0020 lr: 0.02\n",
      "iteration: 222500 loss: 0.0021 lr: 0.02\n",
      "iteration: 222510 loss: 0.0018 lr: 0.02\n",
      "iteration: 222520 loss: 0.0025 lr: 0.02\n",
      "iteration: 222530 loss: 0.0024 lr: 0.02\n",
      "iteration: 222540 loss: 0.0022 lr: 0.02\n",
      "iteration: 222550 loss: 0.0022 lr: 0.02\n",
      "iteration: 222560 loss: 0.0025 lr: 0.02\n",
      "iteration: 222570 loss: 0.0017 lr: 0.02\n",
      "iteration: 222580 loss: 0.0017 lr: 0.02\n",
      "iteration: 222590 loss: 0.0016 lr: 0.02\n",
      "iteration: 222600 loss: 0.0018 lr: 0.02\n",
      "iteration: 222610 loss: 0.0038 lr: 0.02\n",
      "iteration: 222620 loss: 0.0024 lr: 0.02\n",
      "iteration: 222630 loss: 0.0026 lr: 0.02\n",
      "iteration: 222640 loss: 0.0034 lr: 0.02\n",
      "iteration: 222650 loss: 0.0020 lr: 0.02\n",
      "iteration: 222660 loss: 0.0023 lr: 0.02\n",
      "iteration: 222670 loss: 0.0021 lr: 0.02\n",
      "iteration: 222680 loss: 0.0023 lr: 0.02\n",
      "iteration: 222690 loss: 0.0019 lr: 0.02\n",
      "iteration: 222700 loss: 0.0020 lr: 0.02\n",
      "iteration: 222710 loss: 0.0021 lr: 0.02\n",
      "iteration: 222720 loss: 0.0017 lr: 0.02\n",
      "iteration: 222730 loss: 0.0032 lr: 0.02\n",
      "iteration: 222740 loss: 0.0024 lr: 0.02\n",
      "iteration: 222750 loss: 0.0016 lr: 0.02\n",
      "iteration: 222760 loss: 0.0024 lr: 0.02\n",
      "iteration: 222770 loss: 0.0017 lr: 0.02\n",
      "iteration: 222780 loss: 0.0038 lr: 0.02\n",
      "iteration: 222790 loss: 0.0026 lr: 0.02\n",
      "iteration: 222800 loss: 0.0017 lr: 0.02\n",
      "iteration: 222810 loss: 0.0019 lr: 0.02\n",
      "iteration: 222820 loss: 0.0024 lr: 0.02\n",
      "iteration: 222830 loss: 0.0026 lr: 0.02\n",
      "iteration: 222840 loss: 0.0022 lr: 0.02\n",
      "iteration: 222850 loss: 0.0024 lr: 0.02\n",
      "iteration: 222860 loss: 0.0013 lr: 0.02\n",
      "iteration: 222870 loss: 0.0016 lr: 0.02\n",
      "iteration: 222880 loss: 0.0021 lr: 0.02\n",
      "iteration: 222890 loss: 0.0019 lr: 0.02\n",
      "iteration: 222900 loss: 0.0020 lr: 0.02\n",
      "iteration: 222910 loss: 0.0015 lr: 0.02\n",
      "iteration: 222920 loss: 0.0018 lr: 0.02\n",
      "iteration: 222930 loss: 0.0020 lr: 0.02\n",
      "iteration: 222940 loss: 0.0021 lr: 0.02\n",
      "iteration: 222950 loss: 0.0025 lr: 0.02\n",
      "iteration: 222960 loss: 0.0020 lr: 0.02\n",
      "iteration: 222970 loss: 0.0024 lr: 0.02\n",
      "iteration: 222980 loss: 0.0016 lr: 0.02\n",
      "iteration: 222990 loss: 0.0028 lr: 0.02\n",
      "iteration: 223000 loss: 0.0022 lr: 0.02\n",
      "iteration: 223010 loss: 0.0016 lr: 0.02\n",
      "iteration: 223020 loss: 0.0029 lr: 0.02\n",
      "iteration: 223030 loss: 0.0023 lr: 0.02\n",
      "iteration: 223040 loss: 0.0015 lr: 0.02\n",
      "iteration: 223050 loss: 0.0045 lr: 0.02\n",
      "iteration: 223060 loss: 0.0019 lr: 0.02\n",
      "iteration: 223070 loss: 0.0023 lr: 0.02\n",
      "iteration: 223080 loss: 0.0024 lr: 0.02\n",
      "iteration: 223090 loss: 0.0013 lr: 0.02\n",
      "iteration: 223100 loss: 0.0017 lr: 0.02\n",
      "iteration: 223110 loss: 0.0016 lr: 0.02\n",
      "iteration: 223120 loss: 0.0017 lr: 0.02\n",
      "iteration: 223130 loss: 0.0014 lr: 0.02\n",
      "iteration: 223140 loss: 0.0014 lr: 0.02\n",
      "iteration: 223150 loss: 0.0013 lr: 0.02\n",
      "iteration: 223160 loss: 0.0017 lr: 0.02\n",
      "iteration: 223170 loss: 0.0021 lr: 0.02\n",
      "iteration: 223180 loss: 0.0029 lr: 0.02\n",
      "iteration: 223190 loss: 0.0018 lr: 0.02\n",
      "iteration: 223200 loss: 0.0030 lr: 0.02\n",
      "iteration: 223210 loss: 0.0023 lr: 0.02\n",
      "iteration: 223220 loss: 0.0018 lr: 0.02\n",
      "iteration: 223230 loss: 0.0022 lr: 0.02\n",
      "iteration: 223240 loss: 0.0024 lr: 0.02\n",
      "iteration: 223250 loss: 0.0045 lr: 0.02\n",
      "iteration: 223260 loss: 0.0021 lr: 0.02\n",
      "iteration: 223270 loss: 0.0014 lr: 0.02\n",
      "iteration: 223280 loss: 0.0019 lr: 0.02\n",
      "iteration: 223290 loss: 0.0020 lr: 0.02\n",
      "iteration: 223300 loss: 0.0016 lr: 0.02\n",
      "iteration: 223310 loss: 0.0019 lr: 0.02\n",
      "iteration: 223320 loss: 0.0030 lr: 0.02\n",
      "iteration: 223330 loss: 0.0015 lr: 0.02\n",
      "iteration: 223340 loss: 0.0018 lr: 0.02\n",
      "iteration: 223350 loss: 0.0023 lr: 0.02\n",
      "iteration: 223360 loss: 0.0025 lr: 0.02\n",
      "iteration: 223370 loss: 0.0025 lr: 0.02\n",
      "iteration: 223380 loss: 0.0023 lr: 0.02\n",
      "iteration: 223390 loss: 0.0016 lr: 0.02\n",
      "iteration: 223400 loss: 0.0019 lr: 0.02\n",
      "iteration: 223410 loss: 0.0033 lr: 0.02\n",
      "iteration: 223420 loss: 0.0025 lr: 0.02\n",
      "iteration: 223430 loss: 0.0026 lr: 0.02\n",
      "iteration: 223440 loss: 0.0028 lr: 0.02\n",
      "iteration: 223450 loss: 0.0015 lr: 0.02\n",
      "iteration: 223460 loss: 0.0027 lr: 0.02\n",
      "iteration: 223470 loss: 0.0021 lr: 0.02\n",
      "iteration: 223480 loss: 0.0025 lr: 0.02\n",
      "iteration: 223490 loss: 0.0023 lr: 0.02\n",
      "iteration: 223500 loss: 0.0037 lr: 0.02\n",
      "iteration: 223510 loss: 0.0015 lr: 0.02\n",
      "iteration: 223520 loss: 0.0022 lr: 0.02\n",
      "iteration: 223530 loss: 0.0014 lr: 0.02\n",
      "iteration: 223540 loss: 0.0018 lr: 0.02\n",
      "iteration: 223550 loss: 0.0018 lr: 0.02\n",
      "iteration: 223560 loss: 0.0016 lr: 0.02\n",
      "iteration: 223570 loss: 0.0016 lr: 0.02\n",
      "iteration: 223580 loss: 0.0017 lr: 0.02\n",
      "iteration: 223590 loss: 0.0015 lr: 0.02\n",
      "iteration: 223600 loss: 0.0022 lr: 0.02\n",
      "iteration: 223610 loss: 0.0027 lr: 0.02\n",
      "iteration: 223620 loss: 0.0023 lr: 0.02\n",
      "iteration: 223630 loss: 0.0020 lr: 0.02\n",
      "iteration: 223640 loss: 0.0023 lr: 0.02\n",
      "iteration: 223650 loss: 0.0019 lr: 0.02\n",
      "iteration: 223660 loss: 0.0020 lr: 0.02\n",
      "iteration: 223670 loss: 0.0023 lr: 0.02\n",
      "iteration: 223680 loss: 0.0021 lr: 0.02\n",
      "iteration: 223690 loss: 0.0018 lr: 0.02\n",
      "iteration: 223700 loss: 0.0022 lr: 0.02\n",
      "iteration: 223710 loss: 0.0018 lr: 0.02\n",
      "iteration: 223720 loss: 0.0016 lr: 0.02\n",
      "iteration: 223730 loss: 0.0017 lr: 0.02\n",
      "iteration: 223740 loss: 0.0024 lr: 0.02\n",
      "iteration: 223750 loss: 0.0022 lr: 0.02\n",
      "iteration: 223760 loss: 0.0021 lr: 0.02\n",
      "iteration: 223770 loss: 0.0019 lr: 0.02\n",
      "iteration: 223780 loss: 0.0017 lr: 0.02\n",
      "iteration: 223790 loss: 0.0017 lr: 0.02\n",
      "iteration: 223800 loss: 0.0021 lr: 0.02\n",
      "iteration: 223810 loss: 0.0017 lr: 0.02\n",
      "iteration: 223820 loss: 0.0016 lr: 0.02\n",
      "iteration: 223830 loss: 0.0022 lr: 0.02\n",
      "iteration: 223840 loss: 0.0017 lr: 0.02\n",
      "iteration: 223850 loss: 0.0020 lr: 0.02\n",
      "iteration: 223860 loss: 0.0028 lr: 0.02\n",
      "iteration: 223870 loss: 0.0016 lr: 0.02\n",
      "iteration: 223880 loss: 0.0015 lr: 0.02\n",
      "iteration: 223890 loss: 0.0016 lr: 0.02\n",
      "iteration: 223900 loss: 0.0020 lr: 0.02\n",
      "iteration: 223910 loss: 0.0030 lr: 0.02\n",
      "iteration: 223920 loss: 0.0029 lr: 0.02\n",
      "iteration: 223930 loss: 0.0028 lr: 0.02\n",
      "iteration: 223940 loss: 0.0019 lr: 0.02\n",
      "iteration: 223950 loss: 0.0021 lr: 0.02\n",
      "iteration: 223960 loss: 0.0018 lr: 0.02\n",
      "iteration: 223970 loss: 0.0020 lr: 0.02\n",
      "iteration: 223980 loss: 0.0020 lr: 0.02\n",
      "iteration: 223990 loss: 0.0016 lr: 0.02\n",
      "iteration: 224000 loss: 0.0018 lr: 0.02\n",
      "iteration: 224010 loss: 0.0018 lr: 0.02\n",
      "iteration: 224020 loss: 0.0020 lr: 0.02\n",
      "iteration: 224030 loss: 0.0012 lr: 0.02\n",
      "iteration: 224040 loss: 0.0031 lr: 0.02\n",
      "iteration: 224050 loss: 0.0025 lr: 0.02\n",
      "iteration: 224060 loss: 0.0019 lr: 0.02\n",
      "iteration: 224070 loss: 0.0023 lr: 0.02\n",
      "iteration: 224080 loss: 0.0024 lr: 0.02\n",
      "iteration: 224090 loss: 0.0020 lr: 0.02\n",
      "iteration: 224100 loss: 0.0016 lr: 0.02\n",
      "iteration: 224110 loss: 0.0017 lr: 0.02\n",
      "iteration: 224120 loss: 0.0026 lr: 0.02\n",
      "iteration: 224130 loss: 0.0018 lr: 0.02\n",
      "iteration: 224140 loss: 0.0022 lr: 0.02\n",
      "iteration: 224150 loss: 0.0019 lr: 0.02\n",
      "iteration: 224160 loss: 0.0016 lr: 0.02\n",
      "iteration: 224170 loss: 0.0017 lr: 0.02\n",
      "iteration: 224180 loss: 0.0014 lr: 0.02\n",
      "iteration: 224190 loss: 0.0028 lr: 0.02\n",
      "iteration: 224200 loss: 0.0050 lr: 0.02\n",
      "iteration: 224210 loss: 0.0020 lr: 0.02\n",
      "iteration: 224220 loss: 0.0024 lr: 0.02\n",
      "iteration: 224230 loss: 0.0015 lr: 0.02\n",
      "iteration: 224240 loss: 0.0019 lr: 0.02\n",
      "iteration: 224250 loss: 0.0019 lr: 0.02\n",
      "iteration: 224260 loss: 0.0013 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 224270 loss: 0.0018 lr: 0.02\n",
      "iteration: 224280 loss: 0.0019 lr: 0.02\n",
      "iteration: 224290 loss: 0.0017 lr: 0.02\n",
      "iteration: 224300 loss: 0.0017 lr: 0.02\n",
      "iteration: 224310 loss: 0.0021 lr: 0.02\n",
      "iteration: 224320 loss: 0.0015 lr: 0.02\n",
      "iteration: 224330 loss: 0.0016 lr: 0.02\n",
      "iteration: 224340 loss: 0.0022 lr: 0.02\n",
      "iteration: 224350 loss: 0.0018 lr: 0.02\n",
      "iteration: 224360 loss: 0.0020 lr: 0.02\n",
      "iteration: 224370 loss: 0.0019 lr: 0.02\n",
      "iteration: 224380 loss: 0.0018 lr: 0.02\n",
      "iteration: 224390 loss: 0.0021 lr: 0.02\n",
      "iteration: 224400 loss: 0.0019 lr: 0.02\n",
      "iteration: 224410 loss: 0.0024 lr: 0.02\n",
      "iteration: 224420 loss: 0.0022 lr: 0.02\n",
      "iteration: 224430 loss: 0.0023 lr: 0.02\n",
      "iteration: 224440 loss: 0.0022 lr: 0.02\n",
      "iteration: 224450 loss: 0.0026 lr: 0.02\n",
      "iteration: 224460 loss: 0.0020 lr: 0.02\n",
      "iteration: 224470 loss: 0.0016 lr: 0.02\n",
      "iteration: 224480 loss: 0.0020 lr: 0.02\n",
      "iteration: 224490 loss: 0.0018 lr: 0.02\n",
      "iteration: 224500 loss: 0.0017 lr: 0.02\n",
      "iteration: 224510 loss: 0.0018 lr: 0.02\n",
      "iteration: 224520 loss: 0.0023 lr: 0.02\n",
      "iteration: 224530 loss: 0.0025 lr: 0.02\n",
      "iteration: 224540 loss: 0.0031 lr: 0.02\n",
      "iteration: 224550 loss: 0.0017 lr: 0.02\n",
      "iteration: 224560 loss: 0.0022 lr: 0.02\n",
      "iteration: 224570 loss: 0.0014 lr: 0.02\n",
      "iteration: 224580 loss: 0.0015 lr: 0.02\n",
      "iteration: 224590 loss: 0.0019 lr: 0.02\n",
      "iteration: 224600 loss: 0.0029 lr: 0.02\n",
      "iteration: 224610 loss: 0.0024 lr: 0.02\n",
      "iteration: 224620 loss: 0.0026 lr: 0.02\n",
      "iteration: 224630 loss: 0.0019 lr: 0.02\n",
      "iteration: 224640 loss: 0.0019 lr: 0.02\n",
      "iteration: 224650 loss: 0.0016 lr: 0.02\n",
      "iteration: 224660 loss: 0.0020 lr: 0.02\n",
      "iteration: 224670 loss: 0.0033 lr: 0.02\n",
      "iteration: 224680 loss: 0.0024 lr: 0.02\n",
      "iteration: 224690 loss: 0.0016 lr: 0.02\n",
      "iteration: 224700 loss: 0.0016 lr: 0.02\n",
      "iteration: 224710 loss: 0.0015 lr: 0.02\n",
      "iteration: 224720 loss: 0.0030 lr: 0.02\n",
      "iteration: 224730 loss: 0.0017 lr: 0.02\n",
      "iteration: 224740 loss: 0.0034 lr: 0.02\n",
      "iteration: 224750 loss: 0.0020 lr: 0.02\n",
      "iteration: 224760 loss: 0.0019 lr: 0.02\n",
      "iteration: 224770 loss: 0.0038 lr: 0.02\n",
      "iteration: 224780 loss: 0.0020 lr: 0.02\n",
      "iteration: 224790 loss: 0.0032 lr: 0.02\n",
      "iteration: 224800 loss: 0.0015 lr: 0.02\n",
      "iteration: 224810 loss: 0.0023 lr: 0.02\n",
      "iteration: 224820 loss: 0.0026 lr: 0.02\n",
      "iteration: 224830 loss: 0.0022 lr: 0.02\n",
      "iteration: 224840 loss: 0.0024 lr: 0.02\n",
      "iteration: 224850 loss: 0.0029 lr: 0.02\n",
      "iteration: 224860 loss: 0.0040 lr: 0.02\n",
      "iteration: 224870 loss: 0.0018 lr: 0.02\n",
      "iteration: 224880 loss: 0.0023 lr: 0.02\n",
      "iteration: 224890 loss: 0.0023 lr: 0.02\n",
      "iteration: 224900 loss: 0.0025 lr: 0.02\n",
      "iteration: 224910 loss: 0.0023 lr: 0.02\n",
      "iteration: 224920 loss: 0.0025 lr: 0.02\n",
      "iteration: 224930 loss: 0.0019 lr: 0.02\n",
      "iteration: 224940 loss: 0.0018 lr: 0.02\n",
      "iteration: 224950 loss: 0.0014 lr: 0.02\n",
      "iteration: 224960 loss: 0.0016 lr: 0.02\n",
      "iteration: 224970 loss: 0.0019 lr: 0.02\n",
      "iteration: 224980 loss: 0.0024 lr: 0.02\n",
      "iteration: 224990 loss: 0.0024 lr: 0.02\n",
      "iteration: 225000 loss: 0.0018 lr: 0.02\n",
      "iteration: 225010 loss: 0.0020 lr: 0.02\n",
      "iteration: 225020 loss: 0.0021 lr: 0.02\n",
      "iteration: 225030 loss: 0.0023 lr: 0.02\n",
      "iteration: 225040 loss: 0.0026 lr: 0.02\n",
      "iteration: 225050 loss: 0.0032 lr: 0.02\n",
      "iteration: 225060 loss: 0.0021 lr: 0.02\n",
      "iteration: 225070 loss: 0.0018 lr: 0.02\n",
      "iteration: 225080 loss: 0.0014 lr: 0.02\n",
      "iteration: 225090 loss: 0.0017 lr: 0.02\n",
      "iteration: 225100 loss: 0.0020 lr: 0.02\n",
      "iteration: 225110 loss: 0.0022 lr: 0.02\n",
      "iteration: 225120 loss: 0.0019 lr: 0.02\n",
      "iteration: 225130 loss: 0.0017 lr: 0.02\n",
      "iteration: 225140 loss: 0.0027 lr: 0.02\n",
      "iteration: 225150 loss: 0.0025 lr: 0.02\n",
      "iteration: 225160 loss: 0.0013 lr: 0.02\n",
      "iteration: 225170 loss: 0.0030 lr: 0.02\n",
      "iteration: 225180 loss: 0.0022 lr: 0.02\n",
      "iteration: 225190 loss: 0.0028 lr: 0.02\n",
      "iteration: 225200 loss: 0.0018 lr: 0.02\n",
      "iteration: 225210 loss: 0.0017 lr: 0.02\n",
      "iteration: 225220 loss: 0.0019 lr: 0.02\n",
      "iteration: 225230 loss: 0.0018 lr: 0.02\n",
      "iteration: 225240 loss: 0.0019 lr: 0.02\n",
      "iteration: 225250 loss: 0.0019 lr: 0.02\n",
      "iteration: 225260 loss: 0.0019 lr: 0.02\n",
      "iteration: 225270 loss: 0.0021 lr: 0.02\n",
      "iteration: 225280 loss: 0.0021 lr: 0.02\n",
      "iteration: 225290 loss: 0.0022 lr: 0.02\n",
      "iteration: 225300 loss: 0.0012 lr: 0.02\n",
      "iteration: 225310 loss: 0.0019 lr: 0.02\n",
      "iteration: 225320 loss: 0.0023 lr: 0.02\n",
      "iteration: 225330 loss: 0.0022 lr: 0.02\n",
      "iteration: 225340 loss: 0.0031 lr: 0.02\n",
      "iteration: 225350 loss: 0.0025 lr: 0.02\n",
      "iteration: 225360 loss: 0.0021 lr: 0.02\n",
      "iteration: 225370 loss: 0.0021 lr: 0.02\n",
      "iteration: 225380 loss: 0.0018 lr: 0.02\n",
      "iteration: 225390 loss: 0.0021 lr: 0.02\n",
      "iteration: 225400 loss: 0.0017 lr: 0.02\n",
      "iteration: 225410 loss: 0.0022 lr: 0.02\n",
      "iteration: 225420 loss: 0.0024 lr: 0.02\n",
      "iteration: 225430 loss: 0.0014 lr: 0.02\n",
      "iteration: 225440 loss: 0.0032 lr: 0.02\n",
      "iteration: 225450 loss: 0.0018 lr: 0.02\n",
      "iteration: 225460 loss: 0.0022 lr: 0.02\n",
      "iteration: 225470 loss: 0.0019 lr: 0.02\n",
      "iteration: 225480 loss: 0.0012 lr: 0.02\n",
      "iteration: 225490 loss: 0.0016 lr: 0.02\n",
      "iteration: 225500 loss: 0.0025 lr: 0.02\n",
      "iteration: 225510 loss: 0.0029 lr: 0.02\n",
      "iteration: 225520 loss: 0.0019 lr: 0.02\n",
      "iteration: 225530 loss: 0.0017 lr: 0.02\n",
      "iteration: 225540 loss: 0.0020 lr: 0.02\n",
      "iteration: 225550 loss: 0.0017 lr: 0.02\n",
      "iteration: 225560 loss: 0.0026 lr: 0.02\n",
      "iteration: 225570 loss: 0.0017 lr: 0.02\n",
      "iteration: 225580 loss: 0.0020 lr: 0.02\n",
      "iteration: 225590 loss: 0.0026 lr: 0.02\n",
      "iteration: 225600 loss: 0.0017 lr: 0.02\n",
      "iteration: 225610 loss: 0.0023 lr: 0.02\n",
      "iteration: 225620 loss: 0.0014 lr: 0.02\n",
      "iteration: 225630 loss: 0.0014 lr: 0.02\n",
      "iteration: 225640 loss: 0.0019 lr: 0.02\n",
      "iteration: 225650 loss: 0.0019 lr: 0.02\n",
      "iteration: 225660 loss: 0.0020 lr: 0.02\n",
      "iteration: 225670 loss: 0.0017 lr: 0.02\n",
      "iteration: 225680 loss: 0.0025 lr: 0.02\n",
      "iteration: 225690 loss: 0.0014 lr: 0.02\n",
      "iteration: 225700 loss: 0.0014 lr: 0.02\n",
      "iteration: 225710 loss: 0.0021 lr: 0.02\n",
      "iteration: 225720 loss: 0.0022 lr: 0.02\n",
      "iteration: 225730 loss: 0.0014 lr: 0.02\n",
      "iteration: 225740 loss: 0.0023 lr: 0.02\n",
      "iteration: 225750 loss: 0.0022 lr: 0.02\n",
      "iteration: 225760 loss: 0.0024 lr: 0.02\n",
      "iteration: 225770 loss: 0.0021 lr: 0.02\n",
      "iteration: 225780 loss: 0.0016 lr: 0.02\n",
      "iteration: 225790 loss: 0.0015 lr: 0.02\n",
      "iteration: 225800 loss: 0.0016 lr: 0.02\n",
      "iteration: 225810 loss: 0.0023 lr: 0.02\n",
      "iteration: 225820 loss: 0.0020 lr: 0.02\n",
      "iteration: 225830 loss: 0.0014 lr: 0.02\n",
      "iteration: 225840 loss: 0.0020 lr: 0.02\n",
      "iteration: 225850 loss: 0.0032 lr: 0.02\n",
      "iteration: 225860 loss: 0.0019 lr: 0.02\n",
      "iteration: 225870 loss: 0.0020 lr: 0.02\n",
      "iteration: 225880 loss: 0.0019 lr: 0.02\n",
      "iteration: 225890 loss: 0.0020 lr: 0.02\n",
      "iteration: 225900 loss: 0.0020 lr: 0.02\n",
      "iteration: 225910 loss: 0.0022 lr: 0.02\n",
      "iteration: 225920 loss: 0.0020 lr: 0.02\n",
      "iteration: 225930 loss: 0.0024 lr: 0.02\n",
      "iteration: 225940 loss: 0.0026 lr: 0.02\n",
      "iteration: 225950 loss: 0.0026 lr: 0.02\n",
      "iteration: 225960 loss: 0.0018 lr: 0.02\n",
      "iteration: 225970 loss: 0.0025 lr: 0.02\n",
      "iteration: 225980 loss: 0.0012 lr: 0.02\n",
      "iteration: 225990 loss: 0.0021 lr: 0.02\n",
      "iteration: 226000 loss: 0.0015 lr: 0.02\n",
      "iteration: 226010 loss: 0.0021 lr: 0.02\n",
      "iteration: 226020 loss: 0.0016 lr: 0.02\n",
      "iteration: 226030 loss: 0.0017 lr: 0.02\n",
      "iteration: 226040 loss: 0.0019 lr: 0.02\n",
      "iteration: 226050 loss: 0.0016 lr: 0.02\n",
      "iteration: 226060 loss: 0.0026 lr: 0.02\n",
      "iteration: 226070 loss: 0.0020 lr: 0.02\n",
      "iteration: 226080 loss: 0.0013 lr: 0.02\n",
      "iteration: 226090 loss: 0.0031 lr: 0.02\n",
      "iteration: 226100 loss: 0.0020 lr: 0.02\n",
      "iteration: 226110 loss: 0.0022 lr: 0.02\n",
      "iteration: 226120 loss: 0.0032 lr: 0.02\n",
      "iteration: 226130 loss: 0.0023 lr: 0.02\n",
      "iteration: 226140 loss: 0.0026 lr: 0.02\n",
      "iteration: 226150 loss: 0.0021 lr: 0.02\n",
      "iteration: 226160 loss: 0.0016 lr: 0.02\n",
      "iteration: 226170 loss: 0.0022 lr: 0.02\n",
      "iteration: 226180 loss: 0.0020 lr: 0.02\n",
      "iteration: 226190 loss: 0.0019 lr: 0.02\n",
      "iteration: 226200 loss: 0.0015 lr: 0.02\n",
      "iteration: 226210 loss: 0.0021 lr: 0.02\n",
      "iteration: 226220 loss: 0.0020 lr: 0.02\n",
      "iteration: 226230 loss: 0.0023 lr: 0.02\n",
      "iteration: 226240 loss: 0.0034 lr: 0.02\n",
      "iteration: 226250 loss: 0.0022 lr: 0.02\n",
      "iteration: 226260 loss: 0.0020 lr: 0.02\n",
      "iteration: 226270 loss: 0.0022 lr: 0.02\n",
      "iteration: 226280 loss: 0.0015 lr: 0.02\n",
      "iteration: 226290 loss: 0.0020 lr: 0.02\n",
      "iteration: 226300 loss: 0.0020 lr: 0.02\n",
      "iteration: 226310 loss: 0.0024 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 226320 loss: 0.0016 lr: 0.02\n",
      "iteration: 226330 loss: 0.0032 lr: 0.02\n",
      "iteration: 226340 loss: 0.0024 lr: 0.02\n",
      "iteration: 226350 loss: 0.0020 lr: 0.02\n",
      "iteration: 226360 loss: 0.0015 lr: 0.02\n",
      "iteration: 226370 loss: 0.0021 lr: 0.02\n",
      "iteration: 226380 loss: 0.0025 lr: 0.02\n",
      "iteration: 226390 loss: 0.0016 lr: 0.02\n",
      "iteration: 226400 loss: 0.0026 lr: 0.02\n",
      "iteration: 226410 loss: 0.0021 lr: 0.02\n",
      "iteration: 226420 loss: 0.0023 lr: 0.02\n",
      "iteration: 226430 loss: 0.0021 lr: 0.02\n",
      "iteration: 226440 loss: 0.0020 lr: 0.02\n",
      "iteration: 226450 loss: 0.0023 lr: 0.02\n",
      "iteration: 226460 loss: 0.0020 lr: 0.02\n",
      "iteration: 226470 loss: 0.0023 lr: 0.02\n",
      "iteration: 226480 loss: 0.0019 lr: 0.02\n",
      "iteration: 226490 loss: 0.0026 lr: 0.02\n",
      "iteration: 226500 loss: 0.0018 lr: 0.02\n",
      "iteration: 226510 loss: 0.0020 lr: 0.02\n",
      "iteration: 226520 loss: 0.0015 lr: 0.02\n",
      "iteration: 226530 loss: 0.0017 lr: 0.02\n",
      "iteration: 226540 loss: 0.0019 lr: 0.02\n",
      "iteration: 226550 loss: 0.0021 lr: 0.02\n",
      "iteration: 226560 loss: 0.0019 lr: 0.02\n",
      "iteration: 226570 loss: 0.0021 lr: 0.02\n",
      "iteration: 226580 loss: 0.0016 lr: 0.02\n",
      "iteration: 226590 loss: 0.0020 lr: 0.02\n",
      "iteration: 226600 loss: 0.0018 lr: 0.02\n",
      "iteration: 226610 loss: 0.0018 lr: 0.02\n",
      "iteration: 226620 loss: 0.0020 lr: 0.02\n",
      "iteration: 226630 loss: 0.0013 lr: 0.02\n",
      "iteration: 226640 loss: 0.0016 lr: 0.02\n",
      "iteration: 226650 loss: 0.0024 lr: 0.02\n",
      "iteration: 226660 loss: 0.0023 lr: 0.02\n",
      "iteration: 226670 loss: 0.0015 lr: 0.02\n",
      "iteration: 226680 loss: 0.0021 lr: 0.02\n",
      "iteration: 226690 loss: 0.0015 lr: 0.02\n",
      "iteration: 226700 loss: 0.0020 lr: 0.02\n",
      "iteration: 226710 loss: 0.0021 lr: 0.02\n",
      "iteration: 226720 loss: 0.0026 lr: 0.02\n",
      "iteration: 226730 loss: 0.0016 lr: 0.02\n",
      "iteration: 226740 loss: 0.0021 lr: 0.02\n",
      "iteration: 226750 loss: 0.0017 lr: 0.02\n",
      "iteration: 226760 loss: 0.0022 lr: 0.02\n",
      "iteration: 226770 loss: 0.0014 lr: 0.02\n",
      "iteration: 226780 loss: 0.0023 lr: 0.02\n",
      "iteration: 226790 loss: 0.0016 lr: 0.02\n",
      "iteration: 226800 loss: 0.0019 lr: 0.02\n",
      "iteration: 226810 loss: 0.0019 lr: 0.02\n",
      "iteration: 226820 loss: 0.0019 lr: 0.02\n",
      "iteration: 226830 loss: 0.0018 lr: 0.02\n",
      "iteration: 226840 loss: 0.0019 lr: 0.02\n",
      "iteration: 226850 loss: 0.0017 lr: 0.02\n",
      "iteration: 226860 loss: 0.0020 lr: 0.02\n",
      "iteration: 226870 loss: 0.0020 lr: 0.02\n",
      "iteration: 226880 loss: 0.0019 lr: 0.02\n",
      "iteration: 226890 loss: 0.0020 lr: 0.02\n",
      "iteration: 226900 loss: 0.0022 lr: 0.02\n",
      "iteration: 226910 loss: 0.0021 lr: 0.02\n",
      "iteration: 226920 loss: 0.0024 lr: 0.02\n",
      "iteration: 226930 loss: 0.0019 lr: 0.02\n",
      "iteration: 226940 loss: 0.0019 lr: 0.02\n",
      "iteration: 226950 loss: 0.0014 lr: 0.02\n",
      "iteration: 226960 loss: 0.0029 lr: 0.02\n",
      "iteration: 226970 loss: 0.0015 lr: 0.02\n",
      "iteration: 226980 loss: 0.0020 lr: 0.02\n",
      "iteration: 226990 loss: 0.0017 lr: 0.02\n",
      "iteration: 227000 loss: 0.0019 lr: 0.02\n",
      "iteration: 227010 loss: 0.0019 lr: 0.02\n",
      "iteration: 227020 loss: 0.0020 lr: 0.02\n",
      "iteration: 227030 loss: 0.0016 lr: 0.02\n",
      "iteration: 227040 loss: 0.0014 lr: 0.02\n",
      "iteration: 227050 loss: 0.0015 lr: 0.02\n",
      "iteration: 227060 loss: 0.0017 lr: 0.02\n",
      "iteration: 227070 loss: 0.0019 lr: 0.02\n",
      "iteration: 227080 loss: 0.0033 lr: 0.02\n",
      "iteration: 227090 loss: 0.0022 lr: 0.02\n",
      "iteration: 227100 loss: 0.0020 lr: 0.02\n",
      "iteration: 227110 loss: 0.0021 lr: 0.02\n",
      "iteration: 227120 loss: 0.0014 lr: 0.02\n",
      "iteration: 227130 loss: 0.0018 lr: 0.02\n",
      "iteration: 227140 loss: 0.0019 lr: 0.02\n",
      "iteration: 227150 loss: 0.0015 lr: 0.02\n",
      "iteration: 227160 loss: 0.0027 lr: 0.02\n",
      "iteration: 227170 loss: 0.0022 lr: 0.02\n",
      "iteration: 227180 loss: 0.0013 lr: 0.02\n",
      "iteration: 227190 loss: 0.0017 lr: 0.02\n",
      "iteration: 227200 loss: 0.0022 lr: 0.02\n",
      "iteration: 227210 loss: 0.0025 lr: 0.02\n",
      "iteration: 227220 loss: 0.0020 lr: 0.02\n",
      "iteration: 227230 loss: 0.0018 lr: 0.02\n",
      "iteration: 227240 loss: 0.0030 lr: 0.02\n",
      "iteration: 227250 loss: 0.0034 lr: 0.02\n",
      "iteration: 227260 loss: 0.0016 lr: 0.02\n",
      "iteration: 227270 loss: 0.0024 lr: 0.02\n",
      "iteration: 227280 loss: 0.0013 lr: 0.02\n",
      "iteration: 227290 loss: 0.0021 lr: 0.02\n",
      "iteration: 227300 loss: 0.0020 lr: 0.02\n",
      "iteration: 227310 loss: 0.0023 lr: 0.02\n",
      "iteration: 227320 loss: 0.0018 lr: 0.02\n",
      "iteration: 227330 loss: 0.0019 lr: 0.02\n",
      "iteration: 227340 loss: 0.0017 lr: 0.02\n",
      "iteration: 227350 loss: 0.0026 lr: 0.02\n",
      "iteration: 227360 loss: 0.0032 lr: 0.02\n",
      "iteration: 227370 loss: 0.0024 lr: 0.02\n",
      "iteration: 227380 loss: 0.0015 lr: 0.02\n",
      "iteration: 227390 loss: 0.0017 lr: 0.02\n",
      "iteration: 227400 loss: 0.0018 lr: 0.02\n",
      "iteration: 227410 loss: 0.0018 lr: 0.02\n",
      "iteration: 227420 loss: 0.0018 lr: 0.02\n",
      "iteration: 227430 loss: 0.0018 lr: 0.02\n",
      "iteration: 227440 loss: 0.0025 lr: 0.02\n",
      "iteration: 227450 loss: 0.0016 lr: 0.02\n",
      "iteration: 227460 loss: 0.0015 lr: 0.02\n",
      "iteration: 227470 loss: 0.0015 lr: 0.02\n",
      "iteration: 227480 loss: 0.0017 lr: 0.02\n",
      "iteration: 227490 loss: 0.0025 lr: 0.02\n",
      "iteration: 227500 loss: 0.0020 lr: 0.02\n",
      "iteration: 227510 loss: 0.0014 lr: 0.02\n",
      "iteration: 227520 loss: 0.0017 lr: 0.02\n",
      "iteration: 227530 loss: 0.0023 lr: 0.02\n",
      "iteration: 227540 loss: 0.0011 lr: 0.02\n",
      "iteration: 227550 loss: 0.0029 lr: 0.02\n",
      "iteration: 227560 loss: 0.0016 lr: 0.02\n",
      "iteration: 227570 loss: 0.0018 lr: 0.02\n",
      "iteration: 227580 loss: 0.0021 lr: 0.02\n",
      "iteration: 227590 loss: 0.0024 lr: 0.02\n",
      "iteration: 227600 loss: 0.0020 lr: 0.02\n",
      "iteration: 227610 loss: 0.0011 lr: 0.02\n",
      "iteration: 227620 loss: 0.0021 lr: 0.02\n",
      "iteration: 227630 loss: 0.0018 lr: 0.02\n",
      "iteration: 227640 loss: 0.0016 lr: 0.02\n",
      "iteration: 227650 loss: 0.0019 lr: 0.02\n",
      "iteration: 227660 loss: 0.0019 lr: 0.02\n",
      "iteration: 227670 loss: 0.0024 lr: 0.02\n",
      "iteration: 227680 loss: 0.0019 lr: 0.02\n",
      "iteration: 227690 loss: 0.0020 lr: 0.02\n",
      "iteration: 227700 loss: 0.0020 lr: 0.02\n",
      "iteration: 227710 loss: 0.0015 lr: 0.02\n",
      "iteration: 227720 loss: 0.0020 lr: 0.02\n",
      "iteration: 227730 loss: 0.0014 lr: 0.02\n",
      "iteration: 227740 loss: 0.0016 lr: 0.02\n",
      "iteration: 227750 loss: 0.0016 lr: 0.02\n",
      "iteration: 227760 loss: 0.0017 lr: 0.02\n",
      "iteration: 227770 loss: 0.0020 lr: 0.02\n",
      "iteration: 227780 loss: 0.0022 lr: 0.02\n",
      "iteration: 227790 loss: 0.0025 lr: 0.02\n",
      "iteration: 227800 loss: 0.0019 lr: 0.02\n",
      "iteration: 227810 loss: 0.0016 lr: 0.02\n",
      "iteration: 227820 loss: 0.0020 lr: 0.02\n",
      "iteration: 227830 loss: 0.0015 lr: 0.02\n",
      "iteration: 227840 loss: 0.0021 lr: 0.02\n",
      "iteration: 227850 loss: 0.0020 lr: 0.02\n",
      "iteration: 227860 loss: 0.0019 lr: 0.02\n",
      "iteration: 227870 loss: 0.0023 lr: 0.02\n",
      "iteration: 227880 loss: 0.0018 lr: 0.02\n",
      "iteration: 227890 loss: 0.0016 lr: 0.02\n",
      "iteration: 227900 loss: 0.0020 lr: 0.02\n",
      "iteration: 227910 loss: 0.0023 lr: 0.02\n",
      "iteration: 227920 loss: 0.0022 lr: 0.02\n",
      "iteration: 227930 loss: 0.0018 lr: 0.02\n",
      "iteration: 227940 loss: 0.0024 lr: 0.02\n",
      "iteration: 227950 loss: 0.0025 lr: 0.02\n",
      "iteration: 227960 loss: 0.0014 lr: 0.02\n",
      "iteration: 227970 loss: 0.0021 lr: 0.02\n",
      "iteration: 227980 loss: 0.0027 lr: 0.02\n",
      "iteration: 227990 loss: 0.0020 lr: 0.02\n",
      "iteration: 228000 loss: 0.0016 lr: 0.02\n",
      "iteration: 228010 loss: 0.0019 lr: 0.02\n",
      "iteration: 228020 loss: 0.0013 lr: 0.02\n",
      "iteration: 228030 loss: 0.0023 lr: 0.02\n",
      "iteration: 228040 loss: 0.0020 lr: 0.02\n",
      "iteration: 228050 loss: 0.0018 lr: 0.02\n",
      "iteration: 228060 loss: 0.0013 lr: 0.02\n",
      "iteration: 228070 loss: 0.0022 lr: 0.02\n",
      "iteration: 228080 loss: 0.0024 lr: 0.02\n",
      "iteration: 228090 loss: 0.0026 lr: 0.02\n",
      "iteration: 228100 loss: 0.0022 lr: 0.02\n",
      "iteration: 228110 loss: 0.0024 lr: 0.02\n",
      "iteration: 228120 loss: 0.0036 lr: 0.02\n",
      "iteration: 228130 loss: 0.0014 lr: 0.02\n",
      "iteration: 228140 loss: 0.0027 lr: 0.02\n",
      "iteration: 228150 loss: 0.0028 lr: 0.02\n",
      "iteration: 228160 loss: 0.0015 lr: 0.02\n",
      "iteration: 228170 loss: 0.0017 lr: 0.02\n",
      "iteration: 228180 loss: 0.0014 lr: 0.02\n",
      "iteration: 228190 loss: 0.0022 lr: 0.02\n",
      "iteration: 228200 loss: 0.0027 lr: 0.02\n",
      "iteration: 228210 loss: 0.0020 lr: 0.02\n",
      "iteration: 228220 loss: 0.0018 lr: 0.02\n",
      "iteration: 228230 loss: 0.0017 lr: 0.02\n",
      "iteration: 228240 loss: 0.0022 lr: 0.02\n",
      "iteration: 228250 loss: 0.0017 lr: 0.02\n",
      "iteration: 228260 loss: 0.0017 lr: 0.02\n",
      "iteration: 228270 loss: 0.0029 lr: 0.02\n",
      "iteration: 228280 loss: 0.0031 lr: 0.02\n",
      "iteration: 228290 loss: 0.0016 lr: 0.02\n",
      "iteration: 228300 loss: 0.0031 lr: 0.02\n",
      "iteration: 228310 loss: 0.0031 lr: 0.02\n",
      "iteration: 228320 loss: 0.0018 lr: 0.02\n",
      "iteration: 228330 loss: 0.0021 lr: 0.02\n",
      "iteration: 228340 loss: 0.0023 lr: 0.02\n",
      "iteration: 228350 loss: 0.0024 lr: 0.02\n",
      "iteration: 228360 loss: 0.0016 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 228370 loss: 0.0016 lr: 0.02\n",
      "iteration: 228380 loss: 0.0018 lr: 0.02\n",
      "iteration: 228390 loss: 0.0017 lr: 0.02\n",
      "iteration: 228400 loss: 0.0014 lr: 0.02\n",
      "iteration: 228410 loss: 0.0020 lr: 0.02\n",
      "iteration: 228420 loss: 0.0024 lr: 0.02\n",
      "iteration: 228430 loss: 0.0016 lr: 0.02\n",
      "iteration: 228440 loss: 0.0015 lr: 0.02\n",
      "iteration: 228450 loss: 0.0018 lr: 0.02\n",
      "iteration: 228460 loss: 0.0017 lr: 0.02\n",
      "iteration: 228470 loss: 0.0026 lr: 0.02\n",
      "iteration: 228480 loss: 0.0020 lr: 0.02\n",
      "iteration: 228490 loss: 0.0019 lr: 0.02\n",
      "iteration: 228500 loss: 0.0019 lr: 0.02\n",
      "iteration: 228510 loss: 0.0021 lr: 0.02\n",
      "iteration: 228520 loss: 0.0023 lr: 0.02\n",
      "iteration: 228530 loss: 0.0020 lr: 0.02\n",
      "iteration: 228540 loss: 0.0021 lr: 0.02\n",
      "iteration: 228550 loss: 0.0020 lr: 0.02\n",
      "iteration: 228560 loss: 0.0015 lr: 0.02\n",
      "iteration: 228570 loss: 0.0021 lr: 0.02\n",
      "iteration: 228580 loss: 0.0016 lr: 0.02\n",
      "iteration: 228590 loss: 0.0023 lr: 0.02\n",
      "iteration: 228600 loss: 0.0017 lr: 0.02\n",
      "iteration: 228610 loss: 0.0027 lr: 0.02\n",
      "iteration: 228620 loss: 0.0017 lr: 0.02\n",
      "iteration: 228630 loss: 0.0015 lr: 0.02\n",
      "iteration: 228640 loss: 0.0015 lr: 0.02\n",
      "iteration: 228650 loss: 0.0022 lr: 0.02\n",
      "iteration: 228660 loss: 0.0023 lr: 0.02\n",
      "iteration: 228670 loss: 0.0015 lr: 0.02\n",
      "iteration: 228680 loss: 0.0015 lr: 0.02\n",
      "iteration: 228690 loss: 0.0016 lr: 0.02\n",
      "iteration: 228700 loss: 0.0016 lr: 0.02\n",
      "iteration: 228710 loss: 0.0020 lr: 0.02\n",
      "iteration: 228720 loss: 0.0013 lr: 0.02\n",
      "iteration: 228730 loss: 0.0018 lr: 0.02\n",
      "iteration: 228740 loss: 0.0015 lr: 0.02\n",
      "iteration: 228750 loss: 0.0016 lr: 0.02\n",
      "iteration: 228760 loss: 0.0020 lr: 0.02\n",
      "iteration: 228770 loss: 0.0018 lr: 0.02\n",
      "iteration: 228780 loss: 0.0023 lr: 0.02\n",
      "iteration: 228790 loss: 0.0018 lr: 0.02\n",
      "iteration: 228800 loss: 0.0016 lr: 0.02\n",
      "iteration: 228810 loss: 0.0023 lr: 0.02\n",
      "iteration: 228820 loss: 0.0021 lr: 0.02\n",
      "iteration: 228830 loss: 0.0019 lr: 0.02\n",
      "iteration: 228840 loss: 0.0017 lr: 0.02\n",
      "iteration: 228850 loss: 0.0019 lr: 0.02\n",
      "iteration: 228860 loss: 0.0023 lr: 0.02\n",
      "iteration: 228870 loss: 0.0027 lr: 0.02\n",
      "iteration: 228880 loss: 0.0025 lr: 0.02\n",
      "iteration: 228890 loss: 0.0015 lr: 0.02\n",
      "iteration: 228900 loss: 0.0018 lr: 0.02\n",
      "iteration: 228910 loss: 0.0018 lr: 0.02\n",
      "iteration: 228920 loss: 0.0020 lr: 0.02\n",
      "iteration: 228930 loss: 0.0022 lr: 0.02\n",
      "iteration: 228940 loss: 0.0024 lr: 0.02\n",
      "iteration: 228950 loss: 0.0015 lr: 0.02\n",
      "iteration: 228960 loss: 0.0021 lr: 0.02\n",
      "iteration: 228970 loss: 0.0016 lr: 0.02\n",
      "iteration: 228980 loss: 0.0026 lr: 0.02\n",
      "iteration: 228990 loss: 0.0034 lr: 0.02\n",
      "iteration: 229000 loss: 0.0022 lr: 0.02\n",
      "iteration: 229010 loss: 0.0013 lr: 0.02\n",
      "iteration: 229020 loss: 0.0022 lr: 0.02\n",
      "iteration: 229030 loss: 0.0026 lr: 0.02\n",
      "iteration: 229040 loss: 0.0010 lr: 0.02\n",
      "iteration: 229050 loss: 0.0019 lr: 0.02\n",
      "iteration: 229060 loss: 0.0023 lr: 0.02\n",
      "iteration: 229070 loss: 0.0019 lr: 0.02\n",
      "iteration: 229080 loss: 0.0017 lr: 0.02\n",
      "iteration: 229090 loss: 0.0021 lr: 0.02\n",
      "iteration: 229100 loss: 0.0017 lr: 0.02\n",
      "iteration: 229110 loss: 0.0016 lr: 0.02\n",
      "iteration: 229120 loss: 0.0019 lr: 0.02\n",
      "iteration: 229130 loss: 0.0018 lr: 0.02\n",
      "iteration: 229140 loss: 0.0016 lr: 0.02\n",
      "iteration: 229150 loss: 0.0021 lr: 0.02\n",
      "iteration: 229160 loss: 0.0015 lr: 0.02\n",
      "iteration: 229170 loss: 0.0020 lr: 0.02\n",
      "iteration: 229180 loss: 0.0015 lr: 0.02\n",
      "iteration: 229190 loss: 0.0024 lr: 0.02\n",
      "iteration: 229200 loss: 0.0025 lr: 0.02\n",
      "iteration: 229210 loss: 0.0013 lr: 0.02\n",
      "iteration: 229220 loss: 0.0022 lr: 0.02\n",
      "iteration: 229230 loss: 0.0016 lr: 0.02\n",
      "iteration: 229240 loss: 0.0015 lr: 0.02\n",
      "iteration: 229250 loss: 0.0018 lr: 0.02\n",
      "iteration: 229260 loss: 0.0016 lr: 0.02\n",
      "iteration: 229270 loss: 0.0020 lr: 0.02\n",
      "iteration: 229280 loss: 0.0018 lr: 0.02\n",
      "iteration: 229290 loss: 0.0022 lr: 0.02\n",
      "iteration: 229300 loss: 0.0019 lr: 0.02\n",
      "iteration: 229310 loss: 0.0025 lr: 0.02\n",
      "iteration: 229320 loss: 0.0027 lr: 0.02\n",
      "iteration: 229330 loss: 0.0016 lr: 0.02\n",
      "iteration: 229340 loss: 0.0021 lr: 0.02\n",
      "iteration: 229350 loss: 0.0024 lr: 0.02\n",
      "iteration: 229360 loss: 0.0017 lr: 0.02\n",
      "iteration: 229370 loss: 0.0021 lr: 0.02\n",
      "iteration: 229380 loss: 0.0015 lr: 0.02\n",
      "iteration: 229390 loss: 0.0016 lr: 0.02\n",
      "iteration: 229400 loss: 0.0036 lr: 0.02\n",
      "iteration: 229410 loss: 0.0033 lr: 0.02\n",
      "iteration: 229420 loss: 0.0019 lr: 0.02\n",
      "iteration: 229430 loss: 0.0027 lr: 0.02\n",
      "iteration: 229440 loss: 0.0019 lr: 0.02\n",
      "iteration: 229450 loss: 0.0016 lr: 0.02\n",
      "iteration: 229460 loss: 0.0021 lr: 0.02\n",
      "iteration: 229470 loss: 0.0022 lr: 0.02\n",
      "iteration: 229480 loss: 0.0022 lr: 0.02\n",
      "iteration: 229490 loss: 0.0023 lr: 0.02\n",
      "iteration: 229500 loss: 0.0019 lr: 0.02\n",
      "iteration: 229510 loss: 0.0019 lr: 0.02\n",
      "iteration: 229520 loss: 0.0017 lr: 0.02\n",
      "iteration: 229530 loss: 0.0015 lr: 0.02\n",
      "iteration: 229540 loss: 0.0016 lr: 0.02\n",
      "iteration: 229550 loss: 0.0026 lr: 0.02\n",
      "iteration: 229560 loss: 0.0032 lr: 0.02\n",
      "iteration: 229570 loss: 0.0018 lr: 0.02\n",
      "iteration: 229580 loss: 0.0017 lr: 0.02\n",
      "iteration: 229590 loss: 0.0017 lr: 0.02\n",
      "iteration: 229600 loss: 0.0015 lr: 0.02\n",
      "iteration: 229610 loss: 0.0022 lr: 0.02\n",
      "iteration: 229620 loss: 0.0011 lr: 0.02\n",
      "iteration: 229630 loss: 0.0018 lr: 0.02\n",
      "iteration: 229640 loss: 0.0021 lr: 0.02\n",
      "iteration: 229650 loss: 0.0010 lr: 0.02\n",
      "iteration: 229660 loss: 0.0013 lr: 0.02\n",
      "iteration: 229670 loss: 0.0019 lr: 0.02\n",
      "iteration: 229680 loss: 0.0019 lr: 0.02\n",
      "iteration: 229690 loss: 0.0023 lr: 0.02\n",
      "iteration: 229700 loss: 0.0025 lr: 0.02\n",
      "iteration: 229710 loss: 0.0017 lr: 0.02\n",
      "iteration: 229720 loss: 0.0031 lr: 0.02\n",
      "iteration: 229730 loss: 0.0023 lr: 0.02\n",
      "iteration: 229740 loss: 0.0024 lr: 0.02\n",
      "iteration: 229750 loss: 0.0018 lr: 0.02\n",
      "iteration: 229760 loss: 0.0019 lr: 0.02\n",
      "iteration: 229770 loss: 0.0014 lr: 0.02\n",
      "iteration: 229780 loss: 0.0019 lr: 0.02\n",
      "iteration: 229790 loss: 0.0021 lr: 0.02\n",
      "iteration: 229800 loss: 0.0023 lr: 0.02\n",
      "iteration: 229810 loss: 0.0021 lr: 0.02\n",
      "iteration: 229820 loss: 0.0019 lr: 0.02\n",
      "iteration: 229830 loss: 0.0029 lr: 0.02\n",
      "iteration: 229840 loss: 0.0027 lr: 0.02\n",
      "iteration: 229850 loss: 0.0019 lr: 0.02\n",
      "iteration: 229860 loss: 0.0011 lr: 0.02\n",
      "iteration: 229870 loss: 0.0032 lr: 0.02\n",
      "iteration: 229880 loss: 0.0028 lr: 0.02\n",
      "iteration: 229890 loss: 0.0016 lr: 0.02\n",
      "iteration: 229900 loss: 0.0018 lr: 0.02\n",
      "iteration: 229910 loss: 0.0024 lr: 0.02\n",
      "iteration: 229920 loss: 0.0049 lr: 0.02\n",
      "iteration: 229930 loss: 0.0023 lr: 0.02\n",
      "iteration: 229940 loss: 0.0018 lr: 0.02\n",
      "iteration: 229950 loss: 0.0026 lr: 0.02\n",
      "iteration: 229960 loss: 0.0015 lr: 0.02\n",
      "iteration: 229970 loss: 0.0020 lr: 0.02\n",
      "iteration: 229980 loss: 0.0027 lr: 0.02\n",
      "iteration: 229990 loss: 0.0020 lr: 0.02\n",
      "iteration: 230000 loss: 0.0025 lr: 0.02\n",
      "iteration: 230010 loss: 0.0021 lr: 0.02\n",
      "iteration: 230020 loss: 0.0015 lr: 0.02\n",
      "iteration: 230030 loss: 0.0018 lr: 0.02\n",
      "iteration: 230040 loss: 0.0023 lr: 0.02\n",
      "iteration: 230050 loss: 0.0023 lr: 0.02\n",
      "iteration: 230060 loss: 0.0016 lr: 0.02\n",
      "iteration: 230070 loss: 0.0017 lr: 0.02\n",
      "iteration: 230080 loss: 0.0013 lr: 0.02\n",
      "iteration: 230090 loss: 0.0019 lr: 0.02\n",
      "iteration: 230100 loss: 0.0017 lr: 0.02\n",
      "iteration: 230110 loss: 0.0019 lr: 0.02\n",
      "iteration: 230120 loss: 0.0013 lr: 0.02\n",
      "iteration: 230130 loss: 0.0014 lr: 0.02\n",
      "iteration: 230140 loss: 0.0015 lr: 0.02\n",
      "iteration: 230150 loss: 0.0017 lr: 0.02\n",
      "iteration: 230160 loss: 0.0018 lr: 0.02\n",
      "iteration: 230170 loss: 0.0017 lr: 0.02\n",
      "iteration: 230180 loss: 0.0018 lr: 0.02\n",
      "iteration: 230190 loss: 0.0025 lr: 0.02\n",
      "iteration: 230200 loss: 0.0016 lr: 0.02\n",
      "iteration: 230210 loss: 0.0018 lr: 0.02\n",
      "iteration: 230220 loss: 0.0023 lr: 0.02\n",
      "iteration: 230230 loss: 0.0018 lr: 0.02\n",
      "iteration: 230240 loss: 0.0023 lr: 0.02\n",
      "iteration: 230250 loss: 0.0019 lr: 0.02\n",
      "iteration: 230260 loss: 0.0027 lr: 0.02\n",
      "iteration: 230270 loss: 0.0018 lr: 0.02\n",
      "iteration: 230280 loss: 0.0014 lr: 0.02\n",
      "iteration: 230290 loss: 0.0018 lr: 0.02\n",
      "iteration: 230300 loss: 0.0018 lr: 0.02\n",
      "iteration: 230310 loss: 0.0014 lr: 0.02\n",
      "iteration: 230320 loss: 0.0022 lr: 0.02\n",
      "iteration: 230330 loss: 0.0023 lr: 0.02\n",
      "iteration: 230340 loss: 0.0024 lr: 0.02\n",
      "iteration: 230350 loss: 0.0015 lr: 0.02\n",
      "iteration: 230360 loss: 0.0021 lr: 0.02\n",
      "iteration: 230370 loss: 0.0014 lr: 0.02\n",
      "iteration: 230380 loss: 0.0029 lr: 0.02\n",
      "iteration: 230390 loss: 0.0016 lr: 0.02\n",
      "iteration: 230400 loss: 0.0016 lr: 0.02\n",
      "iteration: 230410 loss: 0.0028 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 230420 loss: 0.0021 lr: 0.02\n",
      "iteration: 230430 loss: 0.0024 lr: 0.02\n",
      "iteration: 230440 loss: 0.0014 lr: 0.02\n",
      "iteration: 230450 loss: 0.0019 lr: 0.02\n",
      "iteration: 230460 loss: 0.0023 lr: 0.02\n",
      "iteration: 230470 loss: 0.0029 lr: 0.02\n",
      "iteration: 230480 loss: 0.0019 lr: 0.02\n",
      "iteration: 230490 loss: 0.0019 lr: 0.02\n",
      "iteration: 230500 loss: 0.0024 lr: 0.02\n",
      "iteration: 230510 loss: 0.0022 lr: 0.02\n",
      "iteration: 230520 loss: 0.0028 lr: 0.02\n",
      "iteration: 230530 loss: 0.0018 lr: 0.02\n",
      "iteration: 230540 loss: 0.0024 lr: 0.02\n",
      "iteration: 230550 loss: 0.0021 lr: 0.02\n",
      "iteration: 230560 loss: 0.0025 lr: 0.02\n",
      "iteration: 230570 loss: 0.0057 lr: 0.02\n",
      "iteration: 230580 loss: 0.0036 lr: 0.02\n",
      "iteration: 230590 loss: 0.0035 lr: 0.02\n",
      "iteration: 230600 loss: 0.0019 lr: 0.02\n",
      "iteration: 230610 loss: 0.0028 lr: 0.02\n",
      "iteration: 230620 loss: 0.0028 lr: 0.02\n",
      "iteration: 230630 loss: 0.0022 lr: 0.02\n",
      "iteration: 230640 loss: 0.0026 lr: 0.02\n",
      "iteration: 230650 loss: 0.0019 lr: 0.02\n",
      "iteration: 230660 loss: 0.0022 lr: 0.02\n",
      "iteration: 230670 loss: 0.0018 lr: 0.02\n",
      "iteration: 230680 loss: 0.0018 lr: 0.02\n",
      "iteration: 230690 loss: 0.0014 lr: 0.02\n",
      "iteration: 230700 loss: 0.0017 lr: 0.02\n",
      "iteration: 230710 loss: 0.0024 lr: 0.02\n",
      "iteration: 230720 loss: 0.0019 lr: 0.02\n",
      "iteration: 230730 loss: 0.0021 lr: 0.02\n",
      "iteration: 230740 loss: 0.0018 lr: 0.02\n",
      "iteration: 230750 loss: 0.0026 lr: 0.02\n",
      "iteration: 230760 loss: 0.0025 lr: 0.02\n",
      "iteration: 230770 loss: 0.0033 lr: 0.02\n",
      "iteration: 230780 loss: 0.0018 lr: 0.02\n",
      "iteration: 230790 loss: 0.0047 lr: 0.02\n",
      "iteration: 230800 loss: 0.0017 lr: 0.02\n",
      "iteration: 230810 loss: 0.0022 lr: 0.02\n",
      "iteration: 230820 loss: 0.0030 lr: 0.02\n",
      "iteration: 230830 loss: 0.0020 lr: 0.02\n",
      "iteration: 230840 loss: 0.0020 lr: 0.02\n",
      "iteration: 230850 loss: 0.0022 lr: 0.02\n",
      "iteration: 230860 loss: 0.0024 lr: 0.02\n",
      "iteration: 230870 loss: 0.0022 lr: 0.02\n",
      "iteration: 230880 loss: 0.0019 lr: 0.02\n",
      "iteration: 230890 loss: 0.0018 lr: 0.02\n",
      "iteration: 230900 loss: 0.0015 lr: 0.02\n",
      "iteration: 230910 loss: 0.0018 lr: 0.02\n",
      "iteration: 230920 loss: 0.0032 lr: 0.02\n",
      "iteration: 230930 loss: 0.0022 lr: 0.02\n",
      "iteration: 230940 loss: 0.0027 lr: 0.02\n",
      "iteration: 230950 loss: 0.0016 lr: 0.02\n",
      "iteration: 230960 loss: 0.0018 lr: 0.02\n",
      "iteration: 230970 loss: 0.0022 lr: 0.02\n",
      "iteration: 230980 loss: 0.0029 lr: 0.02\n",
      "iteration: 230990 loss: 0.0027 lr: 0.02\n",
      "iteration: 231000 loss: 0.0022 lr: 0.02\n",
      "iteration: 231010 loss: 0.0018 lr: 0.02\n",
      "iteration: 231020 loss: 0.0021 lr: 0.02\n",
      "iteration: 231030 loss: 0.0022 lr: 0.02\n",
      "iteration: 231040 loss: 0.0019 lr: 0.02\n",
      "iteration: 231050 loss: 0.0020 lr: 0.02\n",
      "iteration: 231060 loss: 0.0019 lr: 0.02\n",
      "iteration: 231070 loss: 0.0024 lr: 0.02\n",
      "iteration: 231080 loss: 0.0018 lr: 0.02\n",
      "iteration: 231090 loss: 0.0024 lr: 0.02\n",
      "iteration: 231100 loss: 0.0020 lr: 0.02\n",
      "iteration: 231110 loss: 0.0019 lr: 0.02\n",
      "iteration: 231120 loss: 0.0029 lr: 0.02\n",
      "iteration: 231130 loss: 0.0029 lr: 0.02\n",
      "iteration: 231140 loss: 0.0022 lr: 0.02\n",
      "iteration: 231150 loss: 0.0022 lr: 0.02\n",
      "iteration: 231160 loss: 0.0018 lr: 0.02\n",
      "iteration: 231170 loss: 0.0019 lr: 0.02\n",
      "iteration: 231180 loss: 0.0025 lr: 0.02\n",
      "iteration: 231190 loss: 0.0022 lr: 0.02\n",
      "iteration: 231200 loss: 0.0029 lr: 0.02\n",
      "iteration: 231210 loss: 0.0024 lr: 0.02\n",
      "iteration: 231220 loss: 0.0025 lr: 0.02\n",
      "iteration: 231230 loss: 0.0022 lr: 0.02\n",
      "iteration: 231240 loss: 0.0017 lr: 0.02\n",
      "iteration: 231250 loss: 0.0018 lr: 0.02\n",
      "iteration: 231260 loss: 0.0015 lr: 0.02\n",
      "iteration: 231270 loss: 0.0017 lr: 0.02\n",
      "iteration: 231280 loss: 0.0023 lr: 0.02\n",
      "iteration: 231290 loss: 0.0014 lr: 0.02\n",
      "iteration: 231300 loss: 0.0015 lr: 0.02\n",
      "iteration: 231310 loss: 0.0029 lr: 0.02\n",
      "iteration: 231320 loss: 0.0018 lr: 0.02\n",
      "iteration: 231330 loss: 0.0017 lr: 0.02\n",
      "iteration: 231340 loss: 0.0020 lr: 0.02\n",
      "iteration: 231350 loss: 0.0019 lr: 0.02\n",
      "iteration: 231360 loss: 0.0021 lr: 0.02\n",
      "iteration: 231370 loss: 0.0015 lr: 0.02\n",
      "iteration: 231380 loss: 0.0024 lr: 0.02\n",
      "iteration: 231390 loss: 0.0019 lr: 0.02\n",
      "iteration: 231400 loss: 0.0026 lr: 0.02\n",
      "iteration: 231410 loss: 0.0020 lr: 0.02\n",
      "iteration: 231420 loss: 0.0017 lr: 0.02\n",
      "iteration: 231430 loss: 0.0022 lr: 0.02\n",
      "iteration: 231440 loss: 0.0013 lr: 0.02\n",
      "iteration: 231450 loss: 0.0019 lr: 0.02\n",
      "iteration: 231460 loss: 0.0013 lr: 0.02\n",
      "iteration: 231470 loss: 0.0019 lr: 0.02\n",
      "iteration: 231480 loss: 0.0027 lr: 0.02\n",
      "iteration: 231490 loss: 0.0021 lr: 0.02\n",
      "iteration: 231500 loss: 0.0024 lr: 0.02\n",
      "iteration: 231510 loss: 0.0024 lr: 0.02\n",
      "iteration: 231520 loss: 0.0022 lr: 0.02\n",
      "iteration: 231530 loss: 0.0020 lr: 0.02\n",
      "iteration: 231540 loss: 0.0016 lr: 0.02\n",
      "iteration: 231550 loss: 0.0024 lr: 0.02\n",
      "iteration: 231560 loss: 0.0018 lr: 0.02\n",
      "iteration: 231570 loss: 0.0021 lr: 0.02\n",
      "iteration: 231580 loss: 0.0022 lr: 0.02\n",
      "iteration: 231590 loss: 0.0021 lr: 0.02\n",
      "iteration: 231600 loss: 0.0039 lr: 0.02\n",
      "iteration: 231610 loss: 0.0018 lr: 0.02\n",
      "iteration: 231620 loss: 0.0020 lr: 0.02\n",
      "iteration: 231630 loss: 0.0025 lr: 0.02\n",
      "iteration: 231640 loss: 0.0020 lr: 0.02\n",
      "iteration: 231650 loss: 0.0015 lr: 0.02\n",
      "iteration: 231660 loss: 0.0028 lr: 0.02\n",
      "iteration: 231670 loss: 0.0017 lr: 0.02\n",
      "iteration: 231680 loss: 0.0016 lr: 0.02\n",
      "iteration: 231690 loss: 0.0015 lr: 0.02\n",
      "iteration: 231700 loss: 0.0018 lr: 0.02\n",
      "iteration: 231710 loss: 0.0019 lr: 0.02\n",
      "iteration: 231720 loss: 0.0021 lr: 0.02\n",
      "iteration: 231730 loss: 0.0022 lr: 0.02\n",
      "iteration: 231740 loss: 0.0018 lr: 0.02\n",
      "iteration: 231750 loss: 0.0020 lr: 0.02\n",
      "iteration: 231760 loss: 0.0015 lr: 0.02\n",
      "iteration: 231770 loss: 0.0013 lr: 0.02\n",
      "iteration: 231780 loss: 0.0016 lr: 0.02\n",
      "iteration: 231790 loss: 0.0021 lr: 0.02\n",
      "iteration: 231800 loss: 0.0018 lr: 0.02\n",
      "iteration: 231810 loss: 0.0019 lr: 0.02\n",
      "iteration: 231820 loss: 0.0018 lr: 0.02\n",
      "iteration: 231830 loss: 0.0019 lr: 0.02\n",
      "iteration: 231840 loss: 0.0020 lr: 0.02\n",
      "iteration: 231850 loss: 0.0015 lr: 0.02\n",
      "iteration: 231860 loss: 0.0022 lr: 0.02\n",
      "iteration: 231870 loss: 0.0022 lr: 0.02\n",
      "iteration: 231880 loss: 0.0018 lr: 0.02\n",
      "iteration: 231890 loss: 0.0024 lr: 0.02\n",
      "iteration: 231900 loss: 0.0017 lr: 0.02\n",
      "iteration: 231910 loss: 0.0022 lr: 0.02\n",
      "iteration: 231920 loss: 0.0020 lr: 0.02\n",
      "iteration: 231930 loss: 0.0030 lr: 0.02\n",
      "iteration: 231940 loss: 0.0020 lr: 0.02\n",
      "iteration: 231950 loss: 0.0019 lr: 0.02\n",
      "iteration: 231960 loss: 0.0018 lr: 0.02\n",
      "iteration: 231970 loss: 0.0015 lr: 0.02\n",
      "iteration: 231980 loss: 0.0028 lr: 0.02\n",
      "iteration: 231990 loss: 0.0019 lr: 0.02\n",
      "iteration: 232000 loss: 0.0014 lr: 0.02\n",
      "iteration: 232010 loss: 0.0025 lr: 0.02\n",
      "iteration: 232020 loss: 0.0017 lr: 0.02\n",
      "iteration: 232030 loss: 0.0020 lr: 0.02\n",
      "iteration: 232040 loss: 0.0015 lr: 0.02\n",
      "iteration: 232050 loss: 0.0016 lr: 0.02\n",
      "iteration: 232060 loss: 0.0023 lr: 0.02\n",
      "iteration: 232070 loss: 0.0015 lr: 0.02\n",
      "iteration: 232080 loss: 0.0014 lr: 0.02\n",
      "iteration: 232090 loss: 0.0020 lr: 0.02\n",
      "iteration: 232100 loss: 0.0028 lr: 0.02\n",
      "iteration: 232110 loss: 0.0017 lr: 0.02\n",
      "iteration: 232120 loss: 0.0020 lr: 0.02\n",
      "iteration: 232130 loss: 0.0020 lr: 0.02\n",
      "iteration: 232140 loss: 0.0023 lr: 0.02\n",
      "iteration: 232150 loss: 0.0018 lr: 0.02\n",
      "iteration: 232160 loss: 0.0028 lr: 0.02\n",
      "iteration: 232170 loss: 0.0020 lr: 0.02\n",
      "iteration: 232180 loss: 0.0024 lr: 0.02\n",
      "iteration: 232190 loss: 0.0024 lr: 0.02\n",
      "iteration: 232200 loss: 0.0025 lr: 0.02\n",
      "iteration: 232210 loss: 0.0023 lr: 0.02\n",
      "iteration: 232220 loss: 0.0024 lr: 0.02\n",
      "iteration: 232230 loss: 0.0028 lr: 0.02\n",
      "iteration: 232240 loss: 0.0033 lr: 0.02\n",
      "iteration: 232250 loss: 0.0021 lr: 0.02\n",
      "iteration: 232260 loss: 0.0012 lr: 0.02\n",
      "iteration: 232270 loss: 0.0018 lr: 0.02\n",
      "iteration: 232280 loss: 0.0036 lr: 0.02\n",
      "iteration: 232290 loss: 0.0018 lr: 0.02\n",
      "iteration: 232300 loss: 0.0020 lr: 0.02\n",
      "iteration: 232310 loss: 0.0044 lr: 0.02\n",
      "iteration: 232320 loss: 0.0025 lr: 0.02\n",
      "iteration: 232330 loss: 0.0036 lr: 0.02\n",
      "iteration: 232340 loss: 0.0016 lr: 0.02\n",
      "iteration: 232350 loss: 0.0014 lr: 0.02\n",
      "iteration: 232360 loss: 0.0020 lr: 0.02\n",
      "iteration: 232370 loss: 0.0021 lr: 0.02\n",
      "iteration: 232380 loss: 0.0025 lr: 0.02\n",
      "iteration: 232390 loss: 0.0016 lr: 0.02\n",
      "iteration: 232400 loss: 0.0014 lr: 0.02\n",
      "iteration: 232410 loss: 0.0018 lr: 0.02\n",
      "iteration: 232420 loss: 0.0020 lr: 0.02\n",
      "iteration: 232430 loss: 0.0017 lr: 0.02\n",
      "iteration: 232440 loss: 0.0034 lr: 0.02\n",
      "iteration: 232450 loss: 0.0017 lr: 0.02\n",
      "iteration: 232460 loss: 0.0014 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 232470 loss: 0.0019 lr: 0.02\n",
      "iteration: 232480 loss: 0.0030 lr: 0.02\n",
      "iteration: 232490 loss: 0.0023 lr: 0.02\n",
      "iteration: 232500 loss: 0.0016 lr: 0.02\n",
      "iteration: 232510 loss: 0.0014 lr: 0.02\n",
      "iteration: 232520 loss: 0.0017 lr: 0.02\n",
      "iteration: 232530 loss: 0.0021 lr: 0.02\n",
      "iteration: 232540 loss: 0.0020 lr: 0.02\n",
      "iteration: 232550 loss: 0.0020 lr: 0.02\n",
      "iteration: 232560 loss: 0.0021 lr: 0.02\n",
      "iteration: 232570 loss: 0.0021 lr: 0.02\n",
      "iteration: 232580 loss: 0.0020 lr: 0.02\n",
      "iteration: 232590 loss: 0.0014 lr: 0.02\n",
      "iteration: 232600 loss: 0.0018 lr: 0.02\n",
      "iteration: 232610 loss: 0.0015 lr: 0.02\n",
      "iteration: 232620 loss: 0.0010 lr: 0.02\n",
      "iteration: 232630 loss: 0.0039 lr: 0.02\n",
      "iteration: 232640 loss: 0.0014 lr: 0.02\n",
      "iteration: 232650 loss: 0.0025 lr: 0.02\n",
      "iteration: 232660 loss: 0.0019 lr: 0.02\n",
      "iteration: 232670 loss: 0.0018 lr: 0.02\n",
      "iteration: 232680 loss: 0.0016 lr: 0.02\n",
      "iteration: 232690 loss: 0.0020 lr: 0.02\n",
      "iteration: 232700 loss: 0.0022 lr: 0.02\n",
      "iteration: 232710 loss: 0.0013 lr: 0.02\n",
      "iteration: 232720 loss: 0.0027 lr: 0.02\n",
      "iteration: 232730 loss: 0.0024 lr: 0.02\n",
      "iteration: 232740 loss: 0.0016 lr: 0.02\n",
      "iteration: 232750 loss: 0.0025 lr: 0.02\n",
      "iteration: 232760 loss: 0.0017 lr: 0.02\n",
      "iteration: 232770 loss: 0.0018 lr: 0.02\n",
      "iteration: 232780 loss: 0.0016 lr: 0.02\n",
      "iteration: 232790 loss: 0.0023 lr: 0.02\n",
      "iteration: 232800 loss: 0.0029 lr: 0.02\n",
      "iteration: 232810 loss: 0.0027 lr: 0.02\n",
      "iteration: 232820 loss: 0.0018 lr: 0.02\n",
      "iteration: 232830 loss: 0.0016 lr: 0.02\n",
      "iteration: 232840 loss: 0.0018 lr: 0.02\n",
      "iteration: 232850 loss: 0.0013 lr: 0.02\n",
      "iteration: 232860 loss: 0.0016 lr: 0.02\n",
      "iteration: 232870 loss: 0.0019 lr: 0.02\n",
      "iteration: 232880 loss: 0.0016 lr: 0.02\n",
      "iteration: 232890 loss: 0.0019 lr: 0.02\n",
      "iteration: 232900 loss: 0.0017 lr: 0.02\n",
      "iteration: 232910 loss: 0.0015 lr: 0.02\n",
      "iteration: 232920 loss: 0.0021 lr: 0.02\n",
      "iteration: 232930 loss: 0.0019 lr: 0.02\n",
      "iteration: 232940 loss: 0.0021 lr: 0.02\n",
      "iteration: 232950 loss: 0.0017 lr: 0.02\n",
      "iteration: 232960 loss: 0.0022 lr: 0.02\n",
      "iteration: 232970 loss: 0.0027 lr: 0.02\n",
      "iteration: 232980 loss: 0.0019 lr: 0.02\n",
      "iteration: 232990 loss: 0.0015 lr: 0.02\n",
      "iteration: 233000 loss: 0.0014 lr: 0.02\n",
      "iteration: 233010 loss: 0.0038 lr: 0.02\n",
      "iteration: 233020 loss: 0.0034 lr: 0.02\n",
      "iteration: 233030 loss: 0.0028 lr: 0.02\n",
      "iteration: 233040 loss: 0.0019 lr: 0.02\n",
      "iteration: 233050 loss: 0.0017 lr: 0.02\n",
      "iteration: 233060 loss: 0.0029 lr: 0.02\n",
      "iteration: 233070 loss: 0.0016 lr: 0.02\n",
      "iteration: 233080 loss: 0.0018 lr: 0.02\n",
      "iteration: 233090 loss: 0.0026 lr: 0.02\n",
      "iteration: 233100 loss: 0.0023 lr: 0.02\n",
      "iteration: 233110 loss: 0.0018 lr: 0.02\n",
      "iteration: 233120 loss: 0.0020 lr: 0.02\n",
      "iteration: 233130 loss: 0.0023 lr: 0.02\n",
      "iteration: 233140 loss: 0.0024 lr: 0.02\n",
      "iteration: 233150 loss: 0.0017 lr: 0.02\n",
      "iteration: 233160 loss: 0.0016 lr: 0.02\n",
      "iteration: 233170 loss: 0.0013 lr: 0.02\n",
      "iteration: 233180 loss: 0.0013 lr: 0.02\n",
      "iteration: 233190 loss: 0.0031 lr: 0.02\n",
      "iteration: 233200 loss: 0.0018 lr: 0.02\n",
      "iteration: 233210 loss: 0.0037 lr: 0.02\n",
      "iteration: 233220 loss: 0.0016 lr: 0.02\n",
      "iteration: 233230 loss: 0.0022 lr: 0.02\n",
      "iteration: 233240 loss: 0.0014 lr: 0.02\n",
      "iteration: 233250 loss: 0.0017 lr: 0.02\n",
      "iteration: 233260 loss: 0.0015 lr: 0.02\n",
      "iteration: 233270 loss: 0.0017 lr: 0.02\n",
      "iteration: 233280 loss: 0.0027 lr: 0.02\n",
      "iteration: 233290 loss: 0.0040 lr: 0.02\n",
      "iteration: 233300 loss: 0.0016 lr: 0.02\n",
      "iteration: 233310 loss: 0.0021 lr: 0.02\n",
      "iteration: 233320 loss: 0.0019 lr: 0.02\n",
      "iteration: 233330 loss: 0.0034 lr: 0.02\n",
      "iteration: 233340 loss: 0.0018 lr: 0.02\n",
      "iteration: 233350 loss: 0.0015 lr: 0.02\n",
      "iteration: 233360 loss: 0.0022 lr: 0.02\n",
      "iteration: 233370 loss: 0.0020 lr: 0.02\n",
      "iteration: 233380 loss: 0.0018 lr: 0.02\n",
      "iteration: 233390 loss: 0.0021 lr: 0.02\n",
      "iteration: 233400 loss: 0.0022 lr: 0.02\n",
      "iteration: 233410 loss: 0.0020 lr: 0.02\n",
      "iteration: 233420 loss: 0.0016 lr: 0.02\n",
      "iteration: 233430 loss: 0.0024 lr: 0.02\n",
      "iteration: 233440 loss: 0.0017 lr: 0.02\n",
      "iteration: 233450 loss: 0.0027 lr: 0.02\n",
      "iteration: 233460 loss: 0.0033 lr: 0.02\n",
      "iteration: 233470 loss: 0.0025 lr: 0.02\n",
      "iteration: 233480 loss: 0.0025 lr: 0.02\n",
      "iteration: 233490 loss: 0.0017 lr: 0.02\n",
      "iteration: 233500 loss: 0.0015 lr: 0.02\n",
      "iteration: 233510 loss: 0.0026 lr: 0.02\n",
      "iteration: 233520 loss: 0.0021 lr: 0.02\n",
      "iteration: 233530 loss: 0.0011 lr: 0.02\n",
      "iteration: 233540 loss: 0.0017 lr: 0.02\n",
      "iteration: 233550 loss: 0.0019 lr: 0.02\n",
      "iteration: 233560 loss: 0.0020 lr: 0.02\n",
      "iteration: 233570 loss: 0.0015 lr: 0.02\n",
      "iteration: 233580 loss: 0.0038 lr: 0.02\n",
      "iteration: 233590 loss: 0.0019 lr: 0.02\n",
      "iteration: 233600 loss: 0.0030 lr: 0.02\n",
      "iteration: 233610 loss: 0.0019 lr: 0.02\n",
      "iteration: 233620 loss: 0.0017 lr: 0.02\n",
      "iteration: 233630 loss: 0.0018 lr: 0.02\n",
      "iteration: 233640 loss: 0.0022 lr: 0.02\n",
      "iteration: 233650 loss: 0.0018 lr: 0.02\n",
      "iteration: 233660 loss: 0.0016 lr: 0.02\n",
      "iteration: 233670 loss: 0.0019 lr: 0.02\n",
      "iteration: 233680 loss: 0.0048 lr: 0.02\n",
      "iteration: 233690 loss: 0.0019 lr: 0.02\n",
      "iteration: 233700 loss: 0.0021 lr: 0.02\n",
      "iteration: 233710 loss: 0.0016 lr: 0.02\n",
      "iteration: 233720 loss: 0.0022 lr: 0.02\n",
      "iteration: 233730 loss: 0.0017 lr: 0.02\n",
      "iteration: 233740 loss: 0.0024 lr: 0.02\n",
      "iteration: 233750 loss: 0.0023 lr: 0.02\n",
      "iteration: 233760 loss: 0.0025 lr: 0.02\n",
      "iteration: 233770 loss: 0.0021 lr: 0.02\n",
      "iteration: 233780 loss: 0.0022 lr: 0.02\n",
      "iteration: 233790 loss: 0.0018 lr: 0.02\n",
      "iteration: 233800 loss: 0.0017 lr: 0.02\n",
      "iteration: 233810 loss: 0.0030 lr: 0.02\n",
      "iteration: 233820 loss: 0.0017 lr: 0.02\n",
      "iteration: 233830 loss: 0.0021 lr: 0.02\n",
      "iteration: 233840 loss: 0.0016 lr: 0.02\n",
      "iteration: 233850 loss: 0.0019 lr: 0.02\n",
      "iteration: 233860 loss: 0.0026 lr: 0.02\n",
      "iteration: 233870 loss: 0.0022 lr: 0.02\n",
      "iteration: 233880 loss: 0.0021 lr: 0.02\n",
      "iteration: 233890 loss: 0.0016 lr: 0.02\n",
      "iteration: 233900 loss: 0.0020 lr: 0.02\n",
      "iteration: 233910 loss: 0.0018 lr: 0.02\n",
      "iteration: 233920 loss: 0.0024 lr: 0.02\n",
      "iteration: 233930 loss: 0.0021 lr: 0.02\n",
      "iteration: 233940 loss: 0.0034 lr: 0.02\n",
      "iteration: 233950 loss: 0.0018 lr: 0.02\n",
      "iteration: 233960 loss: 0.0021 lr: 0.02\n",
      "iteration: 233970 loss: 0.0021 lr: 0.02\n",
      "iteration: 233980 loss: 0.0016 lr: 0.02\n",
      "iteration: 233990 loss: 0.0016 lr: 0.02\n",
      "iteration: 234000 loss: 0.0017 lr: 0.02\n",
      "iteration: 234010 loss: 0.0020 lr: 0.02\n",
      "iteration: 234020 loss: 0.0025 lr: 0.02\n",
      "iteration: 234030 loss: 0.0018 lr: 0.02\n",
      "iteration: 234040 loss: 0.0019 lr: 0.02\n",
      "iteration: 234050 loss: 0.0019 lr: 0.02\n",
      "iteration: 234060 loss: 0.0018 lr: 0.02\n",
      "iteration: 234070 loss: 0.0018 lr: 0.02\n",
      "iteration: 234080 loss: 0.0024 lr: 0.02\n",
      "iteration: 234090 loss: 0.0018 lr: 0.02\n",
      "iteration: 234100 loss: 0.0013 lr: 0.02\n",
      "iteration: 234110 loss: 0.0015 lr: 0.02\n",
      "iteration: 234120 loss: 0.0017 lr: 0.02\n",
      "iteration: 234130 loss: 0.0022 lr: 0.02\n",
      "iteration: 234140 loss: 0.0018 lr: 0.02\n",
      "iteration: 234150 loss: 0.0021 lr: 0.02\n",
      "iteration: 234160 loss: 0.0016 lr: 0.02\n",
      "iteration: 234170 loss: 0.0020 lr: 0.02\n",
      "iteration: 234180 loss: 0.0020 lr: 0.02\n",
      "iteration: 234190 loss: 0.0020 lr: 0.02\n",
      "iteration: 234200 loss: 0.0020 lr: 0.02\n",
      "iteration: 234210 loss: 0.0020 lr: 0.02\n",
      "iteration: 234220 loss: 0.0024 lr: 0.02\n",
      "iteration: 234230 loss: 0.0025 lr: 0.02\n",
      "iteration: 234240 loss: 0.0021 lr: 0.02\n",
      "iteration: 234250 loss: 0.0035 lr: 0.02\n",
      "iteration: 234260 loss: 0.0022 lr: 0.02\n",
      "iteration: 234270 loss: 0.0020 lr: 0.02\n",
      "iteration: 234280 loss: 0.0017 lr: 0.02\n",
      "iteration: 234290 loss: 0.0031 lr: 0.02\n",
      "iteration: 234300 loss: 0.0023 lr: 0.02\n",
      "iteration: 234310 loss: 0.0020 lr: 0.02\n",
      "iteration: 234320 loss: 0.0018 lr: 0.02\n",
      "iteration: 234330 loss: 0.0028 lr: 0.02\n",
      "iteration: 234340 loss: 0.0018 lr: 0.02\n",
      "iteration: 234350 loss: 0.0036 lr: 0.02\n",
      "iteration: 234360 loss: 0.0023 lr: 0.02\n",
      "iteration: 234370 loss: 0.0023 lr: 0.02\n",
      "iteration: 234380 loss: 0.0021 lr: 0.02\n",
      "iteration: 234390 loss: 0.0020 lr: 0.02\n",
      "iteration: 234400 loss: 0.0016 lr: 0.02\n",
      "iteration: 234410 loss: 0.0017 lr: 0.02\n",
      "iteration: 234420 loss: 0.0014 lr: 0.02\n",
      "iteration: 234430 loss: 0.0016 lr: 0.02\n",
      "iteration: 234440 loss: 0.0020 lr: 0.02\n",
      "iteration: 234450 loss: 0.0025 lr: 0.02\n",
      "iteration: 234460 loss: 0.0038 lr: 0.02\n",
      "iteration: 234470 loss: 0.0015 lr: 0.02\n",
      "iteration: 234480 loss: 0.0023 lr: 0.02\n",
      "iteration: 234490 loss: 0.0028 lr: 0.02\n",
      "iteration: 234500 loss: 0.0011 lr: 0.02\n",
      "iteration: 234510 loss: 0.0018 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 234520 loss: 0.0014 lr: 0.02\n",
      "iteration: 234530 loss: 0.0019 lr: 0.02\n",
      "iteration: 234540 loss: 0.0015 lr: 0.02\n",
      "iteration: 234550 loss: 0.0026 lr: 0.02\n",
      "iteration: 234560 loss: 0.0032 lr: 0.02\n",
      "iteration: 234570 loss: 0.0019 lr: 0.02\n",
      "iteration: 234580 loss: 0.0016 lr: 0.02\n",
      "iteration: 234590 loss: 0.0015 lr: 0.02\n",
      "iteration: 234600 loss: 0.0028 lr: 0.02\n",
      "iteration: 234610 loss: 0.0010 lr: 0.02\n",
      "iteration: 234620 loss: 0.0015 lr: 0.02\n",
      "iteration: 234630 loss: 0.0020 lr: 0.02\n",
      "iteration: 234640 loss: 0.0017 lr: 0.02\n",
      "iteration: 234650 loss: 0.0014 lr: 0.02\n",
      "iteration: 234660 loss: 0.0018 lr: 0.02\n",
      "iteration: 234670 loss: 0.0015 lr: 0.02\n",
      "iteration: 234680 loss: 0.0022 lr: 0.02\n",
      "iteration: 234690 loss: 0.0017 lr: 0.02\n",
      "iteration: 234700 loss: 0.0021 lr: 0.02\n",
      "iteration: 234710 loss: 0.0016 lr: 0.02\n",
      "iteration: 234720 loss: 0.0017 lr: 0.02\n",
      "iteration: 234730 loss: 0.0020 lr: 0.02\n",
      "iteration: 234740 loss: 0.0021 lr: 0.02\n",
      "iteration: 234750 loss: 0.0018 lr: 0.02\n",
      "iteration: 234760 loss: 0.0017 lr: 0.02\n",
      "iteration: 234770 loss: 0.0013 lr: 0.02\n",
      "iteration: 234780 loss: 0.0015 lr: 0.02\n",
      "iteration: 234790 loss: 0.0021 lr: 0.02\n",
      "iteration: 234800 loss: 0.0019 lr: 0.02\n",
      "iteration: 234810 loss: 0.0015 lr: 0.02\n",
      "iteration: 234820 loss: 0.0018 lr: 0.02\n",
      "iteration: 234830 loss: 0.0021 lr: 0.02\n",
      "iteration: 234840 loss: 0.0025 lr: 0.02\n",
      "iteration: 234850 loss: 0.0017 lr: 0.02\n",
      "iteration: 234860 loss: 0.0026 lr: 0.02\n",
      "iteration: 234870 loss: 0.0019 lr: 0.02\n",
      "iteration: 234880 loss: 0.0021 lr: 0.02\n",
      "iteration: 234890 loss: 0.0020 lr: 0.02\n",
      "iteration: 234900 loss: 0.0018 lr: 0.02\n",
      "iteration: 234910 loss: 0.0017 lr: 0.02\n",
      "iteration: 234920 loss: 0.0019 lr: 0.02\n",
      "iteration: 234930 loss: 0.0017 lr: 0.02\n",
      "iteration: 234940 loss: 0.0018 lr: 0.02\n",
      "iteration: 234950 loss: 0.0020 lr: 0.02\n",
      "iteration: 234960 loss: 0.0018 lr: 0.02\n",
      "iteration: 234970 loss: 0.0015 lr: 0.02\n",
      "iteration: 234980 loss: 0.0021 lr: 0.02\n",
      "iteration: 234990 loss: 0.0023 lr: 0.02\n",
      "iteration: 235000 loss: 0.0018 lr: 0.02\n",
      "iteration: 235010 loss: 0.0017 lr: 0.02\n",
      "iteration: 235020 loss: 0.0017 lr: 0.02\n",
      "iteration: 235030 loss: 0.0022 lr: 0.02\n",
      "iteration: 235040 loss: 0.0023 lr: 0.02\n",
      "iteration: 235050 loss: 0.0023 lr: 0.02\n",
      "iteration: 235060 loss: 0.0018 lr: 0.02\n",
      "iteration: 235070 loss: 0.0023 lr: 0.02\n",
      "iteration: 235080 loss: 0.0015 lr: 0.02\n",
      "iteration: 235090 loss: 0.0017 lr: 0.02\n",
      "iteration: 235100 loss: 0.0025 lr: 0.02\n",
      "iteration: 235110 loss: 0.0022 lr: 0.02\n",
      "iteration: 235120 loss: 0.0021 lr: 0.02\n",
      "iteration: 235130 loss: 0.0013 lr: 0.02\n",
      "iteration: 235140 loss: 0.0016 lr: 0.02\n",
      "iteration: 235150 loss: 0.0021 lr: 0.02\n",
      "iteration: 235160 loss: 0.0023 lr: 0.02\n",
      "iteration: 235170 loss: 0.0025 lr: 0.02\n",
      "iteration: 235180 loss: 0.0016 lr: 0.02\n",
      "iteration: 235190 loss: 0.0019 lr: 0.02\n",
      "iteration: 235200 loss: 0.0023 lr: 0.02\n",
      "iteration: 235210 loss: 0.0037 lr: 0.02\n",
      "iteration: 235220 loss: 0.0021 lr: 0.02\n",
      "iteration: 235230 loss: 0.0018 lr: 0.02\n",
      "iteration: 235240 loss: 0.0026 lr: 0.02\n",
      "iteration: 235250 loss: 0.0026 lr: 0.02\n",
      "iteration: 235260 loss: 0.0017 lr: 0.02\n",
      "iteration: 235270 loss: 0.0023 lr: 0.02\n",
      "iteration: 235280 loss: 0.0020 lr: 0.02\n",
      "iteration: 235290 loss: 0.0020 lr: 0.02\n",
      "iteration: 235300 loss: 0.0023 lr: 0.02\n",
      "iteration: 235310 loss: 0.0019 lr: 0.02\n",
      "iteration: 235320 loss: 0.0010 lr: 0.02\n",
      "iteration: 235330 loss: 0.0018 lr: 0.02\n",
      "iteration: 235340 loss: 0.0019 lr: 0.02\n",
      "iteration: 235350 loss: 0.0018 lr: 0.02\n",
      "iteration: 235360 loss: 0.0020 lr: 0.02\n",
      "iteration: 235370 loss: 0.0019 lr: 0.02\n",
      "iteration: 235380 loss: 0.0018 lr: 0.02\n",
      "iteration: 235390 loss: 0.0018 lr: 0.02\n",
      "iteration: 235400 loss: 0.0019 lr: 0.02\n",
      "iteration: 235410 loss: 0.0023 lr: 0.02\n",
      "iteration: 235420 loss: 0.0027 lr: 0.02\n",
      "iteration: 235430 loss: 0.0026 lr: 0.02\n",
      "iteration: 235440 loss: 0.0026 lr: 0.02\n",
      "iteration: 235450 loss: 0.0019 lr: 0.02\n",
      "iteration: 235460 loss: 0.0023 lr: 0.02\n",
      "iteration: 235470 loss: 0.0020 lr: 0.02\n",
      "iteration: 235480 loss: 0.0019 lr: 0.02\n",
      "iteration: 235490 loss: 0.0020 lr: 0.02\n",
      "iteration: 235500 loss: 0.0028 lr: 0.02\n",
      "iteration: 235510 loss: 0.0020 lr: 0.02\n",
      "iteration: 235520 loss: 0.0024 lr: 0.02\n",
      "iteration: 235530 loss: 0.0018 lr: 0.02\n",
      "iteration: 235540 loss: 0.0020 lr: 0.02\n",
      "iteration: 235550 loss: 0.0019 lr: 0.02\n",
      "iteration: 235560 loss: 0.0015 lr: 0.02\n",
      "iteration: 235570 loss: 0.0022 lr: 0.02\n",
      "iteration: 235580 loss: 0.0021 lr: 0.02\n",
      "iteration: 235590 loss: 0.0023 lr: 0.02\n",
      "iteration: 235600 loss: 0.0022 lr: 0.02\n",
      "iteration: 235610 loss: 0.0016 lr: 0.02\n",
      "iteration: 235620 loss: 0.0019 lr: 0.02\n",
      "iteration: 235630 loss: 0.0018 lr: 0.02\n",
      "iteration: 235640 loss: 0.0021 lr: 0.02\n",
      "iteration: 235650 loss: 0.0025 lr: 0.02\n",
      "iteration: 235660 loss: 0.0015 lr: 0.02\n",
      "iteration: 235670 loss: 0.0018 lr: 0.02\n",
      "iteration: 235680 loss: 0.0023 lr: 0.02\n",
      "iteration: 235690 loss: 0.0016 lr: 0.02\n",
      "iteration: 235700 loss: 0.0026 lr: 0.02\n",
      "iteration: 235710 loss: 0.0017 lr: 0.02\n",
      "iteration: 235720 loss: 0.0018 lr: 0.02\n",
      "iteration: 235730 loss: 0.0013 lr: 0.02\n",
      "iteration: 235740 loss: 0.0023 lr: 0.02\n",
      "iteration: 235750 loss: 0.0018 lr: 0.02\n",
      "iteration: 235760 loss: 0.0015 lr: 0.02\n",
      "iteration: 235770 loss: 0.0018 lr: 0.02\n",
      "iteration: 235780 loss: 0.0014 lr: 0.02\n",
      "iteration: 235790 loss: 0.0016 lr: 0.02\n",
      "iteration: 235800 loss: 0.0024 lr: 0.02\n",
      "iteration: 235810 loss: 0.0016 lr: 0.02\n",
      "iteration: 235820 loss: 0.0016 lr: 0.02\n",
      "iteration: 235830 loss: 0.0015 lr: 0.02\n",
      "iteration: 235840 loss: 0.0015 lr: 0.02\n",
      "iteration: 235850 loss: 0.0018 lr: 0.02\n",
      "iteration: 235860 loss: 0.0020 lr: 0.02\n",
      "iteration: 235870 loss: 0.0015 lr: 0.02\n",
      "iteration: 235880 loss: 0.0022 lr: 0.02\n",
      "iteration: 235890 loss: 0.0023 lr: 0.02\n",
      "iteration: 235900 loss: 0.0018 lr: 0.02\n",
      "iteration: 235910 loss: 0.0020 lr: 0.02\n",
      "iteration: 235920 loss: 0.0017 lr: 0.02\n",
      "iteration: 235930 loss: 0.0017 lr: 0.02\n",
      "iteration: 235940 loss: 0.0016 lr: 0.02\n",
      "iteration: 235950 loss: 0.0025 lr: 0.02\n",
      "iteration: 235960 loss: 0.0016 lr: 0.02\n",
      "iteration: 235970 loss: 0.0019 lr: 0.02\n",
      "iteration: 235980 loss: 0.0016 lr: 0.02\n",
      "iteration: 235990 loss: 0.0021 lr: 0.02\n",
      "iteration: 236000 loss: 0.0014 lr: 0.02\n",
      "iteration: 236010 loss: 0.0016 lr: 0.02\n",
      "iteration: 236020 loss: 0.0019 lr: 0.02\n",
      "iteration: 236030 loss: 0.0014 lr: 0.02\n",
      "iteration: 236040 loss: 0.0020 lr: 0.02\n",
      "iteration: 236050 loss: 0.0016 lr: 0.02\n",
      "iteration: 236060 loss: 0.0027 lr: 0.02\n",
      "iteration: 236070 loss: 0.0021 lr: 0.02\n",
      "iteration: 236080 loss: 0.0019 lr: 0.02\n",
      "iteration: 236090 loss: 0.0031 lr: 0.02\n",
      "iteration: 236100 loss: 0.0023 lr: 0.02\n",
      "iteration: 236110 loss: 0.0023 lr: 0.02\n",
      "iteration: 236120 loss: 0.0021 lr: 0.02\n",
      "iteration: 236130 loss: 0.0016 lr: 0.02\n",
      "iteration: 236140 loss: 0.0021 lr: 0.02\n",
      "iteration: 236150 loss: 0.0021 lr: 0.02\n",
      "iteration: 236160 loss: 0.0023 lr: 0.02\n",
      "iteration: 236170 loss: 0.0023 lr: 0.02\n",
      "iteration: 236180 loss: 0.0013 lr: 0.02\n",
      "iteration: 236190 loss: 0.0016 lr: 0.02\n",
      "iteration: 236200 loss: 0.0018 lr: 0.02\n",
      "iteration: 236210 loss: 0.0017 lr: 0.02\n",
      "iteration: 236220 loss: 0.0024 lr: 0.02\n",
      "iteration: 236230 loss: 0.0017 lr: 0.02\n",
      "iteration: 236240 loss: 0.0019 lr: 0.02\n",
      "iteration: 236250 loss: 0.0016 lr: 0.02\n",
      "iteration: 236260 loss: 0.0026 lr: 0.02\n",
      "iteration: 236270 loss: 0.0021 lr: 0.02\n",
      "iteration: 236280 loss: 0.0019 lr: 0.02\n",
      "iteration: 236290 loss: 0.0028 lr: 0.02\n",
      "iteration: 236300 loss: 0.0023 lr: 0.02\n",
      "iteration: 236310 loss: 0.0024 lr: 0.02\n",
      "iteration: 236320 loss: 0.0024 lr: 0.02\n",
      "iteration: 236330 loss: 0.0017 lr: 0.02\n",
      "iteration: 236340 loss: 0.0020 lr: 0.02\n",
      "iteration: 236350 loss: 0.0018 lr: 0.02\n",
      "iteration: 236360 loss: 0.0018 lr: 0.02\n",
      "iteration: 236370 loss: 0.0016 lr: 0.02\n",
      "iteration: 236380 loss: 0.0022 lr: 0.02\n",
      "iteration: 236390 loss: 0.0016 lr: 0.02\n",
      "iteration: 236400 loss: 0.0029 lr: 0.02\n",
      "iteration: 236410 loss: 0.0014 lr: 0.02\n",
      "iteration: 236420 loss: 0.0014 lr: 0.02\n",
      "iteration: 236430 loss: 0.0021 lr: 0.02\n",
      "iteration: 236440 loss: 0.0015 lr: 0.02\n",
      "iteration: 236450 loss: 0.0022 lr: 0.02\n",
      "iteration: 236460 loss: 0.0024 lr: 0.02\n",
      "iteration: 236470 loss: 0.0016 lr: 0.02\n",
      "iteration: 236480 loss: 0.0017 lr: 0.02\n",
      "iteration: 236490 loss: 0.0050 lr: 0.02\n",
      "iteration: 236500 loss: 0.0015 lr: 0.02\n",
      "iteration: 236510 loss: 0.0024 lr: 0.02\n",
      "iteration: 236520 loss: 0.0018 lr: 0.02\n",
      "iteration: 236530 loss: 0.0041 lr: 0.02\n",
      "iteration: 236540 loss: 0.0017 lr: 0.02\n",
      "iteration: 236550 loss: 0.0025 lr: 0.02\n",
      "iteration: 236560 loss: 0.0015 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 236570 loss: 0.0022 lr: 0.02\n",
      "iteration: 236580 loss: 0.0017 lr: 0.02\n",
      "iteration: 236590 loss: 0.0027 lr: 0.02\n",
      "iteration: 236600 loss: 0.0018 lr: 0.02\n",
      "iteration: 236610 loss: 0.0019 lr: 0.02\n",
      "iteration: 236620 loss: 0.0017 lr: 0.02\n",
      "iteration: 236630 loss: 0.0024 lr: 0.02\n",
      "iteration: 236640 loss: 0.0019 lr: 0.02\n",
      "iteration: 236650 loss: 0.0018 lr: 0.02\n",
      "iteration: 236660 loss: 0.0043 lr: 0.02\n",
      "iteration: 236670 loss: 0.0013 lr: 0.02\n",
      "iteration: 236680 loss: 0.0021 lr: 0.02\n",
      "iteration: 236690 loss: 0.0014 lr: 0.02\n",
      "iteration: 236700 loss: 0.0021 lr: 0.02\n",
      "iteration: 236710 loss: 0.0016 lr: 0.02\n",
      "iteration: 236720 loss: 0.0020 lr: 0.02\n",
      "iteration: 236730 loss: 0.0016 lr: 0.02\n",
      "iteration: 236740 loss: 0.0017 lr: 0.02\n",
      "iteration: 236750 loss: 0.0022 lr: 0.02\n",
      "iteration: 236760 loss: 0.0013 lr: 0.02\n",
      "iteration: 236770 loss: 0.0016 lr: 0.02\n",
      "iteration: 236780 loss: 0.0023 lr: 0.02\n",
      "iteration: 236790 loss: 0.0015 lr: 0.02\n",
      "iteration: 236800 loss: 0.0015 lr: 0.02\n",
      "iteration: 236810 loss: 0.0018 lr: 0.02\n",
      "iteration: 236820 loss: 0.0023 lr: 0.02\n",
      "iteration: 236830 loss: 0.0017 lr: 0.02\n",
      "iteration: 236840 loss: 0.0023 lr: 0.02\n",
      "iteration: 236850 loss: 0.0017 lr: 0.02\n",
      "iteration: 236860 loss: 0.0023 lr: 0.02\n",
      "iteration: 236870 loss: 0.0012 lr: 0.02\n",
      "iteration: 236880 loss: 0.0020 lr: 0.02\n",
      "iteration: 236890 loss: 0.0032 lr: 0.02\n",
      "iteration: 236900 loss: 0.0017 lr: 0.02\n",
      "iteration: 236910 loss: 0.0017 lr: 0.02\n",
      "iteration: 236920 loss: 0.0022 lr: 0.02\n",
      "iteration: 236930 loss: 0.0029 lr: 0.02\n",
      "iteration: 236940 loss: 0.0019 lr: 0.02\n",
      "iteration: 236950 loss: 0.0023 lr: 0.02\n",
      "iteration: 236960 loss: 0.0021 lr: 0.02\n",
      "iteration: 236970 loss: 0.0028 lr: 0.02\n",
      "iteration: 236980 loss: 0.0016 lr: 0.02\n",
      "iteration: 236990 loss: 0.0029 lr: 0.02\n",
      "iteration: 237000 loss: 0.0024 lr: 0.02\n",
      "iteration: 237010 loss: 0.0021 lr: 0.02\n",
      "iteration: 237020 loss: 0.0020 lr: 0.02\n",
      "iteration: 237030 loss: 0.0016 lr: 0.02\n",
      "iteration: 237040 loss: 0.0016 lr: 0.02\n",
      "iteration: 237050 loss: 0.0013 lr: 0.02\n",
      "iteration: 237060 loss: 0.0015 lr: 0.02\n",
      "iteration: 237070 loss: 0.0028 lr: 0.02\n",
      "iteration: 237080 loss: 0.0020 lr: 0.02\n",
      "iteration: 237090 loss: 0.0027 lr: 0.02\n",
      "iteration: 237100 loss: 0.0019 lr: 0.02\n",
      "iteration: 237110 loss: 0.0014 lr: 0.02\n",
      "iteration: 237120 loss: 0.0012 lr: 0.02\n",
      "iteration: 237130 loss: 0.0019 lr: 0.02\n",
      "iteration: 237140 loss: 0.0025 lr: 0.02\n",
      "iteration: 237150 loss: 0.0019 lr: 0.02\n",
      "iteration: 237160 loss: 0.0028 lr: 0.02\n",
      "iteration: 237170 loss: 0.0022 lr: 0.02\n",
      "iteration: 237180 loss: 0.0025 lr: 0.02\n",
      "iteration: 237190 loss: 0.0018 lr: 0.02\n",
      "iteration: 237200 loss: 0.0016 lr: 0.02\n",
      "iteration: 237210 loss: 0.0013 lr: 0.02\n",
      "iteration: 237220 loss: 0.0015 lr: 0.02\n",
      "iteration: 237230 loss: 0.0019 lr: 0.02\n",
      "iteration: 237240 loss: 0.0020 lr: 0.02\n",
      "iteration: 237250 loss: 0.0015 lr: 0.02\n",
      "iteration: 237260 loss: 0.0017 lr: 0.02\n",
      "iteration: 237270 loss: 0.0012 lr: 0.02\n",
      "iteration: 237280 loss: 0.0026 lr: 0.02\n",
      "iteration: 237290 loss: 0.0016 lr: 0.02\n",
      "iteration: 237300 loss: 0.0026 lr: 0.02\n",
      "iteration: 237310 loss: 0.0014 lr: 0.02\n",
      "iteration: 237320 loss: 0.0019 lr: 0.02\n",
      "iteration: 237330 loss: 0.0015 lr: 0.02\n",
      "iteration: 237340 loss: 0.0020 lr: 0.02\n",
      "iteration: 237350 loss: 0.0030 lr: 0.02\n",
      "iteration: 237360 loss: 0.0026 lr: 0.02\n",
      "iteration: 237370 loss: 0.0017 lr: 0.02\n",
      "iteration: 237380 loss: 0.0013 lr: 0.02\n",
      "iteration: 237390 loss: 0.0026 lr: 0.02\n",
      "iteration: 237400 loss: 0.0022 lr: 0.02\n",
      "iteration: 237410 loss: 0.0021 lr: 0.02\n",
      "iteration: 237420 loss: 0.0013 lr: 0.02\n",
      "iteration: 237430 loss: 0.0017 lr: 0.02\n",
      "iteration: 237440 loss: 0.0021 lr: 0.02\n",
      "iteration: 237450 loss: 0.0023 lr: 0.02\n",
      "iteration: 237460 loss: 0.0016 lr: 0.02\n",
      "iteration: 237470 loss: 0.0017 lr: 0.02\n",
      "iteration: 237480 loss: 0.0014 lr: 0.02\n",
      "iteration: 237490 loss: 0.0019 lr: 0.02\n",
      "iteration: 237500 loss: 0.0030 lr: 0.02\n",
      "iteration: 237510 loss: 0.0022 lr: 0.02\n",
      "iteration: 237520 loss: 0.0018 lr: 0.02\n",
      "iteration: 237530 loss: 0.0016 lr: 0.02\n",
      "iteration: 237540 loss: 0.0021 lr: 0.02\n",
      "iteration: 237550 loss: 0.0020 lr: 0.02\n",
      "iteration: 237560 loss: 0.0012 lr: 0.02\n",
      "iteration: 237570 loss: 0.0020 lr: 0.02\n",
      "iteration: 237580 loss: 0.0017 lr: 0.02\n",
      "iteration: 237590 loss: 0.0015 lr: 0.02\n",
      "iteration: 237600 loss: 0.0011 lr: 0.02\n",
      "iteration: 237610 loss: 0.0018 lr: 0.02\n",
      "iteration: 237620 loss: 0.0047 lr: 0.02\n",
      "iteration: 237630 loss: 0.0017 lr: 0.02\n",
      "iteration: 237640 loss: 0.0009 lr: 0.02\n",
      "iteration: 237650 loss: 0.0029 lr: 0.02\n",
      "iteration: 237660 loss: 0.0020 lr: 0.02\n",
      "iteration: 237670 loss: 0.0016 lr: 0.02\n",
      "iteration: 237680 loss: 0.0018 lr: 0.02\n",
      "iteration: 237690 loss: 0.0016 lr: 0.02\n",
      "iteration: 237700 loss: 0.0011 lr: 0.02\n",
      "iteration: 237710 loss: 0.0020 lr: 0.02\n",
      "iteration: 237720 loss: 0.0014 lr: 0.02\n",
      "iteration: 237730 loss: 0.0016 lr: 0.02\n",
      "iteration: 237740 loss: 0.0018 lr: 0.02\n",
      "iteration: 237750 loss: 0.0025 lr: 0.02\n",
      "iteration: 237760 loss: 0.0016 lr: 0.02\n",
      "iteration: 237770 loss: 0.0030 lr: 0.02\n",
      "iteration: 237780 loss: 0.0027 lr: 0.02\n",
      "iteration: 237790 loss: 0.0019 lr: 0.02\n",
      "iteration: 237800 loss: 0.0020 lr: 0.02\n",
      "iteration: 237810 loss: 0.0022 lr: 0.02\n",
      "iteration: 237820 loss: 0.0024 lr: 0.02\n",
      "iteration: 237830 loss: 0.0022 lr: 0.02\n",
      "iteration: 237840 loss: 0.0017 lr: 0.02\n",
      "iteration: 237850 loss: 0.0017 lr: 0.02\n",
      "iteration: 237860 loss: 0.0016 lr: 0.02\n",
      "iteration: 237870 loss: 0.0015 lr: 0.02\n",
      "iteration: 237880 loss: 0.0020 lr: 0.02\n",
      "iteration: 237890 loss: 0.0017 lr: 0.02\n",
      "iteration: 237900 loss: 0.0016 lr: 0.02\n",
      "iteration: 237910 loss: 0.0017 lr: 0.02\n",
      "iteration: 237920 loss: 0.0018 lr: 0.02\n",
      "iteration: 237930 loss: 0.0028 lr: 0.02\n",
      "iteration: 237940 loss: 0.0019 lr: 0.02\n",
      "iteration: 237950 loss: 0.0021 lr: 0.02\n",
      "iteration: 237960 loss: 0.0021 lr: 0.02\n",
      "iteration: 237970 loss: 0.0018 lr: 0.02\n",
      "iteration: 237980 loss: 0.0019 lr: 0.02\n",
      "iteration: 237990 loss: 0.0024 lr: 0.02\n",
      "iteration: 238000 loss: 0.0018 lr: 0.02\n",
      "iteration: 238010 loss: 0.0033 lr: 0.02\n",
      "iteration: 238020 loss: 0.0020 lr: 0.02\n",
      "iteration: 238030 loss: 0.0026 lr: 0.02\n",
      "iteration: 238040 loss: 0.0020 lr: 0.02\n",
      "iteration: 238050 loss: 0.0017 lr: 0.02\n",
      "iteration: 238060 loss: 0.0025 lr: 0.02\n",
      "iteration: 238070 loss: 0.0023 lr: 0.02\n",
      "iteration: 238080 loss: 0.0016 lr: 0.02\n",
      "iteration: 238090 loss: 0.0024 lr: 0.02\n",
      "iteration: 238100 loss: 0.0018 lr: 0.02\n",
      "iteration: 238110 loss: 0.0019 lr: 0.02\n",
      "iteration: 238120 loss: 0.0022 lr: 0.02\n",
      "iteration: 238130 loss: 0.0025 lr: 0.02\n",
      "iteration: 238140 loss: 0.0033 lr: 0.02\n",
      "iteration: 238150 loss: 0.0020 lr: 0.02\n",
      "iteration: 238160 loss: 0.0010 lr: 0.02\n",
      "iteration: 238170 loss: 0.0017 lr: 0.02\n",
      "iteration: 238180 loss: 0.0018 lr: 0.02\n",
      "iteration: 238190 loss: 0.0026 lr: 0.02\n",
      "iteration: 238200 loss: 0.0015 lr: 0.02\n",
      "iteration: 238210 loss: 0.0018 lr: 0.02\n",
      "iteration: 238220 loss: 0.0015 lr: 0.02\n",
      "iteration: 238230 loss: 0.0017 lr: 0.02\n",
      "iteration: 238240 loss: 0.0016 lr: 0.02\n",
      "iteration: 238250 loss: 0.0026 lr: 0.02\n",
      "iteration: 238260 loss: 0.0016 lr: 0.02\n",
      "iteration: 238270 loss: 0.0018 lr: 0.02\n",
      "iteration: 238280 loss: 0.0016 lr: 0.02\n",
      "iteration: 238290 loss: 0.0023 lr: 0.02\n",
      "iteration: 238300 loss: 0.0018 lr: 0.02\n",
      "iteration: 238310 loss: 0.0022 lr: 0.02\n",
      "iteration: 238320 loss: 0.0036 lr: 0.02\n",
      "iteration: 238330 loss: 0.0027 lr: 0.02\n",
      "iteration: 238340 loss: 0.0021 lr: 0.02\n",
      "iteration: 238350 loss: 0.0022 lr: 0.02\n",
      "iteration: 238360 loss: 0.0016 lr: 0.02\n",
      "iteration: 238370 loss: 0.0014 lr: 0.02\n",
      "iteration: 238380 loss: 0.0018 lr: 0.02\n",
      "iteration: 238390 loss: 0.0025 lr: 0.02\n",
      "iteration: 238400 loss: 0.0022 lr: 0.02\n",
      "iteration: 238410 loss: 0.0020 lr: 0.02\n",
      "iteration: 238420 loss: 0.0014 lr: 0.02\n",
      "iteration: 238430 loss: 0.0014 lr: 0.02\n",
      "iteration: 238440 loss: 0.0026 lr: 0.02\n",
      "iteration: 238450 loss: 0.0024 lr: 0.02\n",
      "iteration: 238460 loss: 0.0016 lr: 0.02\n",
      "iteration: 238470 loss: 0.0026 lr: 0.02\n",
      "iteration: 238480 loss: 0.0023 lr: 0.02\n",
      "iteration: 238490 loss: 0.0018 lr: 0.02\n",
      "iteration: 238500 loss: 0.0022 lr: 0.02\n",
      "iteration: 238510 loss: 0.0017 lr: 0.02\n",
      "iteration: 238520 loss: 0.0022 lr: 0.02\n",
      "iteration: 238530 loss: 0.0021 lr: 0.02\n",
      "iteration: 238540 loss: 0.0030 lr: 0.02\n",
      "iteration: 238550 loss: 0.0022 lr: 0.02\n",
      "iteration: 238560 loss: 0.0026 lr: 0.02\n",
      "iteration: 238570 loss: 0.0014 lr: 0.02\n",
      "iteration: 238580 loss: 0.0021 lr: 0.02\n",
      "iteration: 238590 loss: 0.0022 lr: 0.02\n",
      "iteration: 238600 loss: 0.0038 lr: 0.02\n",
      "iteration: 238610 loss: 0.0022 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 238620 loss: 0.0023 lr: 0.02\n",
      "iteration: 238630 loss: 0.0017 lr: 0.02\n",
      "iteration: 238640 loss: 0.0018 lr: 0.02\n",
      "iteration: 238650 loss: 0.0016 lr: 0.02\n",
      "iteration: 238660 loss: 0.0019 lr: 0.02\n",
      "iteration: 238670 loss: 0.0019 lr: 0.02\n",
      "iteration: 238680 loss: 0.0019 lr: 0.02\n",
      "iteration: 238690 loss: 0.0018 lr: 0.02\n",
      "iteration: 238700 loss: 0.0018 lr: 0.02\n",
      "iteration: 238710 loss: 0.0016 lr: 0.02\n",
      "iteration: 238720 loss: 0.0024 lr: 0.02\n",
      "iteration: 238730 loss: 0.0023 lr: 0.02\n",
      "iteration: 238740 loss: 0.0018 lr: 0.02\n",
      "iteration: 238750 loss: 0.0019 lr: 0.02\n",
      "iteration: 238760 loss: 0.0023 lr: 0.02\n",
      "iteration: 238770 loss: 0.0025 lr: 0.02\n",
      "iteration: 238780 loss: 0.0031 lr: 0.02\n",
      "iteration: 238790 loss: 0.0016 lr: 0.02\n",
      "iteration: 238800 loss: 0.0017 lr: 0.02\n",
      "iteration: 238810 loss: 0.0014 lr: 0.02\n",
      "iteration: 238820 loss: 0.0020 lr: 0.02\n",
      "iteration: 238830 loss: 0.0021 lr: 0.02\n",
      "iteration: 238840 loss: 0.0023 lr: 0.02\n",
      "iteration: 238850 loss: 0.0017 lr: 0.02\n",
      "iteration: 238860 loss: 0.0020 lr: 0.02\n",
      "iteration: 238870 loss: 0.0032 lr: 0.02\n",
      "iteration: 238880 loss: 0.0015 lr: 0.02\n",
      "iteration: 238890 loss: 0.0014 lr: 0.02\n",
      "iteration: 238900 loss: 0.0020 lr: 0.02\n",
      "iteration: 238910 loss: 0.0021 lr: 0.02\n",
      "iteration: 238920 loss: 0.0015 lr: 0.02\n",
      "iteration: 238930 loss: 0.0016 lr: 0.02\n",
      "iteration: 238940 loss: 0.0019 lr: 0.02\n",
      "iteration: 238950 loss: 0.0025 lr: 0.02\n",
      "iteration: 238960 loss: 0.0014 lr: 0.02\n",
      "iteration: 238970 loss: 0.0023 lr: 0.02\n",
      "iteration: 238980 loss: 0.0023 lr: 0.02\n",
      "iteration: 238990 loss: 0.0034 lr: 0.02\n",
      "iteration: 239000 loss: 0.0019 lr: 0.02\n",
      "iteration: 239010 loss: 0.0025 lr: 0.02\n",
      "iteration: 239020 loss: 0.0029 lr: 0.02\n",
      "iteration: 239030 loss: 0.0019 lr: 0.02\n",
      "iteration: 239040 loss: 0.0019 lr: 0.02\n",
      "iteration: 239050 loss: 0.0016 lr: 0.02\n",
      "iteration: 239060 loss: 0.0018 lr: 0.02\n",
      "iteration: 239070 loss: 0.0017 lr: 0.02\n",
      "iteration: 239080 loss: 0.0014 lr: 0.02\n",
      "iteration: 239090 loss: 0.0018 lr: 0.02\n",
      "iteration: 239100 loss: 0.0022 lr: 0.02\n",
      "iteration: 239110 loss: 0.0013 lr: 0.02\n",
      "iteration: 239120 loss: 0.0019 lr: 0.02\n",
      "iteration: 239130 loss: 0.0025 lr: 0.02\n",
      "iteration: 239140 loss: 0.0016 lr: 0.02\n",
      "iteration: 239150 loss: 0.0017 lr: 0.02\n",
      "iteration: 239160 loss: 0.0024 lr: 0.02\n",
      "iteration: 239170 loss: 0.0016 lr: 0.02\n",
      "iteration: 239180 loss: 0.0030 lr: 0.02\n",
      "iteration: 239190 loss: 0.0024 lr: 0.02\n",
      "iteration: 239200 loss: 0.0018 lr: 0.02\n",
      "iteration: 239210 loss: 0.0012 lr: 0.02\n",
      "iteration: 239220 loss: 0.0016 lr: 0.02\n",
      "iteration: 239230 loss: 0.0019 lr: 0.02\n",
      "iteration: 239240 loss: 0.0012 lr: 0.02\n",
      "iteration: 239250 loss: 0.0016 lr: 0.02\n",
      "iteration: 239260 loss: 0.0015 lr: 0.02\n",
      "iteration: 239270 loss: 0.0024 lr: 0.02\n",
      "iteration: 239280 loss: 0.0026 lr: 0.02\n",
      "iteration: 239290 loss: 0.0013 lr: 0.02\n",
      "iteration: 239300 loss: 0.0013 lr: 0.02\n",
      "iteration: 239310 loss: 0.0012 lr: 0.02\n",
      "iteration: 239320 loss: 0.0015 lr: 0.02\n",
      "iteration: 239330 loss: 0.0018 lr: 0.02\n",
      "iteration: 239340 loss: 0.0017 lr: 0.02\n",
      "iteration: 239350 loss: 0.0019 lr: 0.02\n",
      "iteration: 239360 loss: 0.0031 lr: 0.02\n",
      "iteration: 239370 loss: 0.0021 lr: 0.02\n",
      "iteration: 239380 loss: 0.0019 lr: 0.02\n",
      "iteration: 239390 loss: 0.0014 lr: 0.02\n",
      "iteration: 239400 loss: 0.0025 lr: 0.02\n",
      "iteration: 239410 loss: 0.0013 lr: 0.02\n",
      "iteration: 239420 loss: 0.0016 lr: 0.02\n",
      "iteration: 239430 loss: 0.0024 lr: 0.02\n",
      "iteration: 239440 loss: 0.0018 lr: 0.02\n",
      "iteration: 239450 loss: 0.0018 lr: 0.02\n",
      "iteration: 239460 loss: 0.0029 lr: 0.02\n",
      "iteration: 239470 loss: 0.0014 lr: 0.02\n",
      "iteration: 239480 loss: 0.0024 lr: 0.02\n",
      "iteration: 239490 loss: 0.0021 lr: 0.02\n",
      "iteration: 239500 loss: 0.0022 lr: 0.02\n",
      "iteration: 239510 loss: 0.0026 lr: 0.02\n",
      "iteration: 239520 loss: 0.0019 lr: 0.02\n",
      "iteration: 239530 loss: 0.0020 lr: 0.02\n",
      "iteration: 239540 loss: 0.0032 lr: 0.02\n",
      "iteration: 239550 loss: 0.0019 lr: 0.02\n",
      "iteration: 239560 loss: 0.0019 lr: 0.02\n",
      "iteration: 239570 loss: 0.0016 lr: 0.02\n",
      "iteration: 239580 loss: 0.0015 lr: 0.02\n",
      "iteration: 239590 loss: 0.0021 lr: 0.02\n",
      "iteration: 239600 loss: 0.0028 lr: 0.02\n",
      "iteration: 239610 loss: 0.0018 lr: 0.02\n",
      "iteration: 239620 loss: 0.0018 lr: 0.02\n",
      "iteration: 239630 loss: 0.0022 lr: 0.02\n",
      "iteration: 239640 loss: 0.0023 lr: 0.02\n",
      "iteration: 239650 loss: 0.0022 lr: 0.02\n",
      "iteration: 239660 loss: 0.0024 lr: 0.02\n",
      "iteration: 239670 loss: 0.0023 lr: 0.02\n",
      "iteration: 239680 loss: 0.0016 lr: 0.02\n",
      "iteration: 239690 loss: 0.0024 lr: 0.02\n",
      "iteration: 239700 loss: 0.0014 lr: 0.02\n",
      "iteration: 239710 loss: 0.0018 lr: 0.02\n",
      "iteration: 239720 loss: 0.0021 lr: 0.02\n",
      "iteration: 239730 loss: 0.0017 lr: 0.02\n",
      "iteration: 239740 loss: 0.0023 lr: 0.02\n",
      "iteration: 239750 loss: 0.0018 lr: 0.02\n",
      "iteration: 239760 loss: 0.0023 lr: 0.02\n",
      "iteration: 239770 loss: 0.0015 lr: 0.02\n",
      "iteration: 239780 loss: 0.0020 lr: 0.02\n",
      "iteration: 239790 loss: 0.0015 lr: 0.02\n",
      "iteration: 239800 loss: 0.0016 lr: 0.02\n",
      "iteration: 239810 loss: 0.0029 lr: 0.02\n",
      "iteration: 239820 loss: 0.0019 lr: 0.02\n",
      "iteration: 239830 loss: 0.0014 lr: 0.02\n",
      "iteration: 239840 loss: 0.0017 lr: 0.02\n",
      "iteration: 239850 loss: 0.0020 lr: 0.02\n",
      "iteration: 239860 loss: 0.0024 lr: 0.02\n",
      "iteration: 239870 loss: 0.0021 lr: 0.02\n",
      "iteration: 239880 loss: 0.0024 lr: 0.02\n",
      "iteration: 239890 loss: 0.0018 lr: 0.02\n",
      "iteration: 239900 loss: 0.0016 lr: 0.02\n",
      "iteration: 239910 loss: 0.0018 lr: 0.02\n",
      "iteration: 239920 loss: 0.0017 lr: 0.02\n",
      "iteration: 239930 loss: 0.0015 lr: 0.02\n",
      "iteration: 239940 loss: 0.0017 lr: 0.02\n",
      "iteration: 239950 loss: 0.0017 lr: 0.02\n",
      "iteration: 239960 loss: 0.0021 lr: 0.02\n",
      "iteration: 239970 loss: 0.0027 lr: 0.02\n",
      "iteration: 239980 loss: 0.0022 lr: 0.02\n",
      "iteration: 239990 loss: 0.0027 lr: 0.02\n",
      "iteration: 240000 loss: 0.0022 lr: 0.02\n",
      "iteration: 240010 loss: 0.0025 lr: 0.02\n",
      "iteration: 240020 loss: 0.0023 lr: 0.02\n",
      "iteration: 240030 loss: 0.0014 lr: 0.02\n",
      "iteration: 240040 loss: 0.0017 lr: 0.02\n",
      "iteration: 240050 loss: 0.0023 lr: 0.02\n",
      "iteration: 240060 loss: 0.0016 lr: 0.02\n",
      "iteration: 240070 loss: 0.0017 lr: 0.02\n",
      "iteration: 240080 loss: 0.0014 lr: 0.02\n",
      "iteration: 240090 loss: 0.0019 lr: 0.02\n",
      "iteration: 240100 loss: 0.0013 lr: 0.02\n",
      "iteration: 240110 loss: 0.0018 lr: 0.02\n",
      "iteration: 240120 loss: 0.0025 lr: 0.02\n",
      "iteration: 240130 loss: 0.0022 lr: 0.02\n",
      "iteration: 240140 loss: 0.0019 lr: 0.02\n",
      "iteration: 240150 loss: 0.0015 lr: 0.02\n",
      "iteration: 240160 loss: 0.0030 lr: 0.02\n",
      "iteration: 240170 loss: 0.0023 lr: 0.02\n",
      "iteration: 240180 loss: 0.0014 lr: 0.02\n",
      "iteration: 240190 loss: 0.0020 lr: 0.02\n",
      "iteration: 240200 loss: 0.0016 lr: 0.02\n",
      "iteration: 240210 loss: 0.0013 lr: 0.02\n",
      "iteration: 240220 loss: 0.0018 lr: 0.02\n",
      "iteration: 240230 loss: 0.0017 lr: 0.02\n",
      "iteration: 240240 loss: 0.0028 lr: 0.02\n",
      "iteration: 240250 loss: 0.0020 lr: 0.02\n",
      "iteration: 240260 loss: 0.0035 lr: 0.02\n",
      "iteration: 240270 loss: 0.0021 lr: 0.02\n",
      "iteration: 240280 loss: 0.0022 lr: 0.02\n",
      "iteration: 240290 loss: 0.0022 lr: 0.02\n",
      "iteration: 240300 loss: 0.0016 lr: 0.02\n",
      "iteration: 240310 loss: 0.0026 lr: 0.02\n",
      "iteration: 240320 loss: 0.0017 lr: 0.02\n",
      "iteration: 240330 loss: 0.0018 lr: 0.02\n",
      "iteration: 240340 loss: 0.0013 lr: 0.02\n",
      "iteration: 240350 loss: 0.0012 lr: 0.02\n",
      "iteration: 240360 loss: 0.0022 lr: 0.02\n",
      "iteration: 240370 loss: 0.0017 lr: 0.02\n",
      "iteration: 240380 loss: 0.0019 lr: 0.02\n",
      "iteration: 240390 loss: 0.0016 lr: 0.02\n",
      "iteration: 240400 loss: 0.0015 lr: 0.02\n",
      "iteration: 240410 loss: 0.0017 lr: 0.02\n",
      "iteration: 240420 loss: 0.0015 lr: 0.02\n",
      "iteration: 240430 loss: 0.0028 lr: 0.02\n",
      "iteration: 240440 loss: 0.0022 lr: 0.02\n",
      "iteration: 240450 loss: 0.0029 lr: 0.02\n",
      "iteration: 240460 loss: 0.0015 lr: 0.02\n",
      "iteration: 240470 loss: 0.0021 lr: 0.02\n",
      "iteration: 240480 loss: 0.0017 lr: 0.02\n",
      "iteration: 240490 loss: 0.0016 lr: 0.02\n",
      "iteration: 240500 loss: 0.0020 lr: 0.02\n",
      "iteration: 240510 loss: 0.0024 lr: 0.02\n",
      "iteration: 240520 loss: 0.0022 lr: 0.02\n",
      "iteration: 240530 loss: 0.0017 lr: 0.02\n",
      "iteration: 240540 loss: 0.0020 lr: 0.02\n",
      "iteration: 240550 loss: 0.0019 lr: 0.02\n",
      "iteration: 240560 loss: 0.0030 lr: 0.02\n",
      "iteration: 240570 loss: 0.0030 lr: 0.02\n",
      "iteration: 240580 loss: 0.0020 lr: 0.02\n",
      "iteration: 240590 loss: 0.0020 lr: 0.02\n",
      "iteration: 240600 loss: 0.0027 lr: 0.02\n",
      "iteration: 240610 loss: 0.0022 lr: 0.02\n",
      "iteration: 240620 loss: 0.0020 lr: 0.02\n",
      "iteration: 240630 loss: 0.0017 lr: 0.02\n",
      "iteration: 240640 loss: 0.0017 lr: 0.02\n",
      "iteration: 240650 loss: 0.0025 lr: 0.02\n",
      "iteration: 240660 loss: 0.0022 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 240670 loss: 0.0020 lr: 0.02\n",
      "iteration: 240680 loss: 0.0023 lr: 0.02\n",
      "iteration: 240690 loss: 0.0017 lr: 0.02\n",
      "iteration: 240700 loss: 0.0023 lr: 0.02\n",
      "iteration: 240710 loss: 0.0013 lr: 0.02\n",
      "iteration: 240720 loss: 0.0021 lr: 0.02\n",
      "iteration: 240730 loss: 0.0026 lr: 0.02\n",
      "iteration: 240740 loss: 0.0018 lr: 0.02\n",
      "iteration: 240750 loss: 0.0013 lr: 0.02\n",
      "iteration: 240760 loss: 0.0020 lr: 0.02\n",
      "iteration: 240770 loss: 0.0020 lr: 0.02\n",
      "iteration: 240780 loss: 0.0018 lr: 0.02\n",
      "iteration: 240790 loss: 0.0017 lr: 0.02\n",
      "iteration: 240800 loss: 0.0018 lr: 0.02\n",
      "iteration: 240810 loss: 0.0016 lr: 0.02\n",
      "iteration: 240820 loss: 0.0018 lr: 0.02\n",
      "iteration: 240830 loss: 0.0023 lr: 0.02\n",
      "iteration: 240840 loss: 0.0021 lr: 0.02\n",
      "iteration: 240850 loss: 0.0020 lr: 0.02\n",
      "iteration: 240860 loss: 0.0016 lr: 0.02\n",
      "iteration: 240870 loss: 0.0013 lr: 0.02\n",
      "iteration: 240880 loss: 0.0019 lr: 0.02\n",
      "iteration: 240890 loss: 0.0018 lr: 0.02\n",
      "iteration: 240900 loss: 0.0025 lr: 0.02\n",
      "iteration: 240910 loss: 0.0024 lr: 0.02\n",
      "iteration: 240920 loss: 0.0021 lr: 0.02\n",
      "iteration: 240930 loss: 0.0022 lr: 0.02\n",
      "iteration: 240940 loss: 0.0014 lr: 0.02\n",
      "iteration: 240950 loss: 0.0016 lr: 0.02\n",
      "iteration: 240960 loss: 0.0025 lr: 0.02\n",
      "iteration: 240970 loss: 0.0016 lr: 0.02\n",
      "iteration: 240980 loss: 0.0019 lr: 0.02\n",
      "iteration: 240990 loss: 0.0024 lr: 0.02\n",
      "iteration: 241000 loss: 0.0018 lr: 0.02\n",
      "iteration: 241010 loss: 0.0027 lr: 0.02\n",
      "iteration: 241020 loss: 0.0019 lr: 0.02\n",
      "iteration: 241030 loss: 0.0020 lr: 0.02\n",
      "iteration: 241040 loss: 0.0020 lr: 0.02\n",
      "iteration: 241050 loss: 0.0018 lr: 0.02\n",
      "iteration: 241060 loss: 0.0018 lr: 0.02\n",
      "iteration: 241070 loss: 0.0036 lr: 0.02\n",
      "iteration: 241080 loss: 0.0023 lr: 0.02\n",
      "iteration: 241090 loss: 0.0022 lr: 0.02\n",
      "iteration: 241100 loss: 0.0018 lr: 0.02\n",
      "iteration: 241110 loss: 0.0021 lr: 0.02\n",
      "iteration: 241120 loss: 0.0017 lr: 0.02\n",
      "iteration: 241130 loss: 0.0020 lr: 0.02\n",
      "iteration: 241140 loss: 0.0019 lr: 0.02\n",
      "iteration: 241150 loss: 0.0018 lr: 0.02\n",
      "iteration: 241160 loss: 0.0026 lr: 0.02\n",
      "iteration: 241170 loss: 0.0016 lr: 0.02\n",
      "iteration: 241180 loss: 0.0014 lr: 0.02\n",
      "iteration: 241190 loss: 0.0018 lr: 0.02\n",
      "iteration: 241200 loss: 0.0026 lr: 0.02\n",
      "iteration: 241210 loss: 0.0023 lr: 0.02\n",
      "iteration: 241220 loss: 0.0020 lr: 0.02\n",
      "iteration: 241230 loss: 0.0015 lr: 0.02\n",
      "iteration: 241240 loss: 0.0023 lr: 0.02\n",
      "iteration: 241250 loss: 0.0023 lr: 0.02\n",
      "iteration: 241260 loss: 0.0017 lr: 0.02\n",
      "iteration: 241270 loss: 0.0015 lr: 0.02\n",
      "iteration: 241280 loss: 0.0029 lr: 0.02\n",
      "iteration: 241290 loss: 0.0023 lr: 0.02\n",
      "iteration: 241300 loss: 0.0016 lr: 0.02\n",
      "iteration: 241310 loss: 0.0019 lr: 0.02\n",
      "iteration: 241320 loss: 0.0028 lr: 0.02\n",
      "iteration: 241330 loss: 0.0016 lr: 0.02\n",
      "iteration: 241340 loss: 0.0013 lr: 0.02\n",
      "iteration: 241350 loss: 0.0017 lr: 0.02\n",
      "iteration: 241360 loss: 0.0017 lr: 0.02\n",
      "iteration: 241370 loss: 0.0015 lr: 0.02\n",
      "iteration: 241380 loss: 0.0024 lr: 0.02\n",
      "iteration: 241390 loss: 0.0030 lr: 0.02\n",
      "iteration: 241400 loss: 0.0036 lr: 0.02\n",
      "iteration: 241410 loss: 0.0021 lr: 0.02\n",
      "iteration: 241420 loss: 0.0029 lr: 0.02\n",
      "iteration: 241430 loss: 0.0024 lr: 0.02\n",
      "iteration: 241440 loss: 0.0018 lr: 0.02\n",
      "iteration: 241450 loss: 0.0017 lr: 0.02\n",
      "iteration: 241460 loss: 0.0023 lr: 0.02\n",
      "iteration: 241470 loss: 0.0020 lr: 0.02\n",
      "iteration: 241480 loss: 0.0015 lr: 0.02\n",
      "iteration: 241490 loss: 0.0016 lr: 0.02\n",
      "iteration: 241500 loss: 0.0016 lr: 0.02\n",
      "iteration: 241510 loss: 0.0022 lr: 0.02\n",
      "iteration: 241520 loss: 0.0023 lr: 0.02\n",
      "iteration: 241530 loss: 0.0022 lr: 0.02\n",
      "iteration: 241540 loss: 0.0017 lr: 0.02\n",
      "iteration: 241550 loss: 0.0023 lr: 0.02\n",
      "iteration: 241560 loss: 0.0030 lr: 0.02\n",
      "iteration: 241570 loss: 0.0025 lr: 0.02\n",
      "iteration: 241580 loss: 0.0021 lr: 0.02\n",
      "iteration: 241590 loss: 0.0025 lr: 0.02\n",
      "iteration: 241600 loss: 0.0015 lr: 0.02\n",
      "iteration: 241610 loss: 0.0017 lr: 0.02\n",
      "iteration: 241620 loss: 0.0017 lr: 0.02\n",
      "iteration: 241630 loss: 0.0018 lr: 0.02\n",
      "iteration: 241640 loss: 0.0023 lr: 0.02\n",
      "iteration: 241650 loss: 0.0016 lr: 0.02\n",
      "iteration: 241660 loss: 0.0019 lr: 0.02\n",
      "iteration: 241670 loss: 0.0020 lr: 0.02\n",
      "iteration: 241680 loss: 0.0033 lr: 0.02\n",
      "iteration: 241690 loss: 0.0021 lr: 0.02\n",
      "iteration: 241700 loss: 0.0021 lr: 0.02\n",
      "iteration: 241710 loss: 0.0017 lr: 0.02\n",
      "iteration: 241720 loss: 0.0019 lr: 0.02\n",
      "iteration: 241730 loss: 0.0018 lr: 0.02\n",
      "iteration: 241740 loss: 0.0015 lr: 0.02\n",
      "iteration: 241750 loss: 0.0022 lr: 0.02\n",
      "iteration: 241760 loss: 0.0025 lr: 0.02\n",
      "iteration: 241770 loss: 0.0024 lr: 0.02\n",
      "iteration: 241780 loss: 0.0019 lr: 0.02\n",
      "iteration: 241790 loss: 0.0014 lr: 0.02\n",
      "iteration: 241800 loss: 0.0019 lr: 0.02\n",
      "iteration: 241810 loss: 0.0019 lr: 0.02\n",
      "iteration: 241820 loss: 0.0020 lr: 0.02\n",
      "iteration: 241830 loss: 0.0020 lr: 0.02\n",
      "iteration: 241840 loss: 0.0027 lr: 0.02\n",
      "iteration: 241850 loss: 0.0022 lr: 0.02\n",
      "iteration: 241860 loss: 0.0020 lr: 0.02\n",
      "iteration: 241870 loss: 0.0022 lr: 0.02\n",
      "iteration: 241880 loss: 0.0021 lr: 0.02\n",
      "iteration: 241890 loss: 0.0018 lr: 0.02\n",
      "iteration: 241900 loss: 0.0018 lr: 0.02\n",
      "iteration: 241910 loss: 0.0023 lr: 0.02\n",
      "iteration: 241920 loss: 0.0022 lr: 0.02\n",
      "iteration: 241930 loss: 0.0021 lr: 0.02\n",
      "iteration: 241940 loss: 0.0016 lr: 0.02\n",
      "iteration: 241950 loss: 0.0027 lr: 0.02\n",
      "iteration: 241960 loss: 0.0016 lr: 0.02\n",
      "iteration: 241970 loss: 0.0022 lr: 0.02\n",
      "iteration: 241980 loss: 0.0026 lr: 0.02\n",
      "iteration: 241990 loss: 0.0019 lr: 0.02\n",
      "iteration: 242000 loss: 0.0008 lr: 0.02\n",
      "iteration: 242010 loss: 0.0019 lr: 0.02\n",
      "iteration: 242020 loss: 0.0014 lr: 0.02\n",
      "iteration: 242030 loss: 0.0018 lr: 0.02\n",
      "iteration: 242040 loss: 0.0023 lr: 0.02\n",
      "iteration: 242050 loss: 0.0024 lr: 0.02\n",
      "iteration: 242060 loss: 0.0023 lr: 0.02\n",
      "iteration: 242070 loss: 0.0017 lr: 0.02\n",
      "iteration: 242080 loss: 0.0016 lr: 0.02\n",
      "iteration: 242090 loss: 0.0025 lr: 0.02\n",
      "iteration: 242100 loss: 0.0020 lr: 0.02\n",
      "iteration: 242110 loss: 0.0016 lr: 0.02\n",
      "iteration: 242120 loss: 0.0018 lr: 0.02\n",
      "iteration: 242130 loss: 0.0016 lr: 0.02\n",
      "iteration: 242140 loss: 0.0018 lr: 0.02\n",
      "iteration: 242150 loss: 0.0014 lr: 0.02\n",
      "iteration: 242160 loss: 0.0024 lr: 0.02\n",
      "iteration: 242170 loss: 0.0022 lr: 0.02\n",
      "iteration: 242180 loss: 0.0028 lr: 0.02\n",
      "iteration: 242190 loss: 0.0018 lr: 0.02\n",
      "iteration: 242200 loss: 0.0018 lr: 0.02\n",
      "iteration: 242210 loss: 0.0019 lr: 0.02\n",
      "iteration: 242220 loss: 0.0021 lr: 0.02\n",
      "iteration: 242230 loss: 0.0015 lr: 0.02\n",
      "iteration: 242240 loss: 0.0025 lr: 0.02\n",
      "iteration: 242250 loss: 0.0028 lr: 0.02\n",
      "iteration: 242260 loss: 0.0022 lr: 0.02\n",
      "iteration: 242270 loss: 0.0022 lr: 0.02\n",
      "iteration: 242280 loss: 0.0017 lr: 0.02\n",
      "iteration: 242290 loss: 0.0015 lr: 0.02\n",
      "iteration: 242300 loss: 0.0016 lr: 0.02\n",
      "iteration: 242310 loss: 0.0044 lr: 0.02\n",
      "iteration: 242320 loss: 0.0016 lr: 0.02\n",
      "iteration: 242330 loss: 0.0017 lr: 0.02\n",
      "iteration: 242340 loss: 0.0018 lr: 0.02\n",
      "iteration: 242350 loss: 0.0022 lr: 0.02\n",
      "iteration: 242360 loss: 0.0014 lr: 0.02\n",
      "iteration: 242370 loss: 0.0037 lr: 0.02\n",
      "iteration: 242380 loss: 0.0025 lr: 0.02\n",
      "iteration: 242390 loss: 0.0026 lr: 0.02\n",
      "iteration: 242400 loss: 0.0024 lr: 0.02\n",
      "iteration: 242410 loss: 0.0019 lr: 0.02\n",
      "iteration: 242420 loss: 0.0021 lr: 0.02\n",
      "iteration: 242430 loss: 0.0019 lr: 0.02\n",
      "iteration: 242440 loss: 0.0019 lr: 0.02\n",
      "iteration: 242450 loss: 0.0020 lr: 0.02\n",
      "iteration: 242460 loss: 0.0017 lr: 0.02\n",
      "iteration: 242470 loss: 0.0024 lr: 0.02\n",
      "iteration: 242480 loss: 0.0022 lr: 0.02\n",
      "iteration: 242490 loss: 0.0022 lr: 0.02\n",
      "iteration: 242500 loss: 0.0018 lr: 0.02\n",
      "iteration: 242510 loss: 0.0016 lr: 0.02\n",
      "iteration: 242520 loss: 0.0017 lr: 0.02\n",
      "iteration: 242530 loss: 0.0028 lr: 0.02\n",
      "iteration: 242540 loss: 0.0026 lr: 0.02\n",
      "iteration: 242550 loss: 0.0016 lr: 0.02\n",
      "iteration: 242560 loss: 0.0018 lr: 0.02\n",
      "iteration: 242570 loss: 0.0013 lr: 0.02\n",
      "iteration: 242580 loss: 0.0020 lr: 0.02\n",
      "iteration: 242590 loss: 0.0024 lr: 0.02\n",
      "iteration: 242600 loss: 0.0015 lr: 0.02\n",
      "iteration: 242610 loss: 0.0023 lr: 0.02\n",
      "iteration: 242620 loss: 0.0018 lr: 0.02\n",
      "iteration: 242630 loss: 0.0026 lr: 0.02\n",
      "iteration: 242640 loss: 0.0017 lr: 0.02\n",
      "iteration: 242650 loss: 0.0022 lr: 0.02\n",
      "iteration: 242660 loss: 0.0019 lr: 0.02\n",
      "iteration: 242670 loss: 0.0017 lr: 0.02\n",
      "iteration: 242680 loss: 0.0024 lr: 0.02\n",
      "iteration: 242690 loss: 0.0018 lr: 0.02\n",
      "iteration: 242700 loss: 0.0022 lr: 0.02\n",
      "iteration: 242710 loss: 0.0026 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 242720 loss: 0.0021 lr: 0.02\n",
      "iteration: 242730 loss: 0.0014 lr: 0.02\n",
      "iteration: 242740 loss: 0.0018 lr: 0.02\n",
      "iteration: 242750 loss: 0.0039 lr: 0.02\n",
      "iteration: 242760 loss: 0.0017 lr: 0.02\n",
      "iteration: 242770 loss: 0.0016 lr: 0.02\n",
      "iteration: 242780 loss: 0.0013 lr: 0.02\n",
      "iteration: 242790 loss: 0.0015 lr: 0.02\n",
      "iteration: 242800 loss: 0.0015 lr: 0.02\n",
      "iteration: 242810 loss: 0.0014 lr: 0.02\n",
      "iteration: 242820 loss: 0.0034 lr: 0.02\n",
      "iteration: 242830 loss: 0.0023 lr: 0.02\n",
      "iteration: 242840 loss: 0.0015 lr: 0.02\n",
      "iteration: 242850 loss: 0.0021 lr: 0.02\n",
      "iteration: 242860 loss: 0.0023 lr: 0.02\n",
      "iteration: 242870 loss: 0.0019 lr: 0.02\n",
      "iteration: 242880 loss: 0.0021 lr: 0.02\n",
      "iteration: 242890 loss: 0.0017 lr: 0.02\n",
      "iteration: 242900 loss: 0.0022 lr: 0.02\n",
      "iteration: 242910 loss: 0.0018 lr: 0.02\n",
      "iteration: 242920 loss: 0.0017 lr: 0.02\n",
      "iteration: 242930 loss: 0.0016 lr: 0.02\n",
      "iteration: 242940 loss: 0.0015 lr: 0.02\n",
      "iteration: 242950 loss: 0.0025 lr: 0.02\n",
      "iteration: 242960 loss: 0.0022 lr: 0.02\n",
      "iteration: 242970 loss: 0.0015 lr: 0.02\n",
      "iteration: 242980 loss: 0.0019 lr: 0.02\n",
      "iteration: 242990 loss: 0.0025 lr: 0.02\n",
      "iteration: 243000 loss: 0.0024 lr: 0.02\n",
      "iteration: 243010 loss: 0.0020 lr: 0.02\n",
      "iteration: 243020 loss: 0.0014 lr: 0.02\n",
      "iteration: 243030 loss: 0.0014 lr: 0.02\n",
      "iteration: 243040 loss: 0.0017 lr: 0.02\n",
      "iteration: 243050 loss: 0.0018 lr: 0.02\n",
      "iteration: 243060 loss: 0.0020 lr: 0.02\n",
      "iteration: 243070 loss: 0.0015 lr: 0.02\n",
      "iteration: 243080 loss: 0.0018 lr: 0.02\n",
      "iteration: 243090 loss: 0.0017 lr: 0.02\n",
      "iteration: 243100 loss: 0.0021 lr: 0.02\n",
      "iteration: 243110 loss: 0.0019 lr: 0.02\n",
      "iteration: 243120 loss: 0.0016 lr: 0.02\n",
      "iteration: 243130 loss: 0.0018 lr: 0.02\n",
      "iteration: 243140 loss: 0.0022 lr: 0.02\n",
      "iteration: 243150 loss: 0.0016 lr: 0.02\n",
      "iteration: 243160 loss: 0.0024 lr: 0.02\n",
      "iteration: 243170 loss: 0.0022 lr: 0.02\n",
      "iteration: 243180 loss: 0.0027 lr: 0.02\n",
      "iteration: 243190 loss: 0.0019 lr: 0.02\n",
      "iteration: 243200 loss: 0.0024 lr: 0.02\n",
      "iteration: 243210 loss: 0.0021 lr: 0.02\n",
      "iteration: 243220 loss: 0.0018 lr: 0.02\n",
      "iteration: 243230 loss: 0.0018 lr: 0.02\n",
      "iteration: 243240 loss: 0.0026 lr: 0.02\n",
      "iteration: 243250 loss: 0.0024 lr: 0.02\n",
      "iteration: 243260 loss: 0.0019 lr: 0.02\n",
      "iteration: 243270 loss: 0.0018 lr: 0.02\n",
      "iteration: 243280 loss: 0.0019 lr: 0.02\n",
      "iteration: 243290 loss: 0.0014 lr: 0.02\n",
      "iteration: 243300 loss: 0.0021 lr: 0.02\n",
      "iteration: 243310 loss: 0.0019 lr: 0.02\n",
      "iteration: 243320 loss: 0.0025 lr: 0.02\n",
      "iteration: 243330 loss: 0.0024 lr: 0.02\n",
      "iteration: 243340 loss: 0.0018 lr: 0.02\n",
      "iteration: 243350 loss: 0.0025 lr: 0.02\n",
      "iteration: 243360 loss: 0.0018 lr: 0.02\n",
      "iteration: 243370 loss: 0.0028 lr: 0.02\n",
      "iteration: 243380 loss: 0.0025 lr: 0.02\n",
      "iteration: 243390 loss: 0.0013 lr: 0.02\n",
      "iteration: 243400 loss: 0.0018 lr: 0.02\n",
      "iteration: 243410 loss: 0.0019 lr: 0.02\n",
      "iteration: 243420 loss: 0.0025 lr: 0.02\n",
      "iteration: 243430 loss: 0.0016 lr: 0.02\n",
      "iteration: 243440 loss: 0.0016 lr: 0.02\n",
      "iteration: 243450 loss: 0.0015 lr: 0.02\n",
      "iteration: 243460 loss: 0.0022 lr: 0.02\n",
      "iteration: 243470 loss: 0.0020 lr: 0.02\n",
      "iteration: 243480 loss: 0.0021 lr: 0.02\n",
      "iteration: 243490 loss: 0.0016 lr: 0.02\n",
      "iteration: 243500 loss: 0.0020 lr: 0.02\n",
      "iteration: 243510 loss: 0.0020 lr: 0.02\n",
      "iteration: 243520 loss: 0.0019 lr: 0.02\n",
      "iteration: 243530 loss: 0.0030 lr: 0.02\n",
      "iteration: 243540 loss: 0.0020 lr: 0.02\n",
      "iteration: 243550 loss: 0.0018 lr: 0.02\n",
      "iteration: 243560 loss: 0.0019 lr: 0.02\n",
      "iteration: 243570 loss: 0.0014 lr: 0.02\n",
      "iteration: 243580 loss: 0.0016 lr: 0.02\n",
      "iteration: 243590 loss: 0.0016 lr: 0.02\n",
      "iteration: 243600 loss: 0.0015 lr: 0.02\n",
      "iteration: 243610 loss: 0.0016 lr: 0.02\n",
      "iteration: 243620 loss: 0.0021 lr: 0.02\n",
      "iteration: 243630 loss: 0.0015 lr: 0.02\n",
      "iteration: 243640 loss: 0.0019 lr: 0.02\n",
      "iteration: 243650 loss: 0.0019 lr: 0.02\n",
      "iteration: 243660 loss: 0.0017 lr: 0.02\n",
      "iteration: 243670 loss: 0.0018 lr: 0.02\n",
      "iteration: 243680 loss: 0.0012 lr: 0.02\n",
      "iteration: 243690 loss: 0.0021 lr: 0.02\n",
      "iteration: 243700 loss: 0.0015 lr: 0.02\n",
      "iteration: 243710 loss: 0.0016 lr: 0.02\n",
      "iteration: 243720 loss: 0.0021 lr: 0.02\n",
      "iteration: 243730 loss: 0.0019 lr: 0.02\n",
      "iteration: 243740 loss: 0.0017 lr: 0.02\n",
      "iteration: 243750 loss: 0.0019 lr: 0.02\n",
      "iteration: 243760 loss: 0.0015 lr: 0.02\n",
      "iteration: 243770 loss: 0.0011 lr: 0.02\n",
      "iteration: 243780 loss: 0.0019 lr: 0.02\n",
      "iteration: 243790 loss: 0.0016 lr: 0.02\n",
      "iteration: 243800 loss: 0.0019 lr: 0.02\n",
      "iteration: 243810 loss: 0.0019 lr: 0.02\n",
      "iteration: 243820 loss: 0.0026 lr: 0.02\n",
      "iteration: 243830 loss: 0.0012 lr: 0.02\n",
      "iteration: 243840 loss: 0.0020 lr: 0.02\n",
      "iteration: 243850 loss: 0.0020 lr: 0.02\n",
      "iteration: 243860 loss: 0.0020 lr: 0.02\n",
      "iteration: 243870 loss: 0.0015 lr: 0.02\n",
      "iteration: 243880 loss: 0.0019 lr: 0.02\n",
      "iteration: 243890 loss: 0.0024 lr: 0.02\n",
      "iteration: 243900 loss: 0.0015 lr: 0.02\n",
      "iteration: 243910 loss: 0.0017 lr: 0.02\n",
      "iteration: 243920 loss: 0.0018 lr: 0.02\n",
      "iteration: 243930 loss: 0.0015 lr: 0.02\n",
      "iteration: 243940 loss: 0.0028 lr: 0.02\n",
      "iteration: 243950 loss: 0.0020 lr: 0.02\n",
      "iteration: 243960 loss: 0.0034 lr: 0.02\n",
      "iteration: 243970 loss: 0.0014 lr: 0.02\n",
      "iteration: 243980 loss: 0.0019 lr: 0.02\n",
      "iteration: 243990 loss: 0.0015 lr: 0.02\n",
      "iteration: 244000 loss: 0.0024 lr: 0.02\n",
      "iteration: 244010 loss: 0.0018 lr: 0.02\n",
      "iteration: 244020 loss: 0.0017 lr: 0.02\n",
      "iteration: 244030 loss: 0.0029 lr: 0.02\n",
      "iteration: 244040 loss: 0.0015 lr: 0.02\n",
      "iteration: 244050 loss: 0.0018 lr: 0.02\n",
      "iteration: 244060 loss: 0.0023 lr: 0.02\n",
      "iteration: 244070 loss: 0.0021 lr: 0.02\n",
      "iteration: 244080 loss: 0.0020 lr: 0.02\n",
      "iteration: 244090 loss: 0.0025 lr: 0.02\n",
      "iteration: 244100 loss: 0.0017 lr: 0.02\n",
      "iteration: 244110 loss: 0.0021 lr: 0.02\n",
      "iteration: 244120 loss: 0.0019 lr: 0.02\n",
      "iteration: 244130 loss: 0.0018 lr: 0.02\n",
      "iteration: 244140 loss: 0.0014 lr: 0.02\n",
      "iteration: 244150 loss: 0.0016 lr: 0.02\n",
      "iteration: 244160 loss: 0.0031 lr: 0.02\n",
      "iteration: 244170 loss: 0.0018 lr: 0.02\n",
      "iteration: 244180 loss: 0.0017 lr: 0.02\n",
      "iteration: 244190 loss: 0.0019 lr: 0.02\n",
      "iteration: 244200 loss: 0.0023 lr: 0.02\n",
      "iteration: 244210 loss: 0.0012 lr: 0.02\n",
      "iteration: 244220 loss: 0.0017 lr: 0.02\n",
      "iteration: 244230 loss: 0.0015 lr: 0.02\n",
      "iteration: 244240 loss: 0.0018 lr: 0.02\n",
      "iteration: 244250 loss: 0.0018 lr: 0.02\n",
      "iteration: 244260 loss: 0.0017 lr: 0.02\n",
      "iteration: 244270 loss: 0.0019 lr: 0.02\n",
      "iteration: 244280 loss: 0.0017 lr: 0.02\n",
      "iteration: 244290 loss: 0.0017 lr: 0.02\n",
      "iteration: 244300 loss: 0.0019 lr: 0.02\n",
      "iteration: 244310 loss: 0.0015 lr: 0.02\n",
      "iteration: 244320 loss: 0.0020 lr: 0.02\n",
      "iteration: 244330 loss: 0.0016 lr: 0.02\n",
      "iteration: 244340 loss: 0.0022 lr: 0.02\n",
      "iteration: 244350 loss: 0.0016 lr: 0.02\n",
      "iteration: 244360 loss: 0.0037 lr: 0.02\n",
      "iteration: 244370 loss: 0.0019 lr: 0.02\n",
      "iteration: 244380 loss: 0.0017 lr: 0.02\n",
      "iteration: 244390 loss: 0.0025 lr: 0.02\n",
      "iteration: 244400 loss: 0.0018 lr: 0.02\n",
      "iteration: 244410 loss: 0.0019 lr: 0.02\n",
      "iteration: 244420 loss: 0.0025 lr: 0.02\n",
      "iteration: 244430 loss: 0.0018 lr: 0.02\n",
      "iteration: 244440 loss: 0.0018 lr: 0.02\n",
      "iteration: 244450 loss: 0.0026 lr: 0.02\n",
      "iteration: 244460 loss: 0.0015 lr: 0.02\n",
      "iteration: 244470 loss: 0.0019 lr: 0.02\n",
      "iteration: 244480 loss: 0.0020 lr: 0.02\n",
      "iteration: 244490 loss: 0.0018 lr: 0.02\n",
      "iteration: 244500 loss: 0.0020 lr: 0.02\n",
      "iteration: 244510 loss: 0.0018 lr: 0.02\n",
      "iteration: 244520 loss: 0.0015 lr: 0.02\n",
      "iteration: 244530 loss: 0.0014 lr: 0.02\n",
      "iteration: 244540 loss: 0.0020 lr: 0.02\n",
      "iteration: 244550 loss: 0.0024 lr: 0.02\n",
      "iteration: 244560 loss: 0.0017 lr: 0.02\n",
      "iteration: 244570 loss: 0.0017 lr: 0.02\n",
      "iteration: 244580 loss: 0.0014 lr: 0.02\n",
      "iteration: 244590 loss: 0.0014 lr: 0.02\n",
      "iteration: 244600 loss: 0.0022 lr: 0.02\n",
      "iteration: 244610 loss: 0.0017 lr: 0.02\n",
      "iteration: 244620 loss: 0.0020 lr: 0.02\n",
      "iteration: 244630 loss: 0.0019 lr: 0.02\n",
      "iteration: 244640 loss: 0.0014 lr: 0.02\n",
      "iteration: 244650 loss: 0.0024 lr: 0.02\n",
      "iteration: 244660 loss: 0.0022 lr: 0.02\n",
      "iteration: 244670 loss: 0.0017 lr: 0.02\n",
      "iteration: 244680 loss: 0.0016 lr: 0.02\n",
      "iteration: 244690 loss: 0.0014 lr: 0.02\n",
      "iteration: 244700 loss: 0.0014 lr: 0.02\n",
      "iteration: 244710 loss: 0.0028 lr: 0.02\n",
      "iteration: 244720 loss: 0.0015 lr: 0.02\n",
      "iteration: 244730 loss: 0.0022 lr: 0.02\n",
      "iteration: 244740 loss: 0.0017 lr: 0.02\n",
      "iteration: 244750 loss: 0.0017 lr: 0.02\n",
      "iteration: 244760 loss: 0.0031 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 244770 loss: 0.0028 lr: 0.02\n",
      "iteration: 244780 loss: 0.0025 lr: 0.02\n",
      "iteration: 244790 loss: 0.0019 lr: 0.02\n",
      "iteration: 244800 loss: 0.0015 lr: 0.02\n",
      "iteration: 244810 loss: 0.0018 lr: 0.02\n",
      "iteration: 244820 loss: 0.0011 lr: 0.02\n",
      "iteration: 244830 loss: 0.0014 lr: 0.02\n",
      "iteration: 244840 loss: 0.0018 lr: 0.02\n",
      "iteration: 244850 loss: 0.0013 lr: 0.02\n",
      "iteration: 244860 loss: 0.0018 lr: 0.02\n",
      "iteration: 244870 loss: 0.0014 lr: 0.02\n",
      "iteration: 244880 loss: 0.0015 lr: 0.02\n",
      "iteration: 244890 loss: 0.0017 lr: 0.02\n",
      "iteration: 244900 loss: 0.0014 lr: 0.02\n",
      "iteration: 244910 loss: 0.0018 lr: 0.02\n",
      "iteration: 244920 loss: 0.0012 lr: 0.02\n",
      "iteration: 244930 loss: 0.0028 lr: 0.02\n",
      "iteration: 244940 loss: 0.0016 lr: 0.02\n",
      "iteration: 244950 loss: 0.0036 lr: 0.02\n",
      "iteration: 244960 loss: 0.0014 lr: 0.02\n",
      "iteration: 244970 loss: 0.0020 lr: 0.02\n",
      "iteration: 244980 loss: 0.0014 lr: 0.02\n",
      "iteration: 244990 loss: 0.0023 lr: 0.02\n",
      "iteration: 245000 loss: 0.0020 lr: 0.02\n",
      "iteration: 245010 loss: 0.0015 lr: 0.02\n",
      "iteration: 245020 loss: 0.0027 lr: 0.02\n",
      "iteration: 245030 loss: 0.0027 lr: 0.02\n",
      "iteration: 245040 loss: 0.0015 lr: 0.02\n",
      "iteration: 245050 loss: 0.0015 lr: 0.02\n",
      "iteration: 245060 loss: 0.0015 lr: 0.02\n",
      "iteration: 245070 loss: 0.0017 lr: 0.02\n",
      "iteration: 245080 loss: 0.0023 lr: 0.02\n",
      "iteration: 245090 loss: 0.0014 lr: 0.02\n",
      "iteration: 245100 loss: 0.0016 lr: 0.02\n",
      "iteration: 245110 loss: 0.0019 lr: 0.02\n",
      "iteration: 245120 loss: 0.0018 lr: 0.02\n",
      "iteration: 245130 loss: 0.0018 lr: 0.02\n",
      "iteration: 245140 loss: 0.0020 lr: 0.02\n",
      "iteration: 245150 loss: 0.0015 lr: 0.02\n",
      "iteration: 245160 loss: 0.0010 lr: 0.02\n",
      "iteration: 245170 loss: 0.0014 lr: 0.02\n",
      "iteration: 245180 loss: 0.0019 lr: 0.02\n",
      "iteration: 245190 loss: 0.0017 lr: 0.02\n",
      "iteration: 245200 loss: 0.0019 lr: 0.02\n",
      "iteration: 245210 loss: 0.0014 lr: 0.02\n",
      "iteration: 245220 loss: 0.0020 lr: 0.02\n",
      "iteration: 245230 loss: 0.0024 lr: 0.02\n",
      "iteration: 245240 loss: 0.0023 lr: 0.02\n",
      "iteration: 245250 loss: 0.0023 lr: 0.02\n",
      "iteration: 245260 loss: 0.0019 lr: 0.02\n",
      "iteration: 245270 loss: 0.0026 lr: 0.02\n",
      "iteration: 245280 loss: 0.0013 lr: 0.02\n",
      "iteration: 245290 loss: 0.0021 lr: 0.02\n",
      "iteration: 245300 loss: 0.0021 lr: 0.02\n",
      "iteration: 245310 loss: 0.0022 lr: 0.02\n",
      "iteration: 245320 loss: 0.0014 lr: 0.02\n",
      "iteration: 245330 loss: 0.0011 lr: 0.02\n",
      "iteration: 245340 loss: 0.0018 lr: 0.02\n",
      "iteration: 245350 loss: 0.0016 lr: 0.02\n",
      "iteration: 245360 loss: 0.0020 lr: 0.02\n",
      "iteration: 245370 loss: 0.0013 lr: 0.02\n",
      "iteration: 245380 loss: 0.0025 lr: 0.02\n",
      "iteration: 245390 loss: 0.0017 lr: 0.02\n",
      "iteration: 245400 loss: 0.0018 lr: 0.02\n",
      "iteration: 245410 loss: 0.0028 lr: 0.02\n",
      "iteration: 245420 loss: 0.0023 lr: 0.02\n",
      "iteration: 245430 loss: 0.0023 lr: 0.02\n",
      "iteration: 245440 loss: 0.0031 lr: 0.02\n",
      "iteration: 245450 loss: 0.0022 lr: 0.02\n",
      "iteration: 245460 loss: 0.0021 lr: 0.02\n",
      "iteration: 245470 loss: 0.0019 lr: 0.02\n",
      "iteration: 245480 loss: 0.0019 lr: 0.02\n",
      "iteration: 245490 loss: 0.0018 lr: 0.02\n",
      "iteration: 245500 loss: 0.0027 lr: 0.02\n",
      "iteration: 245510 loss: 0.0026 lr: 0.02\n",
      "iteration: 245520 loss: 0.0019 lr: 0.02\n",
      "iteration: 245530 loss: 0.0015 lr: 0.02\n",
      "iteration: 245540 loss: 0.0019 lr: 0.02\n",
      "iteration: 245550 loss: 0.0015 lr: 0.02\n",
      "iteration: 245560 loss: 0.0014 lr: 0.02\n",
      "iteration: 245570 loss: 0.0025 lr: 0.02\n",
      "iteration: 245580 loss: 0.0024 lr: 0.02\n",
      "iteration: 245590 loss: 0.0019 lr: 0.02\n",
      "iteration: 245600 loss: 0.0017 lr: 0.02\n",
      "iteration: 245610 loss: 0.0026 lr: 0.02\n",
      "iteration: 245620 loss: 0.0025 lr: 0.02\n",
      "iteration: 245630 loss: 0.0012 lr: 0.02\n",
      "iteration: 245640 loss: 0.0031 lr: 0.02\n",
      "iteration: 245650 loss: 0.0022 lr: 0.02\n",
      "iteration: 245660 loss: 0.0018 lr: 0.02\n",
      "iteration: 245670 loss: 0.0020 lr: 0.02\n",
      "iteration: 245680 loss: 0.0028 lr: 0.02\n",
      "iteration: 245690 loss: 0.0017 lr: 0.02\n",
      "iteration: 245700 loss: 0.0020 lr: 0.02\n",
      "iteration: 245710 loss: 0.0014 lr: 0.02\n",
      "iteration: 245720 loss: 0.0015 lr: 0.02\n",
      "iteration: 245730 loss: 0.0020 lr: 0.02\n",
      "iteration: 245740 loss: 0.0012 lr: 0.02\n",
      "iteration: 245750 loss: 0.0021 lr: 0.02\n",
      "iteration: 245760 loss: 0.0023 lr: 0.02\n",
      "iteration: 245770 loss: 0.0019 lr: 0.02\n",
      "iteration: 245780 loss: 0.0016 lr: 0.02\n",
      "iteration: 245790 loss: 0.0017 lr: 0.02\n",
      "iteration: 245800 loss: 0.0012 lr: 0.02\n",
      "iteration: 245810 loss: 0.0020 lr: 0.02\n",
      "iteration: 245820 loss: 0.0025 lr: 0.02\n",
      "iteration: 245830 loss: 0.0017 lr: 0.02\n",
      "iteration: 245840 loss: 0.0017 lr: 0.02\n",
      "iteration: 245850 loss: 0.0014 lr: 0.02\n",
      "iteration: 245860 loss: 0.0023 lr: 0.02\n",
      "iteration: 245870 loss: 0.0019 lr: 0.02\n",
      "iteration: 245880 loss: 0.0026 lr: 0.02\n",
      "iteration: 245890 loss: 0.0040 lr: 0.02\n",
      "iteration: 245900 loss: 0.0016 lr: 0.02\n",
      "iteration: 245910 loss: 0.0019 lr: 0.02\n",
      "iteration: 245920 loss: 0.0019 lr: 0.02\n",
      "iteration: 245930 loss: 0.0024 lr: 0.02\n",
      "iteration: 245940 loss: 0.0020 lr: 0.02\n",
      "iteration: 245950 loss: 0.0018 lr: 0.02\n",
      "iteration: 245960 loss: 0.0017 lr: 0.02\n",
      "iteration: 245970 loss: 0.0017 lr: 0.02\n",
      "iteration: 245980 loss: 0.0015 lr: 0.02\n",
      "iteration: 245990 loss: 0.0014 lr: 0.02\n",
      "iteration: 246000 loss: 0.0023 lr: 0.02\n",
      "iteration: 246010 loss: 0.0015 lr: 0.02\n",
      "iteration: 246020 loss: 0.0013 lr: 0.02\n",
      "iteration: 246030 loss: 0.0017 lr: 0.02\n",
      "iteration: 246040 loss: 0.0013 lr: 0.02\n",
      "iteration: 246050 loss: 0.0013 lr: 0.02\n",
      "iteration: 246060 loss: 0.0018 lr: 0.02\n",
      "iteration: 246070 loss: 0.0018 lr: 0.02\n",
      "iteration: 246080 loss: 0.0015 lr: 0.02\n",
      "iteration: 246090 loss: 0.0015 lr: 0.02\n",
      "iteration: 246100 loss: 0.0017 lr: 0.02\n",
      "iteration: 246110 loss: 0.0014 lr: 0.02\n",
      "iteration: 246120 loss: 0.0019 lr: 0.02\n",
      "iteration: 246130 loss: 0.0024 lr: 0.02\n",
      "iteration: 246140 loss: 0.0020 lr: 0.02\n",
      "iteration: 246150 loss: 0.0030 lr: 0.02\n",
      "iteration: 246160 loss: 0.0019 lr: 0.02\n",
      "iteration: 246170 loss: 0.0022 lr: 0.02\n",
      "iteration: 246180 loss: 0.0025 lr: 0.02\n",
      "iteration: 246190 loss: 0.0022 lr: 0.02\n",
      "iteration: 246200 loss: 0.0022 lr: 0.02\n",
      "iteration: 246210 loss: 0.0018 lr: 0.02\n",
      "iteration: 246220 loss: 0.0016 lr: 0.02\n",
      "iteration: 246230 loss: 0.0014 lr: 0.02\n",
      "iteration: 246240 loss: 0.0017 lr: 0.02\n",
      "iteration: 246250 loss: 0.0017 lr: 0.02\n",
      "iteration: 246260 loss: 0.0023 lr: 0.02\n",
      "iteration: 246270 loss: 0.0034 lr: 0.02\n",
      "iteration: 246280 loss: 0.0034 lr: 0.02\n",
      "iteration: 246290 loss: 0.0031 lr: 0.02\n",
      "iteration: 246300 loss: 0.0024 lr: 0.02\n",
      "iteration: 246310 loss: 0.0033 lr: 0.02\n",
      "iteration: 246320 loss: 0.0031 lr: 0.02\n",
      "iteration: 246330 loss: 0.0025 lr: 0.02\n",
      "iteration: 246340 loss: 0.0019 lr: 0.02\n",
      "iteration: 246350 loss: 0.0025 lr: 0.02\n",
      "iteration: 246360 loss: 0.0037 lr: 0.02\n",
      "iteration: 246370 loss: 0.0025 lr: 0.02\n",
      "iteration: 246380 loss: 0.0030 lr: 0.02\n",
      "iteration: 246390 loss: 0.0023 lr: 0.02\n",
      "iteration: 246400 loss: 0.0020 lr: 0.02\n",
      "iteration: 246410 loss: 0.0022 lr: 0.02\n",
      "iteration: 246420 loss: 0.0024 lr: 0.02\n",
      "iteration: 246430 loss: 0.0026 lr: 0.02\n",
      "iteration: 246440 loss: 0.0026 lr: 0.02\n",
      "iteration: 246450 loss: 0.0029 lr: 0.02\n",
      "iteration: 246460 loss: 0.0018 lr: 0.02\n",
      "iteration: 246470 loss: 0.0025 lr: 0.02\n",
      "iteration: 246480 loss: 0.0018 lr: 0.02\n",
      "iteration: 246490 loss: 0.0034 lr: 0.02\n",
      "iteration: 246500 loss: 0.0016 lr: 0.02\n",
      "iteration: 246510 loss: 0.0018 lr: 0.02\n",
      "iteration: 246520 loss: 0.0019 lr: 0.02\n",
      "iteration: 246530 loss: 0.0022 lr: 0.02\n",
      "iteration: 246540 loss: 0.0024 lr: 0.02\n",
      "iteration: 246550 loss: 0.0025 lr: 0.02\n",
      "iteration: 246560 loss: 0.0016 lr: 0.02\n",
      "iteration: 246570 loss: 0.0023 lr: 0.02\n",
      "iteration: 246580 loss: 0.0021 lr: 0.02\n",
      "iteration: 246590 loss: 0.0034 lr: 0.02\n",
      "iteration: 246600 loss: 0.0013 lr: 0.02\n",
      "iteration: 246610 loss: 0.0028 lr: 0.02\n",
      "iteration: 246620 loss: 0.0022 lr: 0.02\n",
      "iteration: 246630 loss: 0.0028 lr: 0.02\n",
      "iteration: 246640 loss: 0.0021 lr: 0.02\n",
      "iteration: 246650 loss: 0.0021 lr: 0.02\n",
      "iteration: 246660 loss: 0.0021 lr: 0.02\n",
      "iteration: 246670 loss: 0.0025 lr: 0.02\n",
      "iteration: 246680 loss: 0.0022 lr: 0.02\n",
      "iteration: 246690 loss: 0.0026 lr: 0.02\n",
      "iteration: 246700 loss: 0.0024 lr: 0.02\n",
      "iteration: 246710 loss: 0.0028 lr: 0.02\n",
      "iteration: 246720 loss: 0.0018 lr: 0.02\n",
      "iteration: 246730 loss: 0.0025 lr: 0.02\n",
      "iteration: 246740 loss: 0.0029 lr: 0.02\n",
      "iteration: 246750 loss: 0.0019 lr: 0.02\n",
      "iteration: 246760 loss: 0.0018 lr: 0.02\n",
      "iteration: 246770 loss: 0.0017 lr: 0.02\n",
      "iteration: 246780 loss: 0.0021 lr: 0.02\n",
      "iteration: 246790 loss: 0.0037 lr: 0.02\n",
      "iteration: 246800 loss: 0.0021 lr: 0.02\n",
      "iteration: 246810 loss: 0.0018 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 246820 loss: 0.0027 lr: 0.02\n",
      "iteration: 246830 loss: 0.0026 lr: 0.02\n",
      "iteration: 246840 loss: 0.0021 lr: 0.02\n",
      "iteration: 246850 loss: 0.0033 lr: 0.02\n",
      "iteration: 246860 loss: 0.0017 lr: 0.02\n",
      "iteration: 246870 loss: 0.0021 lr: 0.02\n",
      "iteration: 246880 loss: 0.0022 lr: 0.02\n",
      "iteration: 246890 loss: 0.0016 lr: 0.02\n",
      "iteration: 246900 loss: 0.0037 lr: 0.02\n",
      "iteration: 246910 loss: 0.0021 lr: 0.02\n",
      "iteration: 246920 loss: 0.0019 lr: 0.02\n",
      "iteration: 246930 loss: 0.0020 lr: 0.02\n",
      "iteration: 246940 loss: 0.0020 lr: 0.02\n",
      "iteration: 246950 loss: 0.0030 lr: 0.02\n",
      "iteration: 246960 loss: 0.0019 lr: 0.02\n",
      "iteration: 246970 loss: 0.0015 lr: 0.02\n",
      "iteration: 246980 loss: 0.0023 lr: 0.02\n",
      "iteration: 246990 loss: 0.0023 lr: 0.02\n",
      "iteration: 247000 loss: 0.0014 lr: 0.02\n",
      "iteration: 247010 loss: 0.0020 lr: 0.02\n",
      "iteration: 247020 loss: 0.0017 lr: 0.02\n",
      "iteration: 247030 loss: 0.0015 lr: 0.02\n",
      "iteration: 247040 loss: 0.0012 lr: 0.02\n",
      "iteration: 247050 loss: 0.0022 lr: 0.02\n",
      "iteration: 247060 loss: 0.0021 lr: 0.02\n",
      "iteration: 247070 loss: 0.0023 lr: 0.02\n",
      "iteration: 247080 loss: 0.0023 lr: 0.02\n",
      "iteration: 247090 loss: 0.0021 lr: 0.02\n",
      "iteration: 247100 loss: 0.0025 lr: 0.02\n",
      "iteration: 247110 loss: 0.0018 lr: 0.02\n",
      "iteration: 247120 loss: 0.0032 lr: 0.02\n",
      "iteration: 247130 loss: 0.0015 lr: 0.02\n",
      "iteration: 247140 loss: 0.0016 lr: 0.02\n",
      "iteration: 247150 loss: 0.0020 lr: 0.02\n",
      "iteration: 247160 loss: 0.0019 lr: 0.02\n",
      "iteration: 247170 loss: 0.0029 lr: 0.02\n",
      "iteration: 247180 loss: 0.0021 lr: 0.02\n",
      "iteration: 247190 loss: 0.0016 lr: 0.02\n",
      "iteration: 247200 loss: 0.0026 lr: 0.02\n",
      "iteration: 247210 loss: 0.0017 lr: 0.02\n",
      "iteration: 247220 loss: 0.0030 lr: 0.02\n",
      "iteration: 247230 loss: 0.0023 lr: 0.02\n",
      "iteration: 247240 loss: 0.0025 lr: 0.02\n",
      "iteration: 247250 loss: 0.0026 lr: 0.02\n",
      "iteration: 247260 loss: 0.0023 lr: 0.02\n",
      "iteration: 247270 loss: 0.0013 lr: 0.02\n",
      "iteration: 247280 loss: 0.0016 lr: 0.02\n",
      "iteration: 247290 loss: 0.0018 lr: 0.02\n",
      "iteration: 247300 loss: 0.0021 lr: 0.02\n",
      "iteration: 247310 loss: 0.0024 lr: 0.02\n",
      "iteration: 247320 loss: 0.0026 lr: 0.02\n",
      "iteration: 247330 loss: 0.0015 lr: 0.02\n",
      "iteration: 247340 loss: 0.0024 lr: 0.02\n",
      "iteration: 247350 loss: 0.0015 lr: 0.02\n",
      "iteration: 247360 loss: 0.0031 lr: 0.02\n",
      "iteration: 247370 loss: 0.0021 lr: 0.02\n",
      "iteration: 247380 loss: 0.0020 lr: 0.02\n",
      "iteration: 247390 loss: 0.0030 lr: 0.02\n",
      "iteration: 247400 loss: 0.0021 lr: 0.02\n",
      "iteration: 247410 loss: 0.0013 lr: 0.02\n",
      "iteration: 247420 loss: 0.0024 lr: 0.02\n",
      "iteration: 247430 loss: 0.0019 lr: 0.02\n",
      "iteration: 247440 loss: 0.0023 lr: 0.02\n",
      "iteration: 247450 loss: 0.0022 lr: 0.02\n",
      "iteration: 247460 loss: 0.0024 lr: 0.02\n",
      "iteration: 247470 loss: 0.0024 lr: 0.02\n",
      "iteration: 247480 loss: 0.0015 lr: 0.02\n",
      "iteration: 247490 loss: 0.0029 lr: 0.02\n",
      "iteration: 247500 loss: 0.0026 lr: 0.02\n",
      "iteration: 247510 loss: 0.0026 lr: 0.02\n",
      "iteration: 247520 loss: 0.0026 lr: 0.02\n",
      "iteration: 247530 loss: 0.0027 lr: 0.02\n",
      "iteration: 247540 loss: 0.0025 lr: 0.02\n",
      "iteration: 247550 loss: 0.0013 lr: 0.02\n",
      "iteration: 247560 loss: 0.0015 lr: 0.02\n",
      "iteration: 247570 loss: 0.0022 lr: 0.02\n",
      "iteration: 247580 loss: 0.0018 lr: 0.02\n",
      "iteration: 247590 loss: 0.0017 lr: 0.02\n",
      "iteration: 247600 loss: 0.0015 lr: 0.02\n",
      "iteration: 247610 loss: 0.0028 lr: 0.02\n",
      "iteration: 247620 loss: 0.0018 lr: 0.02\n",
      "iteration: 247630 loss: 0.0018 lr: 0.02\n",
      "iteration: 247640 loss: 0.0017 lr: 0.02\n",
      "iteration: 247650 loss: 0.0017 lr: 0.02\n",
      "iteration: 247660 loss: 0.0019 lr: 0.02\n",
      "iteration: 247670 loss: 0.0014 lr: 0.02\n",
      "iteration: 247680 loss: 0.0031 lr: 0.02\n",
      "iteration: 247690 loss: 0.0025 lr: 0.02\n",
      "iteration: 247700 loss: 0.0015 lr: 0.02\n",
      "iteration: 247710 loss: 0.0024 lr: 0.02\n",
      "iteration: 247720 loss: 0.0031 lr: 0.02\n",
      "iteration: 247730 loss: 0.0020 lr: 0.02\n",
      "iteration: 247740 loss: 0.0017 lr: 0.02\n",
      "iteration: 247750 loss: 0.0018 lr: 0.02\n",
      "iteration: 247760 loss: 0.0018 lr: 0.02\n",
      "iteration: 247770 loss: 0.0014 lr: 0.02\n",
      "iteration: 247780 loss: 0.0020 lr: 0.02\n",
      "iteration: 247790 loss: 0.0016 lr: 0.02\n",
      "iteration: 247800 loss: 0.0015 lr: 0.02\n",
      "iteration: 247810 loss: 0.0015 lr: 0.02\n",
      "iteration: 247820 loss: 0.0013 lr: 0.02\n",
      "iteration: 247830 loss: 0.0019 lr: 0.02\n",
      "iteration: 247840 loss: 0.0019 lr: 0.02\n",
      "iteration: 247850 loss: 0.0016 lr: 0.02\n",
      "iteration: 247860 loss: 0.0023 lr: 0.02\n",
      "iteration: 247870 loss: 0.0026 lr: 0.02\n",
      "iteration: 247880 loss: 0.0020 lr: 0.02\n",
      "iteration: 247890 loss: 0.0020 lr: 0.02\n",
      "iteration: 247900 loss: 0.0021 lr: 0.02\n",
      "iteration: 247910 loss: 0.0023 lr: 0.02\n",
      "iteration: 247920 loss: 0.0015 lr: 0.02\n",
      "iteration: 247930 loss: 0.0023 lr: 0.02\n",
      "iteration: 247940 loss: 0.0027 lr: 0.02\n",
      "iteration: 247950 loss: 0.0024 lr: 0.02\n",
      "iteration: 247960 loss: 0.0032 lr: 0.02\n",
      "iteration: 247970 loss: 0.0017 lr: 0.02\n",
      "iteration: 247980 loss: 0.0017 lr: 0.02\n",
      "iteration: 247990 loss: 0.0026 lr: 0.02\n",
      "iteration: 248000 loss: 0.0018 lr: 0.02\n",
      "iteration: 248010 loss: 0.0014 lr: 0.02\n",
      "iteration: 248020 loss: 0.0016 lr: 0.02\n",
      "iteration: 248030 loss: 0.0014 lr: 0.02\n",
      "iteration: 248040 loss: 0.0030 lr: 0.02\n",
      "iteration: 248050 loss: 0.0018 lr: 0.02\n",
      "iteration: 248060 loss: 0.0029 lr: 0.02\n",
      "iteration: 248070 loss: 0.0022 lr: 0.02\n",
      "iteration: 248080 loss: 0.0016 lr: 0.02\n",
      "iteration: 248090 loss: 0.0014 lr: 0.02\n",
      "iteration: 248100 loss: 0.0020 lr: 0.02\n",
      "iteration: 248110 loss: 0.0011 lr: 0.02\n",
      "iteration: 248120 loss: 0.0020 lr: 0.02\n",
      "iteration: 248130 loss: 0.0033 lr: 0.02\n",
      "iteration: 248140 loss: 0.0019 lr: 0.02\n",
      "iteration: 248150 loss: 0.0022 lr: 0.02\n",
      "iteration: 248160 loss: 0.0020 lr: 0.02\n",
      "iteration: 248170 loss: 0.0022 lr: 0.02\n",
      "iteration: 248180 loss: 0.0028 lr: 0.02\n",
      "iteration: 248190 loss: 0.0022 lr: 0.02\n",
      "iteration: 248200 loss: 0.0017 lr: 0.02\n",
      "iteration: 248210 loss: 0.0025 lr: 0.02\n",
      "iteration: 248220 loss: 0.0018 lr: 0.02\n",
      "iteration: 248230 loss: 0.0019 lr: 0.02\n",
      "iteration: 248240 loss: 0.0014 lr: 0.02\n",
      "iteration: 248250 loss: 0.0015 lr: 0.02\n",
      "iteration: 248260 loss: 0.0018 lr: 0.02\n",
      "iteration: 248270 loss: 0.0016 lr: 0.02\n",
      "iteration: 248280 loss: 0.0022 lr: 0.02\n",
      "iteration: 248290 loss: 0.0018 lr: 0.02\n",
      "iteration: 248300 loss: 0.0018 lr: 0.02\n",
      "iteration: 248310 loss: 0.0023 lr: 0.02\n",
      "iteration: 248320 loss: 0.0024 lr: 0.02\n",
      "iteration: 248330 loss: 0.0028 lr: 0.02\n",
      "iteration: 248340 loss: 0.0020 lr: 0.02\n",
      "iteration: 248350 loss: 0.0031 lr: 0.02\n",
      "iteration: 248360 loss: 0.0015 lr: 0.02\n",
      "iteration: 248370 loss: 0.0018 lr: 0.02\n",
      "iteration: 248380 loss: 0.0018 lr: 0.02\n",
      "iteration: 248390 loss: 0.0017 lr: 0.02\n",
      "iteration: 248400 loss: 0.0027 lr: 0.02\n",
      "iteration: 248410 loss: 0.0020 lr: 0.02\n",
      "iteration: 248420 loss: 0.0016 lr: 0.02\n",
      "iteration: 248430 loss: 0.0016 lr: 0.02\n",
      "iteration: 248440 loss: 0.0028 lr: 0.02\n",
      "iteration: 248450 loss: 0.0022 lr: 0.02\n",
      "iteration: 248460 loss: 0.0027 lr: 0.02\n",
      "iteration: 248470 loss: 0.0021 lr: 0.02\n",
      "iteration: 248480 loss: 0.0016 lr: 0.02\n",
      "iteration: 248490 loss: 0.0025 lr: 0.02\n",
      "iteration: 248500 loss: 0.0016 lr: 0.02\n",
      "iteration: 248510 loss: 0.0015 lr: 0.02\n",
      "iteration: 248520 loss: 0.0018 lr: 0.02\n",
      "iteration: 248530 loss: 0.0017 lr: 0.02\n",
      "iteration: 248540 loss: 0.0022 lr: 0.02\n",
      "iteration: 248550 loss: 0.0026 lr: 0.02\n",
      "iteration: 248560 loss: 0.0015 lr: 0.02\n",
      "iteration: 248570 loss: 0.0015 lr: 0.02\n",
      "iteration: 248580 loss: 0.0015 lr: 0.02\n",
      "iteration: 248590 loss: 0.0022 lr: 0.02\n",
      "iteration: 248600 loss: 0.0020 lr: 0.02\n",
      "iteration: 248610 loss: 0.0026 lr: 0.02\n",
      "iteration: 248620 loss: 0.0016 lr: 0.02\n",
      "iteration: 248630 loss: 0.0016 lr: 0.02\n",
      "iteration: 248640 loss: 0.0017 lr: 0.02\n",
      "iteration: 248650 loss: 0.0011 lr: 0.02\n",
      "iteration: 248660 loss: 0.0025 lr: 0.02\n",
      "iteration: 248670 loss: 0.0021 lr: 0.02\n",
      "iteration: 248680 loss: 0.0020 lr: 0.02\n",
      "iteration: 248690 loss: 0.0022 lr: 0.02\n",
      "iteration: 248700 loss: 0.0014 lr: 0.02\n",
      "iteration: 248710 loss: 0.0013 lr: 0.02\n",
      "iteration: 248720 loss: 0.0018 lr: 0.02\n",
      "iteration: 248730 loss: 0.0017 lr: 0.02\n",
      "iteration: 248740 loss: 0.0029 lr: 0.02\n",
      "iteration: 248750 loss: 0.0018 lr: 0.02\n",
      "iteration: 248760 loss: 0.0015 lr: 0.02\n",
      "iteration: 248770 loss: 0.0016 lr: 0.02\n",
      "iteration: 248780 loss: 0.0019 lr: 0.02\n",
      "iteration: 248790 loss: 0.0019 lr: 0.02\n",
      "iteration: 248800 loss: 0.0019 lr: 0.02\n",
      "iteration: 248810 loss: 0.0015 lr: 0.02\n",
      "iteration: 248820 loss: 0.0023 lr: 0.02\n",
      "iteration: 248830 loss: 0.0014 lr: 0.02\n",
      "iteration: 248840 loss: 0.0019 lr: 0.02\n",
      "iteration: 248850 loss: 0.0022 lr: 0.02\n",
      "iteration: 248860 loss: 0.0018 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 248870 loss: 0.0020 lr: 0.02\n",
      "iteration: 248880 loss: 0.0017 lr: 0.02\n",
      "iteration: 248890 loss: 0.0018 lr: 0.02\n",
      "iteration: 248900 loss: 0.0014 lr: 0.02\n",
      "iteration: 248910 loss: 0.0016 lr: 0.02\n",
      "iteration: 248920 loss: 0.0022 lr: 0.02\n",
      "iteration: 248930 loss: 0.0022 lr: 0.02\n",
      "iteration: 248940 loss: 0.0017 lr: 0.02\n",
      "iteration: 248950 loss: 0.0018 lr: 0.02\n",
      "iteration: 248960 loss: 0.0016 lr: 0.02\n",
      "iteration: 248970 loss: 0.0017 lr: 0.02\n",
      "iteration: 248980 loss: 0.0013 lr: 0.02\n",
      "iteration: 248990 loss: 0.0027 lr: 0.02\n",
      "iteration: 249000 loss: 0.0040 lr: 0.02\n",
      "iteration: 249010 loss: 0.0018 lr: 0.02\n",
      "iteration: 249020 loss: 0.0022 lr: 0.02\n",
      "iteration: 249030 loss: 0.0023 lr: 0.02\n",
      "iteration: 249040 loss: 0.0027 lr: 0.02\n",
      "iteration: 249050 loss: 0.0014 lr: 0.02\n",
      "iteration: 249060 loss: 0.0020 lr: 0.02\n",
      "iteration: 249070 loss: 0.0019 lr: 0.02\n",
      "iteration: 249080 loss: 0.0019 lr: 0.02\n",
      "iteration: 249090 loss: 0.0020 lr: 0.02\n",
      "iteration: 249100 loss: 0.0021 lr: 0.02\n",
      "iteration: 249110 loss: 0.0018 lr: 0.02\n",
      "iteration: 249120 loss: 0.0019 lr: 0.02\n",
      "iteration: 249130 loss: 0.0039 lr: 0.02\n",
      "iteration: 249140 loss: 0.0022 lr: 0.02\n",
      "iteration: 249150 loss: 0.0015 lr: 0.02\n",
      "iteration: 249160 loss: 0.0022 lr: 0.02\n",
      "iteration: 249170 loss: 0.0022 lr: 0.02\n",
      "iteration: 249180 loss: 0.0023 lr: 0.02\n",
      "iteration: 249190 loss: 0.0020 lr: 0.02\n",
      "iteration: 249200 loss: 0.0016 lr: 0.02\n",
      "iteration: 249210 loss: 0.0017 lr: 0.02\n",
      "iteration: 249220 loss: 0.0020 lr: 0.02\n",
      "iteration: 249230 loss: 0.0012 lr: 0.02\n",
      "iteration: 249240 loss: 0.0022 lr: 0.02\n",
      "iteration: 249250 loss: 0.0015 lr: 0.02\n",
      "iteration: 249260 loss: 0.0021 lr: 0.02\n",
      "iteration: 249270 loss: 0.0024 lr: 0.02\n",
      "iteration: 249280 loss: 0.0024 lr: 0.02\n",
      "iteration: 249290 loss: 0.0018 lr: 0.02\n",
      "iteration: 249300 loss: 0.0026 lr: 0.02\n",
      "iteration: 249310 loss: 0.0022 lr: 0.02\n",
      "iteration: 249320 loss: 0.0016 lr: 0.02\n",
      "iteration: 249330 loss: 0.0020 lr: 0.02\n",
      "iteration: 249340 loss: 0.0019 lr: 0.02\n",
      "iteration: 249350 loss: 0.0029 lr: 0.02\n",
      "iteration: 249360 loss: 0.0020 lr: 0.02\n",
      "iteration: 249370 loss: 0.0019 lr: 0.02\n",
      "iteration: 249380 loss: 0.0015 lr: 0.02\n",
      "iteration: 249390 loss: 0.0030 lr: 0.02\n",
      "iteration: 249400 loss: 0.0039 lr: 0.02\n",
      "iteration: 249410 loss: 0.0013 lr: 0.02\n",
      "iteration: 249420 loss: 0.0023 lr: 0.02\n",
      "iteration: 249430 loss: 0.0027 lr: 0.02\n",
      "iteration: 249440 loss: 0.0016 lr: 0.02\n",
      "iteration: 249450 loss: 0.0014 lr: 0.02\n",
      "iteration: 249460 loss: 0.0022 lr: 0.02\n",
      "iteration: 249470 loss: 0.0017 lr: 0.02\n",
      "iteration: 249480 loss: 0.0015 lr: 0.02\n",
      "iteration: 249490 loss: 0.0019 lr: 0.02\n",
      "iteration: 249500 loss: 0.0022 lr: 0.02\n",
      "iteration: 249510 loss: 0.0024 lr: 0.02\n",
      "iteration: 249520 loss: 0.0018 lr: 0.02\n",
      "iteration: 249530 loss: 0.0020 lr: 0.02\n",
      "iteration: 249540 loss: 0.0017 lr: 0.02\n",
      "iteration: 249550 loss: 0.0019 lr: 0.02\n",
      "iteration: 249560 loss: 0.0023 lr: 0.02\n",
      "iteration: 249570 loss: 0.0014 lr: 0.02\n",
      "iteration: 249580 loss: 0.0016 lr: 0.02\n",
      "iteration: 249590 loss: 0.0018 lr: 0.02\n",
      "iteration: 249600 loss: 0.0022 lr: 0.02\n",
      "iteration: 249610 loss: 0.0017 lr: 0.02\n",
      "iteration: 249620 loss: 0.0013 lr: 0.02\n",
      "iteration: 249630 loss: 0.0015 lr: 0.02\n",
      "iteration: 249640 loss: 0.0018 lr: 0.02\n",
      "iteration: 249650 loss: 0.0018 lr: 0.02\n",
      "iteration: 249660 loss: 0.0014 lr: 0.02\n",
      "iteration: 249670 loss: 0.0014 lr: 0.02\n",
      "iteration: 249680 loss: 0.0021 lr: 0.02\n",
      "iteration: 249690 loss: 0.0016 lr: 0.02\n",
      "iteration: 249700 loss: 0.0020 lr: 0.02\n",
      "iteration: 249710 loss: 0.0014 lr: 0.02\n",
      "iteration: 249720 loss: 0.0023 lr: 0.02\n",
      "iteration: 249730 loss: 0.0018 lr: 0.02\n",
      "iteration: 249740 loss: 0.0020 lr: 0.02\n",
      "iteration: 249750 loss: 0.0017 lr: 0.02\n",
      "iteration: 249760 loss: 0.0017 lr: 0.02\n",
      "iteration: 249770 loss: 0.0016 lr: 0.02\n",
      "iteration: 249780 loss: 0.0019 lr: 0.02\n",
      "iteration: 249790 loss: 0.0015 lr: 0.02\n",
      "iteration: 249800 loss: 0.0033 lr: 0.02\n",
      "iteration: 249810 loss: 0.0021 lr: 0.02\n",
      "iteration: 249820 loss: 0.0019 lr: 0.02\n",
      "iteration: 249830 loss: 0.0026 lr: 0.02\n",
      "iteration: 249840 loss: 0.0020 lr: 0.02\n",
      "iteration: 249850 loss: 0.0024 lr: 0.02\n",
      "iteration: 249860 loss: 0.0014 lr: 0.02\n",
      "iteration: 249870 loss: 0.0015 lr: 0.02\n",
      "iteration: 249880 loss: 0.0015 lr: 0.02\n",
      "iteration: 249890 loss: 0.0019 lr: 0.02\n",
      "iteration: 249900 loss: 0.0021 lr: 0.02\n",
      "iteration: 249910 loss: 0.0019 lr: 0.02\n",
      "iteration: 249920 loss: 0.0019 lr: 0.02\n",
      "iteration: 249930 loss: 0.0018 lr: 0.02\n",
      "iteration: 249940 loss: 0.0015 lr: 0.02\n",
      "iteration: 249950 loss: 0.0011 lr: 0.02\n",
      "iteration: 249960 loss: 0.0021 lr: 0.02\n",
      "iteration: 249970 loss: 0.0017 lr: 0.02\n",
      "iteration: 249980 loss: 0.0022 lr: 0.02\n",
      "iteration: 249990 loss: 0.0027 lr: 0.02\n",
      "iteration: 250000 loss: 0.0023 lr: 0.02\n",
      "iteration: 250010 loss: 0.0019 lr: 0.02\n",
      "iteration: 250020 loss: 0.0021 lr: 0.02\n",
      "iteration: 250030 loss: 0.0023 lr: 0.02\n",
      "iteration: 250040 loss: 0.0014 lr: 0.02\n",
      "iteration: 250050 loss: 0.0014 lr: 0.02\n",
      "iteration: 250060 loss: 0.0012 lr: 0.02\n",
      "iteration: 250070 loss: 0.0020 lr: 0.02\n",
      "iteration: 250080 loss: 0.0019 lr: 0.02\n",
      "iteration: 250090 loss: 0.0022 lr: 0.02\n",
      "iteration: 250100 loss: 0.0032 lr: 0.02\n",
      "iteration: 250110 loss: 0.0016 lr: 0.02\n",
      "iteration: 250120 loss: 0.0018 lr: 0.02\n",
      "iteration: 250130 loss: 0.0024 lr: 0.02\n",
      "iteration: 250140 loss: 0.0014 lr: 0.02\n",
      "iteration: 250150 loss: 0.0026 lr: 0.02\n",
      "iteration: 250160 loss: 0.0023 lr: 0.02\n",
      "iteration: 250170 loss: 0.0018 lr: 0.02\n",
      "iteration: 250180 loss: 0.0019 lr: 0.02\n",
      "iteration: 250190 loss: 0.0032 lr: 0.02\n",
      "iteration: 250200 loss: 0.0015 lr: 0.02\n",
      "iteration: 250210 loss: 0.0018 lr: 0.02\n",
      "iteration: 250220 loss: 0.0022 lr: 0.02\n",
      "iteration: 250230 loss: 0.0017 lr: 0.02\n",
      "iteration: 250240 loss: 0.0019 lr: 0.02\n",
      "iteration: 250250 loss: 0.0016 lr: 0.02\n",
      "iteration: 250260 loss: 0.0020 lr: 0.02\n",
      "iteration: 250270 loss: 0.0027 lr: 0.02\n",
      "iteration: 250280 loss: 0.0016 lr: 0.02\n",
      "iteration: 250290 loss: 0.0021 lr: 0.02\n",
      "iteration: 250300 loss: 0.0025 lr: 0.02\n",
      "iteration: 250310 loss: 0.0021 lr: 0.02\n",
      "iteration: 250320 loss: 0.0017 lr: 0.02\n",
      "iteration: 250330 loss: 0.0020 lr: 0.02\n",
      "iteration: 250340 loss: 0.0018 lr: 0.02\n",
      "iteration: 250350 loss: 0.0019 lr: 0.02\n",
      "iteration: 250360 loss: 0.0016 lr: 0.02\n",
      "iteration: 250370 loss: 0.0020 lr: 0.02\n",
      "iteration: 250380 loss: 0.0023 lr: 0.02\n",
      "iteration: 250390 loss: 0.0019 lr: 0.02\n",
      "iteration: 250400 loss: 0.0017 lr: 0.02\n",
      "iteration: 250410 loss: 0.0019 lr: 0.02\n",
      "iteration: 250420 loss: 0.0023 lr: 0.02\n",
      "iteration: 250430 loss: 0.0020 lr: 0.02\n",
      "iteration: 250440 loss: 0.0016 lr: 0.02\n",
      "iteration: 250450 loss: 0.0015 lr: 0.02\n",
      "iteration: 250460 loss: 0.0014 lr: 0.02\n",
      "iteration: 250470 loss: 0.0018 lr: 0.02\n",
      "iteration: 250480 loss: 0.0019 lr: 0.02\n",
      "iteration: 250490 loss: 0.0016 lr: 0.02\n",
      "iteration: 250500 loss: 0.0016 lr: 0.02\n",
      "iteration: 250510 loss: 0.0016 lr: 0.02\n",
      "iteration: 250520 loss: 0.0015 lr: 0.02\n",
      "iteration: 250530 loss: 0.0020 lr: 0.02\n",
      "iteration: 250540 loss: 0.0017 lr: 0.02\n",
      "iteration: 250550 loss: 0.0017 lr: 0.02\n",
      "iteration: 250560 loss: 0.0017 lr: 0.02\n",
      "iteration: 250570 loss: 0.0024 lr: 0.02\n",
      "iteration: 250580 loss: 0.0013 lr: 0.02\n",
      "iteration: 250590 loss: 0.0017 lr: 0.02\n",
      "iteration: 250600 loss: 0.0018 lr: 0.02\n",
      "iteration: 250610 loss: 0.0020 lr: 0.02\n",
      "iteration: 250620 loss: 0.0019 lr: 0.02\n",
      "iteration: 250630 loss: 0.0023 lr: 0.02\n",
      "iteration: 250640 loss: 0.0019 lr: 0.02\n",
      "iteration: 250650 loss: 0.0018 lr: 0.02\n",
      "iteration: 250660 loss: 0.0021 lr: 0.02\n",
      "iteration: 250670 loss: 0.0013 lr: 0.02\n",
      "iteration: 250680 loss: 0.0019 lr: 0.02\n",
      "iteration: 250690 loss: 0.0017 lr: 0.02\n",
      "iteration: 250700 loss: 0.0022 lr: 0.02\n",
      "iteration: 250710 loss: 0.0024 lr: 0.02\n",
      "iteration: 250720 loss: 0.0017 lr: 0.02\n",
      "iteration: 250730 loss: 0.0020 lr: 0.02\n",
      "iteration: 250740 loss: 0.0018 lr: 0.02\n",
      "iteration: 250750 loss: 0.0016 lr: 0.02\n",
      "iteration: 250760 loss: 0.0019 lr: 0.02\n",
      "iteration: 250770 loss: 0.0019 lr: 0.02\n",
      "iteration: 250780 loss: 0.0012 lr: 0.02\n",
      "iteration: 250790 loss: 0.0016 lr: 0.02\n",
      "iteration: 250800 loss: 0.0012 lr: 0.02\n",
      "iteration: 250810 loss: 0.0017 lr: 0.02\n",
      "iteration: 250820 loss: 0.0016 lr: 0.02\n",
      "iteration: 250830 loss: 0.0014 lr: 0.02\n",
      "iteration: 250840 loss: 0.0023 lr: 0.02\n",
      "iteration: 250850 loss: 0.0019 lr: 0.02\n",
      "iteration: 250860 loss: 0.0019 lr: 0.02\n",
      "iteration: 250870 loss: 0.0014 lr: 0.02\n",
      "iteration: 250880 loss: 0.0025 lr: 0.02\n",
      "iteration: 250890 loss: 0.0018 lr: 0.02\n",
      "iteration: 250900 loss: 0.0021 lr: 0.02\n",
      "iteration: 250910 loss: 0.0015 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 250920 loss: 0.0014 lr: 0.02\n",
      "iteration: 250930 loss: 0.0020 lr: 0.02\n",
      "iteration: 250940 loss: 0.0018 lr: 0.02\n",
      "iteration: 250950 loss: 0.0026 lr: 0.02\n",
      "iteration: 250960 loss: 0.0017 lr: 0.02\n",
      "iteration: 250970 loss: 0.0033 lr: 0.02\n",
      "iteration: 250980 loss: 0.0044 lr: 0.02\n",
      "iteration: 250990 loss: 0.0028 lr: 0.02\n",
      "iteration: 251000 loss: 0.0032 lr: 0.02\n",
      "iteration: 251010 loss: 0.0022 lr: 0.02\n",
      "iteration: 251020 loss: 0.0022 lr: 0.02\n",
      "iteration: 251030 loss: 0.0021 lr: 0.02\n",
      "iteration: 251040 loss: 0.0021 lr: 0.02\n",
      "iteration: 251050 loss: 0.0016 lr: 0.02\n",
      "iteration: 251060 loss: 0.0017 lr: 0.02\n",
      "iteration: 251070 loss: 0.0022 lr: 0.02\n",
      "iteration: 251080 loss: 0.0020 lr: 0.02\n",
      "iteration: 251090 loss: 0.0032 lr: 0.02\n",
      "iteration: 251100 loss: 0.0028 lr: 0.02\n",
      "iteration: 251110 loss: 0.0026 lr: 0.02\n",
      "iteration: 251120 loss: 0.0022 lr: 0.02\n",
      "iteration: 251130 loss: 0.0014 lr: 0.02\n",
      "iteration: 251140 loss: 0.0012 lr: 0.02\n",
      "iteration: 251150 loss: 0.0024 lr: 0.02\n",
      "iteration: 251160 loss: 0.0012 lr: 0.02\n",
      "iteration: 251170 loss: 0.0017 lr: 0.02\n",
      "iteration: 251180 loss: 0.0014 lr: 0.02\n",
      "iteration: 251190 loss: 0.0027 lr: 0.02\n",
      "iteration: 251200 loss: 0.0020 lr: 0.02\n",
      "iteration: 251210 loss: 0.0019 lr: 0.02\n",
      "iteration: 251220 loss: 0.0020 lr: 0.02\n",
      "iteration: 251230 loss: 0.0019 lr: 0.02\n",
      "iteration: 251240 loss: 0.0016 lr: 0.02\n",
      "iteration: 251250 loss: 0.0038 lr: 0.02\n",
      "iteration: 251260 loss: 0.0018 lr: 0.02\n",
      "iteration: 251270 loss: 0.0017 lr: 0.02\n",
      "iteration: 251280 loss: 0.0016 lr: 0.02\n",
      "iteration: 251290 loss: 0.0018 lr: 0.02\n",
      "iteration: 251300 loss: 0.0021 lr: 0.02\n",
      "iteration: 251310 loss: 0.0017 lr: 0.02\n",
      "iteration: 251320 loss: 0.0017 lr: 0.02\n",
      "iteration: 251330 loss: 0.0023 lr: 0.02\n",
      "iteration: 251340 loss: 0.0020 lr: 0.02\n",
      "iteration: 251350 loss: 0.0016 lr: 0.02\n",
      "iteration: 251360 loss: 0.0020 lr: 0.02\n",
      "iteration: 251370 loss: 0.0022 lr: 0.02\n",
      "iteration: 251380 loss: 0.0022 lr: 0.02\n",
      "iteration: 251390 loss: 0.0019 lr: 0.02\n",
      "iteration: 251400 loss: 0.0025 lr: 0.02\n",
      "iteration: 251410 loss: 0.0022 lr: 0.02\n",
      "iteration: 251420 loss: 0.0018 lr: 0.02\n",
      "iteration: 251430 loss: 0.0023 lr: 0.02\n",
      "iteration: 251440 loss: 0.0019 lr: 0.02\n",
      "iteration: 251450 loss: 0.0011 lr: 0.02\n",
      "iteration: 251460 loss: 0.0017 lr: 0.02\n",
      "iteration: 251470 loss: 0.0017 lr: 0.02\n",
      "iteration: 251480 loss: 0.0024 lr: 0.02\n",
      "iteration: 251490 loss: 0.0022 lr: 0.02\n",
      "iteration: 251500 loss: 0.0027 lr: 0.02\n",
      "iteration: 251510 loss: 0.0020 lr: 0.02\n",
      "iteration: 251520 loss: 0.0017 lr: 0.02\n",
      "iteration: 251530 loss: 0.0018 lr: 0.02\n",
      "iteration: 251540 loss: 0.0025 lr: 0.02\n",
      "iteration: 251550 loss: 0.0026 lr: 0.02\n",
      "iteration: 251560 loss: 0.0015 lr: 0.02\n",
      "iteration: 251570 loss: 0.0021 lr: 0.02\n",
      "iteration: 251580 loss: 0.0018 lr: 0.02\n",
      "iteration: 251590 loss: 0.0018 lr: 0.02\n",
      "iteration: 251600 loss: 0.0013 lr: 0.02\n",
      "iteration: 251610 loss: 0.0024 lr: 0.02\n",
      "iteration: 251620 loss: 0.0015 lr: 0.02\n",
      "iteration: 251630 loss: 0.0016 lr: 0.02\n",
      "iteration: 251640 loss: 0.0016 lr: 0.02\n",
      "iteration: 251650 loss: 0.0019 lr: 0.02\n",
      "iteration: 251660 loss: 0.0017 lr: 0.02\n",
      "iteration: 251670 loss: 0.0017 lr: 0.02\n",
      "iteration: 251680 loss: 0.0018 lr: 0.02\n",
      "iteration: 251690 loss: 0.0026 lr: 0.02\n",
      "iteration: 251700 loss: 0.0016 lr: 0.02\n",
      "iteration: 251710 loss: 0.0023 lr: 0.02\n",
      "iteration: 251720 loss: 0.0020 lr: 0.02\n",
      "iteration: 251730 loss: 0.0022 lr: 0.02\n",
      "iteration: 251740 loss: 0.0018 lr: 0.02\n",
      "iteration: 251750 loss: 0.0014 lr: 0.02\n",
      "iteration: 251760 loss: 0.0017 lr: 0.02\n",
      "iteration: 251770 loss: 0.0020 lr: 0.02\n",
      "iteration: 251780 loss: 0.0020 lr: 0.02\n",
      "iteration: 251790 loss: 0.0027 lr: 0.02\n",
      "iteration: 251800 loss: 0.0016 lr: 0.02\n",
      "iteration: 251810 loss: 0.0018 lr: 0.02\n",
      "iteration: 251820 loss: 0.0020 lr: 0.02\n",
      "iteration: 251830 loss: 0.0016 lr: 0.02\n",
      "iteration: 251840 loss: 0.0014 lr: 0.02\n",
      "iteration: 251850 loss: 0.0012 lr: 0.02\n",
      "iteration: 251860 loss: 0.0017 lr: 0.02\n",
      "iteration: 251870 loss: 0.0027 lr: 0.02\n",
      "iteration: 251880 loss: 0.0020 lr: 0.02\n",
      "iteration: 251890 loss: 0.0016 lr: 0.02\n",
      "iteration: 251900 loss: 0.0019 lr: 0.02\n",
      "iteration: 251910 loss: 0.0014 lr: 0.02\n",
      "iteration: 251920 loss: 0.0025 lr: 0.02\n",
      "iteration: 251930 loss: 0.0019 lr: 0.02\n",
      "iteration: 251940 loss: 0.0018 lr: 0.02\n",
      "iteration: 251950 loss: 0.0019 lr: 0.02\n",
      "iteration: 251960 loss: 0.0017 lr: 0.02\n",
      "iteration: 251970 loss: 0.0017 lr: 0.02\n",
      "iteration: 251980 loss: 0.0021 lr: 0.02\n",
      "iteration: 251990 loss: 0.0023 lr: 0.02\n",
      "iteration: 252000 loss: 0.0019 lr: 0.02\n",
      "iteration: 252010 loss: 0.0018 lr: 0.02\n",
      "iteration: 252020 loss: 0.0024 lr: 0.02\n",
      "iteration: 252030 loss: 0.0024 lr: 0.02\n",
      "iteration: 252040 loss: 0.0023 lr: 0.02\n",
      "iteration: 252050 loss: 0.0015 lr: 0.02\n",
      "iteration: 252060 loss: 0.0020 lr: 0.02\n",
      "iteration: 252070 loss: 0.0016 lr: 0.02\n",
      "iteration: 252080 loss: 0.0018 lr: 0.02\n",
      "iteration: 252090 loss: 0.0024 lr: 0.02\n",
      "iteration: 252100 loss: 0.0028 lr: 0.02\n",
      "iteration: 252110 loss: 0.0020 lr: 0.02\n",
      "iteration: 252120 loss: 0.0014 lr: 0.02\n",
      "iteration: 252130 loss: 0.0020 lr: 0.02\n",
      "iteration: 252140 loss: 0.0015 lr: 0.02\n",
      "iteration: 252150 loss: 0.0019 lr: 0.02\n",
      "iteration: 252160 loss: 0.0019 lr: 0.02\n",
      "iteration: 252170 loss: 0.0026 lr: 0.02\n",
      "iteration: 252180 loss: 0.0027 lr: 0.02\n",
      "iteration: 252190 loss: 0.0017 lr: 0.02\n",
      "iteration: 252200 loss: 0.0016 lr: 0.02\n",
      "iteration: 252210 loss: 0.0018 lr: 0.02\n",
      "iteration: 252220 loss: 0.0018 lr: 0.02\n",
      "iteration: 252230 loss: 0.0021 lr: 0.02\n",
      "iteration: 252240 loss: 0.0026 lr: 0.02\n",
      "iteration: 252250 loss: 0.0020 lr: 0.02\n",
      "iteration: 252260 loss: 0.0017 lr: 0.02\n",
      "iteration: 252270 loss: 0.0029 lr: 0.02\n",
      "iteration: 252280 loss: 0.0018 lr: 0.02\n",
      "iteration: 252290 loss: 0.0020 lr: 0.02\n",
      "iteration: 252300 loss: 0.0025 lr: 0.02\n",
      "iteration: 252310 loss: 0.0020 lr: 0.02\n",
      "iteration: 252320 loss: 0.0022 lr: 0.02\n",
      "iteration: 252330 loss: 0.0018 lr: 0.02\n",
      "iteration: 252340 loss: 0.0014 lr: 0.02\n",
      "iteration: 252350 loss: 0.0018 lr: 0.02\n",
      "iteration: 252360 loss: 0.0015 lr: 0.02\n",
      "iteration: 252370 loss: 0.0015 lr: 0.02\n",
      "iteration: 252380 loss: 0.0021 lr: 0.02\n",
      "iteration: 252390 loss: 0.0023 lr: 0.02\n",
      "iteration: 252400 loss: 0.0015 lr: 0.02\n",
      "iteration: 252410 loss: 0.0011 lr: 0.02\n",
      "iteration: 252420 loss: 0.0016 lr: 0.02\n",
      "iteration: 252430 loss: 0.0015 lr: 0.02\n",
      "iteration: 252440 loss: 0.0016 lr: 0.02\n",
      "iteration: 252450 loss: 0.0018 lr: 0.02\n",
      "iteration: 252460 loss: 0.0016 lr: 0.02\n",
      "iteration: 252470 loss: 0.0018 lr: 0.02\n",
      "iteration: 252480 loss: 0.0017 lr: 0.02\n",
      "iteration: 252490 loss: 0.0020 lr: 0.02\n",
      "iteration: 252500 loss: 0.0018 lr: 0.02\n",
      "iteration: 252510 loss: 0.0030 lr: 0.02\n",
      "iteration: 252520 loss: 0.0017 lr: 0.02\n",
      "iteration: 252530 loss: 0.0026 lr: 0.02\n",
      "iteration: 252540 loss: 0.0018 lr: 0.02\n",
      "iteration: 252550 loss: 0.0020 lr: 0.02\n",
      "iteration: 252560 loss: 0.0016 lr: 0.02\n",
      "iteration: 252570 loss: 0.0015 lr: 0.02\n",
      "iteration: 252580 loss: 0.0019 lr: 0.02\n",
      "iteration: 252590 loss: 0.0019 lr: 0.02\n",
      "iteration: 252600 loss: 0.0021 lr: 0.02\n",
      "iteration: 252610 loss: 0.0014 lr: 0.02\n",
      "iteration: 252620 loss: 0.0022 lr: 0.02\n",
      "iteration: 252630 loss: 0.0016 lr: 0.02\n",
      "iteration: 252640 loss: 0.0020 lr: 0.02\n",
      "iteration: 252650 loss: 0.0022 lr: 0.02\n",
      "iteration: 252660 loss: 0.0024 lr: 0.02\n",
      "iteration: 252670 loss: 0.0024 lr: 0.02\n",
      "iteration: 252680 loss: 0.0018 lr: 0.02\n",
      "iteration: 252690 loss: 0.0020 lr: 0.02\n",
      "iteration: 252700 loss: 0.0017 lr: 0.02\n",
      "iteration: 252710 loss: 0.0015 lr: 0.02\n",
      "iteration: 252720 loss: 0.0014 lr: 0.02\n",
      "iteration: 252730 loss: 0.0035 lr: 0.02\n",
      "iteration: 252740 loss: 0.0014 lr: 0.02\n",
      "iteration: 252750 loss: 0.0019 lr: 0.02\n",
      "iteration: 252760 loss: 0.0015 lr: 0.02\n",
      "iteration: 252770 loss: 0.0013 lr: 0.02\n",
      "iteration: 252780 loss: 0.0034 lr: 0.02\n",
      "iteration: 252790 loss: 0.0019 lr: 0.02\n",
      "iteration: 252800 loss: 0.0020 lr: 0.02\n",
      "iteration: 252810 loss: 0.0019 lr: 0.02\n",
      "iteration: 252820 loss: 0.0022 lr: 0.02\n",
      "iteration: 252830 loss: 0.0019 lr: 0.02\n",
      "iteration: 252840 loss: 0.0015 lr: 0.02\n",
      "iteration: 252850 loss: 0.0017 lr: 0.02\n",
      "iteration: 252860 loss: 0.0017 lr: 0.02\n",
      "iteration: 252870 loss: 0.0020 lr: 0.02\n",
      "iteration: 252880 loss: 0.0021 lr: 0.02\n",
      "iteration: 252890 loss: 0.0019 lr: 0.02\n",
      "iteration: 252900 loss: 0.0019 lr: 0.02\n",
      "iteration: 252910 loss: 0.0018 lr: 0.02\n",
      "iteration: 252920 loss: 0.0021 lr: 0.02\n",
      "iteration: 252930 loss: 0.0017 lr: 0.02\n",
      "iteration: 252940 loss: 0.0020 lr: 0.02\n",
      "iteration: 252950 loss: 0.0025 lr: 0.02\n",
      "iteration: 252960 loss: 0.0020 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 252970 loss: 0.0018 lr: 0.02\n",
      "iteration: 252980 loss: 0.0022 lr: 0.02\n",
      "iteration: 252990 loss: 0.0019 lr: 0.02\n",
      "iteration: 253000 loss: 0.0015 lr: 0.02\n",
      "iteration: 253010 loss: 0.0017 lr: 0.02\n",
      "iteration: 253020 loss: 0.0013 lr: 0.02\n",
      "iteration: 253030 loss: 0.0016 lr: 0.02\n",
      "iteration: 253040 loss: 0.0016 lr: 0.02\n",
      "iteration: 253050 loss: 0.0018 lr: 0.02\n",
      "iteration: 253060 loss: 0.0019 lr: 0.02\n",
      "iteration: 253070 loss: 0.0021 lr: 0.02\n",
      "iteration: 253080 loss: 0.0013 lr: 0.02\n",
      "iteration: 253090 loss: 0.0020 lr: 0.02\n",
      "iteration: 253100 loss: 0.0019 lr: 0.02\n",
      "iteration: 253110 loss: 0.0015 lr: 0.02\n",
      "iteration: 253120 loss: 0.0026 lr: 0.02\n",
      "iteration: 253130 loss: 0.0015 lr: 0.02\n",
      "iteration: 253140 loss: 0.0012 lr: 0.02\n",
      "iteration: 253150 loss: 0.0012 lr: 0.02\n",
      "iteration: 253160 loss: 0.0016 lr: 0.02\n",
      "iteration: 253170 loss: 0.0015 lr: 0.02\n",
      "iteration: 253180 loss: 0.0019 lr: 0.02\n",
      "iteration: 253190 loss: 0.0032 lr: 0.02\n",
      "iteration: 253200 loss: 0.0018 lr: 0.02\n",
      "iteration: 253210 loss: 0.0017 lr: 0.02\n",
      "iteration: 253220 loss: 0.0016 lr: 0.02\n",
      "iteration: 253230 loss: 0.0016 lr: 0.02\n",
      "iteration: 253240 loss: 0.0019 lr: 0.02\n",
      "iteration: 253250 loss: 0.0029 lr: 0.02\n",
      "iteration: 253260 loss: 0.0020 lr: 0.02\n",
      "iteration: 253270 loss: 0.0016 lr: 0.02\n",
      "iteration: 253280 loss: 0.0029 lr: 0.02\n",
      "iteration: 253290 loss: 0.0013 lr: 0.02\n",
      "iteration: 253300 loss: 0.0016 lr: 0.02\n",
      "iteration: 253310 loss: 0.0017 lr: 0.02\n",
      "iteration: 253320 loss: 0.0022 lr: 0.02\n",
      "iteration: 253330 loss: 0.0019 lr: 0.02\n",
      "iteration: 253340 loss: 0.0014 lr: 0.02\n",
      "iteration: 253350 loss: 0.0020 lr: 0.02\n",
      "iteration: 253360 loss: 0.0021 lr: 0.02\n",
      "iteration: 253370 loss: 0.0017 lr: 0.02\n",
      "iteration: 253380 loss: 0.0016 lr: 0.02\n",
      "iteration: 253390 loss: 0.0025 lr: 0.02\n",
      "iteration: 253400 loss: 0.0024 lr: 0.02\n",
      "iteration: 253410 loss: 0.0016 lr: 0.02\n",
      "iteration: 253420 loss: 0.0017 lr: 0.02\n",
      "iteration: 253430 loss: 0.0023 lr: 0.02\n",
      "iteration: 253440 loss: 0.0016 lr: 0.02\n",
      "iteration: 253450 loss: 0.0019 lr: 0.02\n",
      "iteration: 253460 loss: 0.0020 lr: 0.02\n",
      "iteration: 253470 loss: 0.0025 lr: 0.02\n",
      "iteration: 253480 loss: 0.0016 lr: 0.02\n",
      "iteration: 253490 loss: 0.0020 lr: 0.02\n",
      "iteration: 253500 loss: 0.0025 lr: 0.02\n",
      "iteration: 253510 loss: 0.0018 lr: 0.02\n",
      "iteration: 253520 loss: 0.0023 lr: 0.02\n",
      "iteration: 253530 loss: 0.0022 lr: 0.02\n",
      "iteration: 253540 loss: 0.0019 lr: 0.02\n",
      "iteration: 253550 loss: 0.0021 lr: 0.02\n",
      "iteration: 253560 loss: 0.0017 lr: 0.02\n",
      "iteration: 253570 loss: 0.0026 lr: 0.02\n",
      "iteration: 253580 loss: 0.0018 lr: 0.02\n",
      "iteration: 253590 loss: 0.0021 lr: 0.02\n",
      "iteration: 253600 loss: 0.0019 lr: 0.02\n",
      "iteration: 253610 loss: 0.0019 lr: 0.02\n",
      "iteration: 253620 loss: 0.0025 lr: 0.02\n",
      "iteration: 253630 loss: 0.0014 lr: 0.02\n",
      "iteration: 253640 loss: 0.0024 lr: 0.02\n",
      "iteration: 253650 loss: 0.0019 lr: 0.02\n",
      "iteration: 253660 loss: 0.0034 lr: 0.02\n",
      "iteration: 253670 loss: 0.0013 lr: 0.02\n",
      "iteration: 253680 loss: 0.0018 lr: 0.02\n",
      "iteration: 253690 loss: 0.0020 lr: 0.02\n",
      "iteration: 253700 loss: 0.0016 lr: 0.02\n",
      "iteration: 253710 loss: 0.0022 lr: 0.02\n",
      "iteration: 253720 loss: 0.0022 lr: 0.02\n",
      "iteration: 253730 loss: 0.0031 lr: 0.02\n",
      "iteration: 253740 loss: 0.0014 lr: 0.02\n",
      "iteration: 253750 loss: 0.0027 lr: 0.02\n",
      "iteration: 253760 loss: 0.0020 lr: 0.02\n",
      "iteration: 253770 loss: 0.0015 lr: 0.02\n",
      "iteration: 253780 loss: 0.0022 lr: 0.02\n",
      "iteration: 253790 loss: 0.0024 lr: 0.02\n",
      "iteration: 253800 loss: 0.0023 lr: 0.02\n",
      "iteration: 253810 loss: 0.0019 lr: 0.02\n",
      "iteration: 253820 loss: 0.0017 lr: 0.02\n",
      "iteration: 253830 loss: 0.0025 lr: 0.02\n",
      "iteration: 253840 loss: 0.0015 lr: 0.02\n",
      "iteration: 253850 loss: 0.0027 lr: 0.02\n",
      "iteration: 253860 loss: 0.0019 lr: 0.02\n",
      "iteration: 253870 loss: 0.0028 lr: 0.02\n",
      "iteration: 253880 loss: 0.0034 lr: 0.02\n",
      "iteration: 253890 loss: 0.0019 lr: 0.02\n",
      "iteration: 253900 loss: 0.0020 lr: 0.02\n",
      "iteration: 253910 loss: 0.0019 lr: 0.02\n",
      "iteration: 253920 loss: 0.0020 lr: 0.02\n",
      "iteration: 253930 loss: 0.0014 lr: 0.02\n",
      "iteration: 253940 loss: 0.0019 lr: 0.02\n",
      "iteration: 253950 loss: 0.0023 lr: 0.02\n",
      "iteration: 253960 loss: 0.0019 lr: 0.02\n",
      "iteration: 253970 loss: 0.0020 lr: 0.02\n",
      "iteration: 253980 loss: 0.0015 lr: 0.02\n",
      "iteration: 253990 loss: 0.0017 lr: 0.02\n",
      "iteration: 254000 loss: 0.0026 lr: 0.02\n",
      "iteration: 254010 loss: 0.0023 lr: 0.02\n",
      "iteration: 254020 loss: 0.0013 lr: 0.02\n",
      "iteration: 254030 loss: 0.0025 lr: 0.02\n",
      "iteration: 254040 loss: 0.0019 lr: 0.02\n",
      "iteration: 254050 loss: 0.0027 lr: 0.02\n",
      "iteration: 254060 loss: 0.0018 lr: 0.02\n",
      "iteration: 254070 loss: 0.0021 lr: 0.02\n",
      "iteration: 254080 loss: 0.0023 lr: 0.02\n",
      "iteration: 254090 loss: 0.0047 lr: 0.02\n",
      "iteration: 254100 loss: 0.0026 lr: 0.02\n",
      "iteration: 254110 loss: 0.0029 lr: 0.02\n",
      "iteration: 254120 loss: 0.0015 lr: 0.02\n",
      "iteration: 254130 loss: 0.0027 lr: 0.02\n",
      "iteration: 254140 loss: 0.0027 lr: 0.02\n",
      "iteration: 254150 loss: 0.0014 lr: 0.02\n",
      "iteration: 254160 loss: 0.0018 lr: 0.02\n",
      "iteration: 254170 loss: 0.0016 lr: 0.02\n",
      "iteration: 254180 loss: 0.0023 lr: 0.02\n",
      "iteration: 254190 loss: 0.0018 lr: 0.02\n",
      "iteration: 254200 loss: 0.0026 lr: 0.02\n",
      "iteration: 254210 loss: 0.0013 lr: 0.02\n",
      "iteration: 254220 loss: 0.0020 lr: 0.02\n",
      "iteration: 254230 loss: 0.0038 lr: 0.02\n",
      "iteration: 254240 loss: 0.0026 lr: 0.02\n",
      "iteration: 254250 loss: 0.0021 lr: 0.02\n",
      "iteration: 254260 loss: 0.0018 lr: 0.02\n",
      "iteration: 254270 loss: 0.0019 lr: 0.02\n",
      "iteration: 254280 loss: 0.0015 lr: 0.02\n",
      "iteration: 254290 loss: 0.0016 lr: 0.02\n",
      "iteration: 254300 loss: 0.0022 lr: 0.02\n",
      "iteration: 254310 loss: 0.0015 lr: 0.02\n",
      "iteration: 254320 loss: 0.0017 lr: 0.02\n",
      "iteration: 254330 loss: 0.0023 lr: 0.02\n",
      "iteration: 254340 loss: 0.0014 lr: 0.02\n",
      "iteration: 254350 loss: 0.0021 lr: 0.02\n",
      "iteration: 254360 loss: 0.0015 lr: 0.02\n",
      "iteration: 254370 loss: 0.0018 lr: 0.02\n",
      "iteration: 254380 loss: 0.0019 lr: 0.02\n",
      "iteration: 254390 loss: 0.0020 lr: 0.02\n",
      "iteration: 254400 loss: 0.0013 lr: 0.02\n",
      "iteration: 254410 loss: 0.0017 lr: 0.02\n",
      "iteration: 254420 loss: 0.0019 lr: 0.02\n",
      "iteration: 254430 loss: 0.0025 lr: 0.02\n",
      "iteration: 254440 loss: 0.0018 lr: 0.02\n",
      "iteration: 254450 loss: 0.0019 lr: 0.02\n",
      "iteration: 254460 loss: 0.0019 lr: 0.02\n",
      "iteration: 254470 loss: 0.0020 lr: 0.02\n",
      "iteration: 254480 loss: 0.0020 lr: 0.02\n",
      "iteration: 254490 loss: 0.0024 lr: 0.02\n",
      "iteration: 254500 loss: 0.0016 lr: 0.02\n",
      "iteration: 254510 loss: 0.0016 lr: 0.02\n",
      "iteration: 254520 loss: 0.0014 lr: 0.02\n",
      "iteration: 254530 loss: 0.0014 lr: 0.02\n",
      "iteration: 254540 loss: 0.0021 lr: 0.02\n",
      "iteration: 254550 loss: 0.0027 lr: 0.02\n",
      "iteration: 254560 loss: 0.0027 lr: 0.02\n",
      "iteration: 254570 loss: 0.0016 lr: 0.02\n",
      "iteration: 254580 loss: 0.0028 lr: 0.02\n",
      "iteration: 254590 loss: 0.0024 lr: 0.02\n",
      "iteration: 254600 loss: 0.0019 lr: 0.02\n",
      "iteration: 254610 loss: 0.0018 lr: 0.02\n",
      "iteration: 254620 loss: 0.0021 lr: 0.02\n",
      "iteration: 254630 loss: 0.0023 lr: 0.02\n",
      "iteration: 254640 loss: 0.0023 lr: 0.02\n",
      "iteration: 254650 loss: 0.0016 lr: 0.02\n",
      "iteration: 254660 loss: 0.0020 lr: 0.02\n",
      "iteration: 254670 loss: 0.0020 lr: 0.02\n",
      "iteration: 254680 loss: 0.0026 lr: 0.02\n",
      "iteration: 254690 loss: 0.0017 lr: 0.02\n",
      "iteration: 254700 loss: 0.0015 lr: 0.02\n",
      "iteration: 254710 loss: 0.0016 lr: 0.02\n",
      "iteration: 254720 loss: 0.0024 lr: 0.02\n",
      "iteration: 254730 loss: 0.0029 lr: 0.02\n",
      "iteration: 254740 loss: 0.0023 lr: 0.02\n",
      "iteration: 254750 loss: 0.0021 lr: 0.02\n",
      "iteration: 254760 loss: 0.0016 lr: 0.02\n",
      "iteration: 254770 loss: 0.0016 lr: 0.02\n",
      "iteration: 254780 loss: 0.0016 lr: 0.02\n",
      "iteration: 254790 loss: 0.0017 lr: 0.02\n",
      "iteration: 254800 loss: 0.0017 lr: 0.02\n",
      "iteration: 254810 loss: 0.0022 lr: 0.02\n",
      "iteration: 254820 loss: 0.0017 lr: 0.02\n",
      "iteration: 254830 loss: 0.0027 lr: 0.02\n",
      "iteration: 254840 loss: 0.0022 lr: 0.02\n",
      "iteration: 254850 loss: 0.0014 lr: 0.02\n",
      "iteration: 254860 loss: 0.0020 lr: 0.02\n",
      "iteration: 254870 loss: 0.0011 lr: 0.02\n",
      "iteration: 254880 loss: 0.0019 lr: 0.02\n",
      "iteration: 254890 loss: 0.0022 lr: 0.02\n",
      "iteration: 254900 loss: 0.0020 lr: 0.02\n",
      "iteration: 254910 loss: 0.0018 lr: 0.02\n",
      "iteration: 254920 loss: 0.0019 lr: 0.02\n",
      "iteration: 254930 loss: 0.0014 lr: 0.02\n",
      "iteration: 254940 loss: 0.0012 lr: 0.02\n",
      "iteration: 254950 loss: 0.0013 lr: 0.02\n",
      "iteration: 254960 loss: 0.0019 lr: 0.02\n",
      "iteration: 254970 loss: 0.0018 lr: 0.02\n",
      "iteration: 254980 loss: 0.0023 lr: 0.02\n",
      "iteration: 254990 loss: 0.0021 lr: 0.02\n",
      "iteration: 255000 loss: 0.0020 lr: 0.02\n",
      "iteration: 255010 loss: 0.0014 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 255020 loss: 0.0023 lr: 0.02\n",
      "iteration: 255030 loss: 0.0014 lr: 0.02\n",
      "iteration: 255040 loss: 0.0014 lr: 0.02\n",
      "iteration: 255050 loss: 0.0020 lr: 0.02\n",
      "iteration: 255060 loss: 0.0014 lr: 0.02\n",
      "iteration: 255070 loss: 0.0013 lr: 0.02\n",
      "iteration: 255080 loss: 0.0017 lr: 0.02\n",
      "iteration: 255090 loss: 0.0019 lr: 0.02\n",
      "iteration: 255100 loss: 0.0017 lr: 0.02\n",
      "iteration: 255110 loss: 0.0021 lr: 0.02\n",
      "iteration: 255120 loss: 0.0020 lr: 0.02\n",
      "iteration: 255130 loss: 0.0017 lr: 0.02\n",
      "iteration: 255140 loss: 0.0015 lr: 0.02\n",
      "iteration: 255150 loss: 0.0020 lr: 0.02\n",
      "iteration: 255160 loss: 0.0015 lr: 0.02\n",
      "iteration: 255170 loss: 0.0016 lr: 0.02\n",
      "iteration: 255180 loss: 0.0013 lr: 0.02\n",
      "iteration: 255190 loss: 0.0021 lr: 0.02\n",
      "iteration: 255200 loss: 0.0019 lr: 0.02\n",
      "iteration: 255210 loss: 0.0023 lr: 0.02\n",
      "iteration: 255220 loss: 0.0023 lr: 0.02\n",
      "iteration: 255230 loss: 0.0020 lr: 0.02\n",
      "iteration: 255240 loss: 0.0019 lr: 0.02\n",
      "iteration: 255250 loss: 0.0017 lr: 0.02\n",
      "iteration: 255260 loss: 0.0020 lr: 0.02\n",
      "iteration: 255270 loss: 0.0011 lr: 0.02\n",
      "iteration: 255280 loss: 0.0021 lr: 0.02\n",
      "iteration: 255290 loss: 0.0017 lr: 0.02\n",
      "iteration: 255300 loss: 0.0019 lr: 0.02\n",
      "iteration: 255310 loss: 0.0018 lr: 0.02\n",
      "iteration: 255320 loss: 0.0015 lr: 0.02\n",
      "iteration: 255330 loss: 0.0019 lr: 0.02\n",
      "iteration: 255340 loss: 0.0019 lr: 0.02\n",
      "iteration: 255350 loss: 0.0025 lr: 0.02\n",
      "iteration: 255360 loss: 0.0015 lr: 0.02\n",
      "iteration: 255370 loss: 0.0020 lr: 0.02\n",
      "iteration: 255380 loss: 0.0031 lr: 0.02\n",
      "iteration: 255390 loss: 0.0016 lr: 0.02\n",
      "iteration: 255400 loss: 0.0017 lr: 0.02\n",
      "iteration: 255410 loss: 0.0021 lr: 0.02\n",
      "iteration: 255420 loss: 0.0026 lr: 0.02\n",
      "iteration: 255430 loss: 0.0019 lr: 0.02\n",
      "iteration: 255440 loss: 0.0028 lr: 0.02\n",
      "iteration: 255450 loss: 0.0013 lr: 0.02\n",
      "iteration: 255460 loss: 0.0018 lr: 0.02\n",
      "iteration: 255470 loss: 0.0019 lr: 0.02\n",
      "iteration: 255480 loss: 0.0015 lr: 0.02\n",
      "iteration: 255490 loss: 0.0016 lr: 0.02\n",
      "iteration: 255500 loss: 0.0016 lr: 0.02\n",
      "iteration: 255510 loss: 0.0023 lr: 0.02\n",
      "iteration: 255520 loss: 0.0017 lr: 0.02\n",
      "iteration: 255530 loss: 0.0015 lr: 0.02\n",
      "iteration: 255540 loss: 0.0023 lr: 0.02\n",
      "iteration: 255550 loss: 0.0018 lr: 0.02\n",
      "iteration: 255560 loss: 0.0020 lr: 0.02\n",
      "iteration: 255570 loss: 0.0019 lr: 0.02\n",
      "iteration: 255580 loss: 0.0015 lr: 0.02\n",
      "iteration: 255590 loss: 0.0020 lr: 0.02\n",
      "iteration: 255600 loss: 0.0026 lr: 0.02\n",
      "iteration: 255610 loss: 0.0016 lr: 0.02\n",
      "iteration: 255620 loss: 0.0025 lr: 0.02\n",
      "iteration: 255630 loss: 0.0014 lr: 0.02\n",
      "iteration: 255640 loss: 0.0021 lr: 0.02\n",
      "iteration: 255650 loss: 0.0022 lr: 0.02\n",
      "iteration: 255660 loss: 0.0018 lr: 0.02\n",
      "iteration: 255670 loss: 0.0015 lr: 0.02\n",
      "iteration: 255680 loss: 0.0019 lr: 0.02\n",
      "iteration: 255690 loss: 0.0016 lr: 0.02\n",
      "iteration: 255700 loss: 0.0016 lr: 0.02\n",
      "iteration: 255710 loss: 0.0016 lr: 0.02\n",
      "iteration: 255720 loss: 0.0013 lr: 0.02\n",
      "iteration: 255730 loss: 0.0020 lr: 0.02\n",
      "iteration: 255740 loss: 0.0020 lr: 0.02\n",
      "iteration: 255750 loss: 0.0019 lr: 0.02\n",
      "iteration: 255760 loss: 0.0014 lr: 0.02\n",
      "iteration: 255770 loss: 0.0017 lr: 0.02\n",
      "iteration: 255780 loss: 0.0018 lr: 0.02\n",
      "iteration: 255790 loss: 0.0016 lr: 0.02\n",
      "iteration: 255800 loss: 0.0015 lr: 0.02\n",
      "iteration: 255810 loss: 0.0017 lr: 0.02\n",
      "iteration: 255820 loss: 0.0023 lr: 0.02\n",
      "iteration: 255830 loss: 0.0021 lr: 0.02\n",
      "iteration: 255840 loss: 0.0026 lr: 0.02\n",
      "iteration: 255850 loss: 0.0022 lr: 0.02\n",
      "iteration: 255860 loss: 0.0026 lr: 0.02\n",
      "iteration: 255870 loss: 0.0020 lr: 0.02\n",
      "iteration: 255880 loss: 0.0021 lr: 0.02\n",
      "iteration: 255890 loss: 0.0021 lr: 0.02\n",
      "iteration: 255900 loss: 0.0019 lr: 0.02\n",
      "iteration: 255910 loss: 0.0017 lr: 0.02\n",
      "iteration: 255920 loss: 0.0021 lr: 0.02\n",
      "iteration: 255930 loss: 0.0018 lr: 0.02\n",
      "iteration: 255940 loss: 0.0019 lr: 0.02\n",
      "iteration: 255950 loss: 0.0017 lr: 0.02\n",
      "iteration: 255960 loss: 0.0023 lr: 0.02\n",
      "iteration: 255970 loss: 0.0019 lr: 0.02\n",
      "iteration: 255980 loss: 0.0019 lr: 0.02\n",
      "iteration: 255990 loss: 0.0020 lr: 0.02\n",
      "iteration: 256000 loss: 0.0020 lr: 0.02\n",
      "iteration: 256010 loss: 0.0013 lr: 0.02\n",
      "iteration: 256020 loss: 0.0027 lr: 0.02\n",
      "iteration: 256030 loss: 0.0021 lr: 0.02\n",
      "iteration: 256040 loss: 0.0017 lr: 0.02\n",
      "iteration: 256050 loss: 0.0016 lr: 0.02\n",
      "iteration: 256060 loss: 0.0017 lr: 0.02\n",
      "iteration: 256070 loss: 0.0017 lr: 0.02\n",
      "iteration: 256080 loss: 0.0019 lr: 0.02\n",
      "iteration: 256090 loss: 0.0016 lr: 0.02\n",
      "iteration: 256100 loss: 0.0011 lr: 0.02\n",
      "iteration: 256110 loss: 0.0014 lr: 0.02\n",
      "iteration: 256120 loss: 0.0017 lr: 0.02\n",
      "iteration: 256130 loss: 0.0022 lr: 0.02\n",
      "iteration: 256140 loss: 0.0015 lr: 0.02\n",
      "iteration: 256150 loss: 0.0024 lr: 0.02\n",
      "iteration: 256160 loss: 0.0020 lr: 0.02\n",
      "iteration: 256170 loss: 0.0018 lr: 0.02\n",
      "iteration: 256180 loss: 0.0027 lr: 0.02\n",
      "iteration: 256190 loss: 0.0027 lr: 0.02\n",
      "iteration: 256200 loss: 0.0024 lr: 0.02\n",
      "iteration: 256210 loss: 0.0015 lr: 0.02\n",
      "iteration: 256220 loss: 0.0017 lr: 0.02\n",
      "iteration: 256230 loss: 0.0025 lr: 0.02\n",
      "iteration: 256240 loss: 0.0018 lr: 0.02\n",
      "iteration: 256250 loss: 0.0022 lr: 0.02\n",
      "iteration: 256260 loss: 0.0023 lr: 0.02\n",
      "iteration: 256270 loss: 0.0018 lr: 0.02\n",
      "iteration: 256280 loss: 0.0018 lr: 0.02\n",
      "iteration: 256290 loss: 0.0011 lr: 0.02\n",
      "iteration: 256300 loss: 0.0016 lr: 0.02\n",
      "iteration: 256310 loss: 0.0017 lr: 0.02\n",
      "iteration: 256320 loss: 0.0017 lr: 0.02\n",
      "iteration: 256330 loss: 0.0020 lr: 0.02\n",
      "iteration: 256340 loss: 0.0017 lr: 0.02\n",
      "iteration: 256350 loss: 0.0024 lr: 0.02\n",
      "iteration: 256360 loss: 0.0020 lr: 0.02\n",
      "iteration: 256370 loss: 0.0016 lr: 0.02\n",
      "iteration: 256380 loss: 0.0016 lr: 0.02\n",
      "iteration: 256390 loss: 0.0018 lr: 0.02\n",
      "iteration: 256400 loss: 0.0019 lr: 0.02\n",
      "iteration: 256410 loss: 0.0027 lr: 0.02\n",
      "iteration: 256420 loss: 0.0023 lr: 0.02\n",
      "iteration: 256430 loss: 0.0016 lr: 0.02\n",
      "iteration: 256440 loss: 0.0014 lr: 0.02\n",
      "iteration: 256450 loss: 0.0021 lr: 0.02\n",
      "iteration: 256460 loss: 0.0014 lr: 0.02\n",
      "iteration: 256470 loss: 0.0019 lr: 0.02\n",
      "iteration: 256480 loss: 0.0015 lr: 0.02\n",
      "iteration: 256490 loss: 0.0015 lr: 0.02\n",
      "iteration: 256500 loss: 0.0023 lr: 0.02\n",
      "iteration: 256510 loss: 0.0015 lr: 0.02\n",
      "iteration: 256520 loss: 0.0019 lr: 0.02\n",
      "iteration: 256530 loss: 0.0018 lr: 0.02\n",
      "iteration: 256540 loss: 0.0019 lr: 0.02\n",
      "iteration: 256550 loss: 0.0018 lr: 0.02\n",
      "iteration: 256560 loss: 0.0017 lr: 0.02\n",
      "iteration: 256570 loss: 0.0015 lr: 0.02\n",
      "iteration: 256580 loss: 0.0017 lr: 0.02\n",
      "iteration: 256590 loss: 0.0015 lr: 0.02\n",
      "iteration: 256600 loss: 0.0024 lr: 0.02\n",
      "iteration: 256610 loss: 0.0023 lr: 0.02\n",
      "iteration: 256620 loss: 0.0019 lr: 0.02\n",
      "iteration: 256630 loss: 0.0030 lr: 0.02\n",
      "iteration: 256640 loss: 0.0026 lr: 0.02\n",
      "iteration: 256650 loss: 0.0023 lr: 0.02\n",
      "iteration: 256660 loss: 0.0016 lr: 0.02\n",
      "iteration: 256670 loss: 0.0019 lr: 0.02\n",
      "iteration: 256680 loss: 0.0027 lr: 0.02\n",
      "iteration: 256690 loss: 0.0021 lr: 0.02\n",
      "iteration: 256700 loss: 0.0013 lr: 0.02\n",
      "iteration: 256710 loss: 0.0013 lr: 0.02\n",
      "iteration: 256720 loss: 0.0015 lr: 0.02\n",
      "iteration: 256730 loss: 0.0016 lr: 0.02\n",
      "iteration: 256740 loss: 0.0021 lr: 0.02\n",
      "iteration: 256750 loss: 0.0014 lr: 0.02\n",
      "iteration: 256760 loss: 0.0020 lr: 0.02\n",
      "iteration: 256770 loss: 0.0017 lr: 0.02\n",
      "iteration: 256780 loss: 0.0028 lr: 0.02\n",
      "iteration: 256790 loss: 0.0017 lr: 0.02\n",
      "iteration: 256800 loss: 0.0019 lr: 0.02\n",
      "iteration: 256810 loss: 0.0027 lr: 0.02\n",
      "iteration: 256820 loss: 0.0017 lr: 0.02\n",
      "iteration: 256830 loss: 0.0018 lr: 0.02\n",
      "iteration: 256840 loss: 0.0015 lr: 0.02\n",
      "iteration: 256850 loss: 0.0012 lr: 0.02\n",
      "iteration: 256860 loss: 0.0013 lr: 0.02\n",
      "iteration: 256870 loss: 0.0023 lr: 0.02\n",
      "iteration: 256880 loss: 0.0016 lr: 0.02\n",
      "iteration: 256890 loss: 0.0019 lr: 0.02\n",
      "iteration: 256900 loss: 0.0016 lr: 0.02\n",
      "iteration: 256910 loss: 0.0017 lr: 0.02\n",
      "iteration: 256920 loss: 0.0022 lr: 0.02\n",
      "iteration: 256930 loss: 0.0015 lr: 0.02\n",
      "iteration: 256940 loss: 0.0017 lr: 0.02\n",
      "iteration: 256950 loss: 0.0022 lr: 0.02\n",
      "iteration: 256960 loss: 0.0018 lr: 0.02\n",
      "iteration: 256970 loss: 0.0015 lr: 0.02\n",
      "iteration: 256980 loss: 0.0014 lr: 0.02\n",
      "iteration: 256990 loss: 0.0031 lr: 0.02\n",
      "iteration: 257000 loss: 0.0023 lr: 0.02\n",
      "iteration: 257010 loss: 0.0017 lr: 0.02\n",
      "iteration: 257020 loss: 0.0013 lr: 0.02\n",
      "iteration: 257030 loss: 0.0016 lr: 0.02\n",
      "iteration: 257040 loss: 0.0019 lr: 0.02\n",
      "iteration: 257050 loss: 0.0028 lr: 0.02\n",
      "iteration: 257060 loss: 0.0016 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 257070 loss: 0.0011 lr: 0.02\n",
      "iteration: 257080 loss: 0.0018 lr: 0.02\n",
      "iteration: 257090 loss: 0.0015 lr: 0.02\n",
      "iteration: 257100 loss: 0.0024 lr: 0.02\n",
      "iteration: 257110 loss: 0.0014 lr: 0.02\n",
      "iteration: 257120 loss: 0.0024 lr: 0.02\n",
      "iteration: 257130 loss: 0.0012 lr: 0.02\n",
      "iteration: 257140 loss: 0.0016 lr: 0.02\n",
      "iteration: 257150 loss: 0.0022 lr: 0.02\n",
      "iteration: 257160 loss: 0.0021 lr: 0.02\n",
      "iteration: 257170 loss: 0.0020 lr: 0.02\n",
      "iteration: 257180 loss: 0.0022 lr: 0.02\n",
      "iteration: 257190 loss: 0.0016 lr: 0.02\n",
      "iteration: 257200 loss: 0.0019 lr: 0.02\n",
      "iteration: 257210 loss: 0.0026 lr: 0.02\n",
      "iteration: 257220 loss: 0.0031 lr: 0.02\n",
      "iteration: 257230 loss: 0.0019 lr: 0.02\n",
      "iteration: 257240 loss: 0.0019 lr: 0.02\n",
      "iteration: 257250 loss: 0.0019 lr: 0.02\n",
      "iteration: 257260 loss: 0.0015 lr: 0.02\n",
      "iteration: 257270 loss: 0.0018 lr: 0.02\n",
      "iteration: 257280 loss: 0.0020 lr: 0.02\n",
      "iteration: 257290 loss: 0.0019 lr: 0.02\n",
      "iteration: 257300 loss: 0.0018 lr: 0.02\n",
      "iteration: 257310 loss: 0.0015 lr: 0.02\n",
      "iteration: 257320 loss: 0.0019 lr: 0.02\n",
      "iteration: 257330 loss: 0.0020 lr: 0.02\n",
      "iteration: 257340 loss: 0.0015 lr: 0.02\n",
      "iteration: 257350 loss: 0.0017 lr: 0.02\n",
      "iteration: 257360 loss: 0.0018 lr: 0.02\n",
      "iteration: 257370 loss: 0.0012 lr: 0.02\n",
      "iteration: 257380 loss: 0.0021 lr: 0.02\n",
      "iteration: 257390 loss: 0.0038 lr: 0.02\n",
      "iteration: 257400 loss: 0.0031 lr: 0.02\n",
      "iteration: 257410 loss: 0.0019 lr: 0.02\n",
      "iteration: 257420 loss: 0.0017 lr: 0.02\n",
      "iteration: 257430 loss: 0.0027 lr: 0.02\n",
      "iteration: 257440 loss: 0.0039 lr: 0.02\n",
      "iteration: 257450 loss: 0.0039 lr: 0.02\n",
      "iteration: 257460 loss: 0.0030 lr: 0.02\n",
      "iteration: 257470 loss: 0.0024 lr: 0.02\n",
      "iteration: 257480 loss: 0.0019 lr: 0.02\n",
      "iteration: 257490 loss: 0.0019 lr: 0.02\n",
      "iteration: 257500 loss: 0.0010 lr: 0.02\n",
      "iteration: 257510 loss: 0.0022 lr: 0.02\n",
      "iteration: 257520 loss: 0.0017 lr: 0.02\n",
      "iteration: 257530 loss: 0.0023 lr: 0.02\n",
      "iteration: 257540 loss: 0.0022 lr: 0.02\n",
      "iteration: 257550 loss: 0.0016 lr: 0.02\n",
      "iteration: 257560 loss: 0.0020 lr: 0.02\n",
      "iteration: 257570 loss: 0.0019 lr: 0.02\n",
      "iteration: 257580 loss: 0.0017 lr: 0.02\n",
      "iteration: 257590 loss: 0.0021 lr: 0.02\n",
      "iteration: 257600 loss: 0.0016 lr: 0.02\n",
      "iteration: 257610 loss: 0.0019 lr: 0.02\n",
      "iteration: 257620 loss: 0.0022 lr: 0.02\n",
      "iteration: 257630 loss: 0.0019 lr: 0.02\n",
      "iteration: 257640 loss: 0.0018 lr: 0.02\n",
      "iteration: 257650 loss: 0.0011 lr: 0.02\n",
      "iteration: 257660 loss: 0.0018 lr: 0.02\n",
      "iteration: 257670 loss: 0.0016 lr: 0.02\n",
      "iteration: 257680 loss: 0.0020 lr: 0.02\n",
      "iteration: 257690 loss: 0.0020 lr: 0.02\n",
      "iteration: 257700 loss: 0.0029 lr: 0.02\n",
      "iteration: 257710 loss: 0.0015 lr: 0.02\n",
      "iteration: 257720 loss: 0.0022 lr: 0.02\n",
      "iteration: 257730 loss: 0.0020 lr: 0.02\n",
      "iteration: 257740 loss: 0.0028 lr: 0.02\n",
      "iteration: 257750 loss: 0.0019 lr: 0.02\n",
      "iteration: 257760 loss: 0.0018 lr: 0.02\n",
      "iteration: 257770 loss: 0.0019 lr: 0.02\n",
      "iteration: 257780 loss: 0.0017 lr: 0.02\n",
      "iteration: 257790 loss: 0.0013 lr: 0.02\n",
      "iteration: 257800 loss: 0.0022 lr: 0.02\n",
      "iteration: 257810 loss: 0.0021 lr: 0.02\n",
      "iteration: 257820 loss: 0.0017 lr: 0.02\n",
      "iteration: 257830 loss: 0.0016 lr: 0.02\n",
      "iteration: 257840 loss: 0.0015 lr: 0.02\n",
      "iteration: 257850 loss: 0.0025 lr: 0.02\n",
      "iteration: 257860 loss: 0.0021 lr: 0.02\n",
      "iteration: 257870 loss: 0.0022 lr: 0.02\n",
      "iteration: 257880 loss: 0.0021 lr: 0.02\n",
      "iteration: 257890 loss: 0.0022 lr: 0.02\n",
      "iteration: 257900 loss: 0.0023 lr: 0.02\n",
      "iteration: 257910 loss: 0.0014 lr: 0.02\n",
      "iteration: 257920 loss: 0.0038 lr: 0.02\n",
      "iteration: 257930 loss: 0.0021 lr: 0.02\n",
      "iteration: 257940 loss: 0.0018 lr: 0.02\n",
      "iteration: 257950 loss: 0.0018 lr: 0.02\n",
      "iteration: 257960 loss: 0.0025 lr: 0.02\n",
      "iteration: 257970 loss: 0.0036 lr: 0.02\n",
      "iteration: 257980 loss: 0.0015 lr: 0.02\n",
      "iteration: 257990 loss: 0.0022 lr: 0.02\n",
      "iteration: 258000 loss: 0.0021 lr: 0.02\n",
      "iteration: 258010 loss: 0.0046 lr: 0.02\n",
      "iteration: 258020 loss: 0.0019 lr: 0.02\n",
      "iteration: 258030 loss: 0.0015 lr: 0.02\n",
      "iteration: 258040 loss: 0.0024 lr: 0.02\n",
      "iteration: 258050 loss: 0.0015 lr: 0.02\n",
      "iteration: 258060 loss: 0.0038 lr: 0.02\n",
      "iteration: 258070 loss: 0.0019 lr: 0.02\n",
      "iteration: 258080 loss: 0.0017 lr: 0.02\n",
      "iteration: 258090 loss: 0.0014 lr: 0.02\n",
      "iteration: 258100 loss: 0.0014 lr: 0.02\n",
      "iteration: 258110 loss: 0.0016 lr: 0.02\n",
      "iteration: 258120 loss: 0.0019 lr: 0.02\n",
      "iteration: 258130 loss: 0.0020 lr: 0.02\n",
      "iteration: 258140 loss: 0.0017 lr: 0.02\n",
      "iteration: 258150 loss: 0.0018 lr: 0.02\n",
      "iteration: 258160 loss: 0.0027 lr: 0.02\n",
      "iteration: 258170 loss: 0.0014 lr: 0.02\n",
      "iteration: 258180 loss: 0.0018 lr: 0.02\n",
      "iteration: 258190 loss: 0.0025 lr: 0.02\n",
      "iteration: 258200 loss: 0.0021 lr: 0.02\n",
      "iteration: 258210 loss: 0.0017 lr: 0.02\n",
      "iteration: 258220 loss: 0.0012 lr: 0.02\n",
      "iteration: 258230 loss: 0.0020 lr: 0.02\n",
      "iteration: 258240 loss: 0.0012 lr: 0.02\n",
      "iteration: 258250 loss: 0.0015 lr: 0.02\n",
      "iteration: 258260 loss: 0.0018 lr: 0.02\n",
      "iteration: 258270 loss: 0.0015 lr: 0.02\n",
      "iteration: 258280 loss: 0.0017 lr: 0.02\n",
      "iteration: 258290 loss: 0.0015 lr: 0.02\n",
      "iteration: 258300 loss: 0.0018 lr: 0.02\n",
      "iteration: 258310 loss: 0.0019 lr: 0.02\n",
      "iteration: 258320 loss: 0.0021 lr: 0.02\n",
      "iteration: 258330 loss: 0.0032 lr: 0.02\n",
      "iteration: 258340 loss: 0.0012 lr: 0.02\n",
      "iteration: 258350 loss: 0.0013 lr: 0.02\n",
      "iteration: 258360 loss: 0.0020 lr: 0.02\n",
      "iteration: 258370 loss: 0.0022 lr: 0.02\n",
      "iteration: 258380 loss: 0.0015 lr: 0.02\n",
      "iteration: 258390 loss: 0.0016 lr: 0.02\n",
      "iteration: 258400 loss: 0.0017 lr: 0.02\n",
      "iteration: 258410 loss: 0.0018 lr: 0.02\n",
      "iteration: 258420 loss: 0.0022 lr: 0.02\n",
      "iteration: 258430 loss: 0.0019 lr: 0.02\n",
      "iteration: 258440 loss: 0.0015 lr: 0.02\n",
      "iteration: 258450 loss: 0.0019 lr: 0.02\n",
      "iteration: 258460 loss: 0.0026 lr: 0.02\n",
      "iteration: 258470 loss: 0.0017 lr: 0.02\n",
      "iteration: 258480 loss: 0.0015 lr: 0.02\n",
      "iteration: 258490 loss: 0.0021 lr: 0.02\n",
      "iteration: 258500 loss: 0.0017 lr: 0.02\n",
      "iteration: 258510 loss: 0.0019 lr: 0.02\n",
      "iteration: 258520 loss: 0.0018 lr: 0.02\n",
      "iteration: 258530 loss: 0.0019 lr: 0.02\n",
      "iteration: 258540 loss: 0.0015 lr: 0.02\n",
      "iteration: 258550 loss: 0.0022 lr: 0.02\n",
      "iteration: 258560 loss: 0.0020 lr: 0.02\n",
      "iteration: 258570 loss: 0.0020 lr: 0.02\n",
      "iteration: 258580 loss: 0.0018 lr: 0.02\n",
      "iteration: 258590 loss: 0.0025 lr: 0.02\n",
      "iteration: 258600 loss: 0.0015 lr: 0.02\n",
      "iteration: 258610 loss: 0.0013 lr: 0.02\n",
      "iteration: 258620 loss: 0.0013 lr: 0.02\n",
      "iteration: 258630 loss: 0.0013 lr: 0.02\n",
      "iteration: 258640 loss: 0.0018 lr: 0.02\n",
      "iteration: 258650 loss: 0.0021 lr: 0.02\n",
      "iteration: 258660 loss: 0.0018 lr: 0.02\n",
      "iteration: 258670 loss: 0.0014 lr: 0.02\n",
      "iteration: 258680 loss: 0.0020 lr: 0.02\n",
      "iteration: 258690 loss: 0.0019 lr: 0.02\n",
      "iteration: 258700 loss: 0.0028 lr: 0.02\n",
      "iteration: 258710 loss: 0.0018 lr: 0.02\n",
      "iteration: 258720 loss: 0.0019 lr: 0.02\n",
      "iteration: 258730 loss: 0.0019 lr: 0.02\n",
      "iteration: 258740 loss: 0.0029 lr: 0.02\n",
      "iteration: 258750 loss: 0.0028 lr: 0.02\n",
      "iteration: 258760 loss: 0.0014 lr: 0.02\n",
      "iteration: 258770 loss: 0.0021 lr: 0.02\n",
      "iteration: 258780 loss: 0.0021 lr: 0.02\n",
      "iteration: 258790 loss: 0.0019 lr: 0.02\n",
      "iteration: 258800 loss: 0.0021 lr: 0.02\n",
      "iteration: 258810 loss: 0.0013 lr: 0.02\n",
      "iteration: 258820 loss: 0.0017 lr: 0.02\n",
      "iteration: 258830 loss: 0.0018 lr: 0.02\n",
      "iteration: 258840 loss: 0.0013 lr: 0.02\n",
      "iteration: 258850 loss: 0.0014 lr: 0.02\n",
      "iteration: 258860 loss: 0.0015 lr: 0.02\n",
      "iteration: 258870 loss: 0.0029 lr: 0.02\n",
      "iteration: 258880 loss: 0.0020 lr: 0.02\n",
      "iteration: 258890 loss: 0.0024 lr: 0.02\n",
      "iteration: 258900 loss: 0.0016 lr: 0.02\n",
      "iteration: 258910 loss: 0.0017 lr: 0.02\n",
      "iteration: 258920 loss: 0.0021 lr: 0.02\n",
      "iteration: 258930 loss: 0.0018 lr: 0.02\n",
      "iteration: 258940 loss: 0.0019 lr: 0.02\n",
      "iteration: 258950 loss: 0.0017 lr: 0.02\n",
      "iteration: 258960 loss: 0.0019 lr: 0.02\n",
      "iteration: 258970 loss: 0.0019 lr: 0.02\n",
      "iteration: 258980 loss: 0.0018 lr: 0.02\n",
      "iteration: 258990 loss: 0.0025 lr: 0.02\n",
      "iteration: 259000 loss: 0.0017 lr: 0.02\n",
      "iteration: 259010 loss: 0.0013 lr: 0.02\n",
      "iteration: 259020 loss: 0.0030 lr: 0.02\n",
      "iteration: 259030 loss: 0.0021 lr: 0.02\n",
      "iteration: 259040 loss: 0.0025 lr: 0.02\n",
      "iteration: 259050 loss: 0.0016 lr: 0.02\n",
      "iteration: 259060 loss: 0.0023 lr: 0.02\n",
      "iteration: 259070 loss: 0.0017 lr: 0.02\n",
      "iteration: 259080 loss: 0.0022 lr: 0.02\n",
      "iteration: 259090 loss: 0.0019 lr: 0.02\n",
      "iteration: 259100 loss: 0.0019 lr: 0.02\n",
      "iteration: 259110 loss: 0.0021 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 259120 loss: 0.0022 lr: 0.02\n",
      "iteration: 259130 loss: 0.0014 lr: 0.02\n",
      "iteration: 259140 loss: 0.0014 lr: 0.02\n",
      "iteration: 259150 loss: 0.0019 lr: 0.02\n",
      "iteration: 259160 loss: 0.0017 lr: 0.02\n",
      "iteration: 259170 loss: 0.0023 lr: 0.02\n",
      "iteration: 259180 loss: 0.0015 lr: 0.02\n",
      "iteration: 259190 loss: 0.0027 lr: 0.02\n",
      "iteration: 259200 loss: 0.0020 lr: 0.02\n",
      "iteration: 259210 loss: 0.0022 lr: 0.02\n",
      "iteration: 259220 loss: 0.0024 lr: 0.02\n",
      "iteration: 259230 loss: 0.0025 lr: 0.02\n",
      "iteration: 259240 loss: 0.0020 lr: 0.02\n",
      "iteration: 259250 loss: 0.0018 lr: 0.02\n",
      "iteration: 259260 loss: 0.0021 lr: 0.02\n",
      "iteration: 259270 loss: 0.0022 lr: 0.02\n",
      "iteration: 259280 loss: 0.0019 lr: 0.02\n",
      "iteration: 259290 loss: 0.0014 lr: 0.02\n",
      "iteration: 259300 loss: 0.0017 lr: 0.02\n",
      "iteration: 259310 loss: 0.0024 lr: 0.02\n",
      "iteration: 259320 loss: 0.0022 lr: 0.02\n",
      "iteration: 259330 loss: 0.0021 lr: 0.02\n",
      "iteration: 259340 loss: 0.0017 lr: 0.02\n",
      "iteration: 259350 loss: 0.0019 lr: 0.02\n",
      "iteration: 259360 loss: 0.0025 lr: 0.02\n",
      "iteration: 259370 loss: 0.0023 lr: 0.02\n",
      "iteration: 259380 loss: 0.0016 lr: 0.02\n",
      "iteration: 259390 loss: 0.0021 lr: 0.02\n",
      "iteration: 259400 loss: 0.0024 lr: 0.02\n",
      "iteration: 259410 loss: 0.0020 lr: 0.02\n",
      "iteration: 259420 loss: 0.0023 lr: 0.02\n",
      "iteration: 259430 loss: 0.0020 lr: 0.02\n",
      "iteration: 259440 loss: 0.0016 lr: 0.02\n",
      "iteration: 259450 loss: 0.0023 lr: 0.02\n",
      "iteration: 259460 loss: 0.0018 lr: 0.02\n",
      "iteration: 259470 loss: 0.0017 lr: 0.02\n",
      "iteration: 259480 loss: 0.0016 lr: 0.02\n",
      "iteration: 259490 loss: 0.0016 lr: 0.02\n",
      "iteration: 259500 loss: 0.0022 lr: 0.02\n",
      "iteration: 259510 loss: 0.0011 lr: 0.02\n",
      "iteration: 259520 loss: 0.0020 lr: 0.02\n",
      "iteration: 259530 loss: 0.0014 lr: 0.02\n",
      "iteration: 259540 loss: 0.0027 lr: 0.02\n",
      "iteration: 259550 loss: 0.0024 lr: 0.02\n",
      "iteration: 259560 loss: 0.0020 lr: 0.02\n",
      "iteration: 259570 loss: 0.0020 lr: 0.02\n",
      "iteration: 259580 loss: 0.0017 lr: 0.02\n",
      "iteration: 259590 loss: 0.0018 lr: 0.02\n",
      "iteration: 259600 loss: 0.0025 lr: 0.02\n",
      "iteration: 259610 loss: 0.0016 lr: 0.02\n",
      "iteration: 259620 loss: 0.0017 lr: 0.02\n",
      "iteration: 259630 loss: 0.0017 lr: 0.02\n",
      "iteration: 259640 loss: 0.0019 lr: 0.02\n",
      "iteration: 259650 loss: 0.0015 lr: 0.02\n",
      "iteration: 259660 loss: 0.0018 lr: 0.02\n",
      "iteration: 259670 loss: 0.0015 lr: 0.02\n",
      "iteration: 259680 loss: 0.0022 lr: 0.02\n",
      "iteration: 259690 loss: 0.0028 lr: 0.02\n",
      "iteration: 259700 loss: 0.0024 lr: 0.02\n",
      "iteration: 259710 loss: 0.0015 lr: 0.02\n",
      "iteration: 259720 loss: 0.0020 lr: 0.02\n",
      "iteration: 259730 loss: 0.0016 lr: 0.02\n",
      "iteration: 259740 loss: 0.0018 lr: 0.02\n",
      "iteration: 259750 loss: 0.0015 lr: 0.02\n",
      "iteration: 259760 loss: 0.0022 lr: 0.02\n",
      "iteration: 259770 loss: 0.0014 lr: 0.02\n",
      "iteration: 259780 loss: 0.0016 lr: 0.02\n",
      "iteration: 259790 loss: 0.0021 lr: 0.02\n",
      "iteration: 259800 loss: 0.0016 lr: 0.02\n",
      "iteration: 259810 loss: 0.0019 lr: 0.02\n",
      "iteration: 259820 loss: 0.0020 lr: 0.02\n",
      "iteration: 259830 loss: 0.0015 lr: 0.02\n",
      "iteration: 259840 loss: 0.0013 lr: 0.02\n",
      "iteration: 259850 loss: 0.0018 lr: 0.02\n",
      "iteration: 259860 loss: 0.0016 lr: 0.02\n",
      "iteration: 259870 loss: 0.0024 lr: 0.02\n",
      "iteration: 259880 loss: 0.0026 lr: 0.02\n",
      "iteration: 259890 loss: 0.0028 lr: 0.02\n",
      "iteration: 259900 loss: 0.0016 lr: 0.02\n",
      "iteration: 259910 loss: 0.0016 lr: 0.02\n",
      "iteration: 259920 loss: 0.0017 lr: 0.02\n",
      "iteration: 259930 loss: 0.0021 lr: 0.02\n",
      "iteration: 259940 loss: 0.0022 lr: 0.02\n",
      "iteration: 259950 loss: 0.0019 lr: 0.02\n",
      "iteration: 259960 loss: 0.0022 lr: 0.02\n",
      "iteration: 259970 loss: 0.0025 lr: 0.02\n",
      "iteration: 259980 loss: 0.0014 lr: 0.02\n",
      "iteration: 259990 loss: 0.0015 lr: 0.02\n",
      "iteration: 260000 loss: 0.0018 lr: 0.02\n",
      "iteration: 260010 loss: 0.0013 lr: 0.02\n",
      "iteration: 260020 loss: 0.0016 lr: 0.02\n",
      "iteration: 260030 loss: 0.0018 lr: 0.02\n",
      "iteration: 260040 loss: 0.0016 lr: 0.02\n",
      "iteration: 260050 loss: 0.0015 lr: 0.02\n",
      "iteration: 260060 loss: 0.0026 lr: 0.02\n",
      "iteration: 260070 loss: 0.0018 lr: 0.02\n",
      "iteration: 260080 loss: 0.0014 lr: 0.02\n",
      "iteration: 260090 loss: 0.0019 lr: 0.02\n",
      "iteration: 260100 loss: 0.0038 lr: 0.02\n",
      "iteration: 260110 loss: 0.0026 lr: 0.02\n",
      "iteration: 260120 loss: 0.0023 lr: 0.02\n",
      "iteration: 260130 loss: 0.0028 lr: 0.02\n",
      "iteration: 260140 loss: 0.0026 lr: 0.02\n",
      "iteration: 260150 loss: 0.0036 lr: 0.02\n",
      "iteration: 260160 loss: 0.0023 lr: 0.02\n",
      "iteration: 260170 loss: 0.0023 lr: 0.02\n",
      "iteration: 260180 loss: 0.0017 lr: 0.02\n",
      "iteration: 260190 loss: 0.0022 lr: 0.02\n",
      "iteration: 260200 loss: 0.0016 lr: 0.02\n",
      "iteration: 260210 loss: 0.0016 lr: 0.02\n",
      "iteration: 260220 loss: 0.0014 lr: 0.02\n",
      "iteration: 260230 loss: 0.0018 lr: 0.02\n",
      "iteration: 260240 loss: 0.0014 lr: 0.02\n",
      "iteration: 260250 loss: 0.0016 lr: 0.02\n",
      "iteration: 260260 loss: 0.0017 lr: 0.02\n",
      "iteration: 260270 loss: 0.0012 lr: 0.02\n",
      "iteration: 260280 loss: 0.0021 lr: 0.02\n",
      "iteration: 260290 loss: 0.0022 lr: 0.02\n",
      "iteration: 260300 loss: 0.0029 lr: 0.02\n",
      "iteration: 260310 loss: 0.0022 lr: 0.02\n",
      "iteration: 260320 loss: 0.0022 lr: 0.02\n",
      "iteration: 260330 loss: 0.0026 lr: 0.02\n",
      "iteration: 260340 loss: 0.0023 lr: 0.02\n",
      "iteration: 260350 loss: 0.0020 lr: 0.02\n",
      "iteration: 260360 loss: 0.0024 lr: 0.02\n",
      "iteration: 260370 loss: 0.0020 lr: 0.02\n",
      "iteration: 260380 loss: 0.0016 lr: 0.02\n",
      "iteration: 260390 loss: 0.0021 lr: 0.02\n",
      "iteration: 260400 loss: 0.0023 lr: 0.02\n",
      "iteration: 260410 loss: 0.0018 lr: 0.02\n",
      "iteration: 260420 loss: 0.0017 lr: 0.02\n",
      "iteration: 260430 loss: 0.0016 lr: 0.02\n",
      "iteration: 260440 loss: 0.0025 lr: 0.02\n",
      "iteration: 260450 loss: 0.0020 lr: 0.02\n",
      "iteration: 260460 loss: 0.0023 lr: 0.02\n",
      "iteration: 260470 loss: 0.0024 lr: 0.02\n",
      "iteration: 260480 loss: 0.0029 lr: 0.02\n",
      "iteration: 260490 loss: 0.0024 lr: 0.02\n",
      "iteration: 260500 loss: 0.0011 lr: 0.02\n",
      "iteration: 260510 loss: 0.0013 lr: 0.02\n",
      "iteration: 260520 loss: 0.0021 lr: 0.02\n",
      "iteration: 260530 loss: 0.0033 lr: 0.02\n",
      "iteration: 260540 loss: 0.0024 lr: 0.02\n",
      "iteration: 260550 loss: 0.0020 lr: 0.02\n",
      "iteration: 260560 loss: 0.0023 lr: 0.02\n",
      "iteration: 260570 loss: 0.0015 lr: 0.02\n",
      "iteration: 260580 loss: 0.0016 lr: 0.02\n",
      "iteration: 260590 loss: 0.0023 lr: 0.02\n",
      "iteration: 260600 loss: 0.0024 lr: 0.02\n",
      "iteration: 260610 loss: 0.0015 lr: 0.02\n",
      "iteration: 260620 loss: 0.0018 lr: 0.02\n",
      "iteration: 260630 loss: 0.0017 lr: 0.02\n",
      "iteration: 260640 loss: 0.0016 lr: 0.02\n",
      "iteration: 260650 loss: 0.0013 lr: 0.02\n",
      "iteration: 260660 loss: 0.0014 lr: 0.02\n",
      "iteration: 260670 loss: 0.0013 lr: 0.02\n",
      "iteration: 260680 loss: 0.0019 lr: 0.02\n",
      "iteration: 260690 loss: 0.0015 lr: 0.02\n",
      "iteration: 260700 loss: 0.0018 lr: 0.02\n",
      "iteration: 260710 loss: 0.0027 lr: 0.02\n",
      "iteration: 260720 loss: 0.0018 lr: 0.02\n",
      "iteration: 260730 loss: 0.0016 lr: 0.02\n",
      "iteration: 260740 loss: 0.0033 lr: 0.02\n",
      "iteration: 260750 loss: 0.0013 lr: 0.02\n",
      "iteration: 260760 loss: 0.0021 lr: 0.02\n",
      "iteration: 260770 loss: 0.0028 lr: 0.02\n",
      "iteration: 260780 loss: 0.0023 lr: 0.02\n",
      "iteration: 260790 loss: 0.0015 lr: 0.02\n",
      "iteration: 260800 loss: 0.0027 lr: 0.02\n",
      "iteration: 260810 loss: 0.0021 lr: 0.02\n",
      "iteration: 260820 loss: 0.0014 lr: 0.02\n",
      "iteration: 260830 loss: 0.0016 lr: 0.02\n",
      "iteration: 260840 loss: 0.0016 lr: 0.02\n",
      "iteration: 260850 loss: 0.0033 lr: 0.02\n",
      "iteration: 260860 loss: 0.0018 lr: 0.02\n",
      "iteration: 260870 loss: 0.0020 lr: 0.02\n",
      "iteration: 260880 loss: 0.0019 lr: 0.02\n",
      "iteration: 260890 loss: 0.0015 lr: 0.02\n",
      "iteration: 260900 loss: 0.0024 lr: 0.02\n",
      "iteration: 260910 loss: 0.0020 lr: 0.02\n",
      "iteration: 260920 loss: 0.0021 lr: 0.02\n",
      "iteration: 260930 loss: 0.0034 lr: 0.02\n",
      "iteration: 260940 loss: 0.0016 lr: 0.02\n",
      "iteration: 260950 loss: 0.0020 lr: 0.02\n",
      "iteration: 260960 loss: 0.0019 lr: 0.02\n",
      "iteration: 260970 loss: 0.0019 lr: 0.02\n",
      "iteration: 260980 loss: 0.0024 lr: 0.02\n",
      "iteration: 260990 loss: 0.0033 lr: 0.02\n",
      "iteration: 261000 loss: 0.0018 lr: 0.02\n",
      "iteration: 261010 loss: 0.0022 lr: 0.02\n",
      "iteration: 261020 loss: 0.0014 lr: 0.02\n",
      "iteration: 261030 loss: 0.0019 lr: 0.02\n",
      "iteration: 261040 loss: 0.0017 lr: 0.02\n",
      "iteration: 261050 loss: 0.0016 lr: 0.02\n",
      "iteration: 261060 loss: 0.0013 lr: 0.02\n",
      "iteration: 261070 loss: 0.0024 lr: 0.02\n",
      "iteration: 261080 loss: 0.0016 lr: 0.02\n",
      "iteration: 261090 loss: 0.0011 lr: 0.02\n",
      "iteration: 261100 loss: 0.0019 lr: 0.02\n",
      "iteration: 261110 loss: 0.0016 lr: 0.02\n",
      "iteration: 261120 loss: 0.0018 lr: 0.02\n",
      "iteration: 261130 loss: 0.0021 lr: 0.02\n",
      "iteration: 261140 loss: 0.0024 lr: 0.02\n",
      "iteration: 261150 loss: 0.0018 lr: 0.02\n",
      "iteration: 261160 loss: 0.0018 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 261170 loss: 0.0017 lr: 0.02\n",
      "iteration: 261180 loss: 0.0018 lr: 0.02\n",
      "iteration: 261190 loss: 0.0022 lr: 0.02\n",
      "iteration: 261200 loss: 0.0034 lr: 0.02\n",
      "iteration: 261210 loss: 0.0024 lr: 0.02\n",
      "iteration: 261220 loss: 0.0018 lr: 0.02\n",
      "iteration: 261230 loss: 0.0021 lr: 0.02\n",
      "iteration: 261240 loss: 0.0023 lr: 0.02\n",
      "iteration: 261250 loss: 0.0018 lr: 0.02\n",
      "iteration: 261260 loss: 0.0019 lr: 0.02\n",
      "iteration: 261270 loss: 0.0030 lr: 0.02\n",
      "iteration: 261280 loss: 0.0014 lr: 0.02\n",
      "iteration: 261290 loss: 0.0015 lr: 0.02\n",
      "iteration: 261300 loss: 0.0013 lr: 0.02\n",
      "iteration: 261310 loss: 0.0017 lr: 0.02\n",
      "iteration: 261320 loss: 0.0013 lr: 0.02\n",
      "iteration: 261330 loss: 0.0014 lr: 0.02\n",
      "iteration: 261340 loss: 0.0015 lr: 0.02\n",
      "iteration: 261350 loss: 0.0016 lr: 0.02\n",
      "iteration: 261360 loss: 0.0017 lr: 0.02\n",
      "iteration: 261370 loss: 0.0020 lr: 0.02\n",
      "iteration: 261380 loss: 0.0015 lr: 0.02\n",
      "iteration: 261390 loss: 0.0017 lr: 0.02\n",
      "iteration: 261400 loss: 0.0014 lr: 0.02\n",
      "iteration: 261410 loss: 0.0017 lr: 0.02\n",
      "iteration: 261420 loss: 0.0031 lr: 0.02\n",
      "iteration: 261430 loss: 0.0017 lr: 0.02\n",
      "iteration: 261440 loss: 0.0017 lr: 0.02\n",
      "iteration: 261450 loss: 0.0015 lr: 0.02\n",
      "iteration: 261460 loss: 0.0031 lr: 0.02\n",
      "iteration: 261470 loss: 0.0017 lr: 0.02\n",
      "iteration: 261480 loss: 0.0021 lr: 0.02\n",
      "iteration: 261490 loss: 0.0024 lr: 0.02\n",
      "iteration: 261500 loss: 0.0024 lr: 0.02\n",
      "iteration: 261510 loss: 0.0022 lr: 0.02\n",
      "iteration: 261520 loss: 0.0026 lr: 0.02\n",
      "iteration: 261530 loss: 0.0014 lr: 0.02\n",
      "iteration: 261540 loss: 0.0028 lr: 0.02\n",
      "iteration: 261550 loss: 0.0022 lr: 0.02\n",
      "iteration: 261560 loss: 0.0022 lr: 0.02\n",
      "iteration: 261570 loss: 0.0019 lr: 0.02\n",
      "iteration: 261580 loss: 0.0015 lr: 0.02\n",
      "iteration: 261590 loss: 0.0015 lr: 0.02\n",
      "iteration: 261600 loss: 0.0014 lr: 0.02\n",
      "iteration: 261610 loss: 0.0019 lr: 0.02\n",
      "iteration: 261620 loss: 0.0018 lr: 0.02\n",
      "iteration: 261630 loss: 0.0025 lr: 0.02\n",
      "iteration: 261640 loss: 0.0016 lr: 0.02\n",
      "iteration: 261650 loss: 0.0020 lr: 0.02\n",
      "iteration: 261660 loss: 0.0039 lr: 0.02\n",
      "iteration: 261670 loss: 0.0014 lr: 0.02\n",
      "iteration: 261680 loss: 0.0024 lr: 0.02\n",
      "iteration: 261690 loss: 0.0013 lr: 0.02\n",
      "iteration: 261700 loss: 0.0022 lr: 0.02\n",
      "iteration: 261710 loss: 0.0022 lr: 0.02\n",
      "iteration: 261720 loss: 0.0015 lr: 0.02\n",
      "iteration: 261730 loss: 0.0016 lr: 0.02\n",
      "iteration: 261740 loss: 0.0019 lr: 0.02\n",
      "iteration: 261750 loss: 0.0021 lr: 0.02\n",
      "iteration: 261760 loss: 0.0015 lr: 0.02\n",
      "iteration: 261770 loss: 0.0020 lr: 0.02\n",
      "iteration: 261780 loss: 0.0018 lr: 0.02\n",
      "iteration: 261790 loss: 0.0021 lr: 0.02\n",
      "iteration: 261800 loss: 0.0016 lr: 0.02\n",
      "iteration: 261810 loss: 0.0022 lr: 0.02\n",
      "iteration: 261820 loss: 0.0022 lr: 0.02\n",
      "iteration: 261830 loss: 0.0018 lr: 0.02\n",
      "iteration: 261840 loss: 0.0021 lr: 0.02\n",
      "iteration: 261850 loss: 0.0027 lr: 0.02\n",
      "iteration: 261860 loss: 0.0014 lr: 0.02\n",
      "iteration: 261870 loss: 0.0015 lr: 0.02\n",
      "iteration: 261880 loss: 0.0022 lr: 0.02\n",
      "iteration: 261890 loss: 0.0031 lr: 0.02\n",
      "iteration: 261900 loss: 0.0016 lr: 0.02\n",
      "iteration: 261910 loss: 0.0018 lr: 0.02\n",
      "iteration: 261920 loss: 0.0018 lr: 0.02\n",
      "iteration: 261930 loss: 0.0016 lr: 0.02\n",
      "iteration: 261940 loss: 0.0020 lr: 0.02\n",
      "iteration: 261950 loss: 0.0018 lr: 0.02\n",
      "iteration: 261960 loss: 0.0018 lr: 0.02\n",
      "iteration: 261970 loss: 0.0026 lr: 0.02\n",
      "iteration: 261980 loss: 0.0037 lr: 0.02\n",
      "iteration: 261990 loss: 0.0030 lr: 0.02\n",
      "iteration: 262000 loss: 0.0018 lr: 0.02\n",
      "iteration: 262010 loss: 0.0017 lr: 0.02\n",
      "iteration: 262020 loss: 0.0026 lr: 0.02\n",
      "iteration: 262030 loss: 0.0026 lr: 0.02\n",
      "iteration: 262040 loss: 0.0016 lr: 0.02\n",
      "iteration: 262050 loss: 0.0011 lr: 0.02\n",
      "iteration: 262060 loss: 0.0016 lr: 0.02\n",
      "iteration: 262070 loss: 0.0015 lr: 0.02\n",
      "iteration: 262080 loss: 0.0013 lr: 0.02\n",
      "iteration: 262090 loss: 0.0016 lr: 0.02\n",
      "iteration: 262100 loss: 0.0024 lr: 0.02\n",
      "iteration: 262110 loss: 0.0038 lr: 0.02\n",
      "iteration: 262120 loss: 0.0029 lr: 0.02\n",
      "iteration: 262130 loss: 0.0021 lr: 0.02\n",
      "iteration: 262140 loss: 0.0016 lr: 0.02\n",
      "iteration: 262150 loss: 0.0019 lr: 0.02\n",
      "iteration: 262160 loss: 0.0018 lr: 0.02\n",
      "iteration: 262170 loss: 0.0021 lr: 0.02\n",
      "iteration: 262180 loss: 0.0017 lr: 0.02\n",
      "iteration: 262190 loss: 0.0015 lr: 0.02\n",
      "iteration: 262200 loss: 0.0018 lr: 0.02\n",
      "iteration: 262210 loss: 0.0025 lr: 0.02\n",
      "iteration: 262220 loss: 0.0015 lr: 0.02\n",
      "iteration: 262230 loss: 0.0015 lr: 0.02\n",
      "iteration: 262240 loss: 0.0014 lr: 0.02\n",
      "iteration: 262250 loss: 0.0022 lr: 0.02\n",
      "iteration: 262260 loss: 0.0021 lr: 0.02\n",
      "iteration: 262270 loss: 0.0018 lr: 0.02\n",
      "iteration: 262280 loss: 0.0021 lr: 0.02\n",
      "iteration: 262290 loss: 0.0014 lr: 0.02\n",
      "iteration: 262300 loss: 0.0018 lr: 0.02\n",
      "iteration: 262310 loss: 0.0019 lr: 0.02\n",
      "iteration: 262320 loss: 0.0023 lr: 0.02\n",
      "iteration: 262330 loss: 0.0027 lr: 0.02\n",
      "iteration: 262340 loss: 0.0035 lr: 0.02\n",
      "iteration: 262350 loss: 0.0014 lr: 0.02\n",
      "iteration: 262360 loss: 0.0017 lr: 0.02\n",
      "iteration: 262370 loss: 0.0028 lr: 0.02\n",
      "iteration: 262380 loss: 0.0025 lr: 0.02\n",
      "iteration: 262390 loss: 0.0019 lr: 0.02\n",
      "iteration: 262400 loss: 0.0015 lr: 0.02\n",
      "iteration: 262410 loss: 0.0017 lr: 0.02\n",
      "iteration: 262420 loss: 0.0014 lr: 0.02\n",
      "iteration: 262430 loss: 0.0012 lr: 0.02\n",
      "iteration: 262440 loss: 0.0032 lr: 0.02\n",
      "iteration: 262450 loss: 0.0015 lr: 0.02\n",
      "iteration: 262460 loss: 0.0013 lr: 0.02\n",
      "iteration: 262470 loss: 0.0018 lr: 0.02\n",
      "iteration: 262480 loss: 0.0019 lr: 0.02\n",
      "iteration: 262490 loss: 0.0010 lr: 0.02\n",
      "iteration: 262500 loss: 0.0017 lr: 0.02\n",
      "iteration: 262510 loss: 0.0024 lr: 0.02\n",
      "iteration: 262520 loss: 0.0021 lr: 0.02\n",
      "iteration: 262530 loss: 0.0017 lr: 0.02\n",
      "iteration: 262540 loss: 0.0021 lr: 0.02\n",
      "iteration: 262550 loss: 0.0020 lr: 0.02\n",
      "iteration: 262560 loss: 0.0016 lr: 0.02\n",
      "iteration: 262570 loss: 0.0015 lr: 0.02\n",
      "iteration: 262580 loss: 0.0017 lr: 0.02\n",
      "iteration: 262590 loss: 0.0013 lr: 0.02\n",
      "iteration: 262600 loss: 0.0010 lr: 0.02\n",
      "iteration: 262610 loss: 0.0016 lr: 0.02\n",
      "iteration: 262620 loss: 0.0014 lr: 0.02\n",
      "iteration: 262630 loss: 0.0019 lr: 0.02\n",
      "iteration: 262640 loss: 0.0024 lr: 0.02\n",
      "iteration: 262650 loss: 0.0026 lr: 0.02\n",
      "iteration: 262660 loss: 0.0015 lr: 0.02\n",
      "iteration: 262670 loss: 0.0019 lr: 0.02\n",
      "iteration: 262680 loss: 0.0016 lr: 0.02\n",
      "iteration: 262690 loss: 0.0023 lr: 0.02\n",
      "iteration: 262700 loss: 0.0021 lr: 0.02\n",
      "iteration: 262710 loss: 0.0025 lr: 0.02\n",
      "iteration: 262720 loss: 0.0023 lr: 0.02\n",
      "iteration: 262730 loss: 0.0017 lr: 0.02\n",
      "iteration: 262740 loss: 0.0011 lr: 0.02\n",
      "iteration: 262750 loss: 0.0019 lr: 0.02\n",
      "iteration: 262760 loss: 0.0016 lr: 0.02\n",
      "iteration: 262770 loss: 0.0029 lr: 0.02\n",
      "iteration: 262780 loss: 0.0019 lr: 0.02\n",
      "iteration: 262790 loss: 0.0020 lr: 0.02\n",
      "iteration: 262800 loss: 0.0014 lr: 0.02\n",
      "iteration: 262810 loss: 0.0018 lr: 0.02\n",
      "iteration: 262820 loss: 0.0016 lr: 0.02\n",
      "iteration: 262830 loss: 0.0014 lr: 0.02\n",
      "iteration: 262840 loss: 0.0023 lr: 0.02\n",
      "iteration: 262850 loss: 0.0019 lr: 0.02\n",
      "iteration: 262860 loss: 0.0019 lr: 0.02\n",
      "iteration: 262870 loss: 0.0017 lr: 0.02\n",
      "iteration: 262880 loss: 0.0013 lr: 0.02\n",
      "iteration: 262890 loss: 0.0017 lr: 0.02\n",
      "iteration: 262900 loss: 0.0015 lr: 0.02\n",
      "iteration: 262910 loss: 0.0016 lr: 0.02\n",
      "iteration: 262920 loss: 0.0017 lr: 0.02\n",
      "iteration: 262930 loss: 0.0016 lr: 0.02\n",
      "iteration: 262940 loss: 0.0021 lr: 0.02\n",
      "iteration: 262950 loss: 0.0031 lr: 0.02\n",
      "iteration: 262960 loss: 0.0015 lr: 0.02\n",
      "iteration: 262970 loss: 0.0018 lr: 0.02\n",
      "iteration: 262980 loss: 0.0024 lr: 0.02\n",
      "iteration: 262990 loss: 0.0018 lr: 0.02\n",
      "iteration: 263000 loss: 0.0020 lr: 0.02\n",
      "iteration: 263010 loss: 0.0021 lr: 0.02\n",
      "iteration: 263020 loss: 0.0019 lr: 0.02\n",
      "iteration: 263030 loss: 0.0021 lr: 0.02\n",
      "iteration: 263040 loss: 0.0015 lr: 0.02\n",
      "iteration: 263050 loss: 0.0017 lr: 0.02\n",
      "iteration: 263060 loss: 0.0010 lr: 0.02\n",
      "iteration: 263070 loss: 0.0020 lr: 0.02\n",
      "iteration: 263080 loss: 0.0011 lr: 0.02\n",
      "iteration: 263090 loss: 0.0015 lr: 0.02\n",
      "iteration: 263100 loss: 0.0017 lr: 0.02\n",
      "iteration: 263110 loss: 0.0016 lr: 0.02\n",
      "iteration: 263120 loss: 0.0015 lr: 0.02\n",
      "iteration: 263130 loss: 0.0013 lr: 0.02\n",
      "iteration: 263140 loss: 0.0020 lr: 0.02\n",
      "iteration: 263150 loss: 0.0018 lr: 0.02\n",
      "iteration: 263160 loss: 0.0018 lr: 0.02\n",
      "iteration: 263170 loss: 0.0017 lr: 0.02\n",
      "iteration: 263180 loss: 0.0011 lr: 0.02\n",
      "iteration: 263190 loss: 0.0017 lr: 0.02\n",
      "iteration: 263200 loss: 0.0019 lr: 0.02\n",
      "iteration: 263210 loss: 0.0022 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 263220 loss: 0.0027 lr: 0.02\n",
      "iteration: 263230 loss: 0.0021 lr: 0.02\n",
      "iteration: 263240 loss: 0.0027 lr: 0.02\n",
      "iteration: 263250 loss: 0.0022 lr: 0.02\n",
      "iteration: 263260 loss: 0.0015 lr: 0.02\n",
      "iteration: 263270 loss: 0.0023 lr: 0.02\n",
      "iteration: 263280 loss: 0.0020 lr: 0.02\n",
      "iteration: 263290 loss: 0.0019 lr: 0.02\n",
      "iteration: 263300 loss: 0.0012 lr: 0.02\n",
      "iteration: 263310 loss: 0.0020 lr: 0.02\n",
      "iteration: 263320 loss: 0.0016 lr: 0.02\n",
      "iteration: 263330 loss: 0.0022 lr: 0.02\n",
      "iteration: 263340 loss: 0.0014 lr: 0.02\n",
      "iteration: 263350 loss: 0.0016 lr: 0.02\n",
      "iteration: 263360 loss: 0.0024 lr: 0.02\n",
      "iteration: 263370 loss: 0.0022 lr: 0.02\n",
      "iteration: 263380 loss: 0.0018 lr: 0.02\n",
      "iteration: 263390 loss: 0.0025 lr: 0.02\n",
      "iteration: 263400 loss: 0.0027 lr: 0.02\n",
      "iteration: 263410 loss: 0.0015 lr: 0.02\n",
      "iteration: 263420 loss: 0.0021 lr: 0.02\n",
      "iteration: 263430 loss: 0.0022 lr: 0.02\n",
      "iteration: 263440 loss: 0.0019 lr: 0.02\n",
      "iteration: 263450 loss: 0.0013 lr: 0.02\n",
      "iteration: 263460 loss: 0.0011 lr: 0.02\n",
      "iteration: 263470 loss: 0.0026 lr: 0.02\n",
      "iteration: 263480 loss: 0.0021 lr: 0.02\n",
      "iteration: 263490 loss: 0.0020 lr: 0.02\n",
      "iteration: 263500 loss: 0.0016 lr: 0.02\n",
      "iteration: 263510 loss: 0.0013 lr: 0.02\n",
      "iteration: 263520 loss: 0.0018 lr: 0.02\n",
      "iteration: 263530 loss: 0.0015 lr: 0.02\n",
      "iteration: 263540 loss: 0.0027 lr: 0.02\n",
      "iteration: 263550 loss: 0.0017 lr: 0.02\n",
      "iteration: 263560 loss: 0.0023 lr: 0.02\n",
      "iteration: 263570 loss: 0.0017 lr: 0.02\n",
      "iteration: 263580 loss: 0.0016 lr: 0.02\n",
      "iteration: 263590 loss: 0.0018 lr: 0.02\n",
      "iteration: 263600 loss: 0.0026 lr: 0.02\n",
      "iteration: 263610 loss: 0.0026 lr: 0.02\n",
      "iteration: 263620 loss: 0.0016 lr: 0.02\n",
      "iteration: 263630 loss: 0.0015 lr: 0.02\n",
      "iteration: 263640 loss: 0.0012 lr: 0.02\n",
      "iteration: 263650 loss: 0.0022 lr: 0.02\n",
      "iteration: 263660 loss: 0.0020 lr: 0.02\n",
      "iteration: 263670 loss: 0.0022 lr: 0.02\n",
      "iteration: 263680 loss: 0.0013 lr: 0.02\n",
      "iteration: 263690 loss: 0.0019 lr: 0.02\n",
      "iteration: 263700 loss: 0.0011 lr: 0.02\n",
      "iteration: 263710 loss: 0.0025 lr: 0.02\n",
      "iteration: 263720 loss: 0.0018 lr: 0.02\n",
      "iteration: 263730 loss: 0.0016 lr: 0.02\n",
      "iteration: 263740 loss: 0.0018 lr: 0.02\n",
      "iteration: 263750 loss: 0.0020 lr: 0.02\n",
      "iteration: 263760 loss: 0.0016 lr: 0.02\n",
      "iteration: 263770 loss: 0.0018 lr: 0.02\n",
      "iteration: 263780 loss: 0.0015 lr: 0.02\n",
      "iteration: 263790 loss: 0.0021 lr: 0.02\n",
      "iteration: 263800 loss: 0.0014 lr: 0.02\n",
      "iteration: 263810 loss: 0.0020 lr: 0.02\n",
      "iteration: 263820 loss: 0.0018 lr: 0.02\n",
      "iteration: 263830 loss: 0.0018 lr: 0.02\n",
      "iteration: 263840 loss: 0.0019 lr: 0.02\n",
      "iteration: 263850 loss: 0.0014 lr: 0.02\n",
      "iteration: 263860 loss: 0.0015 lr: 0.02\n",
      "iteration: 263870 loss: 0.0012 lr: 0.02\n",
      "iteration: 263880 loss: 0.0020 lr: 0.02\n",
      "iteration: 263890 loss: 0.0023 lr: 0.02\n",
      "iteration: 263900 loss: 0.0013 lr: 0.02\n",
      "iteration: 263910 loss: 0.0016 lr: 0.02\n",
      "iteration: 263920 loss: 0.0024 lr: 0.02\n",
      "iteration: 263930 loss: 0.0015 lr: 0.02\n",
      "iteration: 263940 loss: 0.0015 lr: 0.02\n",
      "iteration: 263950 loss: 0.0027 lr: 0.02\n",
      "iteration: 263960 loss: 0.0019 lr: 0.02\n",
      "iteration: 263970 loss: 0.0016 lr: 0.02\n",
      "iteration: 263980 loss: 0.0021 lr: 0.02\n",
      "iteration: 263990 loss: 0.0012 lr: 0.02\n",
      "iteration: 264000 loss: 0.0019 lr: 0.02\n",
      "iteration: 264010 loss: 0.0018 lr: 0.02\n",
      "iteration: 264020 loss: 0.0014 lr: 0.02\n",
      "iteration: 264030 loss: 0.0020 lr: 0.02\n",
      "iteration: 264040 loss: 0.0022 lr: 0.02\n",
      "iteration: 264050 loss: 0.0025 lr: 0.02\n",
      "iteration: 264060 loss: 0.0015 lr: 0.02\n",
      "iteration: 264070 loss: 0.0015 lr: 0.02\n",
      "iteration: 264080 loss: 0.0013 lr: 0.02\n",
      "iteration: 264090 loss: 0.0015 lr: 0.02\n",
      "iteration: 264100 loss: 0.0028 lr: 0.02\n",
      "iteration: 264110 loss: 0.0025 lr: 0.02\n",
      "iteration: 264120 loss: 0.0014 lr: 0.02\n",
      "iteration: 264130 loss: 0.0019 lr: 0.02\n",
      "iteration: 264140 loss: 0.0022 lr: 0.02\n",
      "iteration: 264150 loss: 0.0015 lr: 0.02\n",
      "iteration: 264160 loss: 0.0021 lr: 0.02\n",
      "iteration: 264170 loss: 0.0022 lr: 0.02\n",
      "iteration: 264180 loss: 0.0023 lr: 0.02\n",
      "iteration: 264190 loss: 0.0019 lr: 0.02\n",
      "iteration: 264200 loss: 0.0019 lr: 0.02\n",
      "iteration: 264210 loss: 0.0013 lr: 0.02\n",
      "iteration: 264220 loss: 0.0020 lr: 0.02\n",
      "iteration: 264230 loss: 0.0023 lr: 0.02\n",
      "iteration: 264240 loss: 0.0024 lr: 0.02\n",
      "iteration: 264250 loss: 0.0024 lr: 0.02\n",
      "iteration: 264260 loss: 0.0025 lr: 0.02\n",
      "iteration: 264270 loss: 0.0019 lr: 0.02\n",
      "iteration: 264280 loss: 0.0018 lr: 0.02\n",
      "iteration: 264290 loss: 0.0023 lr: 0.02\n",
      "iteration: 264300 loss: 0.0014 lr: 0.02\n",
      "iteration: 264310 loss: 0.0020 lr: 0.02\n",
      "iteration: 264320 loss: 0.0018 lr: 0.02\n",
      "iteration: 264330 loss: 0.0015 lr: 0.02\n",
      "iteration: 264340 loss: 0.0017 lr: 0.02\n",
      "iteration: 264350 loss: 0.0015 lr: 0.02\n",
      "iteration: 264360 loss: 0.0023 lr: 0.02\n",
      "iteration: 264370 loss: 0.0014 lr: 0.02\n",
      "iteration: 264380 loss: 0.0020 lr: 0.02\n",
      "iteration: 264390 loss: 0.0016 lr: 0.02\n",
      "iteration: 264400 loss: 0.0021 lr: 0.02\n",
      "iteration: 264410 loss: 0.0016 lr: 0.02\n",
      "iteration: 264420 loss: 0.0021 lr: 0.02\n",
      "iteration: 264430 loss: 0.0022 lr: 0.02\n",
      "iteration: 264440 loss: 0.0017 lr: 0.02\n",
      "iteration: 264450 loss: 0.0021 lr: 0.02\n",
      "iteration: 264460 loss: 0.0014 lr: 0.02\n",
      "iteration: 264470 loss: 0.0024 lr: 0.02\n",
      "iteration: 264480 loss: 0.0022 lr: 0.02\n",
      "iteration: 264490 loss: 0.0028 lr: 0.02\n",
      "iteration: 264500 loss: 0.0020 lr: 0.02\n",
      "iteration: 264510 loss: 0.0017 lr: 0.02\n",
      "iteration: 264520 loss: 0.0020 lr: 0.02\n",
      "iteration: 264530 loss: 0.0018 lr: 0.02\n",
      "iteration: 264540 loss: 0.0015 lr: 0.02\n",
      "iteration: 264550 loss: 0.0016 lr: 0.02\n",
      "iteration: 264560 loss: 0.0018 lr: 0.02\n",
      "iteration: 264570 loss: 0.0023 lr: 0.02\n",
      "iteration: 264580 loss: 0.0018 lr: 0.02\n",
      "iteration: 264590 loss: 0.0013 lr: 0.02\n",
      "iteration: 264600 loss: 0.0025 lr: 0.02\n",
      "iteration: 264610 loss: 0.0017 lr: 0.02\n",
      "iteration: 264620 loss: 0.0014 lr: 0.02\n",
      "iteration: 264630 loss: 0.0021 lr: 0.02\n",
      "iteration: 264640 loss: 0.0016 lr: 0.02\n",
      "iteration: 264650 loss: 0.0016 lr: 0.02\n",
      "iteration: 264660 loss: 0.0016 lr: 0.02\n",
      "iteration: 264670 loss: 0.0025 lr: 0.02\n",
      "iteration: 264680 loss: 0.0014 lr: 0.02\n",
      "iteration: 264690 loss: 0.0014 lr: 0.02\n",
      "iteration: 264700 loss: 0.0019 lr: 0.02\n",
      "iteration: 264710 loss: 0.0014 lr: 0.02\n",
      "iteration: 264720 loss: 0.0015 lr: 0.02\n",
      "iteration: 264730 loss: 0.0012 lr: 0.02\n",
      "iteration: 264740 loss: 0.0017 lr: 0.02\n",
      "iteration: 264750 loss: 0.0017 lr: 0.02\n",
      "iteration: 264760 loss: 0.0012 lr: 0.02\n",
      "iteration: 264770 loss: 0.0014 lr: 0.02\n",
      "iteration: 264780 loss: 0.0023 lr: 0.02\n",
      "iteration: 264790 loss: 0.0015 lr: 0.02\n",
      "iteration: 264800 loss: 0.0017 lr: 0.02\n",
      "iteration: 264810 loss: 0.0019 lr: 0.02\n",
      "iteration: 264820 loss: 0.0019 lr: 0.02\n",
      "iteration: 264830 loss: 0.0021 lr: 0.02\n",
      "iteration: 264840 loss: 0.0015 lr: 0.02\n",
      "iteration: 264850 loss: 0.0022 lr: 0.02\n",
      "iteration: 264860 loss: 0.0017 lr: 0.02\n",
      "iteration: 264870 loss: 0.0014 lr: 0.02\n",
      "iteration: 264880 loss: 0.0020 lr: 0.02\n",
      "iteration: 264890 loss: 0.0015 lr: 0.02\n",
      "iteration: 264900 loss: 0.0017 lr: 0.02\n",
      "iteration: 264910 loss: 0.0025 lr: 0.02\n",
      "iteration: 264920 loss: 0.0018 lr: 0.02\n",
      "iteration: 264930 loss: 0.0017 lr: 0.02\n",
      "iteration: 264940 loss: 0.0018 lr: 0.02\n",
      "iteration: 264950 loss: 0.0013 lr: 0.02\n",
      "iteration: 264960 loss: 0.0029 lr: 0.02\n",
      "iteration: 264970 loss: 0.0018 lr: 0.02\n",
      "iteration: 264980 loss: 0.0023 lr: 0.02\n",
      "iteration: 264990 loss: 0.0017 lr: 0.02\n",
      "iteration: 265000 loss: 0.0018 lr: 0.02\n",
      "iteration: 265010 loss: 0.0014 lr: 0.02\n",
      "iteration: 265020 loss: 0.0031 lr: 0.02\n",
      "iteration: 265030 loss: 0.0026 lr: 0.02\n",
      "iteration: 265040 loss: 0.0018 lr: 0.02\n",
      "iteration: 265050 loss: 0.0021 lr: 0.02\n",
      "iteration: 265060 loss: 0.0023 lr: 0.02\n",
      "iteration: 265070 loss: 0.0016 lr: 0.02\n",
      "iteration: 265080 loss: 0.0014 lr: 0.02\n",
      "iteration: 265090 loss: 0.0022 lr: 0.02\n",
      "iteration: 265100 loss: 0.0012 lr: 0.02\n",
      "iteration: 265110 loss: 0.0025 lr: 0.02\n",
      "iteration: 265120 loss: 0.0022 lr: 0.02\n",
      "iteration: 265130 loss: 0.0015 lr: 0.02\n",
      "iteration: 265140 loss: 0.0012 lr: 0.02\n",
      "iteration: 265150 loss: 0.0030 lr: 0.02\n",
      "iteration: 265160 loss: 0.0016 lr: 0.02\n",
      "iteration: 265170 loss: 0.0023 lr: 0.02\n",
      "iteration: 265180 loss: 0.0016 lr: 0.02\n",
      "iteration: 265190 loss: 0.0016 lr: 0.02\n",
      "iteration: 265200 loss: 0.0017 lr: 0.02\n",
      "iteration: 265210 loss: 0.0018 lr: 0.02\n",
      "iteration: 265220 loss: 0.0015 lr: 0.02\n",
      "iteration: 265230 loss: 0.0022 lr: 0.02\n",
      "iteration: 265240 loss: 0.0022 lr: 0.02\n",
      "iteration: 265250 loss: 0.0025 lr: 0.02\n",
      "iteration: 265260 loss: 0.0013 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 265270 loss: 0.0019 lr: 0.02\n",
      "iteration: 265280 loss: 0.0019 lr: 0.02\n",
      "iteration: 265290 loss: 0.0025 lr: 0.02\n",
      "iteration: 265300 loss: 0.0015 lr: 0.02\n",
      "iteration: 265310 loss: 0.0015 lr: 0.02\n",
      "iteration: 265320 loss: 0.0020 lr: 0.02\n",
      "iteration: 265330 loss: 0.0019 lr: 0.02\n",
      "iteration: 265340 loss: 0.0022 lr: 0.02\n",
      "iteration: 265350 loss: 0.0017 lr: 0.02\n",
      "iteration: 265360 loss: 0.0019 lr: 0.02\n",
      "iteration: 265370 loss: 0.0024 lr: 0.02\n",
      "iteration: 265380 loss: 0.0022 lr: 0.02\n",
      "iteration: 265390 loss: 0.0026 lr: 0.02\n",
      "iteration: 265400 loss: 0.0022 lr: 0.02\n",
      "iteration: 265410 loss: 0.0029 lr: 0.02\n",
      "iteration: 265420 loss: 0.0019 lr: 0.02\n",
      "iteration: 265430 loss: 0.0017 lr: 0.02\n",
      "iteration: 265440 loss: 0.0020 lr: 0.02\n",
      "iteration: 265450 loss: 0.0017 lr: 0.02\n",
      "iteration: 265460 loss: 0.0029 lr: 0.02\n",
      "iteration: 265470 loss: 0.0021 lr: 0.02\n",
      "iteration: 265480 loss: 0.0019 lr: 0.02\n",
      "iteration: 265490 loss: 0.0020 lr: 0.02\n",
      "iteration: 265500 loss: 0.0031 lr: 0.02\n",
      "iteration: 265510 loss: 0.0019 lr: 0.02\n",
      "iteration: 265520 loss: 0.0015 lr: 0.02\n",
      "iteration: 265530 loss: 0.0018 lr: 0.02\n",
      "iteration: 265540 loss: 0.0024 lr: 0.02\n",
      "iteration: 265550 loss: 0.0020 lr: 0.02\n",
      "iteration: 265560 loss: 0.0025 lr: 0.02\n",
      "iteration: 265570 loss: 0.0023 lr: 0.02\n",
      "iteration: 265580 loss: 0.0020 lr: 0.02\n",
      "iteration: 265590 loss: 0.0019 lr: 0.02\n",
      "iteration: 265600 loss: 0.0019 lr: 0.02\n",
      "iteration: 265610 loss: 0.0024 lr: 0.02\n",
      "iteration: 265620 loss: 0.0022 lr: 0.02\n",
      "iteration: 265630 loss: 0.0016 lr: 0.02\n",
      "iteration: 265640 loss: 0.0028 lr: 0.02\n",
      "iteration: 265650 loss: 0.0016 lr: 0.02\n",
      "iteration: 265660 loss: 0.0018 lr: 0.02\n",
      "iteration: 265670 loss: 0.0032 lr: 0.02\n",
      "iteration: 265680 loss: 0.0029 lr: 0.02\n",
      "iteration: 265690 loss: 0.0024 lr: 0.02\n",
      "iteration: 265700 loss: 0.0013 lr: 0.02\n",
      "iteration: 265710 loss: 0.0023 lr: 0.02\n",
      "iteration: 265720 loss: 0.0021 lr: 0.02\n",
      "iteration: 265730 loss: 0.0023 lr: 0.02\n",
      "iteration: 265740 loss: 0.0017 lr: 0.02\n",
      "iteration: 265750 loss: 0.0018 lr: 0.02\n",
      "iteration: 265760 loss: 0.0024 lr: 0.02\n",
      "iteration: 265770 loss: 0.0027 lr: 0.02\n",
      "iteration: 265780 loss: 0.0019 lr: 0.02\n",
      "iteration: 265790 loss: 0.0022 lr: 0.02\n",
      "iteration: 265800 loss: 0.0013 lr: 0.02\n",
      "iteration: 265810 loss: 0.0024 lr: 0.02\n",
      "iteration: 265820 loss: 0.0022 lr: 0.02\n",
      "iteration: 265830 loss: 0.0023 lr: 0.02\n",
      "iteration: 265840 loss: 0.0021 lr: 0.02\n",
      "iteration: 265850 loss: 0.0020 lr: 0.02\n",
      "iteration: 265860 loss: 0.0019 lr: 0.02\n",
      "iteration: 265870 loss: 0.0025 lr: 0.02\n",
      "iteration: 265880 loss: 0.0021 lr: 0.02\n",
      "iteration: 265890 loss: 0.0014 lr: 0.02\n",
      "iteration: 265900 loss: 0.0019 lr: 0.02\n",
      "iteration: 265910 loss: 0.0027 lr: 0.02\n",
      "iteration: 265920 loss: 0.0012 lr: 0.02\n",
      "iteration: 265930 loss: 0.0019 lr: 0.02\n",
      "iteration: 265940 loss: 0.0015 lr: 0.02\n",
      "iteration: 265950 loss: 0.0022 lr: 0.02\n",
      "iteration: 265960 loss: 0.0016 lr: 0.02\n",
      "iteration: 265970 loss: 0.0021 lr: 0.02\n",
      "iteration: 265980 loss: 0.0017 lr: 0.02\n",
      "iteration: 265990 loss: 0.0017 lr: 0.02\n",
      "iteration: 266000 loss: 0.0018 lr: 0.02\n",
      "iteration: 266010 loss: 0.0017 lr: 0.02\n",
      "iteration: 266020 loss: 0.0017 lr: 0.02\n",
      "iteration: 266030 loss: 0.0015 lr: 0.02\n",
      "iteration: 266040 loss: 0.0016 lr: 0.02\n",
      "iteration: 266050 loss: 0.0018 lr: 0.02\n",
      "iteration: 266060 loss: 0.0018 lr: 0.02\n",
      "iteration: 266070 loss: 0.0020 lr: 0.02\n",
      "iteration: 266080 loss: 0.0013 lr: 0.02\n",
      "iteration: 266090 loss: 0.0013 lr: 0.02\n",
      "iteration: 266100 loss: 0.0011 lr: 0.02\n",
      "iteration: 266110 loss: 0.0023 lr: 0.02\n",
      "iteration: 266120 loss: 0.0019 lr: 0.02\n",
      "iteration: 266130 loss: 0.0018 lr: 0.02\n",
      "iteration: 266140 loss: 0.0024 lr: 0.02\n",
      "iteration: 266150 loss: 0.0020 lr: 0.02\n",
      "iteration: 266160 loss: 0.0019 lr: 0.02\n",
      "iteration: 266170 loss: 0.0019 lr: 0.02\n",
      "iteration: 266180 loss: 0.0018 lr: 0.02\n",
      "iteration: 266190 loss: 0.0019 lr: 0.02\n",
      "iteration: 266200 loss: 0.0024 lr: 0.02\n",
      "iteration: 266210 loss: 0.0017 lr: 0.02\n",
      "iteration: 266220 loss: 0.0025 lr: 0.02\n",
      "iteration: 266230 loss: 0.0023 lr: 0.02\n",
      "iteration: 266240 loss: 0.0017 lr: 0.02\n",
      "iteration: 266250 loss: 0.0022 lr: 0.02\n",
      "iteration: 266260 loss: 0.0020 lr: 0.02\n",
      "iteration: 266270 loss: 0.0020 lr: 0.02\n",
      "iteration: 266280 loss: 0.0017 lr: 0.02\n",
      "iteration: 266290 loss: 0.0016 lr: 0.02\n",
      "iteration: 266300 loss: 0.0021 lr: 0.02\n",
      "iteration: 266310 loss: 0.0016 lr: 0.02\n",
      "iteration: 266320 loss: 0.0023 lr: 0.02\n",
      "iteration: 266330 loss: 0.0023 lr: 0.02\n",
      "iteration: 266340 loss: 0.0013 lr: 0.02\n",
      "iteration: 266350 loss: 0.0014 lr: 0.02\n",
      "iteration: 266360 loss: 0.0024 lr: 0.02\n",
      "iteration: 266370 loss: 0.0019 lr: 0.02\n",
      "iteration: 266380 loss: 0.0018 lr: 0.02\n",
      "iteration: 266390 loss: 0.0017 lr: 0.02\n",
      "iteration: 266400 loss: 0.0034 lr: 0.02\n",
      "iteration: 266410 loss: 0.0029 lr: 0.02\n",
      "iteration: 266420 loss: 0.0020 lr: 0.02\n",
      "iteration: 266430 loss: 0.0021 lr: 0.02\n",
      "iteration: 266440 loss: 0.0017 lr: 0.02\n",
      "iteration: 266450 loss: 0.0013 lr: 0.02\n",
      "iteration: 266460 loss: 0.0019 lr: 0.02\n",
      "iteration: 266470 loss: 0.0019 lr: 0.02\n",
      "iteration: 266480 loss: 0.0016 lr: 0.02\n",
      "iteration: 266490 loss: 0.0017 lr: 0.02\n",
      "iteration: 266500 loss: 0.0016 lr: 0.02\n",
      "iteration: 266510 loss: 0.0015 lr: 0.02\n",
      "iteration: 266520 loss: 0.0019 lr: 0.02\n",
      "iteration: 266530 loss: 0.0012 lr: 0.02\n",
      "iteration: 266540 loss: 0.0016 lr: 0.02\n",
      "iteration: 266550 loss: 0.0015 lr: 0.02\n",
      "iteration: 266560 loss: 0.0018 lr: 0.02\n",
      "iteration: 266570 loss: 0.0014 lr: 0.02\n",
      "iteration: 266580 loss: 0.0017 lr: 0.02\n",
      "iteration: 266590 loss: 0.0017 lr: 0.02\n",
      "iteration: 266600 loss: 0.0013 lr: 0.02\n",
      "iteration: 266610 loss: 0.0023 lr: 0.02\n",
      "iteration: 266620 loss: 0.0022 lr: 0.02\n",
      "iteration: 266630 loss: 0.0019 lr: 0.02\n",
      "iteration: 266640 loss: 0.0019 lr: 0.02\n",
      "iteration: 266650 loss: 0.0017 lr: 0.02\n",
      "iteration: 266660 loss: 0.0017 lr: 0.02\n",
      "iteration: 266670 loss: 0.0015 lr: 0.02\n",
      "iteration: 266680 loss: 0.0016 lr: 0.02\n",
      "iteration: 266690 loss: 0.0021 lr: 0.02\n",
      "iteration: 266700 loss: 0.0025 lr: 0.02\n",
      "iteration: 266710 loss: 0.0015 lr: 0.02\n",
      "iteration: 266720 loss: 0.0020 lr: 0.02\n",
      "iteration: 266730 loss: 0.0019 lr: 0.02\n",
      "iteration: 266740 loss: 0.0014 lr: 0.02\n",
      "iteration: 266750 loss: 0.0019 lr: 0.02\n",
      "iteration: 266760 loss: 0.0021 lr: 0.02\n",
      "iteration: 266770 loss: 0.0012 lr: 0.02\n",
      "iteration: 266780 loss: 0.0018 lr: 0.02\n",
      "iteration: 266790 loss: 0.0017 lr: 0.02\n",
      "iteration: 266800 loss: 0.0017 lr: 0.02\n",
      "iteration: 266810 loss: 0.0020 lr: 0.02\n",
      "iteration: 266820 loss: 0.0018 lr: 0.02\n",
      "iteration: 266830 loss: 0.0018 lr: 0.02\n",
      "iteration: 266840 loss: 0.0016 lr: 0.02\n",
      "iteration: 266850 loss: 0.0016 lr: 0.02\n",
      "iteration: 266860 loss: 0.0016 lr: 0.02\n",
      "iteration: 266870 loss: 0.0016 lr: 0.02\n",
      "iteration: 266880 loss: 0.0017 lr: 0.02\n",
      "iteration: 266890 loss: 0.0030 lr: 0.02\n",
      "iteration: 266900 loss: 0.0016 lr: 0.02\n",
      "iteration: 266910 loss: 0.0024 lr: 0.02\n",
      "iteration: 266920 loss: 0.0020 lr: 0.02\n",
      "iteration: 266930 loss: 0.0020 lr: 0.02\n",
      "iteration: 266940 loss: 0.0016 lr: 0.02\n",
      "iteration: 266950 loss: 0.0022 lr: 0.02\n",
      "iteration: 266960 loss: 0.0018 lr: 0.02\n",
      "iteration: 266970 loss: 0.0021 lr: 0.02\n",
      "iteration: 266980 loss: 0.0020 lr: 0.02\n",
      "iteration: 266990 loss: 0.0015 lr: 0.02\n",
      "iteration: 267000 loss: 0.0018 lr: 0.02\n",
      "iteration: 267010 loss: 0.0015 lr: 0.02\n",
      "iteration: 267020 loss: 0.0014 lr: 0.02\n",
      "iteration: 267030 loss: 0.0011 lr: 0.02\n",
      "iteration: 267040 loss: 0.0015 lr: 0.02\n",
      "iteration: 267050 loss: 0.0020 lr: 0.02\n",
      "iteration: 267060 loss: 0.0014 lr: 0.02\n",
      "iteration: 267070 loss: 0.0032 lr: 0.02\n",
      "iteration: 267080 loss: 0.0020 lr: 0.02\n",
      "iteration: 267090 loss: 0.0026 lr: 0.02\n",
      "iteration: 267100 loss: 0.0018 lr: 0.02\n",
      "iteration: 267110 loss: 0.0019 lr: 0.02\n",
      "iteration: 267120 loss: 0.0024 lr: 0.02\n",
      "iteration: 267130 loss: 0.0029 lr: 0.02\n",
      "iteration: 267140 loss: 0.0017 lr: 0.02\n",
      "iteration: 267150 loss: 0.0016 lr: 0.02\n",
      "iteration: 267160 loss: 0.0017 lr: 0.02\n",
      "iteration: 267170 loss: 0.0017 lr: 0.02\n",
      "iteration: 267180 loss: 0.0015 lr: 0.02\n",
      "iteration: 267190 loss: 0.0013 lr: 0.02\n",
      "iteration: 267200 loss: 0.0019 lr: 0.02\n",
      "iteration: 267210 loss: 0.0016 lr: 0.02\n",
      "iteration: 267220 loss: 0.0017 lr: 0.02\n",
      "iteration: 267230 loss: 0.0022 lr: 0.02\n",
      "iteration: 267240 loss: 0.0024 lr: 0.02\n",
      "iteration: 267250 loss: 0.0018 lr: 0.02\n",
      "iteration: 267260 loss: 0.0021 lr: 0.02\n",
      "iteration: 267270 loss: 0.0015 lr: 0.02\n",
      "iteration: 267280 loss: 0.0016 lr: 0.02\n",
      "iteration: 267290 loss: 0.0017 lr: 0.02\n",
      "iteration: 267300 loss: 0.0014 lr: 0.02\n",
      "iteration: 267310 loss: 0.0018 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 267320 loss: 0.0018 lr: 0.02\n",
      "iteration: 267330 loss: 0.0019 lr: 0.02\n",
      "iteration: 267340 loss: 0.0014 lr: 0.02\n",
      "iteration: 267350 loss: 0.0023 lr: 0.02\n",
      "iteration: 267360 loss: 0.0014 lr: 0.02\n",
      "iteration: 267370 loss: 0.0021 lr: 0.02\n",
      "iteration: 267380 loss: 0.0021 lr: 0.02\n",
      "iteration: 267390 loss: 0.0019 lr: 0.02\n",
      "iteration: 267400 loss: 0.0015 lr: 0.02\n",
      "iteration: 267410 loss: 0.0020 lr: 0.02\n",
      "iteration: 267420 loss: 0.0019 lr: 0.02\n",
      "iteration: 267430 loss: 0.0021 lr: 0.02\n",
      "iteration: 267440 loss: 0.0013 lr: 0.02\n",
      "iteration: 267450 loss: 0.0019 lr: 0.02\n",
      "iteration: 267460 loss: 0.0023 lr: 0.02\n",
      "iteration: 267470 loss: 0.0017 lr: 0.02\n",
      "iteration: 267480 loss: 0.0027 lr: 0.02\n",
      "iteration: 267490 loss: 0.0011 lr: 0.02\n",
      "iteration: 267500 loss: 0.0016 lr: 0.02\n",
      "iteration: 267510 loss: 0.0017 lr: 0.02\n",
      "iteration: 267520 loss: 0.0012 lr: 0.02\n",
      "iteration: 267530 loss: 0.0018 lr: 0.02\n",
      "iteration: 267540 loss: 0.0018 lr: 0.02\n",
      "iteration: 267550 loss: 0.0017 lr: 0.02\n",
      "iteration: 267560 loss: 0.0014 lr: 0.02\n",
      "iteration: 267570 loss: 0.0019 lr: 0.02\n",
      "iteration: 267580 loss: 0.0014 lr: 0.02\n",
      "iteration: 267590 loss: 0.0012 lr: 0.02\n",
      "iteration: 267600 loss: 0.0024 lr: 0.02\n",
      "iteration: 267610 loss: 0.0017 lr: 0.02\n",
      "iteration: 267620 loss: 0.0015 lr: 0.02\n",
      "iteration: 267630 loss: 0.0014 lr: 0.02\n",
      "iteration: 267640 loss: 0.0020 lr: 0.02\n",
      "iteration: 267650 loss: 0.0013 lr: 0.02\n",
      "iteration: 267660 loss: 0.0015 lr: 0.02\n",
      "iteration: 267670 loss: 0.0013 lr: 0.02\n",
      "iteration: 267680 loss: 0.0012 lr: 0.02\n",
      "iteration: 267690 loss: 0.0015 lr: 0.02\n",
      "iteration: 267700 loss: 0.0015 lr: 0.02\n",
      "iteration: 267710 loss: 0.0033 lr: 0.02\n",
      "iteration: 267720 loss: 0.0020 lr: 0.02\n",
      "iteration: 267730 loss: 0.0023 lr: 0.02\n",
      "iteration: 267740 loss: 0.0018 lr: 0.02\n",
      "iteration: 267750 loss: 0.0014 lr: 0.02\n",
      "iteration: 267760 loss: 0.0014 lr: 0.02\n",
      "iteration: 267770 loss: 0.0017 lr: 0.02\n",
      "iteration: 267780 loss: 0.0026 lr: 0.02\n",
      "iteration: 267790 loss: 0.0015 lr: 0.02\n",
      "iteration: 267800 loss: 0.0014 lr: 0.02\n",
      "iteration: 267810 loss: 0.0017 lr: 0.02\n",
      "iteration: 267820 loss: 0.0023 lr: 0.02\n",
      "iteration: 267830 loss: 0.0025 lr: 0.02\n",
      "iteration: 267840 loss: 0.0018 lr: 0.02\n",
      "iteration: 267850 loss: 0.0016 lr: 0.02\n",
      "iteration: 267860 loss: 0.0021 lr: 0.02\n",
      "iteration: 267870 loss: 0.0015 lr: 0.02\n",
      "iteration: 267880 loss: 0.0019 lr: 0.02\n",
      "iteration: 267890 loss: 0.0018 lr: 0.02\n",
      "iteration: 267900 loss: 0.0016 lr: 0.02\n",
      "iteration: 267910 loss: 0.0019 lr: 0.02\n",
      "iteration: 267920 loss: 0.0019 lr: 0.02\n",
      "iteration: 267930 loss: 0.0020 lr: 0.02\n",
      "iteration: 267940 loss: 0.0016 lr: 0.02\n",
      "iteration: 267950 loss: 0.0019 lr: 0.02\n",
      "iteration: 267960 loss: 0.0031 lr: 0.02\n",
      "iteration: 267970 loss: 0.0020 lr: 0.02\n",
      "iteration: 267980 loss: 0.0018 lr: 0.02\n",
      "iteration: 267990 loss: 0.0016 lr: 0.02\n",
      "iteration: 268000 loss: 0.0018 lr: 0.02\n",
      "iteration: 268010 loss: 0.0019 lr: 0.02\n",
      "iteration: 268020 loss: 0.0022 lr: 0.02\n",
      "iteration: 268030 loss: 0.0024 lr: 0.02\n",
      "iteration: 268040 loss: 0.0020 lr: 0.02\n",
      "iteration: 268050 loss: 0.0027 lr: 0.02\n",
      "iteration: 268060 loss: 0.0023 lr: 0.02\n",
      "iteration: 268070 loss: 0.0018 lr: 0.02\n",
      "iteration: 268080 loss: 0.0013 lr: 0.02\n",
      "iteration: 268090 loss: 0.0016 lr: 0.02\n",
      "iteration: 268100 loss: 0.0011 lr: 0.02\n",
      "iteration: 268110 loss: 0.0011 lr: 0.02\n",
      "iteration: 268120 loss: 0.0019 lr: 0.02\n",
      "iteration: 268130 loss: 0.0028 lr: 0.02\n",
      "iteration: 268140 loss: 0.0018 lr: 0.02\n",
      "iteration: 268150 loss: 0.0036 lr: 0.02\n",
      "iteration: 268160 loss: 0.0015 lr: 0.02\n",
      "iteration: 268170 loss: 0.0015 lr: 0.02\n",
      "iteration: 268180 loss: 0.0024 lr: 0.02\n",
      "iteration: 268190 loss: 0.0019 lr: 0.02\n",
      "iteration: 268200 loss: 0.0013 lr: 0.02\n",
      "iteration: 268210 loss: 0.0022 lr: 0.02\n",
      "iteration: 268220 loss: 0.0021 lr: 0.02\n",
      "iteration: 268230 loss: 0.0020 lr: 0.02\n",
      "iteration: 268240 loss: 0.0025 lr: 0.02\n",
      "iteration: 268250 loss: 0.0015 lr: 0.02\n",
      "iteration: 268260 loss: 0.0015 lr: 0.02\n",
      "iteration: 268270 loss: 0.0017 lr: 0.02\n",
      "iteration: 268280 loss: 0.0022 lr: 0.02\n",
      "iteration: 268290 loss: 0.0019 lr: 0.02\n",
      "iteration: 268300 loss: 0.0021 lr: 0.02\n",
      "iteration: 268310 loss: 0.0020 lr: 0.02\n",
      "iteration: 268320 loss: 0.0015 lr: 0.02\n",
      "iteration: 268330 loss: 0.0017 lr: 0.02\n",
      "iteration: 268340 loss: 0.0020 lr: 0.02\n",
      "iteration: 268350 loss: 0.0014 lr: 0.02\n",
      "iteration: 268360 loss: 0.0017 lr: 0.02\n",
      "iteration: 268370 loss: 0.0028 lr: 0.02\n",
      "iteration: 268380 loss: 0.0017 lr: 0.02\n",
      "iteration: 268390 loss: 0.0018 lr: 0.02\n",
      "iteration: 268400 loss: 0.0015 lr: 0.02\n",
      "iteration: 268410 loss: 0.0017 lr: 0.02\n",
      "iteration: 268420 loss: 0.0016 lr: 0.02\n",
      "iteration: 268430 loss: 0.0018 lr: 0.02\n",
      "iteration: 268440 loss: 0.0014 lr: 0.02\n",
      "iteration: 268450 loss: 0.0016 lr: 0.02\n",
      "iteration: 268460 loss: 0.0019 lr: 0.02\n",
      "iteration: 268470 loss: 0.0014 lr: 0.02\n",
      "iteration: 268480 loss: 0.0023 lr: 0.02\n",
      "iteration: 268490 loss: 0.0014 lr: 0.02\n",
      "iteration: 268500 loss: 0.0018 lr: 0.02\n",
      "iteration: 268510 loss: 0.0016 lr: 0.02\n",
      "iteration: 268520 loss: 0.0020 lr: 0.02\n",
      "iteration: 268530 loss: 0.0030 lr: 0.02\n",
      "iteration: 268540 loss: 0.0019 lr: 0.02\n",
      "iteration: 268550 loss: 0.0013 lr: 0.02\n",
      "iteration: 268560 loss: 0.0022 lr: 0.02\n",
      "iteration: 268570 loss: 0.0018 lr: 0.02\n",
      "iteration: 268580 loss: 0.0015 lr: 0.02\n",
      "iteration: 268590 loss: 0.0012 lr: 0.02\n",
      "iteration: 268600 loss: 0.0013 lr: 0.02\n",
      "iteration: 268610 loss: 0.0014 lr: 0.02\n",
      "iteration: 268620 loss: 0.0024 lr: 0.02\n",
      "iteration: 268630 loss: 0.0013 lr: 0.02\n",
      "iteration: 268640 loss: 0.0013 lr: 0.02\n",
      "iteration: 268650 loss: 0.0022 lr: 0.02\n",
      "iteration: 268660 loss: 0.0016 lr: 0.02\n",
      "iteration: 268670 loss: 0.0013 lr: 0.02\n",
      "iteration: 268680 loss: 0.0019 lr: 0.02\n",
      "iteration: 268690 loss: 0.0010 lr: 0.02\n",
      "iteration: 268700 loss: 0.0018 lr: 0.02\n",
      "iteration: 268710 loss: 0.0015 lr: 0.02\n",
      "iteration: 268720 loss: 0.0019 lr: 0.02\n",
      "iteration: 268730 loss: 0.0021 lr: 0.02\n",
      "iteration: 268740 loss: 0.0018 lr: 0.02\n",
      "iteration: 268750 loss: 0.0030 lr: 0.02\n",
      "iteration: 268760 loss: 0.0023 lr: 0.02\n",
      "iteration: 268770 loss: 0.0028 lr: 0.02\n",
      "iteration: 268780 loss: 0.0021 lr: 0.02\n",
      "iteration: 268790 loss: 0.0017 lr: 0.02\n",
      "iteration: 268800 loss: 0.0019 lr: 0.02\n",
      "iteration: 268810 loss: 0.0023 lr: 0.02\n",
      "iteration: 268820 loss: 0.0024 lr: 0.02\n",
      "iteration: 268830 loss: 0.0015 lr: 0.02\n",
      "iteration: 268840 loss: 0.0016 lr: 0.02\n",
      "iteration: 268850 loss: 0.0021 lr: 0.02\n",
      "iteration: 268860 loss: 0.0019 lr: 0.02\n",
      "iteration: 268870 loss: 0.0017 lr: 0.02\n",
      "iteration: 268880 loss: 0.0025 lr: 0.02\n",
      "iteration: 268890 loss: 0.0021 lr: 0.02\n",
      "iteration: 268900 loss: 0.0020 lr: 0.02\n",
      "iteration: 268910 loss: 0.0020 lr: 0.02\n",
      "iteration: 268920 loss: 0.0016 lr: 0.02\n",
      "iteration: 268930 loss: 0.0017 lr: 0.02\n",
      "iteration: 268940 loss: 0.0017 lr: 0.02\n",
      "iteration: 268950 loss: 0.0018 lr: 0.02\n",
      "iteration: 268960 loss: 0.0031 lr: 0.02\n",
      "iteration: 268970 loss: 0.0031 lr: 0.02\n",
      "iteration: 268980 loss: 0.0017 lr: 0.02\n",
      "iteration: 268990 loss: 0.0017 lr: 0.02\n",
      "iteration: 269000 loss: 0.0024 lr: 0.02\n",
      "iteration: 269010 loss: 0.0018 lr: 0.02\n",
      "iteration: 269020 loss: 0.0015 lr: 0.02\n",
      "iteration: 269030 loss: 0.0015 lr: 0.02\n",
      "iteration: 269040 loss: 0.0015 lr: 0.02\n",
      "iteration: 269050 loss: 0.0022 lr: 0.02\n",
      "iteration: 269060 loss: 0.0024 lr: 0.02\n",
      "iteration: 269070 loss: 0.0021 lr: 0.02\n",
      "iteration: 269080 loss: 0.0017 lr: 0.02\n",
      "iteration: 269090 loss: 0.0020 lr: 0.02\n",
      "iteration: 269100 loss: 0.0015 lr: 0.02\n",
      "iteration: 269110 loss: 0.0023 lr: 0.02\n",
      "iteration: 269120 loss: 0.0015 lr: 0.02\n",
      "iteration: 269130 loss: 0.0016 lr: 0.02\n",
      "iteration: 269140 loss: 0.0032 lr: 0.02\n",
      "iteration: 269150 loss: 0.0012 lr: 0.02\n",
      "iteration: 269160 loss: 0.0019 lr: 0.02\n",
      "iteration: 269170 loss: 0.0020 lr: 0.02\n",
      "iteration: 269180 loss: 0.0014 lr: 0.02\n",
      "iteration: 269190 loss: 0.0041 lr: 0.02\n",
      "iteration: 269200 loss: 0.0015 lr: 0.02\n",
      "iteration: 269210 loss: 0.0017 lr: 0.02\n",
      "iteration: 269220 loss: 0.0017 lr: 0.02\n",
      "iteration: 269230 loss: 0.0016 lr: 0.02\n",
      "iteration: 269240 loss: 0.0018 lr: 0.02\n",
      "iteration: 269250 loss: 0.0017 lr: 0.02\n",
      "iteration: 269260 loss: 0.0011 lr: 0.02\n",
      "iteration: 269270 loss: 0.0018 lr: 0.02\n",
      "iteration: 269280 loss: 0.0019 lr: 0.02\n",
      "iteration: 269290 loss: 0.0021 lr: 0.02\n",
      "iteration: 269300 loss: 0.0022 lr: 0.02\n",
      "iteration: 269310 loss: 0.0020 lr: 0.02\n",
      "iteration: 269320 loss: 0.0013 lr: 0.02\n",
      "iteration: 269330 loss: 0.0023 lr: 0.02\n",
      "iteration: 269340 loss: 0.0017 lr: 0.02\n",
      "iteration: 269350 loss: 0.0016 lr: 0.02\n",
      "iteration: 269360 loss: 0.0022 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 269370 loss: 0.0017 lr: 0.02\n",
      "iteration: 269380 loss: 0.0017 lr: 0.02\n",
      "iteration: 269390 loss: 0.0017 lr: 0.02\n",
      "iteration: 269400 loss: 0.0026 lr: 0.02\n",
      "iteration: 269410 loss: 0.0018 lr: 0.02\n",
      "iteration: 269420 loss: 0.0020 lr: 0.02\n",
      "iteration: 269430 loss: 0.0011 lr: 0.02\n",
      "iteration: 269440 loss: 0.0020 lr: 0.02\n",
      "iteration: 269450 loss: 0.0013 lr: 0.02\n",
      "iteration: 269460 loss: 0.0011 lr: 0.02\n",
      "iteration: 269470 loss: 0.0023 lr: 0.02\n",
      "iteration: 269480 loss: 0.0014 lr: 0.02\n",
      "iteration: 269490 loss: 0.0016 lr: 0.02\n",
      "iteration: 269500 loss: 0.0016 lr: 0.02\n",
      "iteration: 269510 loss: 0.0021 lr: 0.02\n",
      "iteration: 269520 loss: 0.0014 lr: 0.02\n",
      "iteration: 269530 loss: 0.0019 lr: 0.02\n",
      "iteration: 269540 loss: 0.0018 lr: 0.02\n",
      "iteration: 269550 loss: 0.0018 lr: 0.02\n",
      "iteration: 269560 loss: 0.0021 lr: 0.02\n",
      "iteration: 269570 loss: 0.0014 lr: 0.02\n",
      "iteration: 269580 loss: 0.0017 lr: 0.02\n",
      "iteration: 269590 loss: 0.0016 lr: 0.02\n",
      "iteration: 269600 loss: 0.0015 lr: 0.02\n",
      "iteration: 269610 loss: 0.0017 lr: 0.02\n",
      "iteration: 269620 loss: 0.0018 lr: 0.02\n",
      "iteration: 269630 loss: 0.0016 lr: 0.02\n",
      "iteration: 269640 loss: 0.0018 lr: 0.02\n",
      "iteration: 269650 loss: 0.0018 lr: 0.02\n",
      "iteration: 269660 loss: 0.0016 lr: 0.02\n",
      "iteration: 269670 loss: 0.0016 lr: 0.02\n",
      "iteration: 269680 loss: 0.0020 lr: 0.02\n",
      "iteration: 269690 loss: 0.0015 lr: 0.02\n",
      "iteration: 269700 loss: 0.0041 lr: 0.02\n",
      "iteration: 269710 loss: 0.0018 lr: 0.02\n",
      "iteration: 269720 loss: 0.0018 lr: 0.02\n",
      "iteration: 269730 loss: 0.0016 lr: 0.02\n",
      "iteration: 269740 loss: 0.0023 lr: 0.02\n",
      "iteration: 269750 loss: 0.0029 lr: 0.02\n",
      "iteration: 269760 loss: 0.0018 lr: 0.02\n",
      "iteration: 269770 loss: 0.0020 lr: 0.02\n",
      "iteration: 269780 loss: 0.0025 lr: 0.02\n",
      "iteration: 269790 loss: 0.0020 lr: 0.02\n",
      "iteration: 269800 loss: 0.0018 lr: 0.02\n",
      "iteration: 269810 loss: 0.0018 lr: 0.02\n",
      "iteration: 269820 loss: 0.0023 lr: 0.02\n",
      "iteration: 269830 loss: 0.0016 lr: 0.02\n",
      "iteration: 269840 loss: 0.0022 lr: 0.02\n",
      "iteration: 269850 loss: 0.0019 lr: 0.02\n",
      "iteration: 269860 loss: 0.0017 lr: 0.02\n",
      "iteration: 269870 loss: 0.0016 lr: 0.02\n",
      "iteration: 269880 loss: 0.0021 lr: 0.02\n",
      "iteration: 269890 loss: 0.0015 lr: 0.02\n",
      "iteration: 269900 loss: 0.0011 lr: 0.02\n",
      "iteration: 269910 loss: 0.0017 lr: 0.02\n",
      "iteration: 269920 loss: 0.0026 lr: 0.02\n",
      "iteration: 269930 loss: 0.0027 lr: 0.02\n",
      "iteration: 269940 loss: 0.0020 lr: 0.02\n",
      "iteration: 269950 loss: 0.0014 lr: 0.02\n",
      "iteration: 269960 loss: 0.0018 lr: 0.02\n",
      "iteration: 269970 loss: 0.0017 lr: 0.02\n",
      "iteration: 269980 loss: 0.0040 lr: 0.02\n",
      "iteration: 269990 loss: 0.0014 lr: 0.02\n",
      "iteration: 270000 loss: 0.0018 lr: 0.02\n",
      "iteration: 270010 loss: 0.0012 lr: 0.02\n",
      "iteration: 270020 loss: 0.0024 lr: 0.02\n",
      "iteration: 270030 loss: 0.0021 lr: 0.02\n",
      "iteration: 270040 loss: 0.0017 lr: 0.02\n",
      "iteration: 270050 loss: 0.0030 lr: 0.02\n",
      "iteration: 270060 loss: 0.0013 lr: 0.02\n",
      "iteration: 270070 loss: 0.0025 lr: 0.02\n",
      "iteration: 270080 loss: 0.0028 lr: 0.02\n",
      "iteration: 270090 loss: 0.0035 lr: 0.02\n",
      "iteration: 270100 loss: 0.0019 lr: 0.02\n",
      "iteration: 270110 loss: 0.0024 lr: 0.02\n",
      "iteration: 270120 loss: 0.0017 lr: 0.02\n",
      "iteration: 270130 loss: 0.0024 lr: 0.02\n",
      "iteration: 270140 loss: 0.0022 lr: 0.02\n",
      "iteration: 270150 loss: 0.0024 lr: 0.02\n",
      "iteration: 270160 loss: 0.0018 lr: 0.02\n",
      "iteration: 270170 loss: 0.0022 lr: 0.02\n",
      "iteration: 270180 loss: 0.0020 lr: 0.02\n",
      "iteration: 270190 loss: 0.0018 lr: 0.02\n",
      "iteration: 270200 loss: 0.0015 lr: 0.02\n",
      "iteration: 270210 loss: 0.0015 lr: 0.02\n",
      "iteration: 270220 loss: 0.0016 lr: 0.02\n",
      "iteration: 270230 loss: 0.0013 lr: 0.02\n",
      "iteration: 270240 loss: 0.0031 lr: 0.02\n",
      "iteration: 270250 loss: 0.0014 lr: 0.02\n",
      "iteration: 270260 loss: 0.0017 lr: 0.02\n",
      "iteration: 270270 loss: 0.0021 lr: 0.02\n",
      "iteration: 270280 loss: 0.0018 lr: 0.02\n",
      "iteration: 270290 loss: 0.0019 lr: 0.02\n",
      "iteration: 270300 loss: 0.0020 lr: 0.02\n",
      "iteration: 270310 loss: 0.0013 lr: 0.02\n",
      "iteration: 270320 loss: 0.0015 lr: 0.02\n",
      "iteration: 270330 loss: 0.0027 lr: 0.02\n",
      "iteration: 270340 loss: 0.0015 lr: 0.02\n",
      "iteration: 270350 loss: 0.0019 lr: 0.02\n",
      "iteration: 270360 loss: 0.0017 lr: 0.02\n",
      "iteration: 270370 loss: 0.0016 lr: 0.02\n",
      "iteration: 270380 loss: 0.0027 lr: 0.02\n",
      "iteration: 270390 loss: 0.0024 lr: 0.02\n",
      "iteration: 270400 loss: 0.0034 lr: 0.02\n",
      "iteration: 270410 loss: 0.0013 lr: 0.02\n",
      "iteration: 270420 loss: 0.0016 lr: 0.02\n",
      "iteration: 270430 loss: 0.0021 lr: 0.02\n",
      "iteration: 270440 loss: 0.0018 lr: 0.02\n",
      "iteration: 270450 loss: 0.0017 lr: 0.02\n",
      "iteration: 270460 loss: 0.0019 lr: 0.02\n",
      "iteration: 270470 loss: 0.0016 lr: 0.02\n",
      "iteration: 270480 loss: 0.0035 lr: 0.02\n",
      "iteration: 270490 loss: 0.0016 lr: 0.02\n",
      "iteration: 270500 loss: 0.0017 lr: 0.02\n",
      "iteration: 270510 loss: 0.0019 lr: 0.02\n",
      "iteration: 270520 loss: 0.0015 lr: 0.02\n",
      "iteration: 270530 loss: 0.0016 lr: 0.02\n",
      "iteration: 270540 loss: 0.0022 lr: 0.02\n",
      "iteration: 270550 loss: 0.0046 lr: 0.02\n",
      "iteration: 270560 loss: 0.0017 lr: 0.02\n",
      "iteration: 270570 loss: 0.0018 lr: 0.02\n",
      "iteration: 270580 loss: 0.0018 lr: 0.02\n",
      "iteration: 270590 loss: 0.0026 lr: 0.02\n",
      "iteration: 270600 loss: 0.0027 lr: 0.02\n",
      "iteration: 270610 loss: 0.0018 lr: 0.02\n",
      "iteration: 270620 loss: 0.0016 lr: 0.02\n",
      "iteration: 270630 loss: 0.0028 lr: 0.02\n",
      "iteration: 270640 loss: 0.0018 lr: 0.02\n",
      "iteration: 270650 loss: 0.0017 lr: 0.02\n",
      "iteration: 270660 loss: 0.0019 lr: 0.02\n",
      "iteration: 270670 loss: 0.0017 lr: 0.02\n",
      "iteration: 270680 loss: 0.0017 lr: 0.02\n",
      "iteration: 270690 loss: 0.0016 lr: 0.02\n",
      "iteration: 270700 loss: 0.0018 lr: 0.02\n",
      "iteration: 270710 loss: 0.0020 lr: 0.02\n",
      "iteration: 270720 loss: 0.0027 lr: 0.02\n",
      "iteration: 270730 loss: 0.0020 lr: 0.02\n",
      "iteration: 270740 loss: 0.0022 lr: 0.02\n",
      "iteration: 270750 loss: 0.0018 lr: 0.02\n",
      "iteration: 270760 loss: 0.0020 lr: 0.02\n",
      "iteration: 270770 loss: 0.0024 lr: 0.02\n",
      "iteration: 270780 loss: 0.0020 lr: 0.02\n",
      "iteration: 270790 loss: 0.0023 lr: 0.02\n",
      "iteration: 270800 loss: 0.0019 lr: 0.02\n",
      "iteration: 270810 loss: 0.0015 lr: 0.02\n",
      "iteration: 270820 loss: 0.0015 lr: 0.02\n",
      "iteration: 270830 loss: 0.0016 lr: 0.02\n",
      "iteration: 270840 loss: 0.0023 lr: 0.02\n",
      "iteration: 270850 loss: 0.0019 lr: 0.02\n",
      "iteration: 270860 loss: 0.0014 lr: 0.02\n",
      "iteration: 270870 loss: 0.0032 lr: 0.02\n",
      "iteration: 270880 loss: 0.0016 lr: 0.02\n",
      "iteration: 270890 loss: 0.0023 lr: 0.02\n",
      "iteration: 270900 loss: 0.0017 lr: 0.02\n",
      "iteration: 270910 loss: 0.0025 lr: 0.02\n",
      "iteration: 270920 loss: 0.0019 lr: 0.02\n",
      "iteration: 270930 loss: 0.0040 lr: 0.02\n",
      "iteration: 270940 loss: 0.0016 lr: 0.02\n",
      "iteration: 270950 loss: 0.0017 lr: 0.02\n",
      "iteration: 270960 loss: 0.0013 lr: 0.02\n",
      "iteration: 270970 loss: 0.0015 lr: 0.02\n",
      "iteration: 270980 loss: 0.0019 lr: 0.02\n",
      "iteration: 270990 loss: 0.0017 lr: 0.02\n",
      "iteration: 271000 loss: 0.0016 lr: 0.02\n",
      "iteration: 271010 loss: 0.0021 lr: 0.02\n",
      "iteration: 271020 loss: 0.0011 lr: 0.02\n",
      "iteration: 271030 loss: 0.0036 lr: 0.02\n",
      "iteration: 271040 loss: 0.0023 lr: 0.02\n",
      "iteration: 271050 loss: 0.0017 lr: 0.02\n",
      "iteration: 271060 loss: 0.0029 lr: 0.02\n",
      "iteration: 271070 loss: 0.0026 lr: 0.02\n",
      "iteration: 271080 loss: 0.0022 lr: 0.02\n",
      "iteration: 271090 loss: 0.0029 lr: 0.02\n",
      "iteration: 271100 loss: 0.0018 lr: 0.02\n",
      "iteration: 271110 loss: 0.0017 lr: 0.02\n",
      "iteration: 271120 loss: 0.0015 lr: 0.02\n",
      "iteration: 271130 loss: 0.0017 lr: 0.02\n",
      "iteration: 271140 loss: 0.0015 lr: 0.02\n",
      "iteration: 271150 loss: 0.0023 lr: 0.02\n",
      "iteration: 271160 loss: 0.0015 lr: 0.02\n",
      "iteration: 271170 loss: 0.0015 lr: 0.02\n",
      "iteration: 271180 loss: 0.0018 lr: 0.02\n",
      "iteration: 271190 loss: 0.0018 lr: 0.02\n",
      "iteration: 271200 loss: 0.0012 lr: 0.02\n",
      "iteration: 271210 loss: 0.0016 lr: 0.02\n",
      "iteration: 271220 loss: 0.0022 lr: 0.02\n",
      "iteration: 271230 loss: 0.0023 lr: 0.02\n",
      "iteration: 271240 loss: 0.0017 lr: 0.02\n",
      "iteration: 271250 loss: 0.0017 lr: 0.02\n",
      "iteration: 271260 loss: 0.0020 lr: 0.02\n",
      "iteration: 271270 loss: 0.0020 lr: 0.02\n",
      "iteration: 271280 loss: 0.0018 lr: 0.02\n",
      "iteration: 271290 loss: 0.0018 lr: 0.02\n",
      "iteration: 271300 loss: 0.0013 lr: 0.02\n",
      "iteration: 271310 loss: 0.0013 lr: 0.02\n",
      "iteration: 271320 loss: 0.0020 lr: 0.02\n",
      "iteration: 271330 loss: 0.0025 lr: 0.02\n",
      "iteration: 271340 loss: 0.0017 lr: 0.02\n",
      "iteration: 271350 loss: 0.0015 lr: 0.02\n",
      "iteration: 271360 loss: 0.0032 lr: 0.02\n",
      "iteration: 271370 loss: 0.0012 lr: 0.02\n",
      "iteration: 271380 loss: 0.0014 lr: 0.02\n",
      "iteration: 271390 loss: 0.0017 lr: 0.02\n",
      "iteration: 271400 loss: 0.0016 lr: 0.02\n",
      "iteration: 271410 loss: 0.0018 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 271420 loss: 0.0015 lr: 0.02\n",
      "iteration: 271430 loss: 0.0014 lr: 0.02\n",
      "iteration: 271440 loss: 0.0018 lr: 0.02\n",
      "iteration: 271450 loss: 0.0011 lr: 0.02\n",
      "iteration: 271460 loss: 0.0011 lr: 0.02\n",
      "iteration: 271470 loss: 0.0025 lr: 0.02\n",
      "iteration: 271480 loss: 0.0029 lr: 0.02\n",
      "iteration: 271490 loss: 0.0014 lr: 0.02\n",
      "iteration: 271500 loss: 0.0022 lr: 0.02\n",
      "iteration: 271510 loss: 0.0014 lr: 0.02\n",
      "iteration: 271520 loss: 0.0022 lr: 0.02\n",
      "iteration: 271530 loss: 0.0020 lr: 0.02\n",
      "iteration: 271540 loss: 0.0019 lr: 0.02\n",
      "iteration: 271550 loss: 0.0015 lr: 0.02\n",
      "iteration: 271560 loss: 0.0017 lr: 0.02\n",
      "iteration: 271570 loss: 0.0024 lr: 0.02\n",
      "iteration: 271580 loss: 0.0015 lr: 0.02\n",
      "iteration: 271590 loss: 0.0015 lr: 0.02\n",
      "iteration: 271600 loss: 0.0022 lr: 0.02\n",
      "iteration: 271610 loss: 0.0018 lr: 0.02\n",
      "iteration: 271620 loss: 0.0022 lr: 0.02\n",
      "iteration: 271630 loss: 0.0016 lr: 0.02\n",
      "iteration: 271640 loss: 0.0016 lr: 0.02\n",
      "iteration: 271650 loss: 0.0022 lr: 0.02\n",
      "iteration: 271660 loss: 0.0028 lr: 0.02\n",
      "iteration: 271670 loss: 0.0019 lr: 0.02\n",
      "iteration: 271680 loss: 0.0018 lr: 0.02\n",
      "iteration: 271690 loss: 0.0015 lr: 0.02\n",
      "iteration: 271700 loss: 0.0020 lr: 0.02\n",
      "iteration: 271710 loss: 0.0015 lr: 0.02\n",
      "iteration: 271720 loss: 0.0021 lr: 0.02\n",
      "iteration: 271730 loss: 0.0016 lr: 0.02\n",
      "iteration: 271740 loss: 0.0019 lr: 0.02\n",
      "iteration: 271750 loss: 0.0016 lr: 0.02\n",
      "iteration: 271760 loss: 0.0012 lr: 0.02\n",
      "iteration: 271770 loss: 0.0016 lr: 0.02\n",
      "iteration: 271780 loss: 0.0015 lr: 0.02\n",
      "iteration: 271790 loss: 0.0023 lr: 0.02\n",
      "iteration: 271800 loss: 0.0013 lr: 0.02\n",
      "iteration: 271810 loss: 0.0014 lr: 0.02\n",
      "iteration: 271820 loss: 0.0014 lr: 0.02\n",
      "iteration: 271830 loss: 0.0019 lr: 0.02\n",
      "iteration: 271840 loss: 0.0016 lr: 0.02\n",
      "iteration: 271850 loss: 0.0025 lr: 0.02\n",
      "iteration: 271860 loss: 0.0014 lr: 0.02\n",
      "iteration: 271870 loss: 0.0019 lr: 0.02\n",
      "iteration: 271880 loss: 0.0016 lr: 0.02\n",
      "iteration: 271890 loss: 0.0018 lr: 0.02\n",
      "iteration: 271900 loss: 0.0018 lr: 0.02\n",
      "iteration: 271910 loss: 0.0017 lr: 0.02\n",
      "iteration: 271920 loss: 0.0020 lr: 0.02\n",
      "iteration: 271930 loss: 0.0028 lr: 0.02\n",
      "iteration: 271940 loss: 0.0017 lr: 0.02\n",
      "iteration: 271950 loss: 0.0024 lr: 0.02\n",
      "iteration: 271960 loss: 0.0019 lr: 0.02\n",
      "iteration: 271970 loss: 0.0016 lr: 0.02\n",
      "iteration: 271980 loss: 0.0017 lr: 0.02\n",
      "iteration: 271990 loss: 0.0019 lr: 0.02\n",
      "iteration: 272000 loss: 0.0021 lr: 0.02\n",
      "iteration: 272010 loss: 0.0022 lr: 0.02\n",
      "iteration: 272020 loss: 0.0015 lr: 0.02\n",
      "iteration: 272030 loss: 0.0015 lr: 0.02\n",
      "iteration: 272040 loss: 0.0021 lr: 0.02\n",
      "iteration: 272050 loss: 0.0019 lr: 0.02\n",
      "iteration: 272060 loss: 0.0024 lr: 0.02\n",
      "iteration: 272070 loss: 0.0021 lr: 0.02\n",
      "iteration: 272080 loss: 0.0019 lr: 0.02\n",
      "iteration: 272090 loss: 0.0018 lr: 0.02\n",
      "iteration: 272100 loss: 0.0018 lr: 0.02\n",
      "iteration: 272110 loss: 0.0016 lr: 0.02\n",
      "iteration: 272120 loss: 0.0018 lr: 0.02\n",
      "iteration: 272130 loss: 0.0013 lr: 0.02\n",
      "iteration: 272140 loss: 0.0016 lr: 0.02\n",
      "iteration: 272150 loss: 0.0020 lr: 0.02\n",
      "iteration: 272160 loss: 0.0018 lr: 0.02\n",
      "iteration: 272170 loss: 0.0029 lr: 0.02\n",
      "iteration: 272180 loss: 0.0014 lr: 0.02\n",
      "iteration: 272190 loss: 0.0026 lr: 0.02\n",
      "iteration: 272200 loss: 0.0014 lr: 0.02\n",
      "iteration: 272210 loss: 0.0016 lr: 0.02\n",
      "iteration: 272220 loss: 0.0020 lr: 0.02\n",
      "iteration: 272230 loss: 0.0015 lr: 0.02\n",
      "iteration: 272240 loss: 0.0021 lr: 0.02\n",
      "iteration: 272250 loss: 0.0017 lr: 0.02\n",
      "iteration: 272260 loss: 0.0016 lr: 0.02\n",
      "iteration: 272270 loss: 0.0020 lr: 0.02\n",
      "iteration: 272280 loss: 0.0016 lr: 0.02\n",
      "iteration: 272290 loss: 0.0017 lr: 0.02\n",
      "iteration: 272300 loss: 0.0018 lr: 0.02\n",
      "iteration: 272310 loss: 0.0012 lr: 0.02\n",
      "iteration: 272320 loss: 0.0016 lr: 0.02\n",
      "iteration: 272330 loss: 0.0013 lr: 0.02\n",
      "iteration: 272340 loss: 0.0017 lr: 0.02\n",
      "iteration: 272350 loss: 0.0024 lr: 0.02\n",
      "iteration: 272360 loss: 0.0015 lr: 0.02\n",
      "iteration: 272370 loss: 0.0012 lr: 0.02\n",
      "iteration: 272380 loss: 0.0014 lr: 0.02\n",
      "iteration: 272390 loss: 0.0018 lr: 0.02\n",
      "iteration: 272400 loss: 0.0018 lr: 0.02\n",
      "iteration: 272410 loss: 0.0025 lr: 0.02\n",
      "iteration: 272420 loss: 0.0013 lr: 0.02\n",
      "iteration: 272430 loss: 0.0018 lr: 0.02\n",
      "iteration: 272440 loss: 0.0020 lr: 0.02\n",
      "iteration: 272450 loss: 0.0017 lr: 0.02\n",
      "iteration: 272460 loss: 0.0015 lr: 0.02\n",
      "iteration: 272470 loss: 0.0018 lr: 0.02\n",
      "iteration: 272480 loss: 0.0022 lr: 0.02\n",
      "iteration: 272490 loss: 0.0041 lr: 0.02\n",
      "iteration: 272500 loss: 0.0018 lr: 0.02\n",
      "iteration: 272510 loss: 0.0028 lr: 0.02\n",
      "iteration: 272520 loss: 0.0015 lr: 0.02\n",
      "iteration: 272530 loss: 0.0017 lr: 0.02\n",
      "iteration: 272540 loss: 0.0016 lr: 0.02\n",
      "iteration: 272550 loss: 0.0018 lr: 0.02\n",
      "iteration: 272560 loss: 0.0016 lr: 0.02\n",
      "iteration: 272570 loss: 0.0016 lr: 0.02\n",
      "iteration: 272580 loss: 0.0019 lr: 0.02\n",
      "iteration: 272590 loss: 0.0018 lr: 0.02\n",
      "iteration: 272600 loss: 0.0014 lr: 0.02\n",
      "iteration: 272610 loss: 0.0016 lr: 0.02\n",
      "iteration: 272620 loss: 0.0022 lr: 0.02\n",
      "iteration: 272630 loss: 0.0014 lr: 0.02\n",
      "iteration: 272640 loss: 0.0021 lr: 0.02\n",
      "iteration: 272650 loss: 0.0014 lr: 0.02\n",
      "iteration: 272660 loss: 0.0013 lr: 0.02\n",
      "iteration: 272670 loss: 0.0018 lr: 0.02\n",
      "iteration: 272680 loss: 0.0022 lr: 0.02\n",
      "iteration: 272690 loss: 0.0018 lr: 0.02\n",
      "iteration: 272700 loss: 0.0013 lr: 0.02\n",
      "iteration: 272710 loss: 0.0015 lr: 0.02\n",
      "iteration: 272720 loss: 0.0019 lr: 0.02\n",
      "iteration: 272730 loss: 0.0023 lr: 0.02\n",
      "iteration: 272740 loss: 0.0023 lr: 0.02\n",
      "iteration: 272750 loss: 0.0023 lr: 0.02\n",
      "iteration: 272760 loss: 0.0021 lr: 0.02\n",
      "iteration: 272770 loss: 0.0015 lr: 0.02\n",
      "iteration: 272780 loss: 0.0018 lr: 0.02\n",
      "iteration: 272790 loss: 0.0020 lr: 0.02\n",
      "iteration: 272800 loss: 0.0022 lr: 0.02\n",
      "iteration: 272810 loss: 0.0019 lr: 0.02\n",
      "iteration: 272820 loss: 0.0019 lr: 0.02\n",
      "iteration: 272830 loss: 0.0030 lr: 0.02\n",
      "iteration: 272840 loss: 0.0027 lr: 0.02\n",
      "iteration: 272850 loss: 0.0017 lr: 0.02\n",
      "iteration: 272860 loss: 0.0013 lr: 0.02\n",
      "iteration: 272870 loss: 0.0018 lr: 0.02\n",
      "iteration: 272880 loss: 0.0016 lr: 0.02\n",
      "iteration: 272890 loss: 0.0014 lr: 0.02\n",
      "iteration: 272900 loss: 0.0016 lr: 0.02\n",
      "iteration: 272910 loss: 0.0017 lr: 0.02\n",
      "iteration: 272920 loss: 0.0017 lr: 0.02\n",
      "iteration: 272930 loss: 0.0013 lr: 0.02\n",
      "iteration: 272940 loss: 0.0018 lr: 0.02\n",
      "iteration: 272950 loss: 0.0023 lr: 0.02\n",
      "iteration: 272960 loss: 0.0017 lr: 0.02\n",
      "iteration: 272970 loss: 0.0017 lr: 0.02\n",
      "iteration: 272980 loss: 0.0017 lr: 0.02\n",
      "iteration: 272990 loss: 0.0019 lr: 0.02\n",
      "iteration: 273000 loss: 0.0020 lr: 0.02\n",
      "iteration: 273010 loss: 0.0018 lr: 0.02\n",
      "iteration: 273020 loss: 0.0034 lr: 0.02\n",
      "iteration: 273030 loss: 0.0014 lr: 0.02\n",
      "iteration: 273040 loss: 0.0017 lr: 0.02\n",
      "iteration: 273050 loss: 0.0020 lr: 0.02\n",
      "iteration: 273060 loss: 0.0014 lr: 0.02\n",
      "iteration: 273070 loss: 0.0017 lr: 0.02\n",
      "iteration: 273080 loss: 0.0014 lr: 0.02\n",
      "iteration: 273090 loss: 0.0019 lr: 0.02\n",
      "iteration: 273100 loss: 0.0010 lr: 0.02\n",
      "iteration: 273110 loss: 0.0021 lr: 0.02\n",
      "iteration: 273120 loss: 0.0019 lr: 0.02\n",
      "iteration: 273130 loss: 0.0013 lr: 0.02\n",
      "iteration: 273140 loss: 0.0017 lr: 0.02\n",
      "iteration: 273150 loss: 0.0031 lr: 0.02\n",
      "iteration: 273160 loss: 0.0023 lr: 0.02\n",
      "iteration: 273170 loss: 0.0021 lr: 0.02\n",
      "iteration: 273180 loss: 0.0017 lr: 0.02\n",
      "iteration: 273190 loss: 0.0020 lr: 0.02\n",
      "iteration: 273200 loss: 0.0021 lr: 0.02\n",
      "iteration: 273210 loss: 0.0015 lr: 0.02\n",
      "iteration: 273220 loss: 0.0013 lr: 0.02\n",
      "iteration: 273230 loss: 0.0026 lr: 0.02\n",
      "iteration: 273240 loss: 0.0020 lr: 0.02\n",
      "iteration: 273250 loss: 0.0023 lr: 0.02\n",
      "iteration: 273260 loss: 0.0016 lr: 0.02\n",
      "iteration: 273270 loss: 0.0017 lr: 0.02\n",
      "iteration: 273280 loss: 0.0015 lr: 0.02\n",
      "iteration: 273290 loss: 0.0032 lr: 0.02\n",
      "iteration: 273300 loss: 0.0019 lr: 0.02\n",
      "iteration: 273310 loss: 0.0018 lr: 0.02\n",
      "iteration: 273320 loss: 0.0019 lr: 0.02\n",
      "iteration: 273330 loss: 0.0020 lr: 0.02\n",
      "iteration: 273340 loss: 0.0019 lr: 0.02\n",
      "iteration: 273350 loss: 0.0011 lr: 0.02\n",
      "iteration: 273360 loss: 0.0026 lr: 0.02\n",
      "iteration: 273370 loss: 0.0025 lr: 0.02\n",
      "iteration: 273380 loss: 0.0015 lr: 0.02\n",
      "iteration: 273390 loss: 0.0021 lr: 0.02\n",
      "iteration: 273400 loss: 0.0021 lr: 0.02\n",
      "iteration: 273410 loss: 0.0016 lr: 0.02\n",
      "iteration: 273420 loss: 0.0025 lr: 0.02\n",
      "iteration: 273430 loss: 0.0020 lr: 0.02\n",
      "iteration: 273440 loss: 0.0019 lr: 0.02\n",
      "iteration: 273450 loss: 0.0038 lr: 0.02\n",
      "iteration: 273460 loss: 0.0021 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 273470 loss: 0.0022 lr: 0.02\n",
      "iteration: 273480 loss: 0.0015 lr: 0.02\n",
      "iteration: 273490 loss: 0.0024 lr: 0.02\n",
      "iteration: 273500 loss: 0.0014 lr: 0.02\n",
      "iteration: 273510 loss: 0.0017 lr: 0.02\n",
      "iteration: 273520 loss: 0.0015 lr: 0.02\n",
      "iteration: 273530 loss: 0.0015 lr: 0.02\n",
      "iteration: 273540 loss: 0.0019 lr: 0.02\n",
      "iteration: 273550 loss: 0.0016 lr: 0.02\n",
      "iteration: 273560 loss: 0.0019 lr: 0.02\n",
      "iteration: 273570 loss: 0.0034 lr: 0.02\n",
      "iteration: 273580 loss: 0.0018 lr: 0.02\n",
      "iteration: 273590 loss: 0.0017 lr: 0.02\n",
      "iteration: 273600 loss: 0.0017 lr: 0.02\n",
      "iteration: 273610 loss: 0.0030 lr: 0.02\n",
      "iteration: 273620 loss: 0.0019 lr: 0.02\n",
      "iteration: 273630 loss: 0.0015 lr: 0.02\n",
      "iteration: 273640 loss: 0.0015 lr: 0.02\n",
      "iteration: 273650 loss: 0.0015 lr: 0.02\n",
      "iteration: 273660 loss: 0.0021 lr: 0.02\n",
      "iteration: 273670 loss: 0.0019 lr: 0.02\n",
      "iteration: 273680 loss: 0.0020 lr: 0.02\n",
      "iteration: 273690 loss: 0.0016 lr: 0.02\n",
      "iteration: 273700 loss: 0.0019 lr: 0.02\n",
      "iteration: 273710 loss: 0.0016 lr: 0.02\n",
      "iteration: 273720 loss: 0.0018 lr: 0.02\n",
      "iteration: 273730 loss: 0.0015 lr: 0.02\n",
      "iteration: 273740 loss: 0.0012 lr: 0.02\n",
      "iteration: 273750 loss: 0.0023 lr: 0.02\n",
      "iteration: 273760 loss: 0.0020 lr: 0.02\n",
      "iteration: 273770 loss: 0.0014 lr: 0.02\n",
      "iteration: 273780 loss: 0.0011 lr: 0.02\n",
      "iteration: 273790 loss: 0.0017 lr: 0.02\n",
      "iteration: 273800 loss: 0.0017 lr: 0.02\n",
      "iteration: 273810 loss: 0.0012 lr: 0.02\n",
      "iteration: 273820 loss: 0.0019 lr: 0.02\n",
      "iteration: 273830 loss: 0.0018 lr: 0.02\n",
      "iteration: 273840 loss: 0.0014 lr: 0.02\n",
      "iteration: 273850 loss: 0.0015 lr: 0.02\n",
      "iteration: 273860 loss: 0.0024 lr: 0.02\n",
      "iteration: 273870 loss: 0.0013 lr: 0.02\n",
      "iteration: 273880 loss: 0.0025 lr: 0.02\n",
      "iteration: 273890 loss: 0.0018 lr: 0.02\n",
      "iteration: 273900 loss: 0.0021 lr: 0.02\n",
      "iteration: 273910 loss: 0.0028 lr: 0.02\n",
      "iteration: 273920 loss: 0.0018 lr: 0.02\n",
      "iteration: 273930 loss: 0.0019 lr: 0.02\n",
      "iteration: 273940 loss: 0.0023 lr: 0.02\n",
      "iteration: 273950 loss: 0.0014 lr: 0.02\n",
      "iteration: 273960 loss: 0.0015 lr: 0.02\n",
      "iteration: 273970 loss: 0.0022 lr: 0.02\n",
      "iteration: 273980 loss: 0.0016 lr: 0.02\n",
      "iteration: 273990 loss: 0.0017 lr: 0.02\n",
      "iteration: 274000 loss: 0.0012 lr: 0.02\n",
      "iteration: 274010 loss: 0.0019 lr: 0.02\n",
      "iteration: 274020 loss: 0.0017 lr: 0.02\n",
      "iteration: 274030 loss: 0.0018 lr: 0.02\n",
      "iteration: 274040 loss: 0.0014 lr: 0.02\n",
      "iteration: 274050 loss: 0.0012 lr: 0.02\n",
      "iteration: 274060 loss: 0.0015 lr: 0.02\n",
      "iteration: 274070 loss: 0.0015 lr: 0.02\n",
      "iteration: 274080 loss: 0.0017 lr: 0.02\n",
      "iteration: 274090 loss: 0.0014 lr: 0.02\n",
      "iteration: 274100 loss: 0.0014 lr: 0.02\n",
      "iteration: 274110 loss: 0.0017 lr: 0.02\n",
      "iteration: 274120 loss: 0.0017 lr: 0.02\n",
      "iteration: 274130 loss: 0.0015 lr: 0.02\n",
      "iteration: 274140 loss: 0.0021 lr: 0.02\n",
      "iteration: 274150 loss: 0.0015 lr: 0.02\n",
      "iteration: 274160 loss: 0.0018 lr: 0.02\n",
      "iteration: 274170 loss: 0.0011 lr: 0.02\n",
      "iteration: 274180 loss: 0.0016 lr: 0.02\n",
      "iteration: 274190 loss: 0.0020 lr: 0.02\n",
      "iteration: 274200 loss: 0.0019 lr: 0.02\n",
      "iteration: 274210 loss: 0.0026 lr: 0.02\n",
      "iteration: 274220 loss: 0.0017 lr: 0.02\n",
      "iteration: 274230 loss: 0.0019 lr: 0.02\n",
      "iteration: 274240 loss: 0.0027 lr: 0.02\n",
      "iteration: 274250 loss: 0.0018 lr: 0.02\n",
      "iteration: 274260 loss: 0.0018 lr: 0.02\n",
      "iteration: 274270 loss: 0.0020 lr: 0.02\n",
      "iteration: 274280 loss: 0.0016 lr: 0.02\n",
      "iteration: 274290 loss: 0.0025 lr: 0.02\n",
      "iteration: 274300 loss: 0.0014 lr: 0.02\n",
      "iteration: 274310 loss: 0.0019 lr: 0.02\n",
      "iteration: 274320 loss: 0.0016 lr: 0.02\n",
      "iteration: 274330 loss: 0.0019 lr: 0.02\n",
      "iteration: 274340 loss: 0.0015 lr: 0.02\n",
      "iteration: 274350 loss: 0.0014 lr: 0.02\n",
      "iteration: 274360 loss: 0.0014 lr: 0.02\n",
      "iteration: 274370 loss: 0.0014 lr: 0.02\n",
      "iteration: 274380 loss: 0.0017 lr: 0.02\n",
      "iteration: 274390 loss: 0.0018 lr: 0.02\n",
      "iteration: 274400 loss: 0.0011 lr: 0.02\n",
      "iteration: 274410 loss: 0.0012 lr: 0.02\n",
      "iteration: 274420 loss: 0.0015 lr: 0.02\n",
      "iteration: 274430 loss: 0.0014 lr: 0.02\n",
      "iteration: 274440 loss: 0.0016 lr: 0.02\n",
      "iteration: 274450 loss: 0.0019 lr: 0.02\n",
      "iteration: 274460 loss: 0.0021 lr: 0.02\n",
      "iteration: 274470 loss: 0.0022 lr: 0.02\n",
      "iteration: 274480 loss: 0.0022 lr: 0.02\n",
      "iteration: 274490 loss: 0.0022 lr: 0.02\n",
      "iteration: 274500 loss: 0.0017 lr: 0.02\n",
      "iteration: 274510 loss: 0.0016 lr: 0.02\n",
      "iteration: 274520 loss: 0.0024 lr: 0.02\n",
      "iteration: 274530 loss: 0.0014 lr: 0.02\n",
      "iteration: 274540 loss: 0.0015 lr: 0.02\n",
      "iteration: 274550 loss: 0.0023 lr: 0.02\n",
      "iteration: 274560 loss: 0.0019 lr: 0.02\n",
      "iteration: 274570 loss: 0.0015 lr: 0.02\n",
      "iteration: 274580 loss: 0.0020 lr: 0.02\n",
      "iteration: 274590 loss: 0.0020 lr: 0.02\n",
      "iteration: 274600 loss: 0.0016 lr: 0.02\n",
      "iteration: 274610 loss: 0.0021 lr: 0.02\n",
      "iteration: 274620 loss: 0.0014 lr: 0.02\n",
      "iteration: 274630 loss: 0.0016 lr: 0.02\n",
      "iteration: 274640 loss: 0.0015 lr: 0.02\n",
      "iteration: 274650 loss: 0.0017 lr: 0.02\n",
      "iteration: 274660 loss: 0.0016 lr: 0.02\n",
      "iteration: 274670 loss: 0.0021 lr: 0.02\n",
      "iteration: 274680 loss: 0.0025 lr: 0.02\n",
      "iteration: 274690 loss: 0.0020 lr: 0.02\n",
      "iteration: 274700 loss: 0.0022 lr: 0.02\n",
      "iteration: 274710 loss: 0.0015 lr: 0.02\n",
      "iteration: 274720 loss: 0.0029 lr: 0.02\n",
      "iteration: 274730 loss: 0.0019 lr: 0.02\n",
      "iteration: 274740 loss: 0.0021 lr: 0.02\n",
      "iteration: 274750 loss: 0.0015 lr: 0.02\n",
      "iteration: 274760 loss: 0.0019 lr: 0.02\n",
      "iteration: 274770 loss: 0.0016 lr: 0.02\n",
      "iteration: 274780 loss: 0.0015 lr: 0.02\n",
      "iteration: 274790 loss: 0.0020 lr: 0.02\n",
      "iteration: 274800 loss: 0.0015 lr: 0.02\n",
      "iteration: 274810 loss: 0.0021 lr: 0.02\n",
      "iteration: 274820 loss: 0.0020 lr: 0.02\n",
      "iteration: 274830 loss: 0.0016 lr: 0.02\n",
      "iteration: 274840 loss: 0.0015 lr: 0.02\n",
      "iteration: 274850 loss: 0.0020 lr: 0.02\n",
      "iteration: 274860 loss: 0.0016 lr: 0.02\n",
      "iteration: 274870 loss: 0.0019 lr: 0.02\n",
      "iteration: 274880 loss: 0.0024 lr: 0.02\n",
      "iteration: 274890 loss: 0.0024 lr: 0.02\n",
      "iteration: 274900 loss: 0.0018 lr: 0.02\n",
      "iteration: 274910 loss: 0.0018 lr: 0.02\n",
      "iteration: 274920 loss: 0.0015 lr: 0.02\n",
      "iteration: 274930 loss: 0.0023 lr: 0.02\n",
      "iteration: 274940 loss: 0.0019 lr: 0.02\n",
      "iteration: 274950 loss: 0.0015 lr: 0.02\n",
      "iteration: 274960 loss: 0.0020 lr: 0.02\n",
      "iteration: 274970 loss: 0.0020 lr: 0.02\n",
      "iteration: 274980 loss: 0.0018 lr: 0.02\n",
      "iteration: 274990 loss: 0.0020 lr: 0.02\n",
      "iteration: 275000 loss: 0.0025 lr: 0.02\n",
      "iteration: 275010 loss: 0.0017 lr: 0.02\n",
      "iteration: 275020 loss: 0.0014 lr: 0.02\n",
      "iteration: 275030 loss: 0.0021 lr: 0.02\n",
      "iteration: 275040 loss: 0.0019 lr: 0.02\n",
      "iteration: 275050 loss: 0.0017 lr: 0.02\n",
      "iteration: 275060 loss: 0.0017 lr: 0.02\n",
      "iteration: 275070 loss: 0.0014 lr: 0.02\n",
      "iteration: 275080 loss: 0.0021 lr: 0.02\n",
      "iteration: 275090 loss: 0.0016 lr: 0.02\n",
      "iteration: 275100 loss: 0.0024 lr: 0.02\n",
      "iteration: 275110 loss: 0.0033 lr: 0.02\n",
      "iteration: 275120 loss: 0.0022 lr: 0.02\n",
      "iteration: 275130 loss: 0.0024 lr: 0.02\n",
      "iteration: 275140 loss: 0.0040 lr: 0.02\n",
      "iteration: 275150 loss: 0.0023 lr: 0.02\n",
      "iteration: 275160 loss: 0.0019 lr: 0.02\n",
      "iteration: 275170 loss: 0.0050 lr: 0.02\n",
      "iteration: 275180 loss: 0.0019 lr: 0.02\n",
      "iteration: 275190 loss: 0.0024 lr: 0.02\n",
      "iteration: 275200 loss: 0.0026 lr: 0.02\n",
      "iteration: 275210 loss: 0.0015 lr: 0.02\n",
      "iteration: 275220 loss: 0.0025 lr: 0.02\n",
      "iteration: 275230 loss: 0.0019 lr: 0.02\n",
      "iteration: 275240 loss: 0.0013 lr: 0.02\n",
      "iteration: 275250 loss: 0.0014 lr: 0.02\n",
      "iteration: 275260 loss: 0.0019 lr: 0.02\n",
      "iteration: 275270 loss: 0.0016 lr: 0.02\n",
      "iteration: 275280 loss: 0.0018 lr: 0.02\n",
      "iteration: 275290 loss: 0.0012 lr: 0.02\n",
      "iteration: 275300 loss: 0.0022 lr: 0.02\n",
      "iteration: 275310 loss: 0.0021 lr: 0.02\n",
      "iteration: 275320 loss: 0.0014 lr: 0.02\n",
      "iteration: 275330 loss: 0.0024 lr: 0.02\n",
      "iteration: 275340 loss: 0.0019 lr: 0.02\n",
      "iteration: 275350 loss: 0.0023 lr: 0.02\n",
      "iteration: 275360 loss: 0.0017 lr: 0.02\n",
      "iteration: 275370 loss: 0.0022 lr: 0.02\n",
      "iteration: 275380 loss: 0.0018 lr: 0.02\n",
      "iteration: 275390 loss: 0.0014 lr: 0.02\n",
      "iteration: 275400 loss: 0.0020 lr: 0.02\n",
      "iteration: 275410 loss: 0.0016 lr: 0.02\n",
      "iteration: 275420 loss: 0.0030 lr: 0.02\n",
      "iteration: 275430 loss: 0.0018 lr: 0.02\n",
      "iteration: 275440 loss: 0.0013 lr: 0.02\n",
      "iteration: 275450 loss: 0.0018 lr: 0.02\n",
      "iteration: 275460 loss: 0.0019 lr: 0.02\n",
      "iteration: 275470 loss: 0.0021 lr: 0.02\n",
      "iteration: 275480 loss: 0.0022 lr: 0.02\n",
      "iteration: 275490 loss: 0.0016 lr: 0.02\n",
      "iteration: 275500 loss: 0.0019 lr: 0.02\n",
      "iteration: 275510 loss: 0.0026 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 275520 loss: 0.0012 lr: 0.02\n",
      "iteration: 275530 loss: 0.0014 lr: 0.02\n",
      "iteration: 275540 loss: 0.0018 lr: 0.02\n",
      "iteration: 275550 loss: 0.0016 lr: 0.02\n",
      "iteration: 275560 loss: 0.0018 lr: 0.02\n",
      "iteration: 275570 loss: 0.0020 lr: 0.02\n",
      "iteration: 275580 loss: 0.0012 lr: 0.02\n",
      "iteration: 275590 loss: 0.0019 lr: 0.02\n",
      "iteration: 275600 loss: 0.0023 lr: 0.02\n",
      "iteration: 275610 loss: 0.0017 lr: 0.02\n",
      "iteration: 275620 loss: 0.0018 lr: 0.02\n",
      "iteration: 275630 loss: 0.0019 lr: 0.02\n",
      "iteration: 275640 loss: 0.0017 lr: 0.02\n",
      "iteration: 275650 loss: 0.0017 lr: 0.02\n",
      "iteration: 275660 loss: 0.0025 lr: 0.02\n",
      "iteration: 275670 loss: 0.0019 lr: 0.02\n",
      "iteration: 275680 loss: 0.0028 lr: 0.02\n",
      "iteration: 275690 loss: 0.0023 lr: 0.02\n",
      "iteration: 275700 loss: 0.0014 lr: 0.02\n",
      "iteration: 275710 loss: 0.0019 lr: 0.02\n",
      "iteration: 275720 loss: 0.0018 lr: 0.02\n",
      "iteration: 275730 loss: 0.0011 lr: 0.02\n",
      "iteration: 275740 loss: 0.0016 lr: 0.02\n",
      "iteration: 275750 loss: 0.0020 lr: 0.02\n",
      "iteration: 275760 loss: 0.0017 lr: 0.02\n",
      "iteration: 275770 loss: 0.0024 lr: 0.02\n",
      "iteration: 275780 loss: 0.0021 lr: 0.02\n",
      "iteration: 275790 loss: 0.0024 lr: 0.02\n",
      "iteration: 275800 loss: 0.0025 lr: 0.02\n",
      "iteration: 275810 loss: 0.0014 lr: 0.02\n",
      "iteration: 275820 loss: 0.0015 lr: 0.02\n",
      "iteration: 275830 loss: 0.0015 lr: 0.02\n",
      "iteration: 275840 loss: 0.0018 lr: 0.02\n",
      "iteration: 275850 loss: 0.0015 lr: 0.02\n",
      "iteration: 275860 loss: 0.0014 lr: 0.02\n",
      "iteration: 275870 loss: 0.0016 lr: 0.02\n",
      "iteration: 275880 loss: 0.0028 lr: 0.02\n",
      "iteration: 275890 loss: 0.0018 lr: 0.02\n",
      "iteration: 275900 loss: 0.0017 lr: 0.02\n",
      "iteration: 275910 loss: 0.0018 lr: 0.02\n",
      "iteration: 275920 loss: 0.0016 lr: 0.02\n",
      "iteration: 275930 loss: 0.0016 lr: 0.02\n",
      "iteration: 275940 loss: 0.0019 lr: 0.02\n",
      "iteration: 275950 loss: 0.0034 lr: 0.02\n",
      "iteration: 275960 loss: 0.0030 lr: 0.02\n",
      "iteration: 275970 loss: 0.0018 lr: 0.02\n",
      "iteration: 275980 loss: 0.0017 lr: 0.02\n",
      "iteration: 275990 loss: 0.0017 lr: 0.02\n",
      "iteration: 276000 loss: 0.0021 lr: 0.02\n",
      "iteration: 276010 loss: 0.0014 lr: 0.02\n",
      "iteration: 276020 loss: 0.0022 lr: 0.02\n",
      "iteration: 276030 loss: 0.0017 lr: 0.02\n",
      "iteration: 276040 loss: 0.0025 lr: 0.02\n",
      "iteration: 276050 loss: 0.0018 lr: 0.02\n",
      "iteration: 276060 loss: 0.0019 lr: 0.02\n",
      "iteration: 276070 loss: 0.0018 lr: 0.02\n",
      "iteration: 276080 loss: 0.0020 lr: 0.02\n",
      "iteration: 276090 loss: 0.0019 lr: 0.02\n",
      "iteration: 276100 loss: 0.0019 lr: 0.02\n",
      "iteration: 276110 loss: 0.0021 lr: 0.02\n",
      "iteration: 276120 loss: 0.0020 lr: 0.02\n",
      "iteration: 276130 loss: 0.0015 lr: 0.02\n",
      "iteration: 276140 loss: 0.0020 lr: 0.02\n",
      "iteration: 276150 loss: 0.0018 lr: 0.02\n",
      "iteration: 276160 loss: 0.0018 lr: 0.02\n",
      "iteration: 276170 loss: 0.0028 lr: 0.02\n",
      "iteration: 276180 loss: 0.0016 lr: 0.02\n",
      "iteration: 276190 loss: 0.0016 lr: 0.02\n",
      "iteration: 276200 loss: 0.0018 lr: 0.02\n",
      "iteration: 276210 loss: 0.0018 lr: 0.02\n",
      "iteration: 276220 loss: 0.0020 lr: 0.02\n",
      "iteration: 276230 loss: 0.0018 lr: 0.02\n",
      "iteration: 276240 loss: 0.0018 lr: 0.02\n",
      "iteration: 276250 loss: 0.0026 lr: 0.02\n",
      "iteration: 276260 loss: 0.0015 lr: 0.02\n",
      "iteration: 276270 loss: 0.0012 lr: 0.02\n",
      "iteration: 276280 loss: 0.0018 lr: 0.02\n",
      "iteration: 276290 loss: 0.0017 lr: 0.02\n",
      "iteration: 276300 loss: 0.0013 lr: 0.02\n",
      "iteration: 276310 loss: 0.0021 lr: 0.02\n",
      "iteration: 276320 loss: 0.0019 lr: 0.02\n",
      "iteration: 276330 loss: 0.0041 lr: 0.02\n",
      "iteration: 276340 loss: 0.0022 lr: 0.02\n",
      "iteration: 276350 loss: 0.0019 lr: 0.02\n",
      "iteration: 276360 loss: 0.0016 lr: 0.02\n",
      "iteration: 276370 loss: 0.0015 lr: 0.02\n",
      "iteration: 276380 loss: 0.0011 lr: 0.02\n",
      "iteration: 276390 loss: 0.0025 lr: 0.02\n",
      "iteration: 276400 loss: 0.0018 lr: 0.02\n",
      "iteration: 276410 loss: 0.0021 lr: 0.02\n",
      "iteration: 276420 loss: 0.0022 lr: 0.02\n",
      "iteration: 276430 loss: 0.0013 lr: 0.02\n",
      "iteration: 276440 loss: 0.0022 lr: 0.02\n",
      "iteration: 276450 loss: 0.0018 lr: 0.02\n",
      "iteration: 276460 loss: 0.0016 lr: 0.02\n",
      "iteration: 276470 loss: 0.0011 lr: 0.02\n",
      "iteration: 276480 loss: 0.0020 lr: 0.02\n",
      "iteration: 276490 loss: 0.0017 lr: 0.02\n",
      "iteration: 276500 loss: 0.0018 lr: 0.02\n",
      "iteration: 276510 loss: 0.0021 lr: 0.02\n",
      "iteration: 276520 loss: 0.0018 lr: 0.02\n",
      "iteration: 276530 loss: 0.0017 lr: 0.02\n",
      "iteration: 276540 loss: 0.0018 lr: 0.02\n",
      "iteration: 276550 loss: 0.0020 lr: 0.02\n",
      "iteration: 276560 loss: 0.0018 lr: 0.02\n",
      "iteration: 276570 loss: 0.0026 lr: 0.02\n",
      "iteration: 276580 loss: 0.0022 lr: 0.02\n",
      "iteration: 276590 loss: 0.0018 lr: 0.02\n",
      "iteration: 276600 loss: 0.0013 lr: 0.02\n",
      "iteration: 276610 loss: 0.0016 lr: 0.02\n",
      "iteration: 276620 loss: 0.0014 lr: 0.02\n",
      "iteration: 276630 loss: 0.0018 lr: 0.02\n",
      "iteration: 276640 loss: 0.0021 lr: 0.02\n",
      "iteration: 276650 loss: 0.0023 lr: 0.02\n",
      "iteration: 276660 loss: 0.0015 lr: 0.02\n",
      "iteration: 276670 loss: 0.0021 lr: 0.02\n",
      "iteration: 276680 loss: 0.0018 lr: 0.02\n",
      "iteration: 276690 loss: 0.0014 lr: 0.02\n",
      "iteration: 276700 loss: 0.0017 lr: 0.02\n",
      "iteration: 276710 loss: 0.0018 lr: 0.02\n",
      "iteration: 276720 loss: 0.0019 lr: 0.02\n",
      "iteration: 276730 loss: 0.0020 lr: 0.02\n",
      "iteration: 276740 loss: 0.0013 lr: 0.02\n",
      "iteration: 276750 loss: 0.0025 lr: 0.02\n",
      "iteration: 276760 loss: 0.0018 lr: 0.02\n",
      "iteration: 276770 loss: 0.0030 lr: 0.02\n",
      "iteration: 276780 loss: 0.0022 lr: 0.02\n",
      "iteration: 276790 loss: 0.0024 lr: 0.02\n",
      "iteration: 276800 loss: 0.0030 lr: 0.02\n",
      "iteration: 276810 loss: 0.0024 lr: 0.02\n",
      "iteration: 276820 loss: 0.0015 lr: 0.02\n",
      "iteration: 276830 loss: 0.0028 lr: 0.02\n",
      "iteration: 276840 loss: 0.0022 lr: 0.02\n",
      "iteration: 276850 loss: 0.0016 lr: 0.02\n",
      "iteration: 276860 loss: 0.0012 lr: 0.02\n",
      "iteration: 276870 loss: 0.0017 lr: 0.02\n",
      "iteration: 276880 loss: 0.0027 lr: 0.02\n",
      "iteration: 276890 loss: 0.0020 lr: 0.02\n",
      "iteration: 276900 loss: 0.0017 lr: 0.02\n",
      "iteration: 276910 loss: 0.0014 lr: 0.02\n",
      "iteration: 276920 loss: 0.0018 lr: 0.02\n",
      "iteration: 276930 loss: 0.0022 lr: 0.02\n",
      "iteration: 276940 loss: 0.0019 lr: 0.02\n",
      "iteration: 276950 loss: 0.0016 lr: 0.02\n",
      "iteration: 276960 loss: 0.0022 lr: 0.02\n",
      "iteration: 276970 loss: 0.0024 lr: 0.02\n",
      "iteration: 276980 loss: 0.0018 lr: 0.02\n",
      "iteration: 276990 loss: 0.0016 lr: 0.02\n",
      "iteration: 277000 loss: 0.0015 lr: 0.02\n",
      "iteration: 277010 loss: 0.0031 lr: 0.02\n",
      "iteration: 277020 loss: 0.0019 lr: 0.02\n",
      "iteration: 277030 loss: 0.0026 lr: 0.02\n",
      "iteration: 277040 loss: 0.0019 lr: 0.02\n",
      "iteration: 277050 loss: 0.0020 lr: 0.02\n",
      "iteration: 277060 loss: 0.0031 lr: 0.02\n",
      "iteration: 277070 loss: 0.0020 lr: 0.02\n",
      "iteration: 277080 loss: 0.0024 lr: 0.02\n",
      "iteration: 277090 loss: 0.0019 lr: 0.02\n",
      "iteration: 277100 loss: 0.0013 lr: 0.02\n",
      "iteration: 277110 loss: 0.0013 lr: 0.02\n",
      "iteration: 277120 loss: 0.0014 lr: 0.02\n",
      "iteration: 277130 loss: 0.0020 lr: 0.02\n",
      "iteration: 277140 loss: 0.0019 lr: 0.02\n",
      "iteration: 277150 loss: 0.0023 lr: 0.02\n",
      "iteration: 277160 loss: 0.0026 lr: 0.02\n",
      "iteration: 277170 loss: 0.0024 lr: 0.02\n",
      "iteration: 277180 loss: 0.0024 lr: 0.02\n",
      "iteration: 277190 loss: 0.0017 lr: 0.02\n",
      "iteration: 277200 loss: 0.0022 lr: 0.02\n",
      "iteration: 277210 loss: 0.0035 lr: 0.02\n",
      "iteration: 277220 loss: 0.0021 lr: 0.02\n",
      "iteration: 277230 loss: 0.0017 lr: 0.02\n",
      "iteration: 277240 loss: 0.0016 lr: 0.02\n",
      "iteration: 277250 loss: 0.0014 lr: 0.02\n",
      "iteration: 277260 loss: 0.0030 lr: 0.02\n",
      "iteration: 277270 loss: 0.0019 lr: 0.02\n",
      "iteration: 277280 loss: 0.0021 lr: 0.02\n",
      "iteration: 277290 loss: 0.0019 lr: 0.02\n",
      "iteration: 277300 loss: 0.0034 lr: 0.02\n",
      "iteration: 277310 loss: 0.0023 lr: 0.02\n",
      "iteration: 277320 loss: 0.0014 lr: 0.02\n",
      "iteration: 277330 loss: 0.0029 lr: 0.02\n",
      "iteration: 277340 loss: 0.0015 lr: 0.02\n",
      "iteration: 277350 loss: 0.0016 lr: 0.02\n",
      "iteration: 277360 loss: 0.0013 lr: 0.02\n",
      "iteration: 277370 loss: 0.0012 lr: 0.02\n",
      "iteration: 277380 loss: 0.0016 lr: 0.02\n",
      "iteration: 277390 loss: 0.0015 lr: 0.02\n",
      "iteration: 277400 loss: 0.0016 lr: 0.02\n",
      "iteration: 277410 loss: 0.0015 lr: 0.02\n",
      "iteration: 277420 loss: 0.0020 lr: 0.02\n",
      "iteration: 277430 loss: 0.0016 lr: 0.02\n",
      "iteration: 277440 loss: 0.0021 lr: 0.02\n",
      "iteration: 277450 loss: 0.0016 lr: 0.02\n",
      "iteration: 277460 loss: 0.0014 lr: 0.02\n",
      "iteration: 277470 loss: 0.0016 lr: 0.02\n",
      "iteration: 277480 loss: 0.0021 lr: 0.02\n",
      "iteration: 277490 loss: 0.0013 lr: 0.02\n",
      "iteration: 277500 loss: 0.0014 lr: 0.02\n",
      "iteration: 277510 loss: 0.0025 lr: 0.02\n",
      "iteration: 277520 loss: 0.0021 lr: 0.02\n",
      "iteration: 277530 loss: 0.0016 lr: 0.02\n",
      "iteration: 277540 loss: 0.0018 lr: 0.02\n",
      "iteration: 277550 loss: 0.0019 lr: 0.02\n",
      "iteration: 277560 loss: 0.0028 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 277570 loss: 0.0012 lr: 0.02\n",
      "iteration: 277580 loss: 0.0016 lr: 0.02\n",
      "iteration: 277590 loss: 0.0023 lr: 0.02\n",
      "iteration: 277600 loss: 0.0016 lr: 0.02\n",
      "iteration: 277610 loss: 0.0015 lr: 0.02\n",
      "iteration: 277620 loss: 0.0009 lr: 0.02\n",
      "iteration: 277630 loss: 0.0017 lr: 0.02\n",
      "iteration: 277640 loss: 0.0020 lr: 0.02\n",
      "iteration: 277650 loss: 0.0018 lr: 0.02\n",
      "iteration: 277660 loss: 0.0015 lr: 0.02\n",
      "iteration: 277670 loss: 0.0018 lr: 0.02\n",
      "iteration: 277680 loss: 0.0024 lr: 0.02\n",
      "iteration: 277690 loss: 0.0012 lr: 0.02\n",
      "iteration: 277700 loss: 0.0011 lr: 0.02\n",
      "iteration: 277710 loss: 0.0023 lr: 0.02\n",
      "iteration: 277720 loss: 0.0030 lr: 0.02\n",
      "iteration: 277730 loss: 0.0017 lr: 0.02\n",
      "iteration: 277740 loss: 0.0022 lr: 0.02\n",
      "iteration: 277750 loss: 0.0018 lr: 0.02\n",
      "iteration: 277760 loss: 0.0026 lr: 0.02\n",
      "iteration: 277770 loss: 0.0015 lr: 0.02\n",
      "iteration: 277780 loss: 0.0022 lr: 0.02\n",
      "iteration: 277790 loss: 0.0018 lr: 0.02\n",
      "iteration: 277800 loss: 0.0025 lr: 0.02\n",
      "iteration: 277810 loss: 0.0020 lr: 0.02\n",
      "iteration: 277820 loss: 0.0012 lr: 0.02\n",
      "iteration: 277830 loss: 0.0017 lr: 0.02\n",
      "iteration: 277840 loss: 0.0026 lr: 0.02\n",
      "iteration: 277850 loss: 0.0019 lr: 0.02\n",
      "iteration: 277860 loss: 0.0019 lr: 0.02\n",
      "iteration: 277870 loss: 0.0024 lr: 0.02\n",
      "iteration: 277880 loss: 0.0017 lr: 0.02\n",
      "iteration: 277890 loss: 0.0020 lr: 0.02\n",
      "iteration: 277900 loss: 0.0017 lr: 0.02\n",
      "iteration: 277910 loss: 0.0019 lr: 0.02\n",
      "iteration: 277920 loss: 0.0016 lr: 0.02\n",
      "iteration: 277930 loss: 0.0013 lr: 0.02\n",
      "iteration: 277940 loss: 0.0021 lr: 0.02\n",
      "iteration: 277950 loss: 0.0017 lr: 0.02\n",
      "iteration: 277960 loss: 0.0020 lr: 0.02\n",
      "iteration: 277970 loss: 0.0028 lr: 0.02\n",
      "iteration: 277980 loss: 0.0025 lr: 0.02\n",
      "iteration: 277990 loss: 0.0018 lr: 0.02\n",
      "iteration: 278000 loss: 0.0016 lr: 0.02\n",
      "iteration: 278010 loss: 0.0016 lr: 0.02\n",
      "iteration: 278020 loss: 0.0021 lr: 0.02\n",
      "iteration: 278030 loss: 0.0015 lr: 0.02\n",
      "iteration: 278040 loss: 0.0015 lr: 0.02\n",
      "iteration: 278050 loss: 0.0014 lr: 0.02\n",
      "iteration: 278060 loss: 0.0020 lr: 0.02\n",
      "iteration: 278070 loss: 0.0018 lr: 0.02\n",
      "iteration: 278080 loss: 0.0020 lr: 0.02\n",
      "iteration: 278090 loss: 0.0021 lr: 0.02\n",
      "iteration: 278100 loss: 0.0017 lr: 0.02\n",
      "iteration: 278110 loss: 0.0017 lr: 0.02\n",
      "iteration: 278120 loss: 0.0015 lr: 0.02\n",
      "iteration: 278130 loss: 0.0015 lr: 0.02\n",
      "iteration: 278140 loss: 0.0019 lr: 0.02\n",
      "iteration: 278150 loss: 0.0017 lr: 0.02\n",
      "iteration: 278160 loss: 0.0018 lr: 0.02\n",
      "iteration: 278170 loss: 0.0014 lr: 0.02\n",
      "iteration: 278180 loss: 0.0026 lr: 0.02\n",
      "iteration: 278190 loss: 0.0023 lr: 0.02\n",
      "iteration: 278200 loss: 0.0016 lr: 0.02\n",
      "iteration: 278210 loss: 0.0016 lr: 0.02\n",
      "iteration: 278220 loss: 0.0012 lr: 0.02\n",
      "iteration: 278230 loss: 0.0015 lr: 0.02\n",
      "iteration: 278240 loss: 0.0013 lr: 0.02\n",
      "iteration: 278250 loss: 0.0016 lr: 0.02\n",
      "iteration: 278260 loss: 0.0017 lr: 0.02\n",
      "iteration: 278270 loss: 0.0014 lr: 0.02\n",
      "iteration: 278280 loss: 0.0018 lr: 0.02\n",
      "iteration: 278290 loss: 0.0020 lr: 0.02\n",
      "iteration: 278300 loss: 0.0018 lr: 0.02\n",
      "iteration: 278310 loss: 0.0025 lr: 0.02\n",
      "iteration: 278320 loss: 0.0023 lr: 0.02\n",
      "iteration: 278330 loss: 0.0014 lr: 0.02\n",
      "iteration: 278340 loss: 0.0020 lr: 0.02\n",
      "iteration: 278350 loss: 0.0023 lr: 0.02\n",
      "iteration: 278360 loss: 0.0016 lr: 0.02\n",
      "iteration: 278370 loss: 0.0016 lr: 0.02\n",
      "iteration: 278380 loss: 0.0018 lr: 0.02\n",
      "iteration: 278390 loss: 0.0025 lr: 0.02\n",
      "iteration: 278400 loss: 0.0014 lr: 0.02\n",
      "iteration: 278410 loss: 0.0011 lr: 0.02\n",
      "iteration: 278420 loss: 0.0015 lr: 0.02\n",
      "iteration: 278430 loss: 0.0018 lr: 0.02\n",
      "iteration: 278440 loss: 0.0019 lr: 0.02\n",
      "iteration: 278450 loss: 0.0017 lr: 0.02\n",
      "iteration: 278460 loss: 0.0025 lr: 0.02\n",
      "iteration: 278470 loss: 0.0024 lr: 0.02\n",
      "iteration: 278480 loss: 0.0019 lr: 0.02\n",
      "iteration: 278490 loss: 0.0023 lr: 0.02\n",
      "iteration: 278500 loss: 0.0015 lr: 0.02\n",
      "iteration: 278510 loss: 0.0018 lr: 0.02\n",
      "iteration: 278520 loss: 0.0020 lr: 0.02\n",
      "iteration: 278530 loss: 0.0018 lr: 0.02\n",
      "iteration: 278540 loss: 0.0020 lr: 0.02\n",
      "iteration: 278550 loss: 0.0017 lr: 0.02\n",
      "iteration: 278560 loss: 0.0017 lr: 0.02\n",
      "iteration: 278570 loss: 0.0016 lr: 0.02\n",
      "iteration: 278580 loss: 0.0020 lr: 0.02\n",
      "iteration: 278590 loss: 0.0018 lr: 0.02\n",
      "iteration: 278600 loss: 0.0014 lr: 0.02\n",
      "iteration: 278610 loss: 0.0018 lr: 0.02\n",
      "iteration: 278620 loss: 0.0014 lr: 0.02\n",
      "iteration: 278630 loss: 0.0013 lr: 0.02\n",
      "iteration: 278640 loss: 0.0014 lr: 0.02\n",
      "iteration: 278650 loss: 0.0018 lr: 0.02\n",
      "iteration: 278660 loss: 0.0017 lr: 0.02\n",
      "iteration: 278670 loss: 0.0016 lr: 0.02\n",
      "iteration: 278680 loss: 0.0025 lr: 0.02\n",
      "iteration: 278690 loss: 0.0024 lr: 0.02\n",
      "iteration: 278700 loss: 0.0016 lr: 0.02\n",
      "iteration: 278710 loss: 0.0028 lr: 0.02\n",
      "iteration: 278720 loss: 0.0023 lr: 0.02\n",
      "iteration: 278730 loss: 0.0027 lr: 0.02\n",
      "iteration: 278740 loss: 0.0017 lr: 0.02\n",
      "iteration: 278750 loss: 0.0023 lr: 0.02\n",
      "iteration: 278760 loss: 0.0015 lr: 0.02\n",
      "iteration: 278770 loss: 0.0018 lr: 0.02\n",
      "iteration: 278780 loss: 0.0027 lr: 0.02\n",
      "iteration: 278790 loss: 0.0022 lr: 0.02\n",
      "iteration: 278800 loss: 0.0017 lr: 0.02\n",
      "iteration: 278810 loss: 0.0020 lr: 0.02\n",
      "iteration: 278820 loss: 0.0017 lr: 0.02\n",
      "iteration: 278830 loss: 0.0015 lr: 0.02\n",
      "iteration: 278840 loss: 0.0017 lr: 0.02\n",
      "iteration: 278850 loss: 0.0022 lr: 0.02\n",
      "iteration: 278860 loss: 0.0014 lr: 0.02\n",
      "iteration: 278870 loss: 0.0016 lr: 0.02\n",
      "iteration: 278880 loss: 0.0017 lr: 0.02\n",
      "iteration: 278890 loss: 0.0022 lr: 0.02\n",
      "iteration: 278900 loss: 0.0016 lr: 0.02\n",
      "iteration: 278910 loss: 0.0022 lr: 0.02\n",
      "iteration: 278920 loss: 0.0017 lr: 0.02\n",
      "iteration: 278930 loss: 0.0021 lr: 0.02\n",
      "iteration: 278940 loss: 0.0020 lr: 0.02\n",
      "iteration: 278950 loss: 0.0020 lr: 0.02\n",
      "iteration: 278960 loss: 0.0020 lr: 0.02\n",
      "iteration: 278970 loss: 0.0021 lr: 0.02\n",
      "iteration: 278980 loss: 0.0022 lr: 0.02\n",
      "iteration: 278990 loss: 0.0026 lr: 0.02\n",
      "iteration: 279000 loss: 0.0017 lr: 0.02\n",
      "iteration: 279010 loss: 0.0017 lr: 0.02\n",
      "iteration: 279020 loss: 0.0016 lr: 0.02\n",
      "iteration: 279030 loss: 0.0016 lr: 0.02\n",
      "iteration: 279040 loss: 0.0015 lr: 0.02\n",
      "iteration: 279050 loss: 0.0016 lr: 0.02\n",
      "iteration: 279060 loss: 0.0016 lr: 0.02\n",
      "iteration: 279070 loss: 0.0032 lr: 0.02\n",
      "iteration: 279080 loss: 0.0026 lr: 0.02\n",
      "iteration: 279090 loss: 0.0019 lr: 0.02\n",
      "iteration: 279100 loss: 0.0017 lr: 0.02\n",
      "iteration: 279110 loss: 0.0028 lr: 0.02\n",
      "iteration: 279120 loss: 0.0014 lr: 0.02\n",
      "iteration: 279130 loss: 0.0012 lr: 0.02\n",
      "iteration: 279140 loss: 0.0015 lr: 0.02\n",
      "iteration: 279150 loss: 0.0025 lr: 0.02\n",
      "iteration: 279160 loss: 0.0018 lr: 0.02\n",
      "iteration: 279170 loss: 0.0017 lr: 0.02\n",
      "iteration: 279180 loss: 0.0022 lr: 0.02\n",
      "iteration: 279190 loss: 0.0018 lr: 0.02\n",
      "iteration: 279200 loss: 0.0016 lr: 0.02\n",
      "iteration: 279210 loss: 0.0016 lr: 0.02\n",
      "iteration: 279220 loss: 0.0022 lr: 0.02\n",
      "iteration: 279230 loss: 0.0019 lr: 0.02\n",
      "iteration: 279240 loss: 0.0016 lr: 0.02\n",
      "iteration: 279250 loss: 0.0022 lr: 0.02\n",
      "iteration: 279260 loss: 0.0023 lr: 0.02\n",
      "iteration: 279270 loss: 0.0020 lr: 0.02\n",
      "iteration: 279280 loss: 0.0017 lr: 0.02\n",
      "iteration: 279290 loss: 0.0017 lr: 0.02\n",
      "iteration: 279300 loss: 0.0012 lr: 0.02\n",
      "iteration: 279310 loss: 0.0017 lr: 0.02\n",
      "iteration: 279320 loss: 0.0016 lr: 0.02\n",
      "iteration: 279330 loss: 0.0017 lr: 0.02\n",
      "iteration: 279340 loss: 0.0015 lr: 0.02\n",
      "iteration: 279350 loss: 0.0015 lr: 0.02\n",
      "iteration: 279360 loss: 0.0014 lr: 0.02\n",
      "iteration: 279370 loss: 0.0012 lr: 0.02\n",
      "iteration: 279380 loss: 0.0015 lr: 0.02\n",
      "iteration: 279390 loss: 0.0018 lr: 0.02\n",
      "iteration: 279400 loss: 0.0023 lr: 0.02\n",
      "iteration: 279410 loss: 0.0017 lr: 0.02\n",
      "iteration: 279420 loss: 0.0020 lr: 0.02\n",
      "iteration: 279430 loss: 0.0023 lr: 0.02\n",
      "iteration: 279440 loss: 0.0021 lr: 0.02\n",
      "iteration: 279450 loss: 0.0017 lr: 0.02\n",
      "iteration: 279460 loss: 0.0018 lr: 0.02\n",
      "iteration: 279470 loss: 0.0016 lr: 0.02\n",
      "iteration: 279480 loss: 0.0025 lr: 0.02\n",
      "iteration: 279490 loss: 0.0023 lr: 0.02\n",
      "iteration: 279500 loss: 0.0016 lr: 0.02\n",
      "iteration: 279510 loss: 0.0019 lr: 0.02\n",
      "iteration: 279520 loss: 0.0020 lr: 0.02\n",
      "iteration: 279530 loss: 0.0034 lr: 0.02\n",
      "iteration: 279540 loss: 0.0021 lr: 0.02\n",
      "iteration: 279550 loss: 0.0018 lr: 0.02\n",
      "iteration: 279560 loss: 0.0024 lr: 0.02\n",
      "iteration: 279570 loss: 0.0018 lr: 0.02\n",
      "iteration: 279580 loss: 0.0017 lr: 0.02\n",
      "iteration: 279590 loss: 0.0018 lr: 0.02\n",
      "iteration: 279600 loss: 0.0016 lr: 0.02\n",
      "iteration: 279610 loss: 0.0018 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 279620 loss: 0.0017 lr: 0.02\n",
      "iteration: 279630 loss: 0.0019 lr: 0.02\n",
      "iteration: 279640 loss: 0.0015 lr: 0.02\n",
      "iteration: 279650 loss: 0.0019 lr: 0.02\n",
      "iteration: 279660 loss: 0.0013 lr: 0.02\n",
      "iteration: 279670 loss: 0.0015 lr: 0.02\n",
      "iteration: 279680 loss: 0.0025 lr: 0.02\n",
      "iteration: 279690 loss: 0.0025 lr: 0.02\n",
      "iteration: 279700 loss: 0.0014 lr: 0.02\n",
      "iteration: 279710 loss: 0.0018 lr: 0.02\n",
      "iteration: 279720 loss: 0.0016 lr: 0.02\n",
      "iteration: 279730 loss: 0.0023 lr: 0.02\n",
      "iteration: 279740 loss: 0.0025 lr: 0.02\n",
      "iteration: 279750 loss: 0.0030 lr: 0.02\n",
      "iteration: 279760 loss: 0.0020 lr: 0.02\n",
      "iteration: 279770 loss: 0.0025 lr: 0.02\n",
      "iteration: 279780 loss: 0.0016 lr: 0.02\n",
      "iteration: 279790 loss: 0.0018 lr: 0.02\n",
      "iteration: 279800 loss: 0.0018 lr: 0.02\n",
      "iteration: 279810 loss: 0.0017 lr: 0.02\n",
      "iteration: 279820 loss: 0.0025 lr: 0.02\n",
      "iteration: 279830 loss: 0.0018 lr: 0.02\n",
      "iteration: 279840 loss: 0.0019 lr: 0.02\n",
      "iteration: 279850 loss: 0.0029 lr: 0.02\n",
      "iteration: 279860 loss: 0.0019 lr: 0.02\n",
      "iteration: 279870 loss: 0.0020 lr: 0.02\n",
      "iteration: 279880 loss: 0.0021 lr: 0.02\n",
      "iteration: 279890 loss: 0.0020 lr: 0.02\n",
      "iteration: 279900 loss: 0.0016 lr: 0.02\n",
      "iteration: 279910 loss: 0.0025 lr: 0.02\n",
      "iteration: 279920 loss: 0.0032 lr: 0.02\n",
      "iteration: 279930 loss: 0.0019 lr: 0.02\n",
      "iteration: 279940 loss: 0.0022 lr: 0.02\n",
      "iteration: 279950 loss: 0.0025 lr: 0.02\n",
      "iteration: 279960 loss: 0.0027 lr: 0.02\n",
      "iteration: 279970 loss: 0.0024 lr: 0.02\n",
      "iteration: 279980 loss: 0.0015 lr: 0.02\n",
      "iteration: 279990 loss: 0.0016 lr: 0.02\n",
      "iteration: 280000 loss: 0.0047 lr: 0.02\n",
      "iteration: 280010 loss: 0.0022 lr: 0.02\n",
      "iteration: 280020 loss: 0.0013 lr: 0.02\n",
      "iteration: 280030 loss: 0.0010 lr: 0.02\n",
      "iteration: 280040 loss: 0.0019 lr: 0.02\n",
      "iteration: 280050 loss: 0.0022 lr: 0.02\n",
      "iteration: 280060 loss: 0.0017 lr: 0.02\n",
      "iteration: 280070 loss: 0.0021 lr: 0.02\n",
      "iteration: 280080 loss: 0.0025 lr: 0.02\n",
      "iteration: 280090 loss: 0.0025 lr: 0.02\n",
      "iteration: 280100 loss: 0.0014 lr: 0.02\n",
      "iteration: 280110 loss: 0.0029 lr: 0.02\n",
      "iteration: 280120 loss: 0.0033 lr: 0.02\n",
      "iteration: 280130 loss: 0.0019 lr: 0.02\n",
      "iteration: 280140 loss: 0.0014 lr: 0.02\n",
      "iteration: 280150 loss: 0.0016 lr: 0.02\n",
      "iteration: 280160 loss: 0.0013 lr: 0.02\n",
      "iteration: 280170 loss: 0.0019 lr: 0.02\n",
      "iteration: 280180 loss: 0.0019 lr: 0.02\n",
      "iteration: 280190 loss: 0.0023 lr: 0.02\n",
      "iteration: 280200 loss: 0.0015 lr: 0.02\n",
      "iteration: 280210 loss: 0.0018 lr: 0.02\n",
      "iteration: 280220 loss: 0.0012 lr: 0.02\n",
      "iteration: 280230 loss: 0.0015 lr: 0.02\n",
      "iteration: 280240 loss: 0.0016 lr: 0.02\n",
      "iteration: 280250 loss: 0.0014 lr: 0.02\n",
      "iteration: 280260 loss: 0.0017 lr: 0.02\n",
      "iteration: 280270 loss: 0.0014 lr: 0.02\n",
      "iteration: 280280 loss: 0.0016 lr: 0.02\n",
      "iteration: 280290 loss: 0.0017 lr: 0.02\n",
      "iteration: 280300 loss: 0.0019 lr: 0.02\n",
      "iteration: 280310 loss: 0.0016 lr: 0.02\n",
      "iteration: 280320 loss: 0.0014 lr: 0.02\n",
      "iteration: 280330 loss: 0.0026 lr: 0.02\n",
      "iteration: 280340 loss: 0.0013 lr: 0.02\n",
      "iteration: 280350 loss: 0.0022 lr: 0.02\n",
      "iteration: 280360 loss: 0.0012 lr: 0.02\n",
      "iteration: 280370 loss: 0.0020 lr: 0.02\n",
      "iteration: 280380 loss: 0.0021 lr: 0.02\n",
      "iteration: 280390 loss: 0.0019 lr: 0.02\n",
      "iteration: 280400 loss: 0.0024 lr: 0.02\n",
      "iteration: 280410 loss: 0.0020 lr: 0.02\n",
      "iteration: 280420 loss: 0.0019 lr: 0.02\n",
      "iteration: 280430 loss: 0.0019 lr: 0.02\n",
      "iteration: 280440 loss: 0.0023 lr: 0.02\n",
      "iteration: 280450 loss: 0.0021 lr: 0.02\n",
      "iteration: 280460 loss: 0.0029 lr: 0.02\n",
      "iteration: 280470 loss: 0.0016 lr: 0.02\n",
      "iteration: 280480 loss: 0.0015 lr: 0.02\n",
      "iteration: 280490 loss: 0.0014 lr: 0.02\n",
      "iteration: 280500 loss: 0.0019 lr: 0.02\n",
      "iteration: 280510 loss: 0.0017 lr: 0.02\n",
      "iteration: 280520 loss: 0.0020 lr: 0.02\n",
      "iteration: 280530 loss: 0.0013 lr: 0.02\n",
      "iteration: 280540 loss: 0.0017 lr: 0.02\n",
      "iteration: 280550 loss: 0.0016 lr: 0.02\n",
      "iteration: 280560 loss: 0.0012 lr: 0.02\n",
      "iteration: 280570 loss: 0.0016 lr: 0.02\n",
      "iteration: 280580 loss: 0.0013 lr: 0.02\n",
      "iteration: 280590 loss: 0.0012 lr: 0.02\n",
      "iteration: 280600 loss: 0.0016 lr: 0.02\n",
      "iteration: 280610 loss: 0.0017 lr: 0.02\n",
      "iteration: 280620 loss: 0.0016 lr: 0.02\n",
      "iteration: 280630 loss: 0.0017 lr: 0.02\n",
      "iteration: 280640 loss: 0.0014 lr: 0.02\n",
      "iteration: 280650 loss: 0.0021 lr: 0.02\n",
      "iteration: 280660 loss: 0.0023 lr: 0.02\n",
      "iteration: 280670 loss: 0.0017 lr: 0.02\n",
      "iteration: 280680 loss: 0.0031 lr: 0.02\n",
      "iteration: 280690 loss: 0.0019 lr: 0.02\n",
      "iteration: 280700 loss: 0.0011 lr: 0.02\n",
      "iteration: 280710 loss: 0.0019 lr: 0.02\n",
      "iteration: 280720 loss: 0.0015 lr: 0.02\n",
      "iteration: 280730 loss: 0.0012 lr: 0.02\n",
      "iteration: 280740 loss: 0.0014 lr: 0.02\n",
      "iteration: 280750 loss: 0.0016 lr: 0.02\n",
      "iteration: 280760 loss: 0.0026 lr: 0.02\n",
      "iteration: 280770 loss: 0.0021 lr: 0.02\n",
      "iteration: 280780 loss: 0.0018 lr: 0.02\n",
      "iteration: 280790 loss: 0.0026 lr: 0.02\n",
      "iteration: 280800 loss: 0.0022 lr: 0.02\n",
      "iteration: 280810 loss: 0.0016 lr: 0.02\n",
      "iteration: 280820 loss: 0.0018 lr: 0.02\n",
      "iteration: 280830 loss: 0.0021 lr: 0.02\n",
      "iteration: 280840 loss: 0.0018 lr: 0.02\n",
      "iteration: 280850 loss: 0.0016 lr: 0.02\n",
      "iteration: 280860 loss: 0.0018 lr: 0.02\n",
      "iteration: 280870 loss: 0.0016 lr: 0.02\n",
      "iteration: 280880 loss: 0.0020 lr: 0.02\n",
      "iteration: 280890 loss: 0.0018 lr: 0.02\n",
      "iteration: 280900 loss: 0.0022 lr: 0.02\n",
      "iteration: 280910 loss: 0.0016 lr: 0.02\n",
      "iteration: 280920 loss: 0.0019 lr: 0.02\n",
      "iteration: 280930 loss: 0.0014 lr: 0.02\n",
      "iteration: 280940 loss: 0.0020 lr: 0.02\n",
      "iteration: 280950 loss: 0.0014 lr: 0.02\n",
      "iteration: 280960 loss: 0.0015 lr: 0.02\n",
      "iteration: 280970 loss: 0.0015 lr: 0.02\n",
      "iteration: 280980 loss: 0.0013 lr: 0.02\n",
      "iteration: 280990 loss: 0.0013 lr: 0.02\n",
      "iteration: 281000 loss: 0.0014 lr: 0.02\n",
      "iteration: 281010 loss: 0.0015 lr: 0.02\n",
      "iteration: 281020 loss: 0.0017 lr: 0.02\n",
      "iteration: 281030 loss: 0.0014 lr: 0.02\n",
      "iteration: 281040 loss: 0.0021 lr: 0.02\n",
      "iteration: 281050 loss: 0.0031 lr: 0.02\n",
      "iteration: 281060 loss: 0.0023 lr: 0.02\n",
      "iteration: 281070 loss: 0.0017 lr: 0.02\n",
      "iteration: 281080 loss: 0.0014 lr: 0.02\n",
      "iteration: 281090 loss: 0.0013 lr: 0.02\n",
      "iteration: 281100 loss: 0.0016 lr: 0.02\n",
      "iteration: 281110 loss: 0.0020 lr: 0.02\n",
      "iteration: 281120 loss: 0.0023 lr: 0.02\n",
      "iteration: 281130 loss: 0.0017 lr: 0.02\n",
      "iteration: 281140 loss: 0.0021 lr: 0.02\n",
      "iteration: 281150 loss: 0.0018 lr: 0.02\n",
      "iteration: 281160 loss: 0.0013 lr: 0.02\n",
      "iteration: 281170 loss: 0.0017 lr: 0.02\n",
      "iteration: 281180 loss: 0.0016 lr: 0.02\n",
      "iteration: 281190 loss: 0.0022 lr: 0.02\n",
      "iteration: 281200 loss: 0.0014 lr: 0.02\n",
      "iteration: 281210 loss: 0.0015 lr: 0.02\n",
      "iteration: 281220 loss: 0.0019 lr: 0.02\n",
      "iteration: 281230 loss: 0.0013 lr: 0.02\n",
      "iteration: 281240 loss: 0.0019 lr: 0.02\n",
      "iteration: 281250 loss: 0.0020 lr: 0.02\n",
      "iteration: 281260 loss: 0.0019 lr: 0.02\n",
      "iteration: 281270 loss: 0.0017 lr: 0.02\n",
      "iteration: 281280 loss: 0.0022 lr: 0.02\n",
      "iteration: 281290 loss: 0.0018 lr: 0.02\n",
      "iteration: 281300 loss: 0.0015 lr: 0.02\n",
      "iteration: 281310 loss: 0.0016 lr: 0.02\n",
      "iteration: 281320 loss: 0.0016 lr: 0.02\n",
      "iteration: 281330 loss: 0.0017 lr: 0.02\n",
      "iteration: 281340 loss: 0.0015 lr: 0.02\n",
      "iteration: 281350 loss: 0.0021 lr: 0.02\n",
      "iteration: 281360 loss: 0.0013 lr: 0.02\n",
      "iteration: 281370 loss: 0.0012 lr: 0.02\n",
      "iteration: 281380 loss: 0.0019 lr: 0.02\n",
      "iteration: 281390 loss: 0.0015 lr: 0.02\n",
      "iteration: 281400 loss: 0.0017 lr: 0.02\n",
      "iteration: 281410 loss: 0.0026 lr: 0.02\n",
      "iteration: 281420 loss: 0.0017 lr: 0.02\n",
      "iteration: 281430 loss: 0.0020 lr: 0.02\n",
      "iteration: 281440 loss: 0.0021 lr: 0.02\n",
      "iteration: 281450 loss: 0.0017 lr: 0.02\n",
      "iteration: 281460 loss: 0.0014 lr: 0.02\n",
      "iteration: 281470 loss: 0.0021 lr: 0.02\n",
      "iteration: 281480 loss: 0.0019 lr: 0.02\n",
      "iteration: 281490 loss: 0.0010 lr: 0.02\n",
      "iteration: 281500 loss: 0.0018 lr: 0.02\n",
      "iteration: 281510 loss: 0.0017 lr: 0.02\n",
      "iteration: 281520 loss: 0.0017 lr: 0.02\n",
      "iteration: 281530 loss: 0.0030 lr: 0.02\n",
      "iteration: 281540 loss: 0.0021 lr: 0.02\n",
      "iteration: 281550 loss: 0.0017 lr: 0.02\n",
      "iteration: 281560 loss: 0.0020 lr: 0.02\n",
      "iteration: 281570 loss: 0.0019 lr: 0.02\n",
      "iteration: 281580 loss: 0.0017 lr: 0.02\n",
      "iteration: 281590 loss: 0.0018 lr: 0.02\n",
      "iteration: 281600 loss: 0.0023 lr: 0.02\n",
      "iteration: 281610 loss: 0.0021 lr: 0.02\n",
      "iteration: 281620 loss: 0.0012 lr: 0.02\n",
      "iteration: 281630 loss: 0.0016 lr: 0.02\n",
      "iteration: 281640 loss: 0.0014 lr: 0.02\n",
      "iteration: 281650 loss: 0.0018 lr: 0.02\n",
      "iteration: 281660 loss: 0.0016 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 281670 loss: 0.0021 lr: 0.02\n",
      "iteration: 281680 loss: 0.0017 lr: 0.02\n",
      "iteration: 281690 loss: 0.0018 lr: 0.02\n",
      "iteration: 281700 loss: 0.0015 lr: 0.02\n",
      "iteration: 281710 loss: 0.0018 lr: 0.02\n",
      "iteration: 281720 loss: 0.0029 lr: 0.02\n",
      "iteration: 281730 loss: 0.0019 lr: 0.02\n",
      "iteration: 281740 loss: 0.0018 lr: 0.02\n",
      "iteration: 281750 loss: 0.0017 lr: 0.02\n",
      "iteration: 281760 loss: 0.0016 lr: 0.02\n",
      "iteration: 281770 loss: 0.0019 lr: 0.02\n",
      "iteration: 281780 loss: 0.0014 lr: 0.02\n",
      "iteration: 281790 loss: 0.0020 lr: 0.02\n",
      "iteration: 281800 loss: 0.0015 lr: 0.02\n",
      "iteration: 281810 loss: 0.0020 lr: 0.02\n",
      "iteration: 281820 loss: 0.0013 lr: 0.02\n",
      "iteration: 281830 loss: 0.0022 lr: 0.02\n",
      "iteration: 281840 loss: 0.0024 lr: 0.02\n",
      "iteration: 281850 loss: 0.0015 lr: 0.02\n",
      "iteration: 281860 loss: 0.0016 lr: 0.02\n",
      "iteration: 281870 loss: 0.0018 lr: 0.02\n",
      "iteration: 281880 loss: 0.0020 lr: 0.02\n",
      "iteration: 281890 loss: 0.0018 lr: 0.02\n",
      "iteration: 281900 loss: 0.0021 lr: 0.02\n",
      "iteration: 281910 loss: 0.0017 lr: 0.02\n",
      "iteration: 281920 loss: 0.0020 lr: 0.02\n",
      "iteration: 281930 loss: 0.0018 lr: 0.02\n",
      "iteration: 281940 loss: 0.0028 lr: 0.02\n",
      "iteration: 281950 loss: 0.0017 lr: 0.02\n",
      "iteration: 281960 loss: 0.0021 lr: 0.02\n",
      "iteration: 281970 loss: 0.0030 lr: 0.02\n",
      "iteration: 281980 loss: 0.0017 lr: 0.02\n",
      "iteration: 281990 loss: 0.0018 lr: 0.02\n",
      "iteration: 282000 loss: 0.0019 lr: 0.02\n",
      "iteration: 282010 loss: 0.0011 lr: 0.02\n",
      "iteration: 282020 loss: 0.0015 lr: 0.02\n",
      "iteration: 282030 loss: 0.0018 lr: 0.02\n",
      "iteration: 282040 loss: 0.0020 lr: 0.02\n",
      "iteration: 282050 loss: 0.0023 lr: 0.02\n",
      "iteration: 282060 loss: 0.0017 lr: 0.02\n",
      "iteration: 282070 loss: 0.0021 lr: 0.02\n",
      "iteration: 282080 loss: 0.0021 lr: 0.02\n",
      "iteration: 282090 loss: 0.0018 lr: 0.02\n",
      "iteration: 282100 loss: 0.0014 lr: 0.02\n",
      "iteration: 282110 loss: 0.0018 lr: 0.02\n",
      "iteration: 282120 loss: 0.0019 lr: 0.02\n",
      "iteration: 282130 loss: 0.0014 lr: 0.02\n",
      "iteration: 282140 loss: 0.0013 lr: 0.02\n",
      "iteration: 282150 loss: 0.0016 lr: 0.02\n",
      "iteration: 282160 loss: 0.0017 lr: 0.02\n",
      "iteration: 282170 loss: 0.0018 lr: 0.02\n",
      "iteration: 282180 loss: 0.0015 lr: 0.02\n",
      "iteration: 282190 loss: 0.0019 lr: 0.02\n",
      "iteration: 282200 loss: 0.0021 lr: 0.02\n",
      "iteration: 282210 loss: 0.0021 lr: 0.02\n",
      "iteration: 282220 loss: 0.0014 lr: 0.02\n",
      "iteration: 282230 loss: 0.0018 lr: 0.02\n",
      "iteration: 282240 loss: 0.0017 lr: 0.02\n",
      "iteration: 282250 loss: 0.0020 lr: 0.02\n",
      "iteration: 282260 loss: 0.0017 lr: 0.02\n",
      "iteration: 282270 loss: 0.0015 lr: 0.02\n",
      "iteration: 282280 loss: 0.0023 lr: 0.02\n",
      "iteration: 282290 loss: 0.0016 lr: 0.02\n",
      "iteration: 282300 loss: 0.0015 lr: 0.02\n",
      "iteration: 282310 loss: 0.0031 lr: 0.02\n",
      "iteration: 282320 loss: 0.0015 lr: 0.02\n",
      "iteration: 282330 loss: 0.0016 lr: 0.02\n",
      "iteration: 282340 loss: 0.0020 lr: 0.02\n",
      "iteration: 282350 loss: 0.0015 lr: 0.02\n",
      "iteration: 282360 loss: 0.0016 lr: 0.02\n",
      "iteration: 282370 loss: 0.0027 lr: 0.02\n",
      "iteration: 282380 loss: 0.0023 lr: 0.02\n",
      "iteration: 282390 loss: 0.0024 lr: 0.02\n",
      "iteration: 282400 loss: 0.0012 lr: 0.02\n",
      "iteration: 282410 loss: 0.0014 lr: 0.02\n",
      "iteration: 282420 loss: 0.0019 lr: 0.02\n",
      "iteration: 282430 loss: 0.0018 lr: 0.02\n",
      "iteration: 282440 loss: 0.0016 lr: 0.02\n",
      "iteration: 282450 loss: 0.0018 lr: 0.02\n",
      "iteration: 282460 loss: 0.0016 lr: 0.02\n",
      "iteration: 282470 loss: 0.0018 lr: 0.02\n",
      "iteration: 282480 loss: 0.0016 lr: 0.02\n",
      "iteration: 282490 loss: 0.0017 lr: 0.02\n",
      "iteration: 282500 loss: 0.0018 lr: 0.02\n",
      "iteration: 282510 loss: 0.0021 lr: 0.02\n",
      "iteration: 282520 loss: 0.0023 lr: 0.02\n",
      "iteration: 282530 loss: 0.0017 lr: 0.02\n",
      "iteration: 282540 loss: 0.0029 lr: 0.02\n",
      "iteration: 282550 loss: 0.0015 lr: 0.02\n",
      "iteration: 282560 loss: 0.0020 lr: 0.02\n",
      "iteration: 282570 loss: 0.0019 lr: 0.02\n",
      "iteration: 282580 loss: 0.0018 lr: 0.02\n",
      "iteration: 282590 loss: 0.0016 lr: 0.02\n",
      "iteration: 282600 loss: 0.0014 lr: 0.02\n",
      "iteration: 282610 loss: 0.0011 lr: 0.02\n",
      "iteration: 282620 loss: 0.0015 lr: 0.02\n",
      "iteration: 282630 loss: 0.0018 lr: 0.02\n",
      "iteration: 282640 loss: 0.0016 lr: 0.02\n",
      "iteration: 282650 loss: 0.0023 lr: 0.02\n",
      "iteration: 282660 loss: 0.0025 lr: 0.02\n",
      "iteration: 282670 loss: 0.0021 lr: 0.02\n",
      "iteration: 282680 loss: 0.0018 lr: 0.02\n",
      "iteration: 282690 loss: 0.0016 lr: 0.02\n",
      "iteration: 282700 loss: 0.0015 lr: 0.02\n",
      "iteration: 282710 loss: 0.0013 lr: 0.02\n",
      "iteration: 282720 loss: 0.0028 lr: 0.02\n",
      "iteration: 282730 loss: 0.0015 lr: 0.02\n",
      "iteration: 282740 loss: 0.0018 lr: 0.02\n",
      "iteration: 282750 loss: 0.0022 lr: 0.02\n",
      "iteration: 282760 loss: 0.0017 lr: 0.02\n",
      "iteration: 282770 loss: 0.0018 lr: 0.02\n",
      "iteration: 282780 loss: 0.0021 lr: 0.02\n",
      "iteration: 282790 loss: 0.0023 lr: 0.02\n",
      "iteration: 282800 loss: 0.0024 lr: 0.02\n",
      "iteration: 282810 loss: 0.0019 lr: 0.02\n",
      "iteration: 282820 loss: 0.0018 lr: 0.02\n",
      "iteration: 282830 loss: 0.0021 lr: 0.02\n",
      "iteration: 282840 loss: 0.0016 lr: 0.02\n",
      "iteration: 282850 loss: 0.0015 lr: 0.02\n",
      "iteration: 282860 loss: 0.0029 lr: 0.02\n",
      "iteration: 282870 loss: 0.0015 lr: 0.02\n",
      "iteration: 282880 loss: 0.0019 lr: 0.02\n",
      "iteration: 282890 loss: 0.0014 lr: 0.02\n",
      "iteration: 282900 loss: 0.0017 lr: 0.02\n",
      "iteration: 282910 loss: 0.0016 lr: 0.02\n",
      "iteration: 282920 loss: 0.0021 lr: 0.02\n",
      "iteration: 282930 loss: 0.0019 lr: 0.02\n",
      "iteration: 282940 loss: 0.0015 lr: 0.02\n",
      "iteration: 282950 loss: 0.0015 lr: 0.02\n",
      "iteration: 282960 loss: 0.0013 lr: 0.02\n",
      "iteration: 282970 loss: 0.0016 lr: 0.02\n",
      "iteration: 282980 loss: 0.0021 lr: 0.02\n",
      "iteration: 282990 loss: 0.0016 lr: 0.02\n",
      "iteration: 283000 loss: 0.0017 lr: 0.02\n",
      "iteration: 283010 loss: 0.0022 lr: 0.02\n",
      "iteration: 283020 loss: 0.0015 lr: 0.02\n",
      "iteration: 283030 loss: 0.0015 lr: 0.02\n",
      "iteration: 283040 loss: 0.0017 lr: 0.02\n",
      "iteration: 283050 loss: 0.0015 lr: 0.02\n",
      "iteration: 283060 loss: 0.0029 lr: 0.02\n",
      "iteration: 283070 loss: 0.0015 lr: 0.02\n",
      "iteration: 283080 loss: 0.0018 lr: 0.02\n",
      "iteration: 283090 loss: 0.0013 lr: 0.02\n",
      "iteration: 283100 loss: 0.0016 lr: 0.02\n",
      "iteration: 283110 loss: 0.0025 lr: 0.02\n",
      "iteration: 283120 loss: 0.0029 lr: 0.02\n",
      "iteration: 283130 loss: 0.0021 lr: 0.02\n",
      "iteration: 283140 loss: 0.0018 lr: 0.02\n",
      "iteration: 283150 loss: 0.0016 lr: 0.02\n",
      "iteration: 283160 loss: 0.0014 lr: 0.02\n",
      "iteration: 283170 loss: 0.0019 lr: 0.02\n",
      "iteration: 283180 loss: 0.0025 lr: 0.02\n",
      "iteration: 283190 loss: 0.0017 lr: 0.02\n",
      "iteration: 283200 loss: 0.0024 lr: 0.02\n",
      "iteration: 283210 loss: 0.0021 lr: 0.02\n",
      "iteration: 283220 loss: 0.0027 lr: 0.02\n",
      "iteration: 283230 loss: 0.0019 lr: 0.02\n",
      "iteration: 283240 loss: 0.0021 lr: 0.02\n",
      "iteration: 283250 loss: 0.0030 lr: 0.02\n",
      "iteration: 283260 loss: 0.0021 lr: 0.02\n",
      "iteration: 283270 loss: 0.0019 lr: 0.02\n",
      "iteration: 283280 loss: 0.0018 lr: 0.02\n",
      "iteration: 283290 loss: 0.0015 lr: 0.02\n",
      "iteration: 283300 loss: 0.0018 lr: 0.02\n",
      "iteration: 283310 loss: 0.0019 lr: 0.02\n",
      "iteration: 283320 loss: 0.0014 lr: 0.02\n",
      "iteration: 283330 loss: 0.0021 lr: 0.02\n",
      "iteration: 283340 loss: 0.0012 lr: 0.02\n",
      "iteration: 283350 loss: 0.0023 lr: 0.02\n",
      "iteration: 283360 loss: 0.0017 lr: 0.02\n",
      "iteration: 283370 loss: 0.0025 lr: 0.02\n",
      "iteration: 283380 loss: 0.0022 lr: 0.02\n",
      "iteration: 283390 loss: 0.0016 lr: 0.02\n",
      "iteration: 283400 loss: 0.0014 lr: 0.02\n",
      "iteration: 283410 loss: 0.0020 lr: 0.02\n",
      "iteration: 283420 loss: 0.0016 lr: 0.02\n",
      "iteration: 283430 loss: 0.0014 lr: 0.02\n",
      "iteration: 283440 loss: 0.0016 lr: 0.02\n",
      "iteration: 283450 loss: 0.0042 lr: 0.02\n",
      "iteration: 283460 loss: 0.0024 lr: 0.02\n",
      "iteration: 283470 loss: 0.0022 lr: 0.02\n",
      "iteration: 283480 loss: 0.0024 lr: 0.02\n",
      "iteration: 283490 loss: 0.0022 lr: 0.02\n",
      "iteration: 283500 loss: 0.0019 lr: 0.02\n",
      "iteration: 283510 loss: 0.0017 lr: 0.02\n",
      "iteration: 283520 loss: 0.0012 lr: 0.02\n",
      "iteration: 283530 loss: 0.0045 lr: 0.02\n",
      "iteration: 283540 loss: 0.0023 lr: 0.02\n",
      "iteration: 283550 loss: 0.0018 lr: 0.02\n",
      "iteration: 283560 loss: 0.0024 lr: 0.02\n",
      "iteration: 283570 loss: 0.0019 lr: 0.02\n",
      "iteration: 283580 loss: 0.0017 lr: 0.02\n",
      "iteration: 283590 loss: 0.0014 lr: 0.02\n",
      "iteration: 283600 loss: 0.0016 lr: 0.02\n",
      "iteration: 283610 loss: 0.0015 lr: 0.02\n",
      "iteration: 283620 loss: 0.0016 lr: 0.02\n",
      "iteration: 283630 loss: 0.0018 lr: 0.02\n",
      "iteration: 283640 loss: 0.0015 lr: 0.02\n",
      "iteration: 283650 loss: 0.0014 lr: 0.02\n",
      "iteration: 283660 loss: 0.0017 lr: 0.02\n",
      "iteration: 283670 loss: 0.0015 lr: 0.02\n",
      "iteration: 283680 loss: 0.0019 lr: 0.02\n",
      "iteration: 283690 loss: 0.0014 lr: 0.02\n",
      "iteration: 283700 loss: 0.0017 lr: 0.02\n",
      "iteration: 283710 loss: 0.0021 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 283720 loss: 0.0017 lr: 0.02\n",
      "iteration: 283730 loss: 0.0017 lr: 0.02\n",
      "iteration: 283740 loss: 0.0011 lr: 0.02\n",
      "iteration: 283750 loss: 0.0021 lr: 0.02\n",
      "iteration: 283760 loss: 0.0019 lr: 0.02\n",
      "iteration: 283770 loss: 0.0013 lr: 0.02\n",
      "iteration: 283780 loss: 0.0015 lr: 0.02\n",
      "iteration: 283790 loss: 0.0015 lr: 0.02\n",
      "iteration: 283800 loss: 0.0034 lr: 0.02\n",
      "iteration: 283810 loss: 0.0015 lr: 0.02\n",
      "iteration: 283820 loss: 0.0040 lr: 0.02\n",
      "iteration: 283830 loss: 0.0027 lr: 0.02\n",
      "iteration: 283840 loss: 0.0022 lr: 0.02\n",
      "iteration: 283850 loss: 0.0017 lr: 0.02\n",
      "iteration: 283860 loss: 0.0016 lr: 0.02\n",
      "iteration: 283870 loss: 0.0017 lr: 0.02\n",
      "iteration: 283880 loss: 0.0016 lr: 0.02\n",
      "iteration: 283890 loss: 0.0020 lr: 0.02\n",
      "iteration: 283900 loss: 0.0018 lr: 0.02\n",
      "iteration: 283910 loss: 0.0025 lr: 0.02\n",
      "iteration: 283920 loss: 0.0020 lr: 0.02\n",
      "iteration: 283930 loss: 0.0017 lr: 0.02\n",
      "iteration: 283940 loss: 0.0024 lr: 0.02\n",
      "iteration: 283950 loss: 0.0032 lr: 0.02\n",
      "iteration: 283960 loss: 0.0015 lr: 0.02\n",
      "iteration: 283970 loss: 0.0026 lr: 0.02\n",
      "iteration: 283980 loss: 0.0019 lr: 0.02\n",
      "iteration: 283990 loss: 0.0015 lr: 0.02\n",
      "iteration: 284000 loss: 0.0026 lr: 0.02\n",
      "iteration: 284010 loss: 0.0021 lr: 0.02\n",
      "iteration: 284020 loss: 0.0019 lr: 0.02\n",
      "iteration: 284030 loss: 0.0015 lr: 0.02\n",
      "iteration: 284040 loss: 0.0016 lr: 0.02\n",
      "iteration: 284050 loss: 0.0015 lr: 0.02\n",
      "iteration: 284060 loss: 0.0013 lr: 0.02\n",
      "iteration: 284070 loss: 0.0017 lr: 0.02\n",
      "iteration: 284080 loss: 0.0013 lr: 0.02\n",
      "iteration: 284090 loss: 0.0012 lr: 0.02\n",
      "iteration: 284100 loss: 0.0019 lr: 0.02\n",
      "iteration: 284110 loss: 0.0019 lr: 0.02\n",
      "iteration: 284120 loss: 0.0020 lr: 0.02\n",
      "iteration: 284130 loss: 0.0018 lr: 0.02\n",
      "iteration: 284140 loss: 0.0022 lr: 0.02\n",
      "iteration: 284150 loss: 0.0018 lr: 0.02\n",
      "iteration: 284160 loss: 0.0025 lr: 0.02\n",
      "iteration: 284170 loss: 0.0015 lr: 0.02\n",
      "iteration: 284180 loss: 0.0022 lr: 0.02\n",
      "iteration: 284190 loss: 0.0016 lr: 0.02\n",
      "iteration: 284200 loss: 0.0014 lr: 0.02\n",
      "iteration: 284210 loss: 0.0025 lr: 0.02\n",
      "iteration: 284220 loss: 0.0020 lr: 0.02\n",
      "iteration: 284230 loss: 0.0016 lr: 0.02\n",
      "iteration: 284240 loss: 0.0028 lr: 0.02\n",
      "iteration: 284250 loss: 0.0022 lr: 0.02\n",
      "iteration: 284260 loss: 0.0015 lr: 0.02\n",
      "iteration: 284270 loss: 0.0020 lr: 0.02\n",
      "iteration: 284280 loss: 0.0017 lr: 0.02\n",
      "iteration: 284290 loss: 0.0018 lr: 0.02\n",
      "iteration: 284300 loss: 0.0018 lr: 0.02\n",
      "iteration: 284310 loss: 0.0019 lr: 0.02\n",
      "iteration: 284320 loss: 0.0016 lr: 0.02\n",
      "iteration: 284330 loss: 0.0021 lr: 0.02\n",
      "iteration: 284340 loss: 0.0017 lr: 0.02\n",
      "iteration: 284350 loss: 0.0021 lr: 0.02\n",
      "iteration: 284360 loss: 0.0015 lr: 0.02\n",
      "iteration: 284370 loss: 0.0019 lr: 0.02\n",
      "iteration: 284380 loss: 0.0016 lr: 0.02\n",
      "iteration: 284390 loss: 0.0018 lr: 0.02\n",
      "iteration: 284400 loss: 0.0022 lr: 0.02\n",
      "iteration: 284410 loss: 0.0023 lr: 0.02\n",
      "iteration: 284420 loss: 0.0028 lr: 0.02\n",
      "iteration: 284430 loss: 0.0021 lr: 0.02\n",
      "iteration: 284440 loss: 0.0026 lr: 0.02\n",
      "iteration: 284450 loss: 0.0010 lr: 0.02\n",
      "iteration: 284460 loss: 0.0016 lr: 0.02\n",
      "iteration: 284470 loss: 0.0028 lr: 0.02\n",
      "iteration: 284480 loss: 0.0022 lr: 0.02\n",
      "iteration: 284490 loss: 0.0026 lr: 0.02\n",
      "iteration: 284500 loss: 0.0020 lr: 0.02\n",
      "iteration: 284510 loss: 0.0015 lr: 0.02\n",
      "iteration: 284520 loss: 0.0020 lr: 0.02\n",
      "iteration: 284530 loss: 0.0021 lr: 0.02\n",
      "iteration: 284540 loss: 0.0016 lr: 0.02\n",
      "iteration: 284550 loss: 0.0016 lr: 0.02\n",
      "iteration: 284560 loss: 0.0015 lr: 0.02\n",
      "iteration: 284570 loss: 0.0017 lr: 0.02\n",
      "iteration: 284580 loss: 0.0022 lr: 0.02\n",
      "iteration: 284590 loss: 0.0013 lr: 0.02\n",
      "iteration: 284600 loss: 0.0018 lr: 0.02\n",
      "iteration: 284610 loss: 0.0023 lr: 0.02\n",
      "iteration: 284620 loss: 0.0036 lr: 0.02\n",
      "iteration: 284630 loss: 0.0023 lr: 0.02\n",
      "iteration: 284640 loss: 0.0015 lr: 0.02\n",
      "iteration: 284650 loss: 0.0017 lr: 0.02\n",
      "iteration: 284660 loss: 0.0020 lr: 0.02\n",
      "iteration: 284670 loss: 0.0021 lr: 0.02\n",
      "iteration: 284680 loss: 0.0013 lr: 0.02\n",
      "iteration: 284690 loss: 0.0018 lr: 0.02\n",
      "iteration: 284700 loss: 0.0019 lr: 0.02\n",
      "iteration: 284710 loss: 0.0015 lr: 0.02\n",
      "iteration: 284720 loss: 0.0015 lr: 0.02\n",
      "iteration: 284730 loss: 0.0019 lr: 0.02\n",
      "iteration: 284740 loss: 0.0013 lr: 0.02\n",
      "iteration: 284750 loss: 0.0015 lr: 0.02\n",
      "iteration: 284760 loss: 0.0018 lr: 0.02\n",
      "iteration: 284770 loss: 0.0016 lr: 0.02\n",
      "iteration: 284780 loss: 0.0016 lr: 0.02\n",
      "iteration: 284790 loss: 0.0015 lr: 0.02\n",
      "iteration: 284800 loss: 0.0027 lr: 0.02\n",
      "iteration: 284810 loss: 0.0015 lr: 0.02\n",
      "iteration: 284820 loss: 0.0018 lr: 0.02\n",
      "iteration: 284830 loss: 0.0019 lr: 0.02\n",
      "iteration: 284840 loss: 0.0018 lr: 0.02\n",
      "iteration: 284850 loss: 0.0026 lr: 0.02\n",
      "iteration: 284860 loss: 0.0023 lr: 0.02\n",
      "iteration: 284870 loss: 0.0028 lr: 0.02\n",
      "iteration: 284880 loss: 0.0015 lr: 0.02\n",
      "iteration: 284890 loss: 0.0023 lr: 0.02\n",
      "iteration: 284900 loss: 0.0015 lr: 0.02\n",
      "iteration: 284910 loss: 0.0018 lr: 0.02\n",
      "iteration: 284920 loss: 0.0021 lr: 0.02\n",
      "iteration: 284930 loss: 0.0020 lr: 0.02\n",
      "iteration: 284940 loss: 0.0030 lr: 0.02\n",
      "iteration: 284950 loss: 0.0014 lr: 0.02\n",
      "iteration: 284960 loss: 0.0017 lr: 0.02\n",
      "iteration: 284970 loss: 0.0019 lr: 0.02\n",
      "iteration: 284980 loss: 0.0020 lr: 0.02\n",
      "iteration: 284990 loss: 0.0028 lr: 0.02\n",
      "iteration: 285000 loss: 0.0023 lr: 0.02\n",
      "iteration: 285010 loss: 0.0024 lr: 0.02\n",
      "iteration: 285020 loss: 0.0031 lr: 0.02\n",
      "iteration: 285030 loss: 0.0018 lr: 0.02\n",
      "iteration: 285040 loss: 0.0025 lr: 0.02\n",
      "iteration: 285050 loss: 0.0018 lr: 0.02\n",
      "iteration: 285060 loss: 0.0023 lr: 0.02\n",
      "iteration: 285070 loss: 0.0015 lr: 0.02\n",
      "iteration: 285080 loss: 0.0020 lr: 0.02\n",
      "iteration: 285090 loss: 0.0025 lr: 0.02\n",
      "iteration: 285100 loss: 0.0018 lr: 0.02\n",
      "iteration: 285110 loss: 0.0021 lr: 0.02\n",
      "iteration: 285120 loss: 0.0016 lr: 0.02\n",
      "iteration: 285130 loss: 0.0017 lr: 0.02\n",
      "iteration: 285140 loss: 0.0020 lr: 0.02\n",
      "iteration: 285150 loss: 0.0014 lr: 0.02\n",
      "iteration: 285160 loss: 0.0018 lr: 0.02\n",
      "iteration: 285170 loss: 0.0016 lr: 0.02\n",
      "iteration: 285180 loss: 0.0018 lr: 0.02\n",
      "iteration: 285190 loss: 0.0014 lr: 0.02\n",
      "iteration: 285200 loss: 0.0025 lr: 0.02\n",
      "iteration: 285210 loss: 0.0012 lr: 0.02\n",
      "iteration: 285220 loss: 0.0016 lr: 0.02\n",
      "iteration: 285230 loss: 0.0036 lr: 0.02\n",
      "iteration: 285240 loss: 0.0023 lr: 0.02\n",
      "iteration: 285250 loss: 0.0014 lr: 0.02\n",
      "iteration: 285260 loss: 0.0021 lr: 0.02\n",
      "iteration: 285270 loss: 0.0020 lr: 0.02\n",
      "iteration: 285280 loss: 0.0019 lr: 0.02\n",
      "iteration: 285290 loss: 0.0017 lr: 0.02\n",
      "iteration: 285300 loss: 0.0016 lr: 0.02\n",
      "iteration: 285310 loss: 0.0017 lr: 0.02\n",
      "iteration: 285320 loss: 0.0017 lr: 0.02\n",
      "iteration: 285330 loss: 0.0017 lr: 0.02\n",
      "iteration: 285340 loss: 0.0016 lr: 0.02\n",
      "iteration: 285350 loss: 0.0012 lr: 0.02\n",
      "iteration: 285360 loss: 0.0016 lr: 0.02\n",
      "iteration: 285370 loss: 0.0014 lr: 0.02\n",
      "iteration: 285380 loss: 0.0011 lr: 0.02\n",
      "iteration: 285390 loss: 0.0029 lr: 0.02\n",
      "iteration: 285400 loss: 0.0013 lr: 0.02\n",
      "iteration: 285410 loss: 0.0016 lr: 0.02\n",
      "iteration: 285420 loss: 0.0017 lr: 0.02\n",
      "iteration: 285430 loss: 0.0014 lr: 0.02\n",
      "iteration: 285440 loss: 0.0021 lr: 0.02\n",
      "iteration: 285450 loss: 0.0019 lr: 0.02\n",
      "iteration: 285460 loss: 0.0016 lr: 0.02\n",
      "iteration: 285470 loss: 0.0015 lr: 0.02\n",
      "iteration: 285480 loss: 0.0018 lr: 0.02\n",
      "iteration: 285490 loss: 0.0025 lr: 0.02\n",
      "iteration: 285500 loss: 0.0016 lr: 0.02\n",
      "iteration: 285510 loss: 0.0017 lr: 0.02\n",
      "iteration: 285520 loss: 0.0014 lr: 0.02\n",
      "iteration: 285530 loss: 0.0014 lr: 0.02\n",
      "iteration: 285540 loss: 0.0033 lr: 0.02\n",
      "iteration: 285550 loss: 0.0015 lr: 0.02\n",
      "iteration: 285560 loss: 0.0025 lr: 0.02\n",
      "iteration: 285570 loss: 0.0020 lr: 0.02\n",
      "iteration: 285580 loss: 0.0021 lr: 0.02\n",
      "iteration: 285590 loss: 0.0031 lr: 0.02\n",
      "iteration: 285600 loss: 0.0018 lr: 0.02\n",
      "iteration: 285610 loss: 0.0018 lr: 0.02\n",
      "iteration: 285620 loss: 0.0022 lr: 0.02\n",
      "iteration: 285630 loss: 0.0019 lr: 0.02\n",
      "iteration: 285640 loss: 0.0016 lr: 0.02\n",
      "iteration: 285650 loss: 0.0015 lr: 0.02\n",
      "iteration: 285660 loss: 0.0014 lr: 0.02\n",
      "iteration: 285670 loss: 0.0020 lr: 0.02\n",
      "iteration: 285680 loss: 0.0020 lr: 0.02\n",
      "iteration: 285690 loss: 0.0011 lr: 0.02\n",
      "iteration: 285700 loss: 0.0016 lr: 0.02\n",
      "iteration: 285710 loss: 0.0015 lr: 0.02\n",
      "iteration: 285720 loss: 0.0018 lr: 0.02\n",
      "iteration: 285730 loss: 0.0018 lr: 0.02\n",
      "iteration: 285740 loss: 0.0016 lr: 0.02\n",
      "iteration: 285750 loss: 0.0018 lr: 0.02\n",
      "iteration: 285760 loss: 0.0017 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 285770 loss: 0.0020 lr: 0.02\n",
      "iteration: 285780 loss: 0.0020 lr: 0.02\n",
      "iteration: 285790 loss: 0.0012 lr: 0.02\n",
      "iteration: 285800 loss: 0.0012 lr: 0.02\n",
      "iteration: 285810 loss: 0.0017 lr: 0.02\n",
      "iteration: 285820 loss: 0.0020 lr: 0.02\n",
      "iteration: 285830 loss: 0.0015 lr: 0.02\n",
      "iteration: 285840 loss: 0.0022 lr: 0.02\n",
      "iteration: 285850 loss: 0.0018 lr: 0.02\n",
      "iteration: 285860 loss: 0.0017 lr: 0.02\n",
      "iteration: 285870 loss: 0.0025 lr: 0.02\n",
      "iteration: 285880 loss: 0.0015 lr: 0.02\n",
      "iteration: 285890 loss: 0.0011 lr: 0.02\n",
      "iteration: 285900 loss: 0.0025 lr: 0.02\n",
      "iteration: 285910 loss: 0.0013 lr: 0.02\n",
      "iteration: 285920 loss: 0.0023 lr: 0.02\n",
      "iteration: 285930 loss: 0.0013 lr: 0.02\n",
      "iteration: 285940 loss: 0.0021 lr: 0.02\n",
      "iteration: 285950 loss: 0.0018 lr: 0.02\n",
      "iteration: 285960 loss: 0.0023 lr: 0.02\n",
      "iteration: 285970 loss: 0.0019 lr: 0.02\n",
      "iteration: 285980 loss: 0.0013 lr: 0.02\n",
      "iteration: 285990 loss: 0.0024 lr: 0.02\n",
      "iteration: 286000 loss: 0.0015 lr: 0.02\n",
      "iteration: 286010 loss: 0.0021 lr: 0.02\n",
      "iteration: 286020 loss: 0.0015 lr: 0.02\n",
      "iteration: 286030 loss: 0.0015 lr: 0.02\n",
      "iteration: 286040 loss: 0.0024 lr: 0.02\n",
      "iteration: 286050 loss: 0.0026 lr: 0.02\n",
      "iteration: 286060 loss: 0.0017 lr: 0.02\n",
      "iteration: 286070 loss: 0.0020 lr: 0.02\n",
      "iteration: 286080 loss: 0.0013 lr: 0.02\n",
      "iteration: 286090 loss: 0.0023 lr: 0.02\n",
      "iteration: 286100 loss: 0.0016 lr: 0.02\n",
      "iteration: 286110 loss: 0.0012 lr: 0.02\n",
      "iteration: 286120 loss: 0.0025 lr: 0.02\n",
      "iteration: 286130 loss: 0.0026 lr: 0.02\n",
      "iteration: 286140 loss: 0.0022 lr: 0.02\n",
      "iteration: 286150 loss: 0.0022 lr: 0.02\n",
      "iteration: 286160 loss: 0.0016 lr: 0.02\n",
      "iteration: 286170 loss: 0.0023 lr: 0.02\n",
      "iteration: 286180 loss: 0.0015 lr: 0.02\n",
      "iteration: 286190 loss: 0.0017 lr: 0.02\n",
      "iteration: 286200 loss: 0.0020 lr: 0.02\n",
      "iteration: 286210 loss: 0.0016 lr: 0.02\n",
      "iteration: 286220 loss: 0.0017 lr: 0.02\n",
      "iteration: 286230 loss: 0.0032 lr: 0.02\n",
      "iteration: 286240 loss: 0.0019 lr: 0.02\n",
      "iteration: 286250 loss: 0.0021 lr: 0.02\n",
      "iteration: 286260 loss: 0.0016 lr: 0.02\n",
      "iteration: 286270 loss: 0.0015 lr: 0.02\n",
      "iteration: 286280 loss: 0.0015 lr: 0.02\n",
      "iteration: 286290 loss: 0.0021 lr: 0.02\n",
      "iteration: 286300 loss: 0.0020 lr: 0.02\n",
      "iteration: 286310 loss: 0.0018 lr: 0.02\n",
      "iteration: 286320 loss: 0.0023 lr: 0.02\n",
      "iteration: 286330 loss: 0.0016 lr: 0.02\n",
      "iteration: 286340 loss: 0.0017 lr: 0.02\n",
      "iteration: 286350 loss: 0.0020 lr: 0.02\n",
      "iteration: 286360 loss: 0.0018 lr: 0.02\n",
      "iteration: 286370 loss: 0.0023 lr: 0.02\n",
      "iteration: 286380 loss: 0.0032 lr: 0.02\n",
      "iteration: 286390 loss: 0.0031 lr: 0.02\n",
      "iteration: 286400 loss: 0.0013 lr: 0.02\n",
      "iteration: 286410 loss: 0.0019 lr: 0.02\n",
      "iteration: 286420 loss: 0.0013 lr: 0.02\n",
      "iteration: 286430 loss: 0.0015 lr: 0.02\n",
      "iteration: 286440 loss: 0.0014 lr: 0.02\n",
      "iteration: 286450 loss: 0.0018 lr: 0.02\n",
      "iteration: 286460 loss: 0.0021 lr: 0.02\n",
      "iteration: 286470 loss: 0.0017 lr: 0.02\n",
      "iteration: 286480 loss: 0.0016 lr: 0.02\n",
      "iteration: 286490 loss: 0.0013 lr: 0.02\n",
      "iteration: 286500 loss: 0.0019 lr: 0.02\n",
      "iteration: 286510 loss: 0.0013 lr: 0.02\n",
      "iteration: 286520 loss: 0.0029 lr: 0.02\n",
      "iteration: 286530 loss: 0.0031 lr: 0.02\n",
      "iteration: 286540 loss: 0.0022 lr: 0.02\n",
      "iteration: 286550 loss: 0.0014 lr: 0.02\n",
      "iteration: 286560 loss: 0.0013 lr: 0.02\n",
      "iteration: 286570 loss: 0.0019 lr: 0.02\n",
      "iteration: 286580 loss: 0.0019 lr: 0.02\n",
      "iteration: 286590 loss: 0.0023 lr: 0.02\n",
      "iteration: 286600 loss: 0.0029 lr: 0.02\n",
      "iteration: 286610 loss: 0.0014 lr: 0.02\n",
      "iteration: 286620 loss: 0.0012 lr: 0.02\n",
      "iteration: 286630 loss: 0.0018 lr: 0.02\n",
      "iteration: 286640 loss: 0.0016 lr: 0.02\n",
      "iteration: 286650 loss: 0.0013 lr: 0.02\n",
      "iteration: 286660 loss: 0.0015 lr: 0.02\n",
      "iteration: 286670 loss: 0.0017 lr: 0.02\n",
      "iteration: 286680 loss: 0.0019 lr: 0.02\n",
      "iteration: 286690 loss: 0.0020 lr: 0.02\n",
      "iteration: 286700 loss: 0.0020 lr: 0.02\n",
      "iteration: 286710 loss: 0.0016 lr: 0.02\n",
      "iteration: 286720 loss: 0.0009 lr: 0.02\n",
      "iteration: 286730 loss: 0.0026 lr: 0.02\n",
      "iteration: 286740 loss: 0.0022 lr: 0.02\n",
      "iteration: 286750 loss: 0.0013 lr: 0.02\n",
      "iteration: 286760 loss: 0.0015 lr: 0.02\n",
      "iteration: 286770 loss: 0.0023 lr: 0.02\n",
      "iteration: 286780 loss: 0.0021 lr: 0.02\n",
      "iteration: 286790 loss: 0.0019 lr: 0.02\n",
      "iteration: 286800 loss: 0.0014 lr: 0.02\n",
      "iteration: 286810 loss: 0.0016 lr: 0.02\n",
      "iteration: 286820 loss: 0.0018 lr: 0.02\n",
      "iteration: 286830 loss: 0.0015 lr: 0.02\n",
      "iteration: 286840 loss: 0.0023 lr: 0.02\n",
      "iteration: 286850 loss: 0.0015 lr: 0.02\n",
      "iteration: 286860 loss: 0.0017 lr: 0.02\n",
      "iteration: 286870 loss: 0.0013 lr: 0.02\n",
      "iteration: 286880 loss: 0.0025 lr: 0.02\n",
      "iteration: 286890 loss: 0.0019 lr: 0.02\n",
      "iteration: 286900 loss: 0.0019 lr: 0.02\n",
      "iteration: 286910 loss: 0.0017 lr: 0.02\n",
      "iteration: 286920 loss: 0.0017 lr: 0.02\n",
      "iteration: 286930 loss: 0.0016 lr: 0.02\n",
      "iteration: 286940 loss: 0.0014 lr: 0.02\n",
      "iteration: 286950 loss: 0.0020 lr: 0.02\n",
      "iteration: 286960 loss: 0.0022 lr: 0.02\n",
      "iteration: 286970 loss: 0.0026 lr: 0.02\n",
      "iteration: 286980 loss: 0.0019 lr: 0.02\n",
      "iteration: 286990 loss: 0.0016 lr: 0.02\n",
      "iteration: 287000 loss: 0.0019 lr: 0.02\n",
      "iteration: 287010 loss: 0.0017 lr: 0.02\n",
      "iteration: 287020 loss: 0.0016 lr: 0.02\n",
      "iteration: 287030 loss: 0.0019 lr: 0.02\n",
      "iteration: 287040 loss: 0.0017 lr: 0.02\n",
      "iteration: 287050 loss: 0.0017 lr: 0.02\n",
      "iteration: 287060 loss: 0.0019 lr: 0.02\n",
      "iteration: 287070 loss: 0.0018 lr: 0.02\n",
      "iteration: 287080 loss: 0.0022 lr: 0.02\n",
      "iteration: 287090 loss: 0.0014 lr: 0.02\n",
      "iteration: 287100 loss: 0.0017 lr: 0.02\n",
      "iteration: 287110 loss: 0.0021 lr: 0.02\n",
      "iteration: 287120 loss: 0.0024 lr: 0.02\n",
      "iteration: 287130 loss: 0.0014 lr: 0.02\n",
      "iteration: 287140 loss: 0.0018 lr: 0.02\n",
      "iteration: 287150 loss: 0.0015 lr: 0.02\n",
      "iteration: 287160 loss: 0.0025 lr: 0.02\n",
      "iteration: 287170 loss: 0.0017 lr: 0.02\n",
      "iteration: 287180 loss: 0.0016 lr: 0.02\n",
      "iteration: 287190 loss: 0.0015 lr: 0.02\n",
      "iteration: 287200 loss: 0.0020 lr: 0.02\n",
      "iteration: 287210 loss: 0.0015 lr: 0.02\n",
      "iteration: 287220 loss: 0.0017 lr: 0.02\n",
      "iteration: 287230 loss: 0.0017 lr: 0.02\n",
      "iteration: 287240 loss: 0.0028 lr: 0.02\n",
      "iteration: 287250 loss: 0.0020 lr: 0.02\n",
      "iteration: 287260 loss: 0.0016 lr: 0.02\n",
      "iteration: 287270 loss: 0.0017 lr: 0.02\n",
      "iteration: 287280 loss: 0.0014 lr: 0.02\n",
      "iteration: 287290 loss: 0.0013 lr: 0.02\n",
      "iteration: 287300 loss: 0.0016 lr: 0.02\n",
      "iteration: 287310 loss: 0.0023 lr: 0.02\n",
      "iteration: 287320 loss: 0.0020 lr: 0.02\n",
      "iteration: 287330 loss: 0.0020 lr: 0.02\n",
      "iteration: 287340 loss: 0.0022 lr: 0.02\n",
      "iteration: 287350 loss: 0.0020 lr: 0.02\n",
      "iteration: 287360 loss: 0.0012 lr: 0.02\n",
      "iteration: 287370 loss: 0.0014 lr: 0.02\n",
      "iteration: 287380 loss: 0.0016 lr: 0.02\n",
      "iteration: 287390 loss: 0.0015 lr: 0.02\n",
      "iteration: 287400 loss: 0.0024 lr: 0.02\n",
      "iteration: 287410 loss: 0.0020 lr: 0.02\n",
      "iteration: 287420 loss: 0.0014 lr: 0.02\n",
      "iteration: 287430 loss: 0.0016 lr: 0.02\n",
      "iteration: 287440 loss: 0.0018 lr: 0.02\n",
      "iteration: 287450 loss: 0.0016 lr: 0.02\n",
      "iteration: 287460 loss: 0.0018 lr: 0.02\n",
      "iteration: 287470 loss: 0.0016 lr: 0.02\n",
      "iteration: 287480 loss: 0.0020 lr: 0.02\n",
      "iteration: 287490 loss: 0.0030 lr: 0.02\n",
      "iteration: 287500 loss: 0.0026 lr: 0.02\n",
      "iteration: 287510 loss: 0.0017 lr: 0.02\n",
      "iteration: 287520 loss: 0.0015 lr: 0.02\n",
      "iteration: 287530 loss: 0.0021 lr: 0.02\n",
      "iteration: 287540 loss: 0.0025 lr: 0.02\n",
      "iteration: 287550 loss: 0.0020 lr: 0.02\n",
      "iteration: 287560 loss: 0.0013 lr: 0.02\n",
      "iteration: 287570 loss: 0.0016 lr: 0.02\n",
      "iteration: 287580 loss: 0.0019 lr: 0.02\n",
      "iteration: 287590 loss: 0.0027 lr: 0.02\n",
      "iteration: 287600 loss: 0.0020 lr: 0.02\n",
      "iteration: 287610 loss: 0.0012 lr: 0.02\n",
      "iteration: 287620 loss: 0.0012 lr: 0.02\n",
      "iteration: 287630 loss: 0.0016 lr: 0.02\n",
      "iteration: 287640 loss: 0.0013 lr: 0.02\n",
      "iteration: 287650 loss: 0.0014 lr: 0.02\n",
      "iteration: 287660 loss: 0.0015 lr: 0.02\n",
      "iteration: 287670 loss: 0.0020 lr: 0.02\n",
      "iteration: 287680 loss: 0.0018 lr: 0.02\n",
      "iteration: 287690 loss: 0.0022 lr: 0.02\n",
      "iteration: 287700 loss: 0.0019 lr: 0.02\n",
      "iteration: 287710 loss: 0.0014 lr: 0.02\n",
      "iteration: 287720 loss: 0.0012 lr: 0.02\n",
      "iteration: 287730 loss: 0.0025 lr: 0.02\n",
      "iteration: 287740 loss: 0.0011 lr: 0.02\n",
      "iteration: 287750 loss: 0.0020 lr: 0.02\n",
      "iteration: 287760 loss: 0.0012 lr: 0.02\n",
      "iteration: 287770 loss: 0.0021 lr: 0.02\n",
      "iteration: 287780 loss: 0.0016 lr: 0.02\n",
      "iteration: 287790 loss: 0.0019 lr: 0.02\n",
      "iteration: 287800 loss: 0.0025 lr: 0.02\n",
      "iteration: 287810 loss: 0.0017 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 287820 loss: 0.0022 lr: 0.02\n",
      "iteration: 287830 loss: 0.0019 lr: 0.02\n",
      "iteration: 287840 loss: 0.0017 lr: 0.02\n",
      "iteration: 287850 loss: 0.0012 lr: 0.02\n",
      "iteration: 287860 loss: 0.0018 lr: 0.02\n",
      "iteration: 287870 loss: 0.0021 lr: 0.02\n",
      "iteration: 287880 loss: 0.0016 lr: 0.02\n",
      "iteration: 287890 loss: 0.0017 lr: 0.02\n",
      "iteration: 287900 loss: 0.0021 lr: 0.02\n",
      "iteration: 287910 loss: 0.0018 lr: 0.02\n",
      "iteration: 287920 loss: 0.0018 lr: 0.02\n",
      "iteration: 287930 loss: 0.0027 lr: 0.02\n",
      "iteration: 287940 loss: 0.0019 lr: 0.02\n",
      "iteration: 287950 loss: 0.0014 lr: 0.02\n",
      "iteration: 287960 loss: 0.0016 lr: 0.02\n",
      "iteration: 287970 loss: 0.0017 lr: 0.02\n",
      "iteration: 287980 loss: 0.0016 lr: 0.02\n",
      "iteration: 287990 loss: 0.0019 lr: 0.02\n",
      "iteration: 288000 loss: 0.0015 lr: 0.02\n",
      "iteration: 288010 loss: 0.0012 lr: 0.02\n",
      "iteration: 288020 loss: 0.0021 lr: 0.02\n",
      "iteration: 288030 loss: 0.0017 lr: 0.02\n",
      "iteration: 288040 loss: 0.0018 lr: 0.02\n",
      "iteration: 288050 loss: 0.0014 lr: 0.02\n",
      "iteration: 288060 loss: 0.0013 lr: 0.02\n",
      "iteration: 288070 loss: 0.0012 lr: 0.02\n",
      "iteration: 288080 loss: 0.0018 lr: 0.02\n",
      "iteration: 288090 loss: 0.0016 lr: 0.02\n",
      "iteration: 288100 loss: 0.0014 lr: 0.02\n",
      "iteration: 288110 loss: 0.0017 lr: 0.02\n",
      "iteration: 288120 loss: 0.0015 lr: 0.02\n",
      "iteration: 288130 loss: 0.0011 lr: 0.02\n",
      "iteration: 288140 loss: 0.0020 lr: 0.02\n",
      "iteration: 288150 loss: 0.0026 lr: 0.02\n",
      "iteration: 288160 loss: 0.0013 lr: 0.02\n",
      "iteration: 288170 loss: 0.0016 lr: 0.02\n",
      "iteration: 288180 loss: 0.0016 lr: 0.02\n",
      "iteration: 288190 loss: 0.0015 lr: 0.02\n",
      "iteration: 288200 loss: 0.0024 lr: 0.02\n",
      "iteration: 288210 loss: 0.0017 lr: 0.02\n",
      "iteration: 288220 loss: 0.0019 lr: 0.02\n",
      "iteration: 288230 loss: 0.0012 lr: 0.02\n",
      "iteration: 288240 loss: 0.0020 lr: 0.02\n",
      "iteration: 288250 loss: 0.0024 lr: 0.02\n",
      "iteration: 288260 loss: 0.0024 lr: 0.02\n",
      "iteration: 288270 loss: 0.0025 lr: 0.02\n",
      "iteration: 288280 loss: 0.0020 lr: 0.02\n",
      "iteration: 288290 loss: 0.0021 lr: 0.02\n",
      "iteration: 288300 loss: 0.0019 lr: 0.02\n",
      "iteration: 288310 loss: 0.0019 lr: 0.02\n",
      "iteration: 288320 loss: 0.0026 lr: 0.02\n",
      "iteration: 288330 loss: 0.0023 lr: 0.02\n",
      "iteration: 288340 loss: 0.0019 lr: 0.02\n",
      "iteration: 288350 loss: 0.0017 lr: 0.02\n",
      "iteration: 288360 loss: 0.0012 lr: 0.02\n",
      "iteration: 288370 loss: 0.0019 lr: 0.02\n",
      "iteration: 288380 loss: 0.0020 lr: 0.02\n",
      "iteration: 288390 loss: 0.0018 lr: 0.02\n",
      "iteration: 288400 loss: 0.0017 lr: 0.02\n",
      "iteration: 288410 loss: 0.0030 lr: 0.02\n",
      "iteration: 288420 loss: 0.0016 lr: 0.02\n",
      "iteration: 288430 loss: 0.0016 lr: 0.02\n",
      "iteration: 288440 loss: 0.0018 lr: 0.02\n",
      "iteration: 288450 loss: 0.0018 lr: 0.02\n",
      "iteration: 288460 loss: 0.0014 lr: 0.02\n",
      "iteration: 288470 loss: 0.0021 lr: 0.02\n",
      "iteration: 288480 loss: 0.0018 lr: 0.02\n",
      "iteration: 288490 loss: 0.0013 lr: 0.02\n",
      "iteration: 288500 loss: 0.0014 lr: 0.02\n",
      "iteration: 288510 loss: 0.0015 lr: 0.02\n",
      "iteration: 288520 loss: 0.0021 lr: 0.02\n",
      "iteration: 288530 loss: 0.0016 lr: 0.02\n",
      "iteration: 288540 loss: 0.0014 lr: 0.02\n",
      "iteration: 288550 loss: 0.0017 lr: 0.02\n",
      "iteration: 288560 loss: 0.0013 lr: 0.02\n",
      "iteration: 288570 loss: 0.0015 lr: 0.02\n",
      "iteration: 288580 loss: 0.0015 lr: 0.02\n",
      "iteration: 288590 loss: 0.0016 lr: 0.02\n",
      "iteration: 288600 loss: 0.0012 lr: 0.02\n",
      "iteration: 288610 loss: 0.0017 lr: 0.02\n",
      "iteration: 288620 loss: 0.0019 lr: 0.02\n",
      "iteration: 288630 loss: 0.0018 lr: 0.02\n",
      "iteration: 288640 loss: 0.0020 lr: 0.02\n",
      "iteration: 288650 loss: 0.0025 lr: 0.02\n",
      "iteration: 288660 loss: 0.0018 lr: 0.02\n",
      "iteration: 288670 loss: 0.0013 lr: 0.02\n",
      "iteration: 288680 loss: 0.0016 lr: 0.02\n",
      "iteration: 288690 loss: 0.0015 lr: 0.02\n",
      "iteration: 288700 loss: 0.0014 lr: 0.02\n",
      "iteration: 288710 loss: 0.0016 lr: 0.02\n",
      "iteration: 288720 loss: 0.0014 lr: 0.02\n",
      "iteration: 288730 loss: 0.0021 lr: 0.02\n",
      "iteration: 288740 loss: 0.0014 lr: 0.02\n",
      "iteration: 288750 loss: 0.0014 lr: 0.02\n",
      "iteration: 288760 loss: 0.0023 lr: 0.02\n",
      "iteration: 288770 loss: 0.0017 lr: 0.02\n",
      "iteration: 288780 loss: 0.0029 lr: 0.02\n",
      "iteration: 288790 loss: 0.0019 lr: 0.02\n",
      "iteration: 288800 loss: 0.0025 lr: 0.02\n",
      "iteration: 288810 loss: 0.0014 lr: 0.02\n",
      "iteration: 288820 loss: 0.0015 lr: 0.02\n",
      "iteration: 288830 loss: 0.0025 lr: 0.02\n",
      "iteration: 288840 loss: 0.0013 lr: 0.02\n",
      "iteration: 288850 loss: 0.0022 lr: 0.02\n",
      "iteration: 288860 loss: 0.0013 lr: 0.02\n",
      "iteration: 288870 loss: 0.0022 lr: 0.02\n",
      "iteration: 288880 loss: 0.0019 lr: 0.02\n",
      "iteration: 288890 loss: 0.0015 lr: 0.02\n",
      "iteration: 288900 loss: 0.0019 lr: 0.02\n",
      "iteration: 288910 loss: 0.0014 lr: 0.02\n",
      "iteration: 288920 loss: 0.0013 lr: 0.02\n",
      "iteration: 288930 loss: 0.0018 lr: 0.02\n",
      "iteration: 288940 loss: 0.0012 lr: 0.02\n",
      "iteration: 288950 loss: 0.0017 lr: 0.02\n",
      "iteration: 288960 loss: 0.0017 lr: 0.02\n",
      "iteration: 288970 loss: 0.0016 lr: 0.02\n",
      "iteration: 288980 loss: 0.0018 lr: 0.02\n",
      "iteration: 288990 loss: 0.0014 lr: 0.02\n",
      "iteration: 289000 loss: 0.0017 lr: 0.02\n",
      "iteration: 289010 loss: 0.0015 lr: 0.02\n",
      "iteration: 289020 loss: 0.0015 lr: 0.02\n",
      "iteration: 289030 loss: 0.0023 lr: 0.02\n",
      "iteration: 289040 loss: 0.0019 lr: 0.02\n",
      "iteration: 289050 loss: 0.0015 lr: 0.02\n",
      "iteration: 289060 loss: 0.0018 lr: 0.02\n",
      "iteration: 289070 loss: 0.0017 lr: 0.02\n",
      "iteration: 289080 loss: 0.0018 lr: 0.02\n",
      "iteration: 289090 loss: 0.0022 lr: 0.02\n",
      "iteration: 289100 loss: 0.0019 lr: 0.02\n",
      "iteration: 289110 loss: 0.0015 lr: 0.02\n",
      "iteration: 289120 loss: 0.0018 lr: 0.02\n",
      "iteration: 289130 loss: 0.0019 lr: 0.02\n",
      "iteration: 289140 loss: 0.0017 lr: 0.02\n",
      "iteration: 289150 loss: 0.0025 lr: 0.02\n",
      "iteration: 289160 loss: 0.0025 lr: 0.02\n",
      "iteration: 289170 loss: 0.0019 lr: 0.02\n",
      "iteration: 289180 loss: 0.0023 lr: 0.02\n",
      "iteration: 289190 loss: 0.0013 lr: 0.02\n",
      "iteration: 289200 loss: 0.0016 lr: 0.02\n",
      "iteration: 289210 loss: 0.0019 lr: 0.02\n",
      "iteration: 289220 loss: 0.0022 lr: 0.02\n",
      "iteration: 289230 loss: 0.0016 lr: 0.02\n",
      "iteration: 289240 loss: 0.0016 lr: 0.02\n",
      "iteration: 289250 loss: 0.0018 lr: 0.02\n",
      "iteration: 289260 loss: 0.0020 lr: 0.02\n",
      "iteration: 289270 loss: 0.0017 lr: 0.02\n",
      "iteration: 289280 loss: 0.0020 lr: 0.02\n",
      "iteration: 289290 loss: 0.0019 lr: 0.02\n",
      "iteration: 289300 loss: 0.0017 lr: 0.02\n",
      "iteration: 289310 loss: 0.0014 lr: 0.02\n",
      "iteration: 289320 loss: 0.0014 lr: 0.02\n",
      "iteration: 289330 loss: 0.0016 lr: 0.02\n",
      "iteration: 289340 loss: 0.0049 lr: 0.02\n",
      "iteration: 289350 loss: 0.0037 lr: 0.02\n",
      "iteration: 289360 loss: 0.0018 lr: 0.02\n",
      "iteration: 289370 loss: 0.0027 lr: 0.02\n",
      "iteration: 289380 loss: 0.0020 lr: 0.02\n",
      "iteration: 289390 loss: 0.0014 lr: 0.02\n",
      "iteration: 289400 loss: 0.0014 lr: 0.02\n",
      "iteration: 289410 loss: 0.0020 lr: 0.02\n",
      "iteration: 289420 loss: 0.0014 lr: 0.02\n",
      "iteration: 289430 loss: 0.0018 lr: 0.02\n",
      "iteration: 289440 loss: 0.0023 lr: 0.02\n",
      "iteration: 289450 loss: 0.0015 lr: 0.02\n",
      "iteration: 289460 loss: 0.0017 lr: 0.02\n",
      "iteration: 289470 loss: 0.0022 lr: 0.02\n",
      "iteration: 289480 loss: 0.0014 lr: 0.02\n",
      "iteration: 289490 loss: 0.0013 lr: 0.02\n",
      "iteration: 289500 loss: 0.0022 lr: 0.02\n",
      "iteration: 289510 loss: 0.0015 lr: 0.02\n",
      "iteration: 289520 loss: 0.0026 lr: 0.02\n",
      "iteration: 289530 loss: 0.0019 lr: 0.02\n",
      "iteration: 289540 loss: 0.0019 lr: 0.02\n",
      "iteration: 289550 loss: 0.0015 lr: 0.02\n",
      "iteration: 289560 loss: 0.0032 lr: 0.02\n",
      "iteration: 289570 loss: 0.0016 lr: 0.02\n",
      "iteration: 289580 loss: 0.0026 lr: 0.02\n",
      "iteration: 289590 loss: 0.0027 lr: 0.02\n",
      "iteration: 289600 loss: 0.0015 lr: 0.02\n",
      "iteration: 289610 loss: 0.0013 lr: 0.02\n",
      "iteration: 289620 loss: 0.0014 lr: 0.02\n",
      "iteration: 289630 loss: 0.0020 lr: 0.02\n",
      "iteration: 289640 loss: 0.0012 lr: 0.02\n",
      "iteration: 289650 loss: 0.0012 lr: 0.02\n",
      "iteration: 289660 loss: 0.0013 lr: 0.02\n",
      "iteration: 289670 loss: 0.0013 lr: 0.02\n",
      "iteration: 289680 loss: 0.0018 lr: 0.02\n",
      "iteration: 289690 loss: 0.0023 lr: 0.02\n",
      "iteration: 289700 loss: 0.0018 lr: 0.02\n",
      "iteration: 289710 loss: 0.0014 lr: 0.02\n",
      "iteration: 289720 loss: 0.0015 lr: 0.02\n",
      "iteration: 289730 loss: 0.0024 lr: 0.02\n",
      "iteration: 289740 loss: 0.0018 lr: 0.02\n",
      "iteration: 289750 loss: 0.0020 lr: 0.02\n",
      "iteration: 289760 loss: 0.0015 lr: 0.02\n",
      "iteration: 289770 loss: 0.0017 lr: 0.02\n",
      "iteration: 289780 loss: 0.0023 lr: 0.02\n",
      "iteration: 289790 loss: 0.0017 lr: 0.02\n",
      "iteration: 289800 loss: 0.0016 lr: 0.02\n",
      "iteration: 289810 loss: 0.0016 lr: 0.02\n",
      "iteration: 289820 loss: 0.0015 lr: 0.02\n",
      "iteration: 289830 loss: 0.0015 lr: 0.02\n",
      "iteration: 289840 loss: 0.0018 lr: 0.02\n",
      "iteration: 289850 loss: 0.0022 lr: 0.02\n",
      "iteration: 289860 loss: 0.0023 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 289870 loss: 0.0020 lr: 0.02\n",
      "iteration: 289880 loss: 0.0018 lr: 0.02\n",
      "iteration: 289890 loss: 0.0018 lr: 0.02\n",
      "iteration: 289900 loss: 0.0023 lr: 0.02\n",
      "iteration: 289910 loss: 0.0016 lr: 0.02\n",
      "iteration: 289920 loss: 0.0017 lr: 0.02\n",
      "iteration: 289930 loss: 0.0015 lr: 0.02\n",
      "iteration: 289940 loss: 0.0021 lr: 0.02\n",
      "iteration: 289950 loss: 0.0012 lr: 0.02\n",
      "iteration: 289960 loss: 0.0023 lr: 0.02\n",
      "iteration: 289970 loss: 0.0018 lr: 0.02\n",
      "iteration: 289980 loss: 0.0017 lr: 0.02\n",
      "iteration: 289990 loss: 0.0014 lr: 0.02\n",
      "iteration: 290000 loss: 0.0011 lr: 0.02\n",
      "iteration: 290010 loss: 0.0030 lr: 0.02\n",
      "iteration: 290020 loss: 0.0019 lr: 0.02\n",
      "iteration: 290030 loss: 0.0014 lr: 0.02\n",
      "iteration: 290040 loss: 0.0021 lr: 0.02\n",
      "iteration: 290050 loss: 0.0039 lr: 0.02\n",
      "iteration: 290060 loss: 0.0014 lr: 0.02\n",
      "iteration: 290070 loss: 0.0018 lr: 0.02\n",
      "iteration: 290080 loss: 0.0019 lr: 0.02\n",
      "iteration: 290090 loss: 0.0016 lr: 0.02\n",
      "iteration: 290100 loss: 0.0013 lr: 0.02\n",
      "iteration: 290110 loss: 0.0013 lr: 0.02\n",
      "iteration: 290120 loss: 0.0023 lr: 0.02\n",
      "iteration: 290130 loss: 0.0017 lr: 0.02\n",
      "iteration: 290140 loss: 0.0014 lr: 0.02\n",
      "iteration: 290150 loss: 0.0026 lr: 0.02\n",
      "iteration: 290160 loss: 0.0023 lr: 0.02\n",
      "iteration: 290170 loss: 0.0016 lr: 0.02\n",
      "iteration: 290180 loss: 0.0016 lr: 0.02\n",
      "iteration: 290190 loss: 0.0014 lr: 0.02\n",
      "iteration: 290200 loss: 0.0011 lr: 0.02\n",
      "iteration: 290210 loss: 0.0016 lr: 0.02\n",
      "iteration: 290220 loss: 0.0019 lr: 0.02\n",
      "iteration: 290230 loss: 0.0020 lr: 0.02\n",
      "iteration: 290240 loss: 0.0017 lr: 0.02\n",
      "iteration: 290250 loss: 0.0013 lr: 0.02\n",
      "iteration: 290260 loss: 0.0015 lr: 0.02\n",
      "iteration: 290270 loss: 0.0027 lr: 0.02\n",
      "iteration: 290280 loss: 0.0026 lr: 0.02\n",
      "iteration: 290290 loss: 0.0022 lr: 0.02\n",
      "iteration: 290300 loss: 0.0028 lr: 0.02\n",
      "iteration: 290310 loss: 0.0016 lr: 0.02\n",
      "iteration: 290320 loss: 0.0025 lr: 0.02\n",
      "iteration: 290330 loss: 0.0019 lr: 0.02\n",
      "iteration: 290340 loss: 0.0015 lr: 0.02\n",
      "iteration: 290350 loss: 0.0023 lr: 0.02\n",
      "iteration: 290360 loss: 0.0020 lr: 0.02\n",
      "iteration: 290370 loss: 0.0013 lr: 0.02\n",
      "iteration: 290380 loss: 0.0019 lr: 0.02\n",
      "iteration: 290390 loss: 0.0013 lr: 0.02\n",
      "iteration: 290400 loss: 0.0019 lr: 0.02\n",
      "iteration: 290410 loss: 0.0016 lr: 0.02\n",
      "iteration: 290420 loss: 0.0019 lr: 0.02\n",
      "iteration: 290430 loss: 0.0019 lr: 0.02\n",
      "iteration: 290440 loss: 0.0026 lr: 0.02\n",
      "iteration: 290450 loss: 0.0016 lr: 0.02\n",
      "iteration: 290460 loss: 0.0025 lr: 0.02\n",
      "iteration: 290470 loss: 0.0017 lr: 0.02\n",
      "iteration: 290480 loss: 0.0021 lr: 0.02\n",
      "iteration: 290490 loss: 0.0028 lr: 0.02\n",
      "iteration: 290500 loss: 0.0022 lr: 0.02\n",
      "iteration: 290510 loss: 0.0022 lr: 0.02\n",
      "iteration: 290520 loss: 0.0027 lr: 0.02\n",
      "iteration: 290530 loss: 0.0026 lr: 0.02\n",
      "iteration: 290540 loss: 0.0021 lr: 0.02\n",
      "iteration: 290550 loss: 0.0020 lr: 0.02\n",
      "iteration: 290560 loss: 0.0023 lr: 0.02\n",
      "iteration: 290570 loss: 0.0019 lr: 0.02\n",
      "iteration: 290580 loss: 0.0017 lr: 0.02\n",
      "iteration: 290590 loss: 0.0014 lr: 0.02\n",
      "iteration: 290600 loss: 0.0020 lr: 0.02\n",
      "iteration: 290610 loss: 0.0018 lr: 0.02\n",
      "iteration: 290620 loss: 0.0016 lr: 0.02\n",
      "iteration: 290630 loss: 0.0016 lr: 0.02\n",
      "iteration: 290640 loss: 0.0027 lr: 0.02\n",
      "iteration: 290650 loss: 0.0013 lr: 0.02\n",
      "iteration: 290660 loss: 0.0014 lr: 0.02\n",
      "iteration: 290670 loss: 0.0019 lr: 0.02\n",
      "iteration: 290680 loss: 0.0019 lr: 0.02\n",
      "iteration: 290690 loss: 0.0014 lr: 0.02\n",
      "iteration: 290700 loss: 0.0018 lr: 0.02\n",
      "iteration: 290710 loss: 0.0016 lr: 0.02\n",
      "iteration: 290720 loss: 0.0020 lr: 0.02\n",
      "iteration: 290730 loss: 0.0026 lr: 0.02\n",
      "iteration: 290740 loss: 0.0020 lr: 0.02\n",
      "iteration: 290750 loss: 0.0021 lr: 0.02\n",
      "iteration: 290760 loss: 0.0017 lr: 0.02\n",
      "iteration: 290770 loss: 0.0015 lr: 0.02\n",
      "iteration: 290780 loss: 0.0021 lr: 0.02\n",
      "iteration: 290790 loss: 0.0021 lr: 0.02\n",
      "iteration: 290800 loss: 0.0019 lr: 0.02\n",
      "iteration: 290810 loss: 0.0024 lr: 0.02\n",
      "iteration: 290820 loss: 0.0017 lr: 0.02\n",
      "iteration: 290830 loss: 0.0015 lr: 0.02\n",
      "iteration: 290840 loss: 0.0014 lr: 0.02\n",
      "iteration: 290850 loss: 0.0012 lr: 0.02\n",
      "iteration: 290860 loss: 0.0013 lr: 0.02\n",
      "iteration: 290870 loss: 0.0021 lr: 0.02\n",
      "iteration: 290880 loss: 0.0017 lr: 0.02\n",
      "iteration: 290890 loss: 0.0014 lr: 0.02\n",
      "iteration: 290900 loss: 0.0014 lr: 0.02\n",
      "iteration: 290910 loss: 0.0016 lr: 0.02\n",
      "iteration: 290920 loss: 0.0013 lr: 0.02\n",
      "iteration: 290930 loss: 0.0019 lr: 0.02\n",
      "iteration: 290940 loss: 0.0017 lr: 0.02\n",
      "iteration: 290950 loss: 0.0015 lr: 0.02\n",
      "iteration: 290960 loss: 0.0019 lr: 0.02\n",
      "iteration: 290970 loss: 0.0017 lr: 0.02\n",
      "iteration: 290980 loss: 0.0018 lr: 0.02\n",
      "iteration: 290990 loss: 0.0014 lr: 0.02\n",
      "iteration: 291000 loss: 0.0018 lr: 0.02\n",
      "iteration: 291010 loss: 0.0020 lr: 0.02\n",
      "iteration: 291020 loss: 0.0018 lr: 0.02\n",
      "iteration: 291030 loss: 0.0014 lr: 0.02\n",
      "iteration: 291040 loss: 0.0018 lr: 0.02\n",
      "iteration: 291050 loss: 0.0024 lr: 0.02\n",
      "iteration: 291060 loss: 0.0017 lr: 0.02\n",
      "iteration: 291070 loss: 0.0021 lr: 0.02\n",
      "iteration: 291080 loss: 0.0016 lr: 0.02\n",
      "iteration: 291090 loss: 0.0015 lr: 0.02\n",
      "iteration: 291100 loss: 0.0017 lr: 0.02\n",
      "iteration: 291110 loss: 0.0018 lr: 0.02\n",
      "iteration: 291120 loss: 0.0015 lr: 0.02\n",
      "iteration: 291130 loss: 0.0015 lr: 0.02\n",
      "iteration: 291140 loss: 0.0016 lr: 0.02\n",
      "iteration: 291150 loss: 0.0018 lr: 0.02\n",
      "iteration: 291160 loss: 0.0023 lr: 0.02\n",
      "iteration: 291170 loss: 0.0023 lr: 0.02\n",
      "iteration: 291180 loss: 0.0018 lr: 0.02\n",
      "iteration: 291190 loss: 0.0024 lr: 0.02\n",
      "iteration: 291200 loss: 0.0017 lr: 0.02\n",
      "iteration: 291210 loss: 0.0029 lr: 0.02\n",
      "iteration: 291220 loss: 0.0013 lr: 0.02\n",
      "iteration: 291230 loss: 0.0020 lr: 0.02\n",
      "iteration: 291240 loss: 0.0013 lr: 0.02\n",
      "iteration: 291250 loss: 0.0023 lr: 0.02\n",
      "iteration: 291260 loss: 0.0019 lr: 0.02\n",
      "iteration: 291270 loss: 0.0024 lr: 0.02\n",
      "iteration: 291280 loss: 0.0023 lr: 0.02\n",
      "iteration: 291290 loss: 0.0013 lr: 0.02\n",
      "iteration: 291300 loss: 0.0019 lr: 0.02\n",
      "iteration: 291310 loss: 0.0017 lr: 0.02\n",
      "iteration: 291320 loss: 0.0025 lr: 0.02\n",
      "iteration: 291330 loss: 0.0024 lr: 0.02\n",
      "iteration: 291340 loss: 0.0024 lr: 0.02\n",
      "iteration: 291350 loss: 0.0015 lr: 0.02\n",
      "iteration: 291360 loss: 0.0012 lr: 0.02\n",
      "iteration: 291370 loss: 0.0017 lr: 0.02\n",
      "iteration: 291380 loss: 0.0017 lr: 0.02\n",
      "iteration: 291390 loss: 0.0023 lr: 0.02\n",
      "iteration: 291400 loss: 0.0015 lr: 0.02\n",
      "iteration: 291410 loss: 0.0021 lr: 0.02\n",
      "iteration: 291420 loss: 0.0013 lr: 0.02\n",
      "iteration: 291430 loss: 0.0014 lr: 0.02\n",
      "iteration: 291440 loss: 0.0015 lr: 0.02\n",
      "iteration: 291450 loss: 0.0022 lr: 0.02\n",
      "iteration: 291460 loss: 0.0011 lr: 0.02\n",
      "iteration: 291470 loss: 0.0016 lr: 0.02\n",
      "iteration: 291480 loss: 0.0014 lr: 0.02\n",
      "iteration: 291490 loss: 0.0018 lr: 0.02\n",
      "iteration: 291500 loss: 0.0019 lr: 0.02\n",
      "iteration: 291510 loss: 0.0017 lr: 0.02\n",
      "iteration: 291520 loss: 0.0026 lr: 0.02\n",
      "iteration: 291530 loss: 0.0019 lr: 0.02\n",
      "iteration: 291540 loss: 0.0017 lr: 0.02\n",
      "iteration: 291550 loss: 0.0015 lr: 0.02\n",
      "iteration: 291560 loss: 0.0014 lr: 0.02\n",
      "iteration: 291570 loss: 0.0012 lr: 0.02\n",
      "iteration: 291580 loss: 0.0023 lr: 0.02\n",
      "iteration: 291590 loss: 0.0017 lr: 0.02\n",
      "iteration: 291600 loss: 0.0019 lr: 0.02\n",
      "iteration: 291610 loss: 0.0013 lr: 0.02\n",
      "iteration: 291620 loss: 0.0013 lr: 0.02\n",
      "iteration: 291630 loss: 0.0017 lr: 0.02\n",
      "iteration: 291640 loss: 0.0012 lr: 0.02\n",
      "iteration: 291650 loss: 0.0022 lr: 0.02\n",
      "iteration: 291660 loss: 0.0013 lr: 0.02\n",
      "iteration: 291670 loss: 0.0015 lr: 0.02\n",
      "iteration: 291680 loss: 0.0014 lr: 0.02\n",
      "iteration: 291690 loss: 0.0014 lr: 0.02\n",
      "iteration: 291700 loss: 0.0015 lr: 0.02\n",
      "iteration: 291710 loss: 0.0015 lr: 0.02\n",
      "iteration: 291720 loss: 0.0021 lr: 0.02\n",
      "iteration: 291730 loss: 0.0018 lr: 0.02\n",
      "iteration: 291740 loss: 0.0018 lr: 0.02\n",
      "iteration: 291750 loss: 0.0023 lr: 0.02\n",
      "iteration: 291760 loss: 0.0021 lr: 0.02\n",
      "iteration: 291770 loss: 0.0015 lr: 0.02\n",
      "iteration: 291780 loss: 0.0025 lr: 0.02\n",
      "iteration: 291790 loss: 0.0016 lr: 0.02\n",
      "iteration: 291800 loss: 0.0020 lr: 0.02\n",
      "iteration: 291810 loss: 0.0029 lr: 0.02\n",
      "iteration: 291820 loss: 0.0015 lr: 0.02\n",
      "iteration: 291830 loss: 0.0018 lr: 0.02\n",
      "iteration: 291840 loss: 0.0021 lr: 0.02\n",
      "iteration: 291850 loss: 0.0016 lr: 0.02\n",
      "iteration: 291860 loss: 0.0013 lr: 0.02\n",
      "iteration: 291870 loss: 0.0029 lr: 0.02\n",
      "iteration: 291880 loss: 0.0019 lr: 0.02\n",
      "iteration: 291890 loss: 0.0016 lr: 0.02\n",
      "iteration: 291900 loss: 0.0025 lr: 0.02\n",
      "iteration: 291910 loss: 0.0020 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 291920 loss: 0.0022 lr: 0.02\n",
      "iteration: 291930 loss: 0.0020 lr: 0.02\n",
      "iteration: 291940 loss: 0.0017 lr: 0.02\n",
      "iteration: 291950 loss: 0.0020 lr: 0.02\n",
      "iteration: 291960 loss: 0.0021 lr: 0.02\n",
      "iteration: 291970 loss: 0.0017 lr: 0.02\n",
      "iteration: 291980 loss: 0.0027 lr: 0.02\n",
      "iteration: 291990 loss: 0.0015 lr: 0.02\n",
      "iteration: 292000 loss: 0.0016 lr: 0.02\n",
      "iteration: 292010 loss: 0.0014 lr: 0.02\n",
      "iteration: 292020 loss: 0.0023 lr: 0.02\n",
      "iteration: 292030 loss: 0.0015 lr: 0.02\n",
      "iteration: 292040 loss: 0.0016 lr: 0.02\n",
      "iteration: 292050 loss: 0.0021 lr: 0.02\n",
      "iteration: 292060 loss: 0.0016 lr: 0.02\n",
      "iteration: 292070 loss: 0.0018 lr: 0.02\n",
      "iteration: 292080 loss: 0.0013 lr: 0.02\n",
      "iteration: 292090 loss: 0.0016 lr: 0.02\n",
      "iteration: 292100 loss: 0.0034 lr: 0.02\n",
      "iteration: 292110 loss: 0.0017 lr: 0.02\n",
      "iteration: 292120 loss: 0.0018 lr: 0.02\n",
      "iteration: 292130 loss: 0.0026 lr: 0.02\n",
      "iteration: 292140 loss: 0.0025 lr: 0.02\n",
      "iteration: 292150 loss: 0.0019 lr: 0.02\n",
      "iteration: 292160 loss: 0.0020 lr: 0.02\n",
      "iteration: 292170 loss: 0.0015 lr: 0.02\n",
      "iteration: 292180 loss: 0.0023 lr: 0.02\n",
      "iteration: 292190 loss: 0.0020 lr: 0.02\n",
      "iteration: 292200 loss: 0.0014 lr: 0.02\n",
      "iteration: 292210 loss: 0.0018 lr: 0.02\n",
      "iteration: 292220 loss: 0.0019 lr: 0.02\n",
      "iteration: 292230 loss: 0.0027 lr: 0.02\n",
      "iteration: 292240 loss: 0.0017 lr: 0.02\n",
      "iteration: 292250 loss: 0.0022 lr: 0.02\n",
      "iteration: 292260 loss: 0.0021 lr: 0.02\n",
      "iteration: 292270 loss: 0.0014 lr: 0.02\n",
      "iteration: 292280 loss: 0.0021 lr: 0.02\n",
      "iteration: 292290 loss: 0.0024 lr: 0.02\n",
      "iteration: 292300 loss: 0.0016 lr: 0.02\n",
      "iteration: 292310 loss: 0.0016 lr: 0.02\n",
      "iteration: 292320 loss: 0.0015 lr: 0.02\n",
      "iteration: 292330 loss: 0.0020 lr: 0.02\n",
      "iteration: 292340 loss: 0.0012 lr: 0.02\n",
      "iteration: 292350 loss: 0.0023 lr: 0.02\n",
      "iteration: 292360 loss: 0.0018 lr: 0.02\n",
      "iteration: 292370 loss: 0.0016 lr: 0.02\n",
      "iteration: 292380 loss: 0.0014 lr: 0.02\n",
      "iteration: 292390 loss: 0.0021 lr: 0.02\n",
      "iteration: 292400 loss: 0.0021 lr: 0.02\n",
      "iteration: 292410 loss: 0.0017 lr: 0.02\n",
      "iteration: 292420 loss: 0.0014 lr: 0.02\n",
      "iteration: 292430 loss: 0.0013 lr: 0.02\n",
      "iteration: 292440 loss: 0.0013 lr: 0.02\n",
      "iteration: 292450 loss: 0.0015 lr: 0.02\n",
      "iteration: 292460 loss: 0.0019 lr: 0.02\n",
      "iteration: 292470 loss: 0.0020 lr: 0.02\n",
      "iteration: 292480 loss: 0.0013 lr: 0.02\n",
      "iteration: 292490 loss: 0.0013 lr: 0.02\n",
      "iteration: 292500 loss: 0.0012 lr: 0.02\n",
      "iteration: 292510 loss: 0.0014 lr: 0.02\n",
      "iteration: 292520 loss: 0.0018 lr: 0.02\n",
      "iteration: 292530 loss: 0.0030 lr: 0.02\n",
      "iteration: 292540 loss: 0.0021 lr: 0.02\n",
      "iteration: 292550 loss: 0.0018 lr: 0.02\n",
      "iteration: 292560 loss: 0.0035 lr: 0.02\n",
      "iteration: 292570 loss: 0.0016 lr: 0.02\n",
      "iteration: 292580 loss: 0.0021 lr: 0.02\n",
      "iteration: 292590 loss: 0.0017 lr: 0.02\n",
      "iteration: 292600 loss: 0.0017 lr: 0.02\n",
      "iteration: 292610 loss: 0.0030 lr: 0.02\n",
      "iteration: 292620 loss: 0.0024 lr: 0.02\n",
      "iteration: 292630 loss: 0.0018 lr: 0.02\n",
      "iteration: 292640 loss: 0.0017 lr: 0.02\n",
      "iteration: 292650 loss: 0.0013 lr: 0.02\n",
      "iteration: 292660 loss: 0.0017 lr: 0.02\n",
      "iteration: 292670 loss: 0.0025 lr: 0.02\n",
      "iteration: 292680 loss: 0.0017 lr: 0.02\n",
      "iteration: 292690 loss: 0.0025 lr: 0.02\n",
      "iteration: 292700 loss: 0.0017 lr: 0.02\n",
      "iteration: 292710 loss: 0.0021 lr: 0.02\n",
      "iteration: 292720 loss: 0.0016 lr: 0.02\n",
      "iteration: 292730 loss: 0.0021 lr: 0.02\n",
      "iteration: 292740 loss: 0.0019 lr: 0.02\n",
      "iteration: 292750 loss: 0.0016 lr: 0.02\n",
      "iteration: 292760 loss: 0.0019 lr: 0.02\n",
      "iteration: 292770 loss: 0.0023 lr: 0.02\n",
      "iteration: 292780 loss: 0.0017 lr: 0.02\n",
      "iteration: 292790 loss: 0.0020 lr: 0.02\n",
      "iteration: 292800 loss: 0.0016 lr: 0.02\n",
      "iteration: 292810 loss: 0.0013 lr: 0.02\n",
      "iteration: 292820 loss: 0.0021 lr: 0.02\n",
      "iteration: 292830 loss: 0.0015 lr: 0.02\n",
      "iteration: 292840 loss: 0.0018 lr: 0.02\n",
      "iteration: 292850 loss: 0.0017 lr: 0.02\n",
      "iteration: 292860 loss: 0.0025 lr: 0.02\n",
      "iteration: 292870 loss: 0.0012 lr: 0.02\n",
      "iteration: 292880 loss: 0.0021 lr: 0.02\n",
      "iteration: 292890 loss: 0.0019 lr: 0.02\n",
      "iteration: 292900 loss: 0.0016 lr: 0.02\n",
      "iteration: 292910 loss: 0.0018 lr: 0.02\n",
      "iteration: 292920 loss: 0.0018 lr: 0.02\n",
      "iteration: 292930 loss: 0.0021 lr: 0.02\n",
      "iteration: 292940 loss: 0.0014 lr: 0.02\n",
      "iteration: 292950 loss: 0.0021 lr: 0.02\n",
      "iteration: 292960 loss: 0.0015 lr: 0.02\n",
      "iteration: 292970 loss: 0.0024 lr: 0.02\n",
      "iteration: 292980 loss: 0.0019 lr: 0.02\n",
      "iteration: 292990 loss: 0.0018 lr: 0.02\n",
      "iteration: 293000 loss: 0.0014 lr: 0.02\n",
      "iteration: 293010 loss: 0.0024 lr: 0.02\n",
      "iteration: 293020 loss: 0.0018 lr: 0.02\n",
      "iteration: 293030 loss: 0.0015 lr: 0.02\n",
      "iteration: 293040 loss: 0.0017 lr: 0.02\n",
      "iteration: 293050 loss: 0.0015 lr: 0.02\n",
      "iteration: 293060 loss: 0.0015 lr: 0.02\n",
      "iteration: 293070 loss: 0.0015 lr: 0.02\n",
      "iteration: 293080 loss: 0.0019 lr: 0.02\n",
      "iteration: 293090 loss: 0.0013 lr: 0.02\n",
      "iteration: 293100 loss: 0.0016 lr: 0.02\n",
      "iteration: 293110 loss: 0.0015 lr: 0.02\n",
      "iteration: 293120 loss: 0.0018 lr: 0.02\n",
      "iteration: 293130 loss: 0.0015 lr: 0.02\n",
      "iteration: 293140 loss: 0.0025 lr: 0.02\n",
      "iteration: 293150 loss: 0.0022 lr: 0.02\n",
      "iteration: 293160 loss: 0.0019 lr: 0.02\n",
      "iteration: 293170 loss: 0.0014 lr: 0.02\n",
      "iteration: 293180 loss: 0.0009 lr: 0.02\n",
      "iteration: 293190 loss: 0.0015 lr: 0.02\n",
      "iteration: 293200 loss: 0.0016 lr: 0.02\n",
      "iteration: 293210 loss: 0.0021 lr: 0.02\n",
      "iteration: 293220 loss: 0.0014 lr: 0.02\n",
      "iteration: 293230 loss: 0.0018 lr: 0.02\n",
      "iteration: 293240 loss: 0.0020 lr: 0.02\n",
      "iteration: 293250 loss: 0.0016 lr: 0.02\n",
      "iteration: 293260 loss: 0.0015 lr: 0.02\n",
      "iteration: 293270 loss: 0.0021 lr: 0.02\n",
      "iteration: 293280 loss: 0.0020 lr: 0.02\n",
      "iteration: 293290 loss: 0.0016 lr: 0.02\n",
      "iteration: 293300 loss: 0.0019 lr: 0.02\n",
      "iteration: 293310 loss: 0.0023 lr: 0.02\n",
      "iteration: 293320 loss: 0.0019 lr: 0.02\n",
      "iteration: 293330 loss: 0.0018 lr: 0.02\n",
      "iteration: 293340 loss: 0.0016 lr: 0.02\n",
      "iteration: 293350 loss: 0.0013 lr: 0.02\n",
      "iteration: 293360 loss: 0.0013 lr: 0.02\n",
      "iteration: 293370 loss: 0.0017 lr: 0.02\n",
      "iteration: 293380 loss: 0.0024 lr: 0.02\n",
      "iteration: 293390 loss: 0.0018 lr: 0.02\n",
      "iteration: 293400 loss: 0.0015 lr: 0.02\n",
      "iteration: 293410 loss: 0.0017 lr: 0.02\n",
      "iteration: 293420 loss: 0.0022 lr: 0.02\n",
      "iteration: 293430 loss: 0.0017 lr: 0.02\n",
      "iteration: 293440 loss: 0.0014 lr: 0.02\n",
      "iteration: 293450 loss: 0.0019 lr: 0.02\n",
      "iteration: 293460 loss: 0.0015 lr: 0.02\n",
      "iteration: 293470 loss: 0.0025 lr: 0.02\n",
      "iteration: 293480 loss: 0.0018 lr: 0.02\n",
      "iteration: 293490 loss: 0.0015 lr: 0.02\n",
      "iteration: 293500 loss: 0.0030 lr: 0.02\n",
      "iteration: 293510 loss: 0.0011 lr: 0.02\n",
      "iteration: 293520 loss: 0.0015 lr: 0.02\n",
      "iteration: 293530 loss: 0.0021 lr: 0.02\n",
      "iteration: 293540 loss: 0.0023 lr: 0.02\n",
      "iteration: 293550 loss: 0.0019 lr: 0.02\n",
      "iteration: 293560 loss: 0.0017 lr: 0.02\n",
      "iteration: 293570 loss: 0.0013 lr: 0.02\n",
      "iteration: 293580 loss: 0.0013 lr: 0.02\n",
      "iteration: 293590 loss: 0.0016 lr: 0.02\n",
      "iteration: 293600 loss: 0.0012 lr: 0.02\n",
      "iteration: 293610 loss: 0.0019 lr: 0.02\n",
      "iteration: 293620 loss: 0.0020 lr: 0.02\n",
      "iteration: 293630 loss: 0.0015 lr: 0.02\n",
      "iteration: 293640 loss: 0.0023 lr: 0.02\n",
      "iteration: 293650 loss: 0.0014 lr: 0.02\n",
      "iteration: 293660 loss: 0.0023 lr: 0.02\n",
      "iteration: 293670 loss: 0.0018 lr: 0.02\n",
      "iteration: 293680 loss: 0.0018 lr: 0.02\n",
      "iteration: 293690 loss: 0.0022 lr: 0.02\n",
      "iteration: 293700 loss: 0.0013 lr: 0.02\n",
      "iteration: 293710 loss: 0.0021 lr: 0.02\n",
      "iteration: 293720 loss: 0.0019 lr: 0.02\n",
      "iteration: 293730 loss: 0.0013 lr: 0.02\n",
      "iteration: 293740 loss: 0.0015 lr: 0.02\n",
      "iteration: 293750 loss: 0.0024 lr: 0.02\n",
      "iteration: 293760 loss: 0.0011 lr: 0.02\n",
      "iteration: 293770 loss: 0.0022 lr: 0.02\n",
      "iteration: 293780 loss: 0.0018 lr: 0.02\n",
      "iteration: 293790 loss: 0.0017 lr: 0.02\n",
      "iteration: 293800 loss: 0.0015 lr: 0.02\n",
      "iteration: 293810 loss: 0.0017 lr: 0.02\n",
      "iteration: 293820 loss: 0.0022 lr: 0.02\n",
      "iteration: 293830 loss: 0.0016 lr: 0.02\n",
      "iteration: 293840 loss: 0.0014 lr: 0.02\n",
      "iteration: 293850 loss: 0.0012 lr: 0.02\n",
      "iteration: 293860 loss: 0.0025 lr: 0.02\n",
      "iteration: 293870 loss: 0.0017 lr: 0.02\n",
      "iteration: 293880 loss: 0.0016 lr: 0.02\n",
      "iteration: 293890 loss: 0.0025 lr: 0.02\n",
      "iteration: 293900 loss: 0.0021 lr: 0.02\n",
      "iteration: 293910 loss: 0.0016 lr: 0.02\n",
      "iteration: 293920 loss: 0.0015 lr: 0.02\n",
      "iteration: 293930 loss: 0.0014 lr: 0.02\n",
      "iteration: 293940 loss: 0.0012 lr: 0.02\n",
      "iteration: 293950 loss: 0.0015 lr: 0.02\n",
      "iteration: 293960 loss: 0.0013 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 293970 loss: 0.0020 lr: 0.02\n",
      "iteration: 293980 loss: 0.0021 lr: 0.02\n",
      "iteration: 293990 loss: 0.0022 lr: 0.02\n",
      "iteration: 294000 loss: 0.0016 lr: 0.02\n",
      "iteration: 294010 loss: 0.0021 lr: 0.02\n",
      "iteration: 294020 loss: 0.0017 lr: 0.02\n",
      "iteration: 294030 loss: 0.0020 lr: 0.02\n",
      "iteration: 294040 loss: 0.0014 lr: 0.02\n",
      "iteration: 294050 loss: 0.0013 lr: 0.02\n",
      "iteration: 294060 loss: 0.0016 lr: 0.02\n",
      "iteration: 294070 loss: 0.0019 lr: 0.02\n",
      "iteration: 294080 loss: 0.0016 lr: 0.02\n",
      "iteration: 294090 loss: 0.0021 lr: 0.02\n",
      "iteration: 294100 loss: 0.0015 lr: 0.02\n",
      "iteration: 294110 loss: 0.0014 lr: 0.02\n",
      "iteration: 294120 loss: 0.0024 lr: 0.02\n",
      "iteration: 294130 loss: 0.0019 lr: 0.02\n",
      "iteration: 294140 loss: 0.0014 lr: 0.02\n",
      "iteration: 294150 loss: 0.0021 lr: 0.02\n",
      "iteration: 294160 loss: 0.0017 lr: 0.02\n",
      "iteration: 294170 loss: 0.0014 lr: 0.02\n",
      "iteration: 294180 loss: 0.0017 lr: 0.02\n",
      "iteration: 294190 loss: 0.0021 lr: 0.02\n",
      "iteration: 294200 loss: 0.0015 lr: 0.02\n",
      "iteration: 294210 loss: 0.0015 lr: 0.02\n",
      "iteration: 294220 loss: 0.0013 lr: 0.02\n",
      "iteration: 294230 loss: 0.0025 lr: 0.02\n",
      "iteration: 294240 loss: 0.0014 lr: 0.02\n",
      "iteration: 294250 loss: 0.0022 lr: 0.02\n",
      "iteration: 294260 loss: 0.0018 lr: 0.02\n",
      "iteration: 294270 loss: 0.0011 lr: 0.02\n",
      "iteration: 294280 loss: 0.0025 lr: 0.02\n",
      "iteration: 294290 loss: 0.0024 lr: 0.02\n",
      "iteration: 294300 loss: 0.0022 lr: 0.02\n",
      "iteration: 294310 loss: 0.0026 lr: 0.02\n",
      "iteration: 294320 loss: 0.0036 lr: 0.02\n",
      "iteration: 294330 loss: 0.0014 lr: 0.02\n",
      "iteration: 294340 loss: 0.0013 lr: 0.02\n",
      "iteration: 294350 loss: 0.0019 lr: 0.02\n",
      "iteration: 294360 loss: 0.0019 lr: 0.02\n",
      "iteration: 294370 loss: 0.0024 lr: 0.02\n",
      "iteration: 294380 loss: 0.0015 lr: 0.02\n",
      "iteration: 294390 loss: 0.0015 lr: 0.02\n",
      "iteration: 294400 loss: 0.0025 lr: 0.02\n",
      "iteration: 294410 loss: 0.0028 lr: 0.02\n",
      "iteration: 294420 loss: 0.0029 lr: 0.02\n",
      "iteration: 294430 loss: 0.0018 lr: 0.02\n",
      "iteration: 294440 loss: 0.0014 lr: 0.02\n",
      "iteration: 294450 loss: 0.0018 lr: 0.02\n",
      "iteration: 294460 loss: 0.0022 lr: 0.02\n",
      "iteration: 294470 loss: 0.0016 lr: 0.02\n",
      "iteration: 294480 loss: 0.0023 lr: 0.02\n",
      "iteration: 294490 loss: 0.0027 lr: 0.02\n",
      "iteration: 294500 loss: 0.0013 lr: 0.02\n",
      "iteration: 294510 loss: 0.0019 lr: 0.02\n",
      "iteration: 294520 loss: 0.0018 lr: 0.02\n",
      "iteration: 294530 loss: 0.0013 lr: 0.02\n",
      "iteration: 294540 loss: 0.0019 lr: 0.02\n",
      "iteration: 294550 loss: 0.0025 lr: 0.02\n",
      "iteration: 294560 loss: 0.0017 lr: 0.02\n",
      "iteration: 294570 loss: 0.0024 lr: 0.02\n",
      "iteration: 294580 loss: 0.0017 lr: 0.02\n",
      "iteration: 294590 loss: 0.0025 lr: 0.02\n",
      "iteration: 294600 loss: 0.0012 lr: 0.02\n",
      "iteration: 294610 loss: 0.0014 lr: 0.02\n",
      "iteration: 294620 loss: 0.0017 lr: 0.02\n",
      "iteration: 294630 loss: 0.0016 lr: 0.02\n",
      "iteration: 294640 loss: 0.0024 lr: 0.02\n",
      "iteration: 294650 loss: 0.0016 lr: 0.02\n",
      "iteration: 294660 loss: 0.0021 lr: 0.02\n",
      "iteration: 294670 loss: 0.0021 lr: 0.02\n",
      "iteration: 294680 loss: 0.0012 lr: 0.02\n",
      "iteration: 294690 loss: 0.0014 lr: 0.02\n",
      "iteration: 294700 loss: 0.0020 lr: 0.02\n",
      "iteration: 294710 loss: 0.0017 lr: 0.02\n",
      "iteration: 294720 loss: 0.0025 lr: 0.02\n",
      "iteration: 294730 loss: 0.0020 lr: 0.02\n",
      "iteration: 294740 loss: 0.0016 lr: 0.02\n",
      "iteration: 294750 loss: 0.0017 lr: 0.02\n",
      "iteration: 294760 loss: 0.0021 lr: 0.02\n",
      "iteration: 294770 loss: 0.0019 lr: 0.02\n",
      "iteration: 294780 loss: 0.0022 lr: 0.02\n",
      "iteration: 294790 loss: 0.0017 lr: 0.02\n",
      "iteration: 294800 loss: 0.0016 lr: 0.02\n",
      "iteration: 294810 loss: 0.0021 lr: 0.02\n",
      "iteration: 294820 loss: 0.0024 lr: 0.02\n",
      "iteration: 294830 loss: 0.0022 lr: 0.02\n",
      "iteration: 294840 loss: 0.0016 lr: 0.02\n",
      "iteration: 294850 loss: 0.0032 lr: 0.02\n",
      "iteration: 294860 loss: 0.0014 lr: 0.02\n",
      "iteration: 294870 loss: 0.0016 lr: 0.02\n",
      "iteration: 294880 loss: 0.0020 lr: 0.02\n",
      "iteration: 294890 loss: 0.0022 lr: 0.02\n",
      "iteration: 294900 loss: 0.0026 lr: 0.02\n",
      "iteration: 294910 loss: 0.0019 lr: 0.02\n",
      "iteration: 294920 loss: 0.0020 lr: 0.02\n",
      "iteration: 294930 loss: 0.0033 lr: 0.02\n",
      "iteration: 294940 loss: 0.0021 lr: 0.02\n",
      "iteration: 294950 loss: 0.0015 lr: 0.02\n",
      "iteration: 294960 loss: 0.0017 lr: 0.02\n",
      "iteration: 294970 loss: 0.0018 lr: 0.02\n",
      "iteration: 294980 loss: 0.0018 lr: 0.02\n",
      "iteration: 294990 loss: 0.0015 lr: 0.02\n",
      "iteration: 295000 loss: 0.0034 lr: 0.02\n",
      "iteration: 295010 loss: 0.0019 lr: 0.02\n",
      "iteration: 295020 loss: 0.0027 lr: 0.02\n",
      "iteration: 295030 loss: 0.0020 lr: 0.02\n",
      "iteration: 295040 loss: 0.0014 lr: 0.02\n",
      "iteration: 295050 loss: 0.0011 lr: 0.02\n",
      "iteration: 295060 loss: 0.0016 lr: 0.02\n",
      "iteration: 295070 loss: 0.0015 lr: 0.02\n",
      "iteration: 295080 loss: 0.0020 lr: 0.02\n",
      "iteration: 295090 loss: 0.0013 lr: 0.02\n",
      "iteration: 295100 loss: 0.0016 lr: 0.02\n",
      "iteration: 295110 loss: 0.0018 lr: 0.02\n",
      "iteration: 295120 loss: 0.0016 lr: 0.02\n",
      "iteration: 295130 loss: 0.0015 lr: 0.02\n",
      "iteration: 295140 loss: 0.0016 lr: 0.02\n",
      "iteration: 295150 loss: 0.0018 lr: 0.02\n",
      "iteration: 295160 loss: 0.0020 lr: 0.02\n",
      "iteration: 295170 loss: 0.0019 lr: 0.02\n",
      "iteration: 295180 loss: 0.0022 lr: 0.02\n",
      "iteration: 295190 loss: 0.0019 lr: 0.02\n",
      "iteration: 295200 loss: 0.0015 lr: 0.02\n",
      "iteration: 295210 loss: 0.0011 lr: 0.02\n",
      "iteration: 295220 loss: 0.0042 lr: 0.02\n",
      "iteration: 295230 loss: 0.0017 lr: 0.02\n",
      "iteration: 295240 loss: 0.0020 lr: 0.02\n",
      "iteration: 295250 loss: 0.0015 lr: 0.02\n",
      "iteration: 295260 loss: 0.0011 lr: 0.02\n",
      "iteration: 295270 loss: 0.0019 lr: 0.02\n",
      "iteration: 295280 loss: 0.0009 lr: 0.02\n",
      "iteration: 295290 loss: 0.0017 lr: 0.02\n",
      "iteration: 295300 loss: 0.0026 lr: 0.02\n",
      "iteration: 295310 loss: 0.0018 lr: 0.02\n",
      "iteration: 295320 loss: 0.0021 lr: 0.02\n",
      "iteration: 295330 loss: 0.0022 lr: 0.02\n",
      "iteration: 295340 loss: 0.0017 lr: 0.02\n",
      "iteration: 295350 loss: 0.0022 lr: 0.02\n",
      "iteration: 295360 loss: 0.0020 lr: 0.02\n",
      "iteration: 295370 loss: 0.0027 lr: 0.02\n",
      "iteration: 295380 loss: 0.0017 lr: 0.02\n",
      "iteration: 295390 loss: 0.0017 lr: 0.02\n",
      "iteration: 295400 loss: 0.0017 lr: 0.02\n",
      "iteration: 295410 loss: 0.0013 lr: 0.02\n",
      "iteration: 295420 loss: 0.0019 lr: 0.02\n",
      "iteration: 295430 loss: 0.0011 lr: 0.02\n",
      "iteration: 295440 loss: 0.0014 lr: 0.02\n",
      "iteration: 295450 loss: 0.0018 lr: 0.02\n",
      "iteration: 295460 loss: 0.0014 lr: 0.02\n",
      "iteration: 295470 loss: 0.0019 lr: 0.02\n",
      "iteration: 295480 loss: 0.0016 lr: 0.02\n",
      "iteration: 295490 loss: 0.0019 lr: 0.02\n",
      "iteration: 295500 loss: 0.0021 lr: 0.02\n",
      "iteration: 295510 loss: 0.0018 lr: 0.02\n",
      "iteration: 295520 loss: 0.0012 lr: 0.02\n",
      "iteration: 295530 loss: 0.0013 lr: 0.02\n",
      "iteration: 295540 loss: 0.0015 lr: 0.02\n",
      "iteration: 295550 loss: 0.0017 lr: 0.02\n",
      "iteration: 295560 loss: 0.0013 lr: 0.02\n",
      "iteration: 295570 loss: 0.0020 lr: 0.02\n",
      "iteration: 295580 loss: 0.0016 lr: 0.02\n",
      "iteration: 295590 loss: 0.0013 lr: 0.02\n",
      "iteration: 295600 loss: 0.0022 lr: 0.02\n",
      "iteration: 295610 loss: 0.0022 lr: 0.02\n",
      "iteration: 295620 loss: 0.0015 lr: 0.02\n",
      "iteration: 295630 loss: 0.0012 lr: 0.02\n",
      "iteration: 295640 loss: 0.0013 lr: 0.02\n",
      "iteration: 295650 loss: 0.0019 lr: 0.02\n",
      "iteration: 295660 loss: 0.0016 lr: 0.02\n",
      "iteration: 295670 loss: 0.0019 lr: 0.02\n",
      "iteration: 295680 loss: 0.0019 lr: 0.02\n",
      "iteration: 295690 loss: 0.0019 lr: 0.02\n",
      "iteration: 295700 loss: 0.0021 lr: 0.02\n",
      "iteration: 295710 loss: 0.0019 lr: 0.02\n",
      "iteration: 295720 loss: 0.0019 lr: 0.02\n",
      "iteration: 295730 loss: 0.0021 lr: 0.02\n",
      "iteration: 295740 loss: 0.0012 lr: 0.02\n",
      "iteration: 295750 loss: 0.0016 lr: 0.02\n",
      "iteration: 295760 loss: 0.0030 lr: 0.02\n",
      "iteration: 295770 loss: 0.0026 lr: 0.02\n",
      "iteration: 295780 loss: 0.0012 lr: 0.02\n",
      "iteration: 295790 loss: 0.0016 lr: 0.02\n",
      "iteration: 295800 loss: 0.0014 lr: 0.02\n",
      "iteration: 295810 loss: 0.0012 lr: 0.02\n",
      "iteration: 295820 loss: 0.0014 lr: 0.02\n",
      "iteration: 295830 loss: 0.0015 lr: 0.02\n",
      "iteration: 295840 loss: 0.0019 lr: 0.02\n",
      "iteration: 295850 loss: 0.0014 lr: 0.02\n",
      "iteration: 295860 loss: 0.0016 lr: 0.02\n",
      "iteration: 295870 loss: 0.0018 lr: 0.02\n",
      "iteration: 295880 loss: 0.0017 lr: 0.02\n",
      "iteration: 295890 loss: 0.0020 lr: 0.02\n",
      "iteration: 295900 loss: 0.0015 lr: 0.02\n",
      "iteration: 295910 loss: 0.0021 lr: 0.02\n",
      "iteration: 295920 loss: 0.0026 lr: 0.02\n",
      "iteration: 295930 loss: 0.0023 lr: 0.02\n",
      "iteration: 295940 loss: 0.0025 lr: 0.02\n",
      "iteration: 295950 loss: 0.0021 lr: 0.02\n",
      "iteration: 295960 loss: 0.0016 lr: 0.02\n",
      "iteration: 295970 loss: 0.0019 lr: 0.02\n",
      "iteration: 295980 loss: 0.0020 lr: 0.02\n",
      "iteration: 295990 loss: 0.0016 lr: 0.02\n",
      "iteration: 296000 loss: 0.0014 lr: 0.02\n",
      "iteration: 296010 loss: 0.0018 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 296020 loss: 0.0013 lr: 0.02\n",
      "iteration: 296030 loss: 0.0015 lr: 0.02\n",
      "iteration: 296040 loss: 0.0020 lr: 0.02\n",
      "iteration: 296050 loss: 0.0014 lr: 0.02\n",
      "iteration: 296060 loss: 0.0017 lr: 0.02\n",
      "iteration: 296070 loss: 0.0018 lr: 0.02\n",
      "iteration: 296080 loss: 0.0017 lr: 0.02\n",
      "iteration: 296090 loss: 0.0016 lr: 0.02\n",
      "iteration: 296100 loss: 0.0019 lr: 0.02\n",
      "iteration: 296110 loss: 0.0015 lr: 0.02\n",
      "iteration: 296120 loss: 0.0016 lr: 0.02\n",
      "iteration: 296130 loss: 0.0013 lr: 0.02\n",
      "iteration: 296140 loss: 0.0017 lr: 0.02\n",
      "iteration: 296150 loss: 0.0018 lr: 0.02\n",
      "iteration: 296160 loss: 0.0015 lr: 0.02\n",
      "iteration: 296170 loss: 0.0019 lr: 0.02\n",
      "iteration: 296180 loss: 0.0015 lr: 0.02\n",
      "iteration: 296190 loss: 0.0023 lr: 0.02\n",
      "iteration: 296200 loss: 0.0017 lr: 0.02\n",
      "iteration: 296210 loss: 0.0025 lr: 0.02\n",
      "iteration: 296220 loss: 0.0017 lr: 0.02\n",
      "iteration: 296230 loss: 0.0018 lr: 0.02\n",
      "iteration: 296240 loss: 0.0025 lr: 0.02\n",
      "iteration: 296250 loss: 0.0020 lr: 0.02\n",
      "iteration: 296260 loss: 0.0030 lr: 0.02\n",
      "iteration: 296270 loss: 0.0020 lr: 0.02\n",
      "iteration: 296280 loss: 0.0022 lr: 0.02\n",
      "iteration: 296290 loss: 0.0016 lr: 0.02\n",
      "iteration: 296300 loss: 0.0026 lr: 0.02\n",
      "iteration: 296310 loss: 0.0020 lr: 0.02\n",
      "iteration: 296320 loss: 0.0015 lr: 0.02\n",
      "iteration: 296330 loss: 0.0020 lr: 0.02\n",
      "iteration: 296340 loss: 0.0016 lr: 0.02\n",
      "iteration: 296350 loss: 0.0017 lr: 0.02\n",
      "iteration: 296360 loss: 0.0015 lr: 0.02\n",
      "iteration: 296370 loss: 0.0022 lr: 0.02\n",
      "iteration: 296380 loss: 0.0017 lr: 0.02\n",
      "iteration: 296390 loss: 0.0014 lr: 0.02\n",
      "iteration: 296400 loss: 0.0021 lr: 0.02\n",
      "iteration: 296410 loss: 0.0026 lr: 0.02\n",
      "iteration: 296420 loss: 0.0018 lr: 0.02\n",
      "iteration: 296430 loss: 0.0016 lr: 0.02\n",
      "iteration: 296440 loss: 0.0021 lr: 0.02\n",
      "iteration: 296450 loss: 0.0029 lr: 0.02\n",
      "iteration: 296460 loss: 0.0022 lr: 0.02\n",
      "iteration: 296470 loss: 0.0016 lr: 0.02\n",
      "iteration: 296480 loss: 0.0020 lr: 0.02\n",
      "iteration: 296490 loss: 0.0027 lr: 0.02\n",
      "iteration: 296500 loss: 0.0021 lr: 0.02\n",
      "iteration: 296510 loss: 0.0015 lr: 0.02\n",
      "iteration: 296520 loss: 0.0029 lr: 0.02\n",
      "iteration: 296530 loss: 0.0012 lr: 0.02\n",
      "iteration: 296540 loss: 0.0024 lr: 0.02\n",
      "iteration: 296550 loss: 0.0037 lr: 0.02\n",
      "iteration: 296560 loss: 0.0018 lr: 0.02\n",
      "iteration: 296570 loss: 0.0019 lr: 0.02\n",
      "iteration: 296580 loss: 0.0024 lr: 0.02\n",
      "iteration: 296590 loss: 0.0023 lr: 0.02\n",
      "iteration: 296600 loss: 0.0020 lr: 0.02\n",
      "iteration: 296610 loss: 0.0012 lr: 0.02\n",
      "iteration: 296620 loss: 0.0025 lr: 0.02\n",
      "iteration: 296630 loss: 0.0023 lr: 0.02\n",
      "iteration: 296640 loss: 0.0020 lr: 0.02\n",
      "iteration: 296650 loss: 0.0016 lr: 0.02\n",
      "iteration: 296660 loss: 0.0014 lr: 0.02\n",
      "iteration: 296670 loss: 0.0021 lr: 0.02\n",
      "iteration: 296680 loss: 0.0016 lr: 0.02\n",
      "iteration: 296690 loss: 0.0015 lr: 0.02\n",
      "iteration: 296700 loss: 0.0014 lr: 0.02\n",
      "iteration: 296710 loss: 0.0011 lr: 0.02\n",
      "iteration: 296720 loss: 0.0016 lr: 0.02\n",
      "iteration: 296730 loss: 0.0020 lr: 0.02\n",
      "iteration: 296740 loss: 0.0022 lr: 0.02\n",
      "iteration: 296750 loss: 0.0024 lr: 0.02\n",
      "iteration: 296760 loss: 0.0018 lr: 0.02\n",
      "iteration: 296770 loss: 0.0030 lr: 0.02\n",
      "iteration: 296780 loss: 0.0014 lr: 0.02\n",
      "iteration: 296790 loss: 0.0019 lr: 0.02\n",
      "iteration: 296800 loss: 0.0019 lr: 0.02\n",
      "iteration: 296810 loss: 0.0025 lr: 0.02\n",
      "iteration: 296820 loss: 0.0022 lr: 0.02\n",
      "iteration: 296830 loss: 0.0010 lr: 0.02\n",
      "iteration: 296840 loss: 0.0020 lr: 0.02\n",
      "iteration: 296850 loss: 0.0024 lr: 0.02\n",
      "iteration: 296860 loss: 0.0020 lr: 0.02\n",
      "iteration: 296870 loss: 0.0014 lr: 0.02\n",
      "iteration: 296880 loss: 0.0024 lr: 0.02\n",
      "iteration: 296890 loss: 0.0023 lr: 0.02\n",
      "iteration: 296900 loss: 0.0014 lr: 0.02\n",
      "iteration: 296910 loss: 0.0018 lr: 0.02\n",
      "iteration: 296920 loss: 0.0016 lr: 0.02\n",
      "iteration: 296930 loss: 0.0015 lr: 0.02\n",
      "iteration: 296940 loss: 0.0019 lr: 0.02\n",
      "iteration: 296950 loss: 0.0024 lr: 0.02\n",
      "iteration: 296960 loss: 0.0016 lr: 0.02\n",
      "iteration: 296970 loss: 0.0019 lr: 0.02\n",
      "iteration: 296980 loss: 0.0021 lr: 0.02\n",
      "iteration: 296990 loss: 0.0021 lr: 0.02\n",
      "iteration: 297000 loss: 0.0023 lr: 0.02\n",
      "iteration: 297010 loss: 0.0020 lr: 0.02\n",
      "iteration: 297020 loss: 0.0019 lr: 0.02\n",
      "iteration: 297030 loss: 0.0015 lr: 0.02\n",
      "iteration: 297040 loss: 0.0023 lr: 0.02\n",
      "iteration: 297050 loss: 0.0018 lr: 0.02\n",
      "iteration: 297060 loss: 0.0014 lr: 0.02\n",
      "iteration: 297070 loss: 0.0020 lr: 0.02\n",
      "iteration: 297080 loss: 0.0020 lr: 0.02\n",
      "iteration: 297090 loss: 0.0019 lr: 0.02\n",
      "iteration: 297100 loss: 0.0013 lr: 0.02\n",
      "iteration: 297110 loss: 0.0018 lr: 0.02\n",
      "iteration: 297120 loss: 0.0016 lr: 0.02\n",
      "iteration: 297130 loss: 0.0020 lr: 0.02\n",
      "iteration: 297140 loss: 0.0016 lr: 0.02\n",
      "iteration: 297150 loss: 0.0018 lr: 0.02\n",
      "iteration: 297160 loss: 0.0022 lr: 0.02\n",
      "iteration: 297170 loss: 0.0023 lr: 0.02\n",
      "iteration: 297180 loss: 0.0012 lr: 0.02\n",
      "iteration: 297190 loss: 0.0017 lr: 0.02\n",
      "iteration: 297200 loss: 0.0015 lr: 0.02\n",
      "iteration: 297210 loss: 0.0021 lr: 0.02\n",
      "iteration: 297220 loss: 0.0016 lr: 0.02\n",
      "iteration: 297230 loss: 0.0016 lr: 0.02\n",
      "iteration: 297240 loss: 0.0015 lr: 0.02\n",
      "iteration: 297250 loss: 0.0018 lr: 0.02\n",
      "iteration: 297260 loss: 0.0015 lr: 0.02\n",
      "iteration: 297270 loss: 0.0026 lr: 0.02\n",
      "iteration: 297280 loss: 0.0020 lr: 0.02\n",
      "iteration: 297290 loss: 0.0016 lr: 0.02\n",
      "iteration: 297300 loss: 0.0018 lr: 0.02\n",
      "iteration: 297310 loss: 0.0014 lr: 0.02\n",
      "iteration: 297320 loss: 0.0017 lr: 0.02\n",
      "iteration: 297330 loss: 0.0018 lr: 0.02\n",
      "iteration: 297340 loss: 0.0010 lr: 0.02\n",
      "iteration: 297350 loss: 0.0019 lr: 0.02\n",
      "iteration: 297360 loss: 0.0023 lr: 0.02\n",
      "iteration: 297370 loss: 0.0016 lr: 0.02\n",
      "iteration: 297380 loss: 0.0015 lr: 0.02\n",
      "iteration: 297390 loss: 0.0013 lr: 0.02\n",
      "iteration: 297400 loss: 0.0016 lr: 0.02\n",
      "iteration: 297410 loss: 0.0015 lr: 0.02\n",
      "iteration: 297420 loss: 0.0019 lr: 0.02\n",
      "iteration: 297430 loss: 0.0018 lr: 0.02\n",
      "iteration: 297440 loss: 0.0017 lr: 0.02\n",
      "iteration: 297450 loss: 0.0014 lr: 0.02\n",
      "iteration: 297460 loss: 0.0017 lr: 0.02\n",
      "iteration: 297470 loss: 0.0019 lr: 0.02\n",
      "iteration: 297480 loss: 0.0014 lr: 0.02\n",
      "iteration: 297490 loss: 0.0027 lr: 0.02\n",
      "iteration: 297500 loss: 0.0015 lr: 0.02\n",
      "iteration: 297510 loss: 0.0015 lr: 0.02\n",
      "iteration: 297520 loss: 0.0021 lr: 0.02\n",
      "iteration: 297530 loss: 0.0015 lr: 0.02\n",
      "iteration: 297540 loss: 0.0018 lr: 0.02\n",
      "iteration: 297550 loss: 0.0019 lr: 0.02\n",
      "iteration: 297560 loss: 0.0015 lr: 0.02\n",
      "iteration: 297570 loss: 0.0024 lr: 0.02\n",
      "iteration: 297580 loss: 0.0019 lr: 0.02\n",
      "iteration: 297590 loss: 0.0019 lr: 0.02\n",
      "iteration: 297600 loss: 0.0017 lr: 0.02\n",
      "iteration: 297610 loss: 0.0022 lr: 0.02\n",
      "iteration: 297620 loss: 0.0015 lr: 0.02\n",
      "iteration: 297630 loss: 0.0018 lr: 0.02\n",
      "iteration: 297640 loss: 0.0022 lr: 0.02\n",
      "iteration: 297650 loss: 0.0018 lr: 0.02\n",
      "iteration: 297660 loss: 0.0014 lr: 0.02\n",
      "iteration: 297670 loss: 0.0019 lr: 0.02\n",
      "iteration: 297680 loss: 0.0016 lr: 0.02\n",
      "iteration: 297690 loss: 0.0016 lr: 0.02\n",
      "iteration: 297700 loss: 0.0019 lr: 0.02\n",
      "iteration: 297710 loss: 0.0014 lr: 0.02\n",
      "iteration: 297720 loss: 0.0016 lr: 0.02\n",
      "iteration: 297730 loss: 0.0014 lr: 0.02\n",
      "iteration: 297740 loss: 0.0018 lr: 0.02\n",
      "iteration: 297750 loss: 0.0015 lr: 0.02\n",
      "iteration: 297760 loss: 0.0016 lr: 0.02\n",
      "iteration: 297770 loss: 0.0020 lr: 0.02\n",
      "iteration: 297780 loss: 0.0021 lr: 0.02\n",
      "iteration: 297790 loss: 0.0020 lr: 0.02\n",
      "iteration: 297800 loss: 0.0016 lr: 0.02\n",
      "iteration: 297810 loss: 0.0024 lr: 0.02\n",
      "iteration: 297820 loss: 0.0011 lr: 0.02\n",
      "iteration: 297830 loss: 0.0016 lr: 0.02\n",
      "iteration: 297840 loss: 0.0015 lr: 0.02\n",
      "iteration: 297850 loss: 0.0019 lr: 0.02\n",
      "iteration: 297860 loss: 0.0015 lr: 0.02\n",
      "iteration: 297870 loss: 0.0024 lr: 0.02\n",
      "iteration: 297880 loss: 0.0023 lr: 0.02\n",
      "iteration: 297890 loss: 0.0017 lr: 0.02\n",
      "iteration: 297900 loss: 0.0011 lr: 0.02\n",
      "iteration: 297910 loss: 0.0013 lr: 0.02\n",
      "iteration: 297920 loss: 0.0018 lr: 0.02\n",
      "iteration: 297930 loss: 0.0014 lr: 0.02\n",
      "iteration: 297940 loss: 0.0017 lr: 0.02\n",
      "iteration: 297950 loss: 0.0020 lr: 0.02\n",
      "iteration: 297960 loss: 0.0013 lr: 0.02\n",
      "iteration: 297970 loss: 0.0018 lr: 0.02\n",
      "iteration: 297980 loss: 0.0014 lr: 0.02\n",
      "iteration: 297990 loss: 0.0014 lr: 0.02\n",
      "iteration: 298000 loss: 0.0015 lr: 0.02\n",
      "iteration: 298010 loss: 0.0016 lr: 0.02\n",
      "iteration: 298020 loss: 0.0015 lr: 0.02\n",
      "iteration: 298030 loss: 0.0026 lr: 0.02\n",
      "iteration: 298040 loss: 0.0019 lr: 0.02\n",
      "iteration: 298050 loss: 0.0017 lr: 0.02\n",
      "iteration: 298060 loss: 0.0020 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 298070 loss: 0.0013 lr: 0.02\n",
      "iteration: 298080 loss: 0.0019 lr: 0.02\n",
      "iteration: 298090 loss: 0.0011 lr: 0.02\n",
      "iteration: 298100 loss: 0.0017 lr: 0.02\n",
      "iteration: 298110 loss: 0.0026 lr: 0.02\n",
      "iteration: 298120 loss: 0.0020 lr: 0.02\n",
      "iteration: 298130 loss: 0.0019 lr: 0.02\n",
      "iteration: 298140 loss: 0.0017 lr: 0.02\n",
      "iteration: 298150 loss: 0.0018 lr: 0.02\n",
      "iteration: 298160 loss: 0.0017 lr: 0.02\n",
      "iteration: 298170 loss: 0.0028 lr: 0.02\n",
      "iteration: 298180 loss: 0.0015 lr: 0.02\n",
      "iteration: 298190 loss: 0.0019 lr: 0.02\n",
      "iteration: 298200 loss: 0.0015 lr: 0.02\n",
      "iteration: 298210 loss: 0.0022 lr: 0.02\n",
      "iteration: 298220 loss: 0.0016 lr: 0.02\n",
      "iteration: 298230 loss: 0.0013 lr: 0.02\n",
      "iteration: 298240 loss: 0.0017 lr: 0.02\n",
      "iteration: 298250 loss: 0.0021 lr: 0.02\n",
      "iteration: 298260 loss: 0.0018 lr: 0.02\n",
      "iteration: 298270 loss: 0.0017 lr: 0.02\n",
      "iteration: 298280 loss: 0.0022 lr: 0.02\n",
      "iteration: 298290 loss: 0.0018 lr: 0.02\n",
      "iteration: 298300 loss: 0.0023 lr: 0.02\n",
      "iteration: 298310 loss: 0.0020 lr: 0.02\n",
      "iteration: 298320 loss: 0.0014 lr: 0.02\n",
      "iteration: 298330 loss: 0.0020 lr: 0.02\n",
      "iteration: 298340 loss: 0.0014 lr: 0.02\n",
      "iteration: 298350 loss: 0.0025 lr: 0.02\n",
      "iteration: 298360 loss: 0.0016 lr: 0.02\n",
      "iteration: 298370 loss: 0.0013 lr: 0.02\n",
      "iteration: 298380 loss: 0.0018 lr: 0.02\n",
      "iteration: 298390 loss: 0.0021 lr: 0.02\n",
      "iteration: 298400 loss: 0.0021 lr: 0.02\n",
      "iteration: 298410 loss: 0.0014 lr: 0.02\n",
      "iteration: 298420 loss: 0.0015 lr: 0.02\n",
      "iteration: 298430 loss: 0.0025 lr: 0.02\n",
      "iteration: 298440 loss: 0.0019 lr: 0.02\n",
      "iteration: 298450 loss: 0.0017 lr: 0.02\n",
      "iteration: 298460 loss: 0.0014 lr: 0.02\n",
      "iteration: 298470 loss: 0.0016 lr: 0.02\n",
      "iteration: 298480 loss: 0.0026 lr: 0.02\n",
      "iteration: 298490 loss: 0.0012 lr: 0.02\n",
      "iteration: 298500 loss: 0.0020 lr: 0.02\n",
      "iteration: 298510 loss: 0.0017 lr: 0.02\n",
      "iteration: 298520 loss: 0.0019 lr: 0.02\n",
      "iteration: 298530 loss: 0.0026 lr: 0.02\n",
      "iteration: 298540 loss: 0.0017 lr: 0.02\n",
      "iteration: 298550 loss: 0.0015 lr: 0.02\n",
      "iteration: 298560 loss: 0.0012 lr: 0.02\n",
      "iteration: 298570 loss: 0.0022 lr: 0.02\n",
      "iteration: 298580 loss: 0.0018 lr: 0.02\n",
      "iteration: 298590 loss: 0.0022 lr: 0.02\n",
      "iteration: 298600 loss: 0.0020 lr: 0.02\n",
      "iteration: 298610 loss: 0.0012 lr: 0.02\n",
      "iteration: 298620 loss: 0.0017 lr: 0.02\n",
      "iteration: 298630 loss: 0.0013 lr: 0.02\n",
      "iteration: 298640 loss: 0.0019 lr: 0.02\n",
      "iteration: 298650 loss: 0.0018 lr: 0.02\n",
      "iteration: 298660 loss: 0.0016 lr: 0.02\n",
      "iteration: 298670 loss: 0.0016 lr: 0.02\n",
      "iteration: 298680 loss: 0.0016 lr: 0.02\n",
      "iteration: 298690 loss: 0.0013 lr: 0.02\n",
      "iteration: 298700 loss: 0.0014 lr: 0.02\n",
      "iteration: 298710 loss: 0.0021 lr: 0.02\n",
      "iteration: 298720 loss: 0.0020 lr: 0.02\n",
      "iteration: 298730 loss: 0.0023 lr: 0.02\n",
      "iteration: 298740 loss: 0.0019 lr: 0.02\n",
      "iteration: 298750 loss: 0.0024 lr: 0.02\n",
      "iteration: 298760 loss: 0.0022 lr: 0.02\n",
      "iteration: 298770 loss: 0.0032 lr: 0.02\n",
      "iteration: 298780 loss: 0.0018 lr: 0.02\n",
      "iteration: 298790 loss: 0.0014 lr: 0.02\n",
      "iteration: 298800 loss: 0.0016 lr: 0.02\n",
      "iteration: 298810 loss: 0.0020 lr: 0.02\n",
      "iteration: 298820 loss: 0.0013 lr: 0.02\n",
      "iteration: 298830 loss: 0.0020 lr: 0.02\n",
      "iteration: 298840 loss: 0.0014 lr: 0.02\n",
      "iteration: 298850 loss: 0.0016 lr: 0.02\n",
      "iteration: 298860 loss: 0.0017 lr: 0.02\n",
      "iteration: 298870 loss: 0.0035 lr: 0.02\n",
      "iteration: 298880 loss: 0.0011 lr: 0.02\n",
      "iteration: 298890 loss: 0.0016 lr: 0.02\n",
      "iteration: 298900 loss: 0.0020 lr: 0.02\n",
      "iteration: 298910 loss: 0.0018 lr: 0.02\n",
      "iteration: 298920 loss: 0.0018 lr: 0.02\n",
      "iteration: 298930 loss: 0.0015 lr: 0.02\n",
      "iteration: 298940 loss: 0.0011 lr: 0.02\n",
      "iteration: 298950 loss: 0.0020 lr: 0.02\n",
      "iteration: 298960 loss: 0.0021 lr: 0.02\n",
      "iteration: 298970 loss: 0.0014 lr: 0.02\n",
      "iteration: 298980 loss: 0.0017 lr: 0.02\n",
      "iteration: 298990 loss: 0.0016 lr: 0.02\n",
      "iteration: 299000 loss: 0.0026 lr: 0.02\n",
      "iteration: 299010 loss: 0.0019 lr: 0.02\n",
      "iteration: 299020 loss: 0.0020 lr: 0.02\n",
      "iteration: 299030 loss: 0.0017 lr: 0.02\n",
      "iteration: 299040 loss: 0.0014 lr: 0.02\n",
      "iteration: 299050 loss: 0.0015 lr: 0.02\n",
      "iteration: 299060 loss: 0.0021 lr: 0.02\n",
      "iteration: 299070 loss: 0.0014 lr: 0.02\n",
      "iteration: 299080 loss: 0.0018 lr: 0.02\n",
      "iteration: 299090 loss: 0.0024 lr: 0.02\n",
      "iteration: 299100 loss: 0.0014 lr: 0.02\n",
      "iteration: 299110 loss: 0.0022 lr: 0.02\n",
      "iteration: 299120 loss: 0.0031 lr: 0.02\n",
      "iteration: 299130 loss: 0.0023 lr: 0.02\n",
      "iteration: 299140 loss: 0.0020 lr: 0.02\n",
      "iteration: 299150 loss: 0.0021 lr: 0.02\n",
      "iteration: 299160 loss: 0.0018 lr: 0.02\n",
      "iteration: 299170 loss: 0.0012 lr: 0.02\n",
      "iteration: 299180 loss: 0.0019 lr: 0.02\n",
      "iteration: 299190 loss: 0.0019 lr: 0.02\n",
      "iteration: 299200 loss: 0.0014 lr: 0.02\n",
      "iteration: 299210 loss: 0.0018 lr: 0.02\n",
      "iteration: 299220 loss: 0.0018 lr: 0.02\n",
      "iteration: 299230 loss: 0.0018 lr: 0.02\n",
      "iteration: 299240 loss: 0.0015 lr: 0.02\n",
      "iteration: 299250 loss: 0.0018 lr: 0.02\n",
      "iteration: 299260 loss: 0.0019 lr: 0.02\n",
      "iteration: 299270 loss: 0.0022 lr: 0.02\n",
      "iteration: 299280 loss: 0.0014 lr: 0.02\n",
      "iteration: 299290 loss: 0.0013 lr: 0.02\n",
      "iteration: 299300 loss: 0.0017 lr: 0.02\n",
      "iteration: 299310 loss: 0.0014 lr: 0.02\n",
      "iteration: 299320 loss: 0.0027 lr: 0.02\n",
      "iteration: 299330 loss: 0.0015 lr: 0.02\n",
      "iteration: 299340 loss: 0.0020 lr: 0.02\n",
      "iteration: 299350 loss: 0.0016 lr: 0.02\n",
      "iteration: 299360 loss: 0.0019 lr: 0.02\n",
      "iteration: 299370 loss: 0.0020 lr: 0.02\n",
      "iteration: 299380 loss: 0.0017 lr: 0.02\n",
      "iteration: 299390 loss: 0.0017 lr: 0.02\n",
      "iteration: 299400 loss: 0.0022 lr: 0.02\n",
      "iteration: 299410 loss: 0.0028 lr: 0.02\n",
      "iteration: 299420 loss: 0.0016 lr: 0.02\n",
      "iteration: 299430 loss: 0.0015 lr: 0.02\n",
      "iteration: 299440 loss: 0.0022 lr: 0.02\n",
      "iteration: 299450 loss: 0.0021 lr: 0.02\n",
      "iteration: 299460 loss: 0.0013 lr: 0.02\n",
      "iteration: 299470 loss: 0.0018 lr: 0.02\n",
      "iteration: 299480 loss: 0.0024 lr: 0.02\n",
      "iteration: 299490 loss: 0.0014 lr: 0.02\n",
      "iteration: 299500 loss: 0.0013 lr: 0.02\n",
      "iteration: 299510 loss: 0.0021 lr: 0.02\n",
      "iteration: 299520 loss: 0.0019 lr: 0.02\n",
      "iteration: 299530 loss: 0.0012 lr: 0.02\n",
      "iteration: 299540 loss: 0.0023 lr: 0.02\n",
      "iteration: 299550 loss: 0.0016 lr: 0.02\n",
      "iteration: 299560 loss: 0.0014 lr: 0.02\n",
      "iteration: 299570 loss: 0.0016 lr: 0.02\n",
      "iteration: 299580 loss: 0.0017 lr: 0.02\n",
      "iteration: 299590 loss: 0.0020 lr: 0.02\n",
      "iteration: 299600 loss: 0.0012 lr: 0.02\n",
      "iteration: 299610 loss: 0.0021 lr: 0.02\n",
      "iteration: 299620 loss: 0.0012 lr: 0.02\n",
      "iteration: 299630 loss: 0.0017 lr: 0.02\n",
      "iteration: 299640 loss: 0.0025 lr: 0.02\n",
      "iteration: 299650 loss: 0.0016 lr: 0.02\n",
      "iteration: 299660 loss: 0.0015 lr: 0.02\n",
      "iteration: 299670 loss: 0.0020 lr: 0.02\n",
      "iteration: 299680 loss: 0.0014 lr: 0.02\n",
      "iteration: 299690 loss: 0.0018 lr: 0.02\n",
      "iteration: 299700 loss: 0.0014 lr: 0.02\n",
      "iteration: 299710 loss: 0.0019 lr: 0.02\n",
      "iteration: 299720 loss: 0.0032 lr: 0.02\n",
      "iteration: 299730 loss: 0.0015 lr: 0.02\n",
      "iteration: 299740 loss: 0.0018 lr: 0.02\n",
      "iteration: 299750 loss: 0.0024 lr: 0.02\n",
      "iteration: 299760 loss: 0.0019 lr: 0.02\n",
      "iteration: 299770 loss: 0.0021 lr: 0.02\n",
      "iteration: 299780 loss: 0.0014 lr: 0.02\n",
      "iteration: 299790 loss: 0.0023 lr: 0.02\n",
      "iteration: 299800 loss: 0.0022 lr: 0.02\n",
      "iteration: 299810 loss: 0.0024 lr: 0.02\n",
      "iteration: 299820 loss: 0.0019 lr: 0.02\n",
      "iteration: 299830 loss: 0.0029 lr: 0.02\n",
      "iteration: 299840 loss: 0.0023 lr: 0.02\n",
      "iteration: 299850 loss: 0.0017 lr: 0.02\n",
      "iteration: 299860 loss: 0.0017 lr: 0.02\n",
      "iteration: 299870 loss: 0.0020 lr: 0.02\n",
      "iteration: 299880 loss: 0.0016 lr: 0.02\n",
      "iteration: 299890 loss: 0.0015 lr: 0.02\n",
      "iteration: 299900 loss: 0.0015 lr: 0.02\n",
      "iteration: 299910 loss: 0.0012 lr: 0.02\n",
      "iteration: 299920 loss: 0.0021 lr: 0.02\n",
      "iteration: 299930 loss: 0.0021 lr: 0.02\n",
      "iteration: 299940 loss: 0.0017 lr: 0.02\n",
      "iteration: 299950 loss: 0.0022 lr: 0.02\n",
      "iteration: 299960 loss: 0.0018 lr: 0.02\n",
      "iteration: 299970 loss: 0.0025 lr: 0.02\n",
      "iteration: 299980 loss: 0.0018 lr: 0.02\n",
      "iteration: 299990 loss: 0.0015 lr: 0.02\n",
      "iteration: 300000 loss: 0.0018 lr: 0.02\n",
      "iteration: 300010 loss: 0.0022 lr: 0.02\n",
      "iteration: 300020 loss: 0.0020 lr: 0.02\n",
      "iteration: 300030 loss: 0.0021 lr: 0.02\n",
      "iteration: 300040 loss: 0.0027 lr: 0.02\n",
      "iteration: 300050 loss: 0.0013 lr: 0.02\n",
      "iteration: 300060 loss: 0.0011 lr: 0.02\n",
      "iteration: 300070 loss: 0.0011 lr: 0.02\n",
      "iteration: 300080 loss: 0.0013 lr: 0.02\n",
      "iteration: 300090 loss: 0.0016 lr: 0.02\n",
      "iteration: 300100 loss: 0.0015 lr: 0.02\n",
      "iteration: 300110 loss: 0.0014 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 300120 loss: 0.0017 lr: 0.02\n",
      "iteration: 300130 loss: 0.0016 lr: 0.02\n",
      "iteration: 300140 loss: 0.0016 lr: 0.02\n",
      "iteration: 300150 loss: 0.0015 lr: 0.02\n",
      "iteration: 300160 loss: 0.0010 lr: 0.02\n",
      "iteration: 300170 loss: 0.0012 lr: 0.02\n",
      "iteration: 300180 loss: 0.0015 lr: 0.02\n",
      "iteration: 300190 loss: 0.0015 lr: 0.02\n",
      "iteration: 300200 loss: 0.0030 lr: 0.02\n",
      "iteration: 300210 loss: 0.0017 lr: 0.02\n",
      "iteration: 300220 loss: 0.0015 lr: 0.02\n",
      "iteration: 300230 loss: 0.0017 lr: 0.02\n",
      "iteration: 300240 loss: 0.0020 lr: 0.02\n",
      "iteration: 300250 loss: 0.0024 lr: 0.02\n",
      "iteration: 300260 loss: 0.0020 lr: 0.02\n",
      "iteration: 300270 loss: 0.0020 lr: 0.02\n",
      "iteration: 300280 loss: 0.0021 lr: 0.02\n",
      "iteration: 300290 loss: 0.0016 lr: 0.02\n",
      "iteration: 300300 loss: 0.0021 lr: 0.02\n",
      "iteration: 300310 loss: 0.0014 lr: 0.02\n",
      "iteration: 300320 loss: 0.0021 lr: 0.02\n",
      "iteration: 300330 loss: 0.0013 lr: 0.02\n",
      "iteration: 300340 loss: 0.0018 lr: 0.02\n",
      "iteration: 300350 loss: 0.0043 lr: 0.02\n",
      "iteration: 300360 loss: 0.0018 lr: 0.02\n",
      "iteration: 300370 loss: 0.0019 lr: 0.02\n",
      "iteration: 300380 loss: 0.0014 lr: 0.02\n",
      "iteration: 300390 loss: 0.0017 lr: 0.02\n",
      "iteration: 300400 loss: 0.0016 lr: 0.02\n",
      "iteration: 300410 loss: 0.0013 lr: 0.02\n",
      "iteration: 300420 loss: 0.0015 lr: 0.02\n",
      "iteration: 300430 loss: 0.0017 lr: 0.02\n",
      "iteration: 300440 loss: 0.0022 lr: 0.02\n",
      "iteration: 300450 loss: 0.0018 lr: 0.02\n",
      "iteration: 300460 loss: 0.0021 lr: 0.02\n",
      "iteration: 300470 loss: 0.0013 lr: 0.02\n",
      "iteration: 300480 loss: 0.0017 lr: 0.02\n",
      "iteration: 300490 loss: 0.0014 lr: 0.02\n",
      "iteration: 300500 loss: 0.0015 lr: 0.02\n",
      "iteration: 300510 loss: 0.0015 lr: 0.02\n",
      "iteration: 300520 loss: 0.0017 lr: 0.02\n",
      "iteration: 300530 loss: 0.0015 lr: 0.02\n",
      "iteration: 300540 loss: 0.0016 lr: 0.02\n",
      "iteration: 300550 loss: 0.0013 lr: 0.02\n",
      "iteration: 300560 loss: 0.0013 lr: 0.02\n",
      "iteration: 300570 loss: 0.0016 lr: 0.02\n",
      "iteration: 300580 loss: 0.0023 lr: 0.02\n",
      "iteration: 300590 loss: 0.0022 lr: 0.02\n",
      "iteration: 300600 loss: 0.0016 lr: 0.02\n",
      "iteration: 300610 loss: 0.0017 lr: 0.02\n",
      "iteration: 300620 loss: 0.0019 lr: 0.02\n",
      "iteration: 300630 loss: 0.0013 lr: 0.02\n",
      "iteration: 300640 loss: 0.0014 lr: 0.02\n",
      "iteration: 300650 loss: 0.0015 lr: 0.02\n",
      "iteration: 300660 loss: 0.0020 lr: 0.02\n",
      "iteration: 300670 loss: 0.0016 lr: 0.02\n",
      "iteration: 300680 loss: 0.0014 lr: 0.02\n",
      "iteration: 300690 loss: 0.0022 lr: 0.02\n",
      "iteration: 300700 loss: 0.0018 lr: 0.02\n",
      "iteration: 300710 loss: 0.0015 lr: 0.02\n",
      "iteration: 300720 loss: 0.0013 lr: 0.02\n",
      "iteration: 300730 loss: 0.0016 lr: 0.02\n",
      "iteration: 300740 loss: 0.0019 lr: 0.02\n",
      "iteration: 300750 loss: 0.0022 lr: 0.02\n",
      "iteration: 300760 loss: 0.0018 lr: 0.02\n",
      "iteration: 300770 loss: 0.0014 lr: 0.02\n",
      "iteration: 300780 loss: 0.0021 lr: 0.02\n",
      "iteration: 300790 loss: 0.0021 lr: 0.02\n",
      "iteration: 300800 loss: 0.0014 lr: 0.02\n",
      "iteration: 300810 loss: 0.0016 lr: 0.02\n",
      "iteration: 300820 loss: 0.0026 lr: 0.02\n",
      "iteration: 300830 loss: 0.0011 lr: 0.02\n",
      "iteration: 300840 loss: 0.0021 lr: 0.02\n",
      "iteration: 300850 loss: 0.0015 lr: 0.02\n",
      "iteration: 300860 loss: 0.0021 lr: 0.02\n",
      "iteration: 300870 loss: 0.0028 lr: 0.02\n",
      "iteration: 300880 loss: 0.0016 lr: 0.02\n",
      "iteration: 300890 loss: 0.0021 lr: 0.02\n",
      "iteration: 300900 loss: 0.0011 lr: 0.02\n",
      "iteration: 300910 loss: 0.0017 lr: 0.02\n",
      "iteration: 300920 loss: 0.0029 lr: 0.02\n",
      "iteration: 300930 loss: 0.0021 lr: 0.02\n",
      "iteration: 300940 loss: 0.0018 lr: 0.02\n",
      "iteration: 300950 loss: 0.0023 lr: 0.02\n",
      "iteration: 300960 loss: 0.0017 lr: 0.02\n",
      "iteration: 300970 loss: 0.0020 lr: 0.02\n",
      "iteration: 300980 loss: 0.0013 lr: 0.02\n",
      "iteration: 300990 loss: 0.0013 lr: 0.02\n",
      "iteration: 301000 loss: 0.0017 lr: 0.02\n",
      "iteration: 301010 loss: 0.0014 lr: 0.02\n",
      "iteration: 301020 loss: 0.0016 lr: 0.02\n",
      "iteration: 301030 loss: 0.0021 lr: 0.02\n",
      "iteration: 301040 loss: 0.0016 lr: 0.02\n",
      "iteration: 301050 loss: 0.0010 lr: 0.02\n",
      "iteration: 301060 loss: 0.0018 lr: 0.02\n",
      "iteration: 301070 loss: 0.0014 lr: 0.02\n",
      "iteration: 301080 loss: 0.0012 lr: 0.02\n",
      "iteration: 301090 loss: 0.0017 lr: 0.02\n",
      "iteration: 301100 loss: 0.0026 lr: 0.02\n",
      "iteration: 301110 loss: 0.0015 lr: 0.02\n",
      "iteration: 301120 loss: 0.0021 lr: 0.02\n",
      "iteration: 301130 loss: 0.0023 lr: 0.02\n",
      "iteration: 301140 loss: 0.0016 lr: 0.02\n",
      "iteration: 301150 loss: 0.0017 lr: 0.02\n",
      "iteration: 301160 loss: 0.0020 lr: 0.02\n",
      "iteration: 301170 loss: 0.0020 lr: 0.02\n",
      "iteration: 301180 loss: 0.0014 lr: 0.02\n",
      "iteration: 301190 loss: 0.0018 lr: 0.02\n",
      "iteration: 301200 loss: 0.0022 lr: 0.02\n",
      "iteration: 301210 loss: 0.0015 lr: 0.02\n",
      "iteration: 301220 loss: 0.0018 lr: 0.02\n",
      "iteration: 301230 loss: 0.0016 lr: 0.02\n",
      "iteration: 301240 loss: 0.0019 lr: 0.02\n",
      "iteration: 301250 loss: 0.0015 lr: 0.02\n",
      "iteration: 301260 loss: 0.0023 lr: 0.02\n",
      "iteration: 301270 loss: 0.0028 lr: 0.02\n",
      "iteration: 301280 loss: 0.0023 lr: 0.02\n",
      "iteration: 301290 loss: 0.0013 lr: 0.02\n",
      "iteration: 301300 loss: 0.0022 lr: 0.02\n",
      "iteration: 301310 loss: 0.0017 lr: 0.02\n",
      "iteration: 301320 loss: 0.0020 lr: 0.02\n",
      "iteration: 301330 loss: 0.0021 lr: 0.02\n",
      "iteration: 301340 loss: 0.0019 lr: 0.02\n",
      "iteration: 301350 loss: 0.0017 lr: 0.02\n",
      "iteration: 301360 loss: 0.0021 lr: 0.02\n",
      "iteration: 301370 loss: 0.0013 lr: 0.02\n",
      "iteration: 301380 loss: 0.0017 lr: 0.02\n",
      "iteration: 301390 loss: 0.0011 lr: 0.02\n",
      "iteration: 301400 loss: 0.0013 lr: 0.02\n",
      "iteration: 301410 loss: 0.0015 lr: 0.02\n",
      "iteration: 301420 loss: 0.0012 lr: 0.02\n",
      "iteration: 301430 loss: 0.0013 lr: 0.02\n",
      "iteration: 301440 loss: 0.0014 lr: 0.02\n",
      "iteration: 301450 loss: 0.0014 lr: 0.02\n",
      "iteration: 301460 loss: 0.0016 lr: 0.02\n",
      "iteration: 301470 loss: 0.0015 lr: 0.02\n",
      "iteration: 301480 loss: 0.0010 lr: 0.02\n",
      "iteration: 301490 loss: 0.0019 lr: 0.02\n",
      "iteration: 301500 loss: 0.0026 lr: 0.02\n",
      "iteration: 301510 loss: 0.0021 lr: 0.02\n",
      "iteration: 301520 loss: 0.0018 lr: 0.02\n",
      "iteration: 301530 loss: 0.0017 lr: 0.02\n",
      "iteration: 301540 loss: 0.0013 lr: 0.02\n",
      "iteration: 301550 loss: 0.0017 lr: 0.02\n",
      "iteration: 301560 loss: 0.0017 lr: 0.02\n",
      "iteration: 301570 loss: 0.0014 lr: 0.02\n",
      "iteration: 301580 loss: 0.0013 lr: 0.02\n",
      "iteration: 301590 loss: 0.0017 lr: 0.02\n",
      "iteration: 301600 loss: 0.0024 lr: 0.02\n",
      "iteration: 301610 loss: 0.0021 lr: 0.02\n",
      "iteration: 301620 loss: 0.0018 lr: 0.02\n",
      "iteration: 301630 loss: 0.0015 lr: 0.02\n",
      "iteration: 301640 loss: 0.0021 lr: 0.02\n",
      "iteration: 301650 loss: 0.0015 lr: 0.02\n",
      "iteration: 301660 loss: 0.0014 lr: 0.02\n",
      "iteration: 301670 loss: 0.0017 lr: 0.02\n",
      "iteration: 301680 loss: 0.0015 lr: 0.02\n",
      "iteration: 301690 loss: 0.0020 lr: 0.02\n",
      "iteration: 301700 loss: 0.0013 lr: 0.02\n",
      "iteration: 301710 loss: 0.0019 lr: 0.02\n",
      "iteration: 301720 loss: 0.0011 lr: 0.02\n",
      "iteration: 301730 loss: 0.0018 lr: 0.02\n",
      "iteration: 301740 loss: 0.0020 lr: 0.02\n",
      "iteration: 301750 loss: 0.0020 lr: 0.02\n",
      "iteration: 301760 loss: 0.0013 lr: 0.02\n",
      "iteration: 301770 loss: 0.0025 lr: 0.02\n",
      "iteration: 301780 loss: 0.0019 lr: 0.02\n",
      "iteration: 301790 loss: 0.0019 lr: 0.02\n",
      "iteration: 301800 loss: 0.0017 lr: 0.02\n",
      "iteration: 301810 loss: 0.0015 lr: 0.02\n",
      "iteration: 301820 loss: 0.0021 lr: 0.02\n",
      "iteration: 301830 loss: 0.0016 lr: 0.02\n",
      "iteration: 301840 loss: 0.0016 lr: 0.02\n",
      "iteration: 301850 loss: 0.0020 lr: 0.02\n",
      "iteration: 301860 loss: 0.0019 lr: 0.02\n",
      "iteration: 301870 loss: 0.0024 lr: 0.02\n",
      "iteration: 301880 loss: 0.0018 lr: 0.02\n",
      "iteration: 301890 loss: 0.0021 lr: 0.02\n",
      "iteration: 301900 loss: 0.0015 lr: 0.02\n",
      "iteration: 301910 loss: 0.0022 lr: 0.02\n",
      "iteration: 301920 loss: 0.0031 lr: 0.02\n",
      "iteration: 301930 loss: 0.0021 lr: 0.02\n",
      "iteration: 301940 loss: 0.0012 lr: 0.02\n",
      "iteration: 301950 loss: 0.0016 lr: 0.02\n",
      "iteration: 301960 loss: 0.0026 lr: 0.02\n",
      "iteration: 301970 loss: 0.0018 lr: 0.02\n",
      "iteration: 301980 loss: 0.0019 lr: 0.02\n",
      "iteration: 301990 loss: 0.0016 lr: 0.02\n",
      "iteration: 302000 loss: 0.0014 lr: 0.02\n",
      "iteration: 302010 loss: 0.0018 lr: 0.02\n",
      "iteration: 302020 loss: 0.0017 lr: 0.02\n",
      "iteration: 302030 loss: 0.0013 lr: 0.02\n",
      "iteration: 302040 loss: 0.0031 lr: 0.02\n",
      "iteration: 302050 loss: 0.0022 lr: 0.02\n",
      "iteration: 302060 loss: 0.0019 lr: 0.02\n",
      "iteration: 302070 loss: 0.0014 lr: 0.02\n",
      "iteration: 302080 loss: 0.0014 lr: 0.02\n",
      "iteration: 302090 loss: 0.0016 lr: 0.02\n",
      "iteration: 302100 loss: 0.0022 lr: 0.02\n",
      "iteration: 302110 loss: 0.0018 lr: 0.02\n",
      "iteration: 302120 loss: 0.0019 lr: 0.02\n",
      "iteration: 302130 loss: 0.0016 lr: 0.02\n",
      "iteration: 302140 loss: 0.0017 lr: 0.02\n",
      "iteration: 302150 loss: 0.0017 lr: 0.02\n",
      "iteration: 302160 loss: 0.0018 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 302170 loss: 0.0015 lr: 0.02\n",
      "iteration: 302180 loss: 0.0030 lr: 0.02\n",
      "iteration: 302190 loss: 0.0023 lr: 0.02\n",
      "iteration: 302200 loss: 0.0022 lr: 0.02\n",
      "iteration: 302210 loss: 0.0017 lr: 0.02\n",
      "iteration: 302220 loss: 0.0022 lr: 0.02\n",
      "iteration: 302230 loss: 0.0021 lr: 0.02\n",
      "iteration: 302240 loss: 0.0021 lr: 0.02\n",
      "iteration: 302250 loss: 0.0019 lr: 0.02\n",
      "iteration: 302260 loss: 0.0014 lr: 0.02\n",
      "iteration: 302270 loss: 0.0020 lr: 0.02\n",
      "iteration: 302280 loss: 0.0018 lr: 0.02\n",
      "iteration: 302290 loss: 0.0020 lr: 0.02\n",
      "iteration: 302300 loss: 0.0015 lr: 0.02\n",
      "iteration: 302310 loss: 0.0017 lr: 0.02\n",
      "iteration: 302320 loss: 0.0015 lr: 0.02\n",
      "iteration: 302330 loss: 0.0016 lr: 0.02\n",
      "iteration: 302340 loss: 0.0029 lr: 0.02\n",
      "iteration: 302350 loss: 0.0020 lr: 0.02\n",
      "iteration: 302360 loss: 0.0016 lr: 0.02\n",
      "iteration: 302370 loss: 0.0014 lr: 0.02\n",
      "iteration: 302380 loss: 0.0015 lr: 0.02\n",
      "iteration: 302390 loss: 0.0012 lr: 0.02\n",
      "iteration: 302400 loss: 0.0018 lr: 0.02\n",
      "iteration: 302410 loss: 0.0020 lr: 0.02\n",
      "iteration: 302420 loss: 0.0019 lr: 0.02\n",
      "iteration: 302430 loss: 0.0015 lr: 0.02\n",
      "iteration: 302440 loss: 0.0017 lr: 0.02\n",
      "iteration: 302450 loss: 0.0015 lr: 0.02\n",
      "iteration: 302460 loss: 0.0016 lr: 0.02\n",
      "iteration: 302470 loss: 0.0017 lr: 0.02\n",
      "iteration: 302480 loss: 0.0018 lr: 0.02\n",
      "iteration: 302490 loss: 0.0023 lr: 0.02\n",
      "iteration: 302500 loss: 0.0011 lr: 0.02\n",
      "iteration: 302510 loss: 0.0027 lr: 0.02\n",
      "iteration: 302520 loss: 0.0019 lr: 0.02\n",
      "iteration: 302530 loss: 0.0015 lr: 0.02\n",
      "iteration: 302540 loss: 0.0026 lr: 0.02\n",
      "iteration: 302550 loss: 0.0019 lr: 0.02\n",
      "iteration: 302560 loss: 0.0025 lr: 0.02\n",
      "iteration: 302570 loss: 0.0014 lr: 0.02\n",
      "iteration: 302580 loss: 0.0027 lr: 0.02\n",
      "iteration: 302590 loss: 0.0019 lr: 0.02\n",
      "iteration: 302600 loss: 0.0018 lr: 0.02\n",
      "iteration: 302610 loss: 0.0018 lr: 0.02\n",
      "iteration: 302620 loss: 0.0013 lr: 0.02\n",
      "iteration: 302630 loss: 0.0018 lr: 0.02\n",
      "iteration: 302640 loss: 0.0027 lr: 0.02\n",
      "iteration: 302650 loss: 0.0023 lr: 0.02\n",
      "iteration: 302660 loss: 0.0026 lr: 0.02\n",
      "iteration: 302670 loss: 0.0024 lr: 0.02\n",
      "iteration: 302680 loss: 0.0019 lr: 0.02\n",
      "iteration: 302690 loss: 0.0024 lr: 0.02\n",
      "iteration: 302700 loss: 0.0021 lr: 0.02\n",
      "iteration: 302710 loss: 0.0015 lr: 0.02\n",
      "iteration: 302720 loss: 0.0018 lr: 0.02\n",
      "iteration: 302730 loss: 0.0021 lr: 0.02\n",
      "iteration: 302740 loss: 0.0024 lr: 0.02\n",
      "iteration: 302750 loss: 0.0013 lr: 0.02\n",
      "iteration: 302760 loss: 0.0012 lr: 0.02\n",
      "iteration: 302770 loss: 0.0016 lr: 0.02\n",
      "iteration: 302780 loss: 0.0013 lr: 0.02\n",
      "iteration: 302790 loss: 0.0013 lr: 0.02\n",
      "iteration: 302800 loss: 0.0012 lr: 0.02\n",
      "iteration: 302810 loss: 0.0021 lr: 0.02\n",
      "iteration: 302820 loss: 0.0016 lr: 0.02\n",
      "iteration: 302830 loss: 0.0020 lr: 0.02\n",
      "iteration: 302840 loss: 0.0013 lr: 0.02\n",
      "iteration: 302850 loss: 0.0018 lr: 0.02\n",
      "iteration: 302860 loss: 0.0019 lr: 0.02\n",
      "iteration: 302870 loss: 0.0014 lr: 0.02\n",
      "iteration: 302880 loss: 0.0026 lr: 0.02\n",
      "iteration: 302890 loss: 0.0017 lr: 0.02\n",
      "iteration: 302900 loss: 0.0012 lr: 0.02\n",
      "iteration: 302910 loss: 0.0016 lr: 0.02\n",
      "iteration: 302920 loss: 0.0015 lr: 0.02\n",
      "iteration: 302930 loss: 0.0014 lr: 0.02\n",
      "iteration: 302940 loss: 0.0014 lr: 0.02\n",
      "iteration: 302950 loss: 0.0014 lr: 0.02\n",
      "iteration: 302960 loss: 0.0023 lr: 0.02\n",
      "iteration: 302970 loss: 0.0015 lr: 0.02\n",
      "iteration: 302980 loss: 0.0023 lr: 0.02\n",
      "iteration: 302990 loss: 0.0028 lr: 0.02\n",
      "iteration: 303000 loss: 0.0014 lr: 0.02\n",
      "iteration: 303010 loss: 0.0012 lr: 0.02\n",
      "iteration: 303020 loss: 0.0013 lr: 0.02\n",
      "iteration: 303030 loss: 0.0016 lr: 0.02\n",
      "iteration: 303040 loss: 0.0017 lr: 0.02\n",
      "iteration: 303050 loss: 0.0016 lr: 0.02\n",
      "iteration: 303060 loss: 0.0013 lr: 0.02\n",
      "iteration: 303070 loss: 0.0024 lr: 0.02\n",
      "iteration: 303080 loss: 0.0028 lr: 0.02\n",
      "iteration: 303090 loss: 0.0026 lr: 0.02\n",
      "iteration: 303100 loss: 0.0012 lr: 0.02\n",
      "iteration: 303110 loss: 0.0020 lr: 0.02\n",
      "iteration: 303120 loss: 0.0015 lr: 0.02\n",
      "iteration: 303130 loss: 0.0019 lr: 0.02\n",
      "iteration: 303140 loss: 0.0024 lr: 0.02\n",
      "iteration: 303150 loss: 0.0012 lr: 0.02\n",
      "iteration: 303160 loss: 0.0022 lr: 0.02\n",
      "iteration: 303170 loss: 0.0018 lr: 0.02\n",
      "iteration: 303180 loss: 0.0014 lr: 0.02\n",
      "iteration: 303190 loss: 0.0020 lr: 0.02\n",
      "iteration: 303200 loss: 0.0023 lr: 0.02\n",
      "iteration: 303210 loss: 0.0020 lr: 0.02\n",
      "iteration: 303220 loss: 0.0026 lr: 0.02\n",
      "iteration: 303230 loss: 0.0019 lr: 0.02\n",
      "iteration: 303240 loss: 0.0027 lr: 0.02\n",
      "iteration: 303250 loss: 0.0016 lr: 0.02\n",
      "iteration: 303260 loss: 0.0017 lr: 0.02\n",
      "iteration: 303270 loss: 0.0016 lr: 0.02\n",
      "iteration: 303280 loss: 0.0017 lr: 0.02\n",
      "iteration: 303290 loss: 0.0017 lr: 0.02\n",
      "iteration: 303300 loss: 0.0011 lr: 0.02\n",
      "iteration: 303310 loss: 0.0022 lr: 0.02\n",
      "iteration: 303320 loss: 0.0017 lr: 0.02\n",
      "iteration: 303330 loss: 0.0017 lr: 0.02\n",
      "iteration: 303340 loss: 0.0017 lr: 0.02\n",
      "iteration: 303350 loss: 0.0020 lr: 0.02\n",
      "iteration: 303360 loss: 0.0020 lr: 0.02\n",
      "iteration: 303370 loss: 0.0028 lr: 0.02\n",
      "iteration: 303380 loss: 0.0018 lr: 0.02\n",
      "iteration: 303390 loss: 0.0024 lr: 0.02\n",
      "iteration: 303400 loss: 0.0019 lr: 0.02\n",
      "iteration: 303410 loss: 0.0016 lr: 0.02\n",
      "iteration: 303420 loss: 0.0016 lr: 0.02\n",
      "iteration: 303430 loss: 0.0015 lr: 0.02\n",
      "iteration: 303440 loss: 0.0015 lr: 0.02\n",
      "iteration: 303450 loss: 0.0015 lr: 0.02\n",
      "iteration: 303460 loss: 0.0017 lr: 0.02\n",
      "iteration: 303470 loss: 0.0014 lr: 0.02\n",
      "iteration: 303480 loss: 0.0025 lr: 0.02\n",
      "iteration: 303490 loss: 0.0018 lr: 0.02\n",
      "iteration: 303500 loss: 0.0015 lr: 0.02\n",
      "iteration: 303510 loss: 0.0016 lr: 0.02\n",
      "iteration: 303520 loss: 0.0020 lr: 0.02\n",
      "iteration: 303530 loss: 0.0019 lr: 0.02\n",
      "iteration: 303540 loss: 0.0016 lr: 0.02\n",
      "iteration: 303550 loss: 0.0015 lr: 0.02\n",
      "iteration: 303560 loss: 0.0015 lr: 0.02\n",
      "iteration: 303570 loss: 0.0012 lr: 0.02\n",
      "iteration: 303580 loss: 0.0022 lr: 0.02\n",
      "iteration: 303590 loss: 0.0018 lr: 0.02\n",
      "iteration: 303600 loss: 0.0014 lr: 0.02\n",
      "iteration: 303610 loss: 0.0017 lr: 0.02\n",
      "iteration: 303620 loss: 0.0015 lr: 0.02\n",
      "iteration: 303630 loss: 0.0013 lr: 0.02\n",
      "iteration: 303640 loss: 0.0014 lr: 0.02\n",
      "iteration: 303650 loss: 0.0012 lr: 0.02\n",
      "iteration: 303660 loss: 0.0019 lr: 0.02\n",
      "iteration: 303670 loss: 0.0017 lr: 0.02\n",
      "iteration: 303680 loss: 0.0014 lr: 0.02\n",
      "iteration: 303690 loss: 0.0022 lr: 0.02\n",
      "iteration: 303700 loss: 0.0018 lr: 0.02\n",
      "iteration: 303710 loss: 0.0016 lr: 0.02\n",
      "iteration: 303720 loss: 0.0019 lr: 0.02\n",
      "iteration: 303730 loss: 0.0022 lr: 0.02\n",
      "iteration: 303740 loss: 0.0017 lr: 0.02\n",
      "iteration: 303750 loss: 0.0015 lr: 0.02\n",
      "iteration: 303760 loss: 0.0015 lr: 0.02\n",
      "iteration: 303770 loss: 0.0019 lr: 0.02\n",
      "iteration: 303780 loss: 0.0021 lr: 0.02\n",
      "iteration: 303790 loss: 0.0026 lr: 0.02\n",
      "iteration: 303800 loss: 0.0022 lr: 0.02\n",
      "iteration: 303810 loss: 0.0014 lr: 0.02\n",
      "iteration: 303820 loss: 0.0015 lr: 0.02\n",
      "iteration: 303830 loss: 0.0017 lr: 0.02\n",
      "iteration: 303840 loss: 0.0020 lr: 0.02\n",
      "iteration: 303850 loss: 0.0018 lr: 0.02\n",
      "iteration: 303860 loss: 0.0018 lr: 0.02\n",
      "iteration: 303870 loss: 0.0018 lr: 0.02\n",
      "iteration: 303880 loss: 0.0013 lr: 0.02\n",
      "iteration: 303890 loss: 0.0018 lr: 0.02\n",
      "iteration: 303900 loss: 0.0012 lr: 0.02\n",
      "iteration: 303910 loss: 0.0016 lr: 0.02\n",
      "iteration: 303920 loss: 0.0018 lr: 0.02\n",
      "iteration: 303930 loss: 0.0021 lr: 0.02\n",
      "iteration: 303940 loss: 0.0015 lr: 0.02\n",
      "iteration: 303950 loss: 0.0014 lr: 0.02\n",
      "iteration: 303960 loss: 0.0019 lr: 0.02\n",
      "iteration: 303970 loss: 0.0020 lr: 0.02\n",
      "iteration: 303980 loss: 0.0015 lr: 0.02\n",
      "iteration: 303990 loss: 0.0013 lr: 0.02\n",
      "iteration: 304000 loss: 0.0018 lr: 0.02\n",
      "iteration: 304010 loss: 0.0020 lr: 0.02\n",
      "iteration: 304020 loss: 0.0016 lr: 0.02\n",
      "iteration: 304030 loss: 0.0012 lr: 0.02\n",
      "iteration: 304040 loss: 0.0012 lr: 0.02\n",
      "iteration: 304050 loss: 0.0019 lr: 0.02\n",
      "iteration: 304060 loss: 0.0018 lr: 0.02\n",
      "iteration: 304070 loss: 0.0020 lr: 0.02\n",
      "iteration: 304080 loss: 0.0013 lr: 0.02\n",
      "iteration: 304090 loss: 0.0015 lr: 0.02\n",
      "iteration: 304100 loss: 0.0030 lr: 0.02\n",
      "iteration: 304110 loss: 0.0021 lr: 0.02\n",
      "iteration: 304120 loss: 0.0013 lr: 0.02\n",
      "iteration: 304130 loss: 0.0015 lr: 0.02\n",
      "iteration: 304140 loss: 0.0019 lr: 0.02\n",
      "iteration: 304150 loss: 0.0016 lr: 0.02\n",
      "iteration: 304160 loss: 0.0023 lr: 0.02\n",
      "iteration: 304170 loss: 0.0015 lr: 0.02\n",
      "iteration: 304180 loss: 0.0020 lr: 0.02\n",
      "iteration: 304190 loss: 0.0040 lr: 0.02\n",
      "iteration: 304200 loss: 0.0027 lr: 0.02\n",
      "iteration: 304210 loss: 0.0019 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 304220 loss: 0.0019 lr: 0.02\n",
      "iteration: 304230 loss: 0.0013 lr: 0.02\n",
      "iteration: 304240 loss: 0.0020 lr: 0.02\n",
      "iteration: 304250 loss: 0.0021 lr: 0.02\n",
      "iteration: 304260 loss: 0.0017 lr: 0.02\n",
      "iteration: 304270 loss: 0.0017 lr: 0.02\n",
      "iteration: 304280 loss: 0.0016 lr: 0.02\n",
      "iteration: 304290 loss: 0.0023 lr: 0.02\n",
      "iteration: 304300 loss: 0.0018 lr: 0.02\n",
      "iteration: 304310 loss: 0.0014 lr: 0.02\n",
      "iteration: 304320 loss: 0.0027 lr: 0.02\n",
      "iteration: 304330 loss: 0.0018 lr: 0.02\n",
      "iteration: 304340 loss: 0.0012 lr: 0.02\n",
      "iteration: 304350 loss: 0.0030 lr: 0.02\n",
      "iteration: 304360 loss: 0.0022 lr: 0.02\n",
      "iteration: 304370 loss: 0.0022 lr: 0.02\n",
      "iteration: 304380 loss: 0.0016 lr: 0.02\n",
      "iteration: 304390 loss: 0.0019 lr: 0.02\n",
      "iteration: 304400 loss: 0.0016 lr: 0.02\n",
      "iteration: 304410 loss: 0.0018 lr: 0.02\n",
      "iteration: 304420 loss: 0.0016 lr: 0.02\n",
      "iteration: 304430 loss: 0.0019 lr: 0.02\n",
      "iteration: 304440 loss: 0.0020 lr: 0.02\n",
      "iteration: 304450 loss: 0.0018 lr: 0.02\n",
      "iteration: 304460 loss: 0.0021 lr: 0.02\n",
      "iteration: 304470 loss: 0.0019 lr: 0.02\n",
      "iteration: 304480 loss: 0.0016 lr: 0.02\n",
      "iteration: 304490 loss: 0.0014 lr: 0.02\n",
      "iteration: 304500 loss: 0.0016 lr: 0.02\n",
      "iteration: 304510 loss: 0.0021 lr: 0.02\n",
      "iteration: 304520 loss: 0.0016 lr: 0.02\n",
      "iteration: 304530 loss: 0.0020 lr: 0.02\n",
      "iteration: 304540 loss: 0.0014 lr: 0.02\n",
      "iteration: 304550 loss: 0.0016 lr: 0.02\n",
      "iteration: 304560 loss: 0.0015 lr: 0.02\n",
      "iteration: 304570 loss: 0.0023 lr: 0.02\n",
      "iteration: 304580 loss: 0.0014 lr: 0.02\n",
      "iteration: 304590 loss: 0.0013 lr: 0.02\n",
      "iteration: 304600 loss: 0.0013 lr: 0.02\n",
      "iteration: 304610 loss: 0.0014 lr: 0.02\n",
      "iteration: 304620 loss: 0.0015 lr: 0.02\n",
      "iteration: 304630 loss: 0.0022 lr: 0.02\n",
      "iteration: 304640 loss: 0.0019 lr: 0.02\n",
      "iteration: 304650 loss: 0.0013 lr: 0.02\n",
      "iteration: 304660 loss: 0.0015 lr: 0.02\n",
      "iteration: 304670 loss: 0.0017 lr: 0.02\n",
      "iteration: 304680 loss: 0.0016 lr: 0.02\n",
      "iteration: 304690 loss: 0.0016 lr: 0.02\n",
      "iteration: 304700 loss: 0.0017 lr: 0.02\n",
      "iteration: 304710 loss: 0.0019 lr: 0.02\n",
      "iteration: 304720 loss: 0.0015 lr: 0.02\n",
      "iteration: 304730 loss: 0.0015 lr: 0.02\n",
      "iteration: 304740 loss: 0.0016 lr: 0.02\n",
      "iteration: 304750 loss: 0.0015 lr: 0.02\n",
      "iteration: 304760 loss: 0.0022 lr: 0.02\n",
      "iteration: 304770 loss: 0.0039 lr: 0.02\n",
      "iteration: 304780 loss: 0.0018 lr: 0.02\n",
      "iteration: 304790 loss: 0.0017 lr: 0.02\n",
      "iteration: 304800 loss: 0.0014 lr: 0.02\n",
      "iteration: 304810 loss: 0.0016 lr: 0.02\n",
      "iteration: 304820 loss: 0.0017 lr: 0.02\n",
      "iteration: 304830 loss: 0.0015 lr: 0.02\n",
      "iteration: 304840 loss: 0.0021 lr: 0.02\n",
      "iteration: 304850 loss: 0.0016 lr: 0.02\n",
      "iteration: 304860 loss: 0.0015 lr: 0.02\n",
      "iteration: 304870 loss: 0.0013 lr: 0.02\n",
      "iteration: 304880 loss: 0.0016 lr: 0.02\n",
      "iteration: 304890 loss: 0.0017 lr: 0.02\n",
      "iteration: 304900 loss: 0.0014 lr: 0.02\n",
      "iteration: 304910 loss: 0.0012 lr: 0.02\n",
      "iteration: 304920 loss: 0.0018 lr: 0.02\n",
      "iteration: 304930 loss: 0.0013 lr: 0.02\n",
      "iteration: 304940 loss: 0.0013 lr: 0.02\n",
      "iteration: 304950 loss: 0.0020 lr: 0.02\n",
      "iteration: 304960 loss: 0.0020 lr: 0.02\n",
      "iteration: 304970 loss: 0.0019 lr: 0.02\n",
      "iteration: 304980 loss: 0.0017 lr: 0.02\n",
      "iteration: 304990 loss: 0.0017 lr: 0.02\n",
      "iteration: 305000 loss: 0.0012 lr: 0.02\n",
      "iteration: 305010 loss: 0.0014 lr: 0.02\n",
      "iteration: 305020 loss: 0.0021 lr: 0.02\n",
      "iteration: 305030 loss: 0.0013 lr: 0.02\n",
      "iteration: 305040 loss: 0.0012 lr: 0.02\n",
      "iteration: 305050 loss: 0.0012 lr: 0.02\n",
      "iteration: 305060 loss: 0.0015 lr: 0.02\n",
      "iteration: 305070 loss: 0.0019 lr: 0.02\n",
      "iteration: 305080 loss: 0.0020 lr: 0.02\n",
      "iteration: 305090 loss: 0.0021 lr: 0.02\n",
      "iteration: 305100 loss: 0.0021 lr: 0.02\n",
      "iteration: 305110 loss: 0.0016 lr: 0.02\n",
      "iteration: 305120 loss: 0.0018 lr: 0.02\n",
      "iteration: 305130 loss: 0.0016 lr: 0.02\n",
      "iteration: 305140 loss: 0.0015 lr: 0.02\n",
      "iteration: 305150 loss: 0.0014 lr: 0.02\n",
      "iteration: 305160 loss: 0.0020 lr: 0.02\n",
      "iteration: 305170 loss: 0.0016 lr: 0.02\n",
      "iteration: 305180 loss: 0.0020 lr: 0.02\n",
      "iteration: 305190 loss: 0.0014 lr: 0.02\n",
      "iteration: 305200 loss: 0.0021 lr: 0.02\n",
      "iteration: 305210 loss: 0.0015 lr: 0.02\n",
      "iteration: 305220 loss: 0.0021 lr: 0.02\n",
      "iteration: 305230 loss: 0.0021 lr: 0.02\n",
      "iteration: 305240 loss: 0.0021 lr: 0.02\n",
      "iteration: 305250 loss: 0.0026 lr: 0.02\n",
      "iteration: 305260 loss: 0.0036 lr: 0.02\n",
      "iteration: 305270 loss: 0.0014 lr: 0.02\n",
      "iteration: 305280 loss: 0.0019 lr: 0.02\n",
      "iteration: 305290 loss: 0.0018 lr: 0.02\n",
      "iteration: 305300 loss: 0.0016 lr: 0.02\n",
      "iteration: 305310 loss: 0.0020 lr: 0.02\n",
      "iteration: 305320 loss: 0.0016 lr: 0.02\n",
      "iteration: 305330 loss: 0.0025 lr: 0.02\n",
      "iteration: 305340 loss: 0.0024 lr: 0.02\n",
      "iteration: 305350 loss: 0.0017 lr: 0.02\n",
      "iteration: 305360 loss: 0.0017 lr: 0.02\n",
      "iteration: 305370 loss: 0.0018 lr: 0.02\n",
      "iteration: 305380 loss: 0.0014 lr: 0.02\n",
      "iteration: 305390 loss: 0.0011 lr: 0.02\n",
      "iteration: 305400 loss: 0.0016 lr: 0.02\n",
      "iteration: 305410 loss: 0.0016 lr: 0.02\n",
      "iteration: 305420 loss: 0.0015 lr: 0.02\n",
      "iteration: 305430 loss: 0.0016 lr: 0.02\n",
      "iteration: 305440 loss: 0.0015 lr: 0.02\n",
      "iteration: 305450 loss: 0.0014 lr: 0.02\n",
      "iteration: 305460 loss: 0.0021 lr: 0.02\n",
      "iteration: 305470 loss: 0.0016 lr: 0.02\n",
      "iteration: 305480 loss: 0.0023 lr: 0.02\n",
      "iteration: 305490 loss: 0.0019 lr: 0.02\n",
      "iteration: 305500 loss: 0.0024 lr: 0.02\n",
      "iteration: 305510 loss: 0.0023 lr: 0.02\n",
      "iteration: 305520 loss: 0.0017 lr: 0.02\n",
      "iteration: 305530 loss: 0.0042 lr: 0.02\n",
      "iteration: 305540 loss: 0.0017 lr: 0.02\n",
      "iteration: 305550 loss: 0.0016 lr: 0.02\n",
      "iteration: 305560 loss: 0.0013 lr: 0.02\n",
      "iteration: 305570 loss: 0.0016 lr: 0.02\n",
      "iteration: 305580 loss: 0.0018 lr: 0.02\n",
      "iteration: 305590 loss: 0.0016 lr: 0.02\n",
      "iteration: 305600 loss: 0.0019 lr: 0.02\n",
      "iteration: 305610 loss: 0.0016 lr: 0.02\n",
      "iteration: 305620 loss: 0.0026 lr: 0.02\n",
      "iteration: 305630 loss: 0.0024 lr: 0.02\n",
      "iteration: 305640 loss: 0.0017 lr: 0.02\n",
      "iteration: 305650 loss: 0.0022 lr: 0.02\n",
      "iteration: 305660 loss: 0.0022 lr: 0.02\n",
      "iteration: 305670 loss: 0.0020 lr: 0.02\n",
      "iteration: 305680 loss: 0.0013 lr: 0.02\n",
      "iteration: 305690 loss: 0.0023 lr: 0.02\n",
      "iteration: 305700 loss: 0.0016 lr: 0.02\n",
      "iteration: 305710 loss: 0.0017 lr: 0.02\n",
      "iteration: 305720 loss: 0.0018 lr: 0.02\n",
      "iteration: 305730 loss: 0.0019 lr: 0.02\n",
      "iteration: 305740 loss: 0.0014 lr: 0.02\n",
      "iteration: 305750 loss: 0.0020 lr: 0.02\n",
      "iteration: 305760 loss: 0.0014 lr: 0.02\n",
      "iteration: 305770 loss: 0.0014 lr: 0.02\n",
      "iteration: 305780 loss: 0.0015 lr: 0.02\n",
      "iteration: 305790 loss: 0.0019 lr: 0.02\n",
      "iteration: 305800 loss: 0.0023 lr: 0.02\n",
      "iteration: 305810 loss: 0.0018 lr: 0.02\n",
      "iteration: 305820 loss: 0.0017 lr: 0.02\n",
      "iteration: 305830 loss: 0.0016 lr: 0.02\n",
      "iteration: 305840 loss: 0.0016 lr: 0.02\n",
      "iteration: 305850 loss: 0.0012 lr: 0.02\n",
      "iteration: 305860 loss: 0.0017 lr: 0.02\n",
      "iteration: 305870 loss: 0.0021 lr: 0.02\n",
      "iteration: 305880 loss: 0.0011 lr: 0.02\n",
      "iteration: 305890 loss: 0.0020 lr: 0.02\n",
      "iteration: 305900 loss: 0.0017 lr: 0.02\n",
      "iteration: 305910 loss: 0.0017 lr: 0.02\n",
      "iteration: 305920 loss: 0.0023 lr: 0.02\n",
      "iteration: 305930 loss: 0.0021 lr: 0.02\n",
      "iteration: 305940 loss: 0.0016 lr: 0.02\n",
      "iteration: 305950 loss: 0.0025 lr: 0.02\n",
      "iteration: 305960 loss: 0.0018 lr: 0.02\n",
      "iteration: 305970 loss: 0.0012 lr: 0.02\n",
      "iteration: 305980 loss: 0.0017 lr: 0.02\n",
      "iteration: 305990 loss: 0.0019 lr: 0.02\n",
      "iteration: 306000 loss: 0.0024 lr: 0.02\n",
      "iteration: 306010 loss: 0.0019 lr: 0.02\n",
      "iteration: 306020 loss: 0.0021 lr: 0.02\n",
      "iteration: 306030 loss: 0.0017 lr: 0.02\n",
      "iteration: 306040 loss: 0.0013 lr: 0.02\n",
      "iteration: 306050 loss: 0.0018 lr: 0.02\n",
      "iteration: 306060 loss: 0.0015 lr: 0.02\n",
      "iteration: 306070 loss: 0.0021 lr: 0.02\n",
      "iteration: 306080 loss: 0.0023 lr: 0.02\n",
      "iteration: 306090 loss: 0.0024 lr: 0.02\n",
      "iteration: 306100 loss: 0.0012 lr: 0.02\n",
      "iteration: 306110 loss: 0.0018 lr: 0.02\n",
      "iteration: 306120 loss: 0.0014 lr: 0.02\n",
      "iteration: 306130 loss: 0.0014 lr: 0.02\n",
      "iteration: 306140 loss: 0.0022 lr: 0.02\n",
      "iteration: 306150 loss: 0.0014 lr: 0.02\n",
      "iteration: 306160 loss: 0.0018 lr: 0.02\n",
      "iteration: 306170 loss: 0.0029 lr: 0.02\n",
      "iteration: 306180 loss: 0.0019 lr: 0.02\n",
      "iteration: 306190 loss: 0.0023 lr: 0.02\n",
      "iteration: 306200 loss: 0.0018 lr: 0.02\n",
      "iteration: 306210 loss: 0.0018 lr: 0.02\n",
      "iteration: 306220 loss: 0.0016 lr: 0.02\n",
      "iteration: 306230 loss: 0.0018 lr: 0.02\n",
      "iteration: 306240 loss: 0.0019 lr: 0.02\n",
      "iteration: 306250 loss: 0.0016 lr: 0.02\n",
      "iteration: 306260 loss: 0.0027 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 306270 loss: 0.0035 lr: 0.02\n",
      "iteration: 306280 loss: 0.0021 lr: 0.02\n",
      "iteration: 306290 loss: 0.0017 lr: 0.02\n",
      "iteration: 306300 loss: 0.0011 lr: 0.02\n",
      "iteration: 306310 loss: 0.0015 lr: 0.02\n",
      "iteration: 306320 loss: 0.0021 lr: 0.02\n",
      "iteration: 306330 loss: 0.0019 lr: 0.02\n",
      "iteration: 306340 loss: 0.0024 lr: 0.02\n",
      "iteration: 306350 loss: 0.0015 lr: 0.02\n",
      "iteration: 306360 loss: 0.0014 lr: 0.02\n",
      "iteration: 306370 loss: 0.0020 lr: 0.02\n",
      "iteration: 306380 loss: 0.0015 lr: 0.02\n",
      "iteration: 306390 loss: 0.0018 lr: 0.02\n",
      "iteration: 306400 loss: 0.0019 lr: 0.02\n",
      "iteration: 306410 loss: 0.0016 lr: 0.02\n",
      "iteration: 306420 loss: 0.0024 lr: 0.02\n",
      "iteration: 306430 loss: 0.0015 lr: 0.02\n",
      "iteration: 306440 loss: 0.0015 lr: 0.02\n",
      "iteration: 306450 loss: 0.0018 lr: 0.02\n",
      "iteration: 306460 loss: 0.0014 lr: 0.02\n",
      "iteration: 306470 loss: 0.0018 lr: 0.02\n",
      "iteration: 306480 loss: 0.0017 lr: 0.02\n",
      "iteration: 306490 loss: 0.0020 lr: 0.02\n",
      "iteration: 306500 loss: 0.0011 lr: 0.02\n",
      "iteration: 306510 loss: 0.0011 lr: 0.02\n",
      "iteration: 306520 loss: 0.0015 lr: 0.02\n",
      "iteration: 306530 loss: 0.0016 lr: 0.02\n",
      "iteration: 306540 loss: 0.0018 lr: 0.02\n",
      "iteration: 306550 loss: 0.0019 lr: 0.02\n",
      "iteration: 306560 loss: 0.0010 lr: 0.02\n",
      "iteration: 306570 loss: 0.0016 lr: 0.02\n",
      "iteration: 306580 loss: 0.0017 lr: 0.02\n",
      "iteration: 306590 loss: 0.0015 lr: 0.02\n",
      "iteration: 306600 loss: 0.0014 lr: 0.02\n",
      "iteration: 306610 loss: 0.0012 lr: 0.02\n",
      "iteration: 306620 loss: 0.0016 lr: 0.02\n",
      "iteration: 306630 loss: 0.0021 lr: 0.02\n",
      "iteration: 306640 loss: 0.0015 lr: 0.02\n",
      "iteration: 306650 loss: 0.0018 lr: 0.02\n",
      "iteration: 306660 loss: 0.0014 lr: 0.02\n",
      "iteration: 306670 loss: 0.0014 lr: 0.02\n",
      "iteration: 306680 loss: 0.0026 lr: 0.02\n",
      "iteration: 306690 loss: 0.0016 lr: 0.02\n",
      "iteration: 306700 loss: 0.0014 lr: 0.02\n",
      "iteration: 306710 loss: 0.0014 lr: 0.02\n",
      "iteration: 306720 loss: 0.0020 lr: 0.02\n",
      "iteration: 306730 loss: 0.0013 lr: 0.02\n",
      "iteration: 306740 loss: 0.0015 lr: 0.02\n",
      "iteration: 306750 loss: 0.0016 lr: 0.02\n",
      "iteration: 306760 loss: 0.0013 lr: 0.02\n",
      "iteration: 306770 loss: 0.0019 lr: 0.02\n",
      "iteration: 306780 loss: 0.0012 lr: 0.02\n",
      "iteration: 306790 loss: 0.0020 lr: 0.02\n",
      "iteration: 306800 loss: 0.0013 lr: 0.02\n",
      "iteration: 306810 loss: 0.0015 lr: 0.02\n",
      "iteration: 306820 loss: 0.0017 lr: 0.02\n",
      "iteration: 306830 loss: 0.0021 lr: 0.02\n",
      "iteration: 306840 loss: 0.0019 lr: 0.02\n",
      "iteration: 306850 loss: 0.0015 lr: 0.02\n",
      "iteration: 306860 loss: 0.0016 lr: 0.02\n",
      "iteration: 306870 loss: 0.0017 lr: 0.02\n",
      "iteration: 306880 loss: 0.0023 lr: 0.02\n",
      "iteration: 306890 loss: 0.0022 lr: 0.02\n",
      "iteration: 306900 loss: 0.0019 lr: 0.02\n",
      "iteration: 306910 loss: 0.0014 lr: 0.02\n",
      "iteration: 306920 loss: 0.0014 lr: 0.02\n",
      "iteration: 306930 loss: 0.0017 lr: 0.02\n",
      "iteration: 306940 loss: 0.0013 lr: 0.02\n",
      "iteration: 306950 loss: 0.0025 lr: 0.02\n",
      "iteration: 306960 loss: 0.0020 lr: 0.02\n",
      "iteration: 306970 loss: 0.0016 lr: 0.02\n",
      "iteration: 306980 loss: 0.0024 lr: 0.02\n",
      "iteration: 306990 loss: 0.0017 lr: 0.02\n",
      "iteration: 307000 loss: 0.0014 lr: 0.02\n",
      "iteration: 307010 loss: 0.0017 lr: 0.02\n",
      "iteration: 307020 loss: 0.0016 lr: 0.02\n",
      "iteration: 307030 loss: 0.0016 lr: 0.02\n",
      "iteration: 307040 loss: 0.0017 lr: 0.02\n",
      "iteration: 307050 loss: 0.0019 lr: 0.02\n",
      "iteration: 307060 loss: 0.0013 lr: 0.02\n",
      "iteration: 307070 loss: 0.0018 lr: 0.02\n",
      "iteration: 307080 loss: 0.0026 lr: 0.02\n",
      "iteration: 307090 loss: 0.0018 lr: 0.02\n",
      "iteration: 307100 loss: 0.0012 lr: 0.02\n",
      "iteration: 307110 loss: 0.0014 lr: 0.02\n",
      "iteration: 307120 loss: 0.0015 lr: 0.02\n",
      "iteration: 307130 loss: 0.0013 lr: 0.02\n",
      "iteration: 307140 loss: 0.0014 lr: 0.02\n",
      "iteration: 307150 loss: 0.0018 lr: 0.02\n",
      "iteration: 307160 loss: 0.0010 lr: 0.02\n",
      "iteration: 307170 loss: 0.0023 lr: 0.02\n",
      "iteration: 307180 loss: 0.0018 lr: 0.02\n",
      "iteration: 307190 loss: 0.0014 lr: 0.02\n",
      "iteration: 307200 loss: 0.0026 lr: 0.02\n",
      "iteration: 307210 loss: 0.0016 lr: 0.02\n",
      "iteration: 307220 loss: 0.0019 lr: 0.02\n",
      "iteration: 307230 loss: 0.0018 lr: 0.02\n",
      "iteration: 307240 loss: 0.0018 lr: 0.02\n",
      "iteration: 307250 loss: 0.0015 lr: 0.02\n",
      "iteration: 307260 loss: 0.0025 lr: 0.02\n",
      "iteration: 307270 loss: 0.0014 lr: 0.02\n",
      "iteration: 307280 loss: 0.0015 lr: 0.02\n",
      "iteration: 307290 loss: 0.0025 lr: 0.02\n",
      "iteration: 307300 loss: 0.0013 lr: 0.02\n",
      "iteration: 307310 loss: 0.0014 lr: 0.02\n",
      "iteration: 307320 loss: 0.0015 lr: 0.02\n",
      "iteration: 307330 loss: 0.0021 lr: 0.02\n",
      "iteration: 307340 loss: 0.0016 lr: 0.02\n",
      "iteration: 307350 loss: 0.0016 lr: 0.02\n",
      "iteration: 307360 loss: 0.0015 lr: 0.02\n",
      "iteration: 307370 loss: 0.0020 lr: 0.02\n",
      "iteration: 307380 loss: 0.0014 lr: 0.02\n",
      "iteration: 307390 loss: 0.0016 lr: 0.02\n",
      "iteration: 307400 loss: 0.0014 lr: 0.02\n",
      "iteration: 307410 loss: 0.0017 lr: 0.02\n",
      "iteration: 307420 loss: 0.0014 lr: 0.02\n",
      "iteration: 307430 loss: 0.0012 lr: 0.02\n",
      "iteration: 307440 loss: 0.0017 lr: 0.02\n",
      "iteration: 307450 loss: 0.0018 lr: 0.02\n",
      "iteration: 307460 loss: 0.0014 lr: 0.02\n",
      "iteration: 307470 loss: 0.0018 lr: 0.02\n",
      "iteration: 307480 loss: 0.0016 lr: 0.02\n",
      "iteration: 307490 loss: 0.0010 lr: 0.02\n",
      "iteration: 307500 loss: 0.0013 lr: 0.02\n",
      "iteration: 307510 loss: 0.0013 lr: 0.02\n",
      "iteration: 307520 loss: 0.0029 lr: 0.02\n",
      "iteration: 307530 loss: 0.0022 lr: 0.02\n",
      "iteration: 307540 loss: 0.0023 lr: 0.02\n",
      "iteration: 307550 loss: 0.0015 lr: 0.02\n",
      "iteration: 307560 loss: 0.0017 lr: 0.02\n",
      "iteration: 307570 loss: 0.0020 lr: 0.02\n",
      "iteration: 307580 loss: 0.0020 lr: 0.02\n",
      "iteration: 307590 loss: 0.0015 lr: 0.02\n",
      "iteration: 307600 loss: 0.0023 lr: 0.02\n",
      "iteration: 307610 loss: 0.0022 lr: 0.02\n",
      "iteration: 307620 loss: 0.0025 lr: 0.02\n",
      "iteration: 307630 loss: 0.0025 lr: 0.02\n",
      "iteration: 307640 loss: 0.0016 lr: 0.02\n",
      "iteration: 307650 loss: 0.0016 lr: 0.02\n",
      "iteration: 307660 loss: 0.0014 lr: 0.02\n",
      "iteration: 307670 loss: 0.0018 lr: 0.02\n",
      "iteration: 307680 loss: 0.0016 lr: 0.02\n",
      "iteration: 307690 loss: 0.0015 lr: 0.02\n",
      "iteration: 307700 loss: 0.0031 lr: 0.02\n",
      "iteration: 307710 loss: 0.0015 lr: 0.02\n",
      "iteration: 307720 loss: 0.0014 lr: 0.02\n",
      "iteration: 307730 loss: 0.0019 lr: 0.02\n",
      "iteration: 307740 loss: 0.0023 lr: 0.02\n",
      "iteration: 307750 loss: 0.0017 lr: 0.02\n",
      "iteration: 307760 loss: 0.0014 lr: 0.02\n",
      "iteration: 307770 loss: 0.0020 lr: 0.02\n",
      "iteration: 307780 loss: 0.0019 lr: 0.02\n",
      "iteration: 307790 loss: 0.0017 lr: 0.02\n",
      "iteration: 307800 loss: 0.0016 lr: 0.02\n",
      "iteration: 307810 loss: 0.0016 lr: 0.02\n",
      "iteration: 307820 loss: 0.0024 lr: 0.02\n",
      "iteration: 307830 loss: 0.0015 lr: 0.02\n",
      "iteration: 307840 loss: 0.0014 lr: 0.02\n",
      "iteration: 307850 loss: 0.0016 lr: 0.02\n",
      "iteration: 307860 loss: 0.0015 lr: 0.02\n",
      "iteration: 307870 loss: 0.0013 lr: 0.02\n",
      "iteration: 307880 loss: 0.0017 lr: 0.02\n",
      "iteration: 307890 loss: 0.0025 lr: 0.02\n",
      "iteration: 307900 loss: 0.0016 lr: 0.02\n",
      "iteration: 307910 loss: 0.0021 lr: 0.02\n",
      "iteration: 307920 loss: 0.0014 lr: 0.02\n",
      "iteration: 307930 loss: 0.0014 lr: 0.02\n",
      "iteration: 307940 loss: 0.0033 lr: 0.02\n",
      "iteration: 307950 loss: 0.0021 lr: 0.02\n",
      "iteration: 307960 loss: 0.0021 lr: 0.02\n",
      "iteration: 307970 loss: 0.0014 lr: 0.02\n",
      "iteration: 307980 loss: 0.0017 lr: 0.02\n",
      "iteration: 307990 loss: 0.0014 lr: 0.02\n",
      "iteration: 308000 loss: 0.0020 lr: 0.02\n",
      "iteration: 308010 loss: 0.0018 lr: 0.02\n",
      "iteration: 308020 loss: 0.0013 lr: 0.02\n",
      "iteration: 308030 loss: 0.0014 lr: 0.02\n",
      "iteration: 308040 loss: 0.0016 lr: 0.02\n",
      "iteration: 308050 loss: 0.0019 lr: 0.02\n",
      "iteration: 308060 loss: 0.0024 lr: 0.02\n",
      "iteration: 308070 loss: 0.0014 lr: 0.02\n",
      "iteration: 308080 loss: 0.0013 lr: 0.02\n",
      "iteration: 308090 loss: 0.0027 lr: 0.02\n",
      "iteration: 308100 loss: 0.0016 lr: 0.02\n",
      "iteration: 308110 loss: 0.0016 lr: 0.02\n",
      "iteration: 308120 loss: 0.0019 lr: 0.02\n",
      "iteration: 308130 loss: 0.0027 lr: 0.02\n",
      "iteration: 308140 loss: 0.0022 lr: 0.02\n",
      "iteration: 308150 loss: 0.0017 lr: 0.02\n",
      "iteration: 308160 loss: 0.0022 lr: 0.02\n",
      "iteration: 308170 loss: 0.0025 lr: 0.02\n",
      "iteration: 308180 loss: 0.0019 lr: 0.02\n",
      "iteration: 308190 loss: 0.0020 lr: 0.02\n",
      "iteration: 308200 loss: 0.0019 lr: 0.02\n",
      "iteration: 308210 loss: 0.0014 lr: 0.02\n",
      "iteration: 308220 loss: 0.0015 lr: 0.02\n",
      "iteration: 308230 loss: 0.0014 lr: 0.02\n",
      "iteration: 308240 loss: 0.0017 lr: 0.02\n",
      "iteration: 308250 loss: 0.0025 lr: 0.02\n",
      "iteration: 308260 loss: 0.0025 lr: 0.02\n",
      "iteration: 308270 loss: 0.0017 lr: 0.02\n",
      "iteration: 308280 loss: 0.0023 lr: 0.02\n",
      "iteration: 308290 loss: 0.0023 lr: 0.02\n",
      "iteration: 308300 loss: 0.0022 lr: 0.02\n",
      "iteration: 308310 loss: 0.0014 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 308320 loss: 0.0013 lr: 0.02\n",
      "iteration: 308330 loss: 0.0014 lr: 0.02\n",
      "iteration: 308340 loss: 0.0016 lr: 0.02\n",
      "iteration: 308350 loss: 0.0018 lr: 0.02\n",
      "iteration: 308360 loss: 0.0014 lr: 0.02\n",
      "iteration: 308370 loss: 0.0015 lr: 0.02\n",
      "iteration: 308380 loss: 0.0018 lr: 0.02\n",
      "iteration: 308390 loss: 0.0017 lr: 0.02\n",
      "iteration: 308400 loss: 0.0015 lr: 0.02\n",
      "iteration: 308410 loss: 0.0018 lr: 0.02\n",
      "iteration: 308420 loss: 0.0016 lr: 0.02\n",
      "iteration: 308430 loss: 0.0021 lr: 0.02\n",
      "iteration: 308440 loss: 0.0017 lr: 0.02\n",
      "iteration: 308450 loss: 0.0019 lr: 0.02\n",
      "iteration: 308460 loss: 0.0012 lr: 0.02\n",
      "iteration: 308470 loss: 0.0017 lr: 0.02\n",
      "iteration: 308480 loss: 0.0016 lr: 0.02\n",
      "iteration: 308490 loss: 0.0018 lr: 0.02\n",
      "iteration: 308500 loss: 0.0017 lr: 0.02\n",
      "iteration: 308510 loss: 0.0021 lr: 0.02\n",
      "iteration: 308520 loss: 0.0015 lr: 0.02\n",
      "iteration: 308530 loss: 0.0023 lr: 0.02\n",
      "iteration: 308540 loss: 0.0019 lr: 0.02\n",
      "iteration: 308550 loss: 0.0021 lr: 0.02\n",
      "iteration: 308560 loss: 0.0026 lr: 0.02\n",
      "iteration: 308570 loss: 0.0019 lr: 0.02\n",
      "iteration: 308580 loss: 0.0018 lr: 0.02\n",
      "iteration: 308590 loss: 0.0020 lr: 0.02\n",
      "iteration: 308600 loss: 0.0012 lr: 0.02\n",
      "iteration: 308610 loss: 0.0017 lr: 0.02\n",
      "iteration: 308620 loss: 0.0022 lr: 0.02\n",
      "iteration: 308630 loss: 0.0018 lr: 0.02\n",
      "iteration: 308640 loss: 0.0017 lr: 0.02\n",
      "iteration: 308650 loss: 0.0018 lr: 0.02\n",
      "iteration: 308660 loss: 0.0015 lr: 0.02\n",
      "iteration: 308670 loss: 0.0019 lr: 0.02\n",
      "iteration: 308680 loss: 0.0019 lr: 0.02\n",
      "iteration: 308690 loss: 0.0019 lr: 0.02\n",
      "iteration: 308700 loss: 0.0014 lr: 0.02\n",
      "iteration: 308710 loss: 0.0016 lr: 0.02\n",
      "iteration: 308720 loss: 0.0012 lr: 0.02\n",
      "iteration: 308730 loss: 0.0024 lr: 0.02\n",
      "iteration: 308740 loss: 0.0022 lr: 0.02\n",
      "iteration: 308750 loss: 0.0016 lr: 0.02\n",
      "iteration: 308760 loss: 0.0018 lr: 0.02\n",
      "iteration: 308770 loss: 0.0021 lr: 0.02\n",
      "iteration: 308780 loss: 0.0018 lr: 0.02\n",
      "iteration: 308790 loss: 0.0013 lr: 0.02\n",
      "iteration: 308800 loss: 0.0013 lr: 0.02\n",
      "iteration: 308810 loss: 0.0030 lr: 0.02\n",
      "iteration: 308820 loss: 0.0016 lr: 0.02\n",
      "iteration: 308830 loss: 0.0017 lr: 0.02\n",
      "iteration: 308840 loss: 0.0014 lr: 0.02\n",
      "iteration: 308850 loss: 0.0011 lr: 0.02\n",
      "iteration: 308860 loss: 0.0016 lr: 0.02\n",
      "iteration: 308870 loss: 0.0020 lr: 0.02\n",
      "iteration: 308880 loss: 0.0022 lr: 0.02\n",
      "iteration: 308890 loss: 0.0016 lr: 0.02\n",
      "iteration: 308900 loss: 0.0018 lr: 0.02\n",
      "iteration: 308910 loss: 0.0015 lr: 0.02\n",
      "iteration: 308920 loss: 0.0018 lr: 0.02\n",
      "iteration: 308930 loss: 0.0014 lr: 0.02\n",
      "iteration: 308940 loss: 0.0023 lr: 0.02\n",
      "iteration: 308950 loss: 0.0014 lr: 0.02\n",
      "iteration: 308960 loss: 0.0015 lr: 0.02\n",
      "iteration: 308970 loss: 0.0017 lr: 0.02\n",
      "iteration: 308980 loss: 0.0039 lr: 0.02\n",
      "iteration: 308990 loss: 0.0016 lr: 0.02\n",
      "iteration: 309000 loss: 0.0018 lr: 0.02\n",
      "iteration: 309010 loss: 0.0020 lr: 0.02\n",
      "iteration: 309020 loss: 0.0022 lr: 0.02\n",
      "iteration: 309030 loss: 0.0015 lr: 0.02\n",
      "iteration: 309040 loss: 0.0020 lr: 0.02\n",
      "iteration: 309050 loss: 0.0019 lr: 0.02\n",
      "iteration: 309060 loss: 0.0013 lr: 0.02\n",
      "iteration: 309070 loss: 0.0016 lr: 0.02\n",
      "iteration: 309080 loss: 0.0023 lr: 0.02\n",
      "iteration: 309090 loss: 0.0028 lr: 0.02\n",
      "iteration: 309100 loss: 0.0010 lr: 0.02\n",
      "iteration: 309110 loss: 0.0018 lr: 0.02\n",
      "iteration: 309120 loss: 0.0020 lr: 0.02\n",
      "iteration: 309130 loss: 0.0018 lr: 0.02\n",
      "iteration: 309140 loss: 0.0047 lr: 0.02\n",
      "iteration: 309150 loss: 0.0017 lr: 0.02\n",
      "iteration: 309160 loss: 0.0012 lr: 0.02\n",
      "iteration: 309170 loss: 0.0021 lr: 0.02\n",
      "iteration: 309180 loss: 0.0013 lr: 0.02\n",
      "iteration: 309190 loss: 0.0017 lr: 0.02\n",
      "iteration: 309200 loss: 0.0014 lr: 0.02\n",
      "iteration: 309210 loss: 0.0017 lr: 0.02\n",
      "iteration: 309220 loss: 0.0020 lr: 0.02\n",
      "iteration: 309230 loss: 0.0021 lr: 0.02\n",
      "iteration: 309240 loss: 0.0017 lr: 0.02\n",
      "iteration: 309250 loss: 0.0010 lr: 0.02\n",
      "iteration: 309260 loss: 0.0018 lr: 0.02\n",
      "iteration: 309270 loss: 0.0011 lr: 0.02\n",
      "iteration: 309280 loss: 0.0024 lr: 0.02\n",
      "iteration: 309290 loss: 0.0023 lr: 0.02\n",
      "iteration: 309300 loss: 0.0016 lr: 0.02\n",
      "iteration: 309310 loss: 0.0022 lr: 0.02\n",
      "iteration: 309320 loss: 0.0020 lr: 0.02\n",
      "iteration: 309330 loss: 0.0023 lr: 0.02\n",
      "iteration: 309340 loss: 0.0018 lr: 0.02\n",
      "iteration: 309350 loss: 0.0025 lr: 0.02\n",
      "iteration: 309360 loss: 0.0018 lr: 0.02\n",
      "iteration: 309370 loss: 0.0021 lr: 0.02\n",
      "iteration: 309380 loss: 0.0011 lr: 0.02\n",
      "iteration: 309390 loss: 0.0023 lr: 0.02\n",
      "iteration: 309400 loss: 0.0029 lr: 0.02\n",
      "iteration: 309410 loss: 0.0017 lr: 0.02\n",
      "iteration: 309420 loss: 0.0014 lr: 0.02\n",
      "iteration: 309430 loss: 0.0025 lr: 0.02\n",
      "iteration: 309440 loss: 0.0025 lr: 0.02\n",
      "iteration: 309450 loss: 0.0024 lr: 0.02\n",
      "iteration: 309460 loss: 0.0017 lr: 0.02\n",
      "iteration: 309470 loss: 0.0020 lr: 0.02\n",
      "iteration: 309480 loss: 0.0021 lr: 0.02\n",
      "iteration: 309490 loss: 0.0022 lr: 0.02\n",
      "iteration: 309500 loss: 0.0019 lr: 0.02\n",
      "iteration: 309510 loss: 0.0022 lr: 0.02\n",
      "iteration: 309520 loss: 0.0015 lr: 0.02\n",
      "iteration: 309530 loss: 0.0015 lr: 0.02\n",
      "iteration: 309540 loss: 0.0016 lr: 0.02\n",
      "iteration: 309550 loss: 0.0017 lr: 0.02\n",
      "iteration: 309560 loss: 0.0017 lr: 0.02\n",
      "iteration: 309570 loss: 0.0018 lr: 0.02\n",
      "iteration: 309580 loss: 0.0026 lr: 0.02\n",
      "iteration: 309590 loss: 0.0014 lr: 0.02\n",
      "iteration: 309600 loss: 0.0016 lr: 0.02\n",
      "iteration: 309610 loss: 0.0018 lr: 0.02\n",
      "iteration: 309620 loss: 0.0021 lr: 0.02\n",
      "iteration: 309630 loss: 0.0016 lr: 0.02\n",
      "iteration: 309640 loss: 0.0016 lr: 0.02\n",
      "iteration: 309650 loss: 0.0023 lr: 0.02\n",
      "iteration: 309660 loss: 0.0017 lr: 0.02\n",
      "iteration: 309670 loss: 0.0012 lr: 0.02\n",
      "iteration: 309680 loss: 0.0018 lr: 0.02\n",
      "iteration: 309690 loss: 0.0026 lr: 0.02\n",
      "iteration: 309700 loss: 0.0022 lr: 0.02\n",
      "iteration: 309710 loss: 0.0017 lr: 0.02\n",
      "iteration: 309720 loss: 0.0017 lr: 0.02\n",
      "iteration: 309730 loss: 0.0018 lr: 0.02\n",
      "iteration: 309740 loss: 0.0016 lr: 0.02\n",
      "iteration: 309750 loss: 0.0028 lr: 0.02\n",
      "iteration: 309760 loss: 0.0016 lr: 0.02\n",
      "iteration: 309770 loss: 0.0014 lr: 0.02\n",
      "iteration: 309780 loss: 0.0019 lr: 0.02\n",
      "iteration: 309790 loss: 0.0018 lr: 0.02\n",
      "iteration: 309800 loss: 0.0015 lr: 0.02\n",
      "iteration: 309810 loss: 0.0020 lr: 0.02\n",
      "iteration: 309820 loss: 0.0028 lr: 0.02\n",
      "iteration: 309830 loss: 0.0023 lr: 0.02\n",
      "iteration: 309840 loss: 0.0017 lr: 0.02\n",
      "iteration: 309850 loss: 0.0023 lr: 0.02\n",
      "iteration: 309860 loss: 0.0017 lr: 0.02\n",
      "iteration: 309870 loss: 0.0018 lr: 0.02\n",
      "iteration: 309880 loss: 0.0012 lr: 0.02\n",
      "iteration: 309890 loss: 0.0015 lr: 0.02\n",
      "iteration: 309900 loss: 0.0019 lr: 0.02\n",
      "iteration: 309910 loss: 0.0017 lr: 0.02\n",
      "iteration: 309920 loss: 0.0015 lr: 0.02\n",
      "iteration: 309930 loss: 0.0017 lr: 0.02\n",
      "iteration: 309940 loss: 0.0020 lr: 0.02\n",
      "iteration: 309950 loss: 0.0024 lr: 0.02\n",
      "iteration: 309960 loss: 0.0017 lr: 0.02\n",
      "iteration: 309970 loss: 0.0015 lr: 0.02\n",
      "iteration: 309980 loss: 0.0015 lr: 0.02\n",
      "iteration: 309990 loss: 0.0016 lr: 0.02\n",
      "iteration: 310000 loss: 0.0027 lr: 0.02\n",
      "iteration: 310010 loss: 0.0016 lr: 0.02\n",
      "iteration: 310020 loss: 0.0012 lr: 0.02\n",
      "iteration: 310030 loss: 0.0027 lr: 0.02\n",
      "iteration: 310040 loss: 0.0014 lr: 0.02\n",
      "iteration: 310050 loss: 0.0023 lr: 0.02\n",
      "iteration: 310060 loss: 0.0017 lr: 0.02\n",
      "iteration: 310070 loss: 0.0026 lr: 0.02\n",
      "iteration: 310080 loss: 0.0019 lr: 0.02\n",
      "iteration: 310090 loss: 0.0015 lr: 0.02\n",
      "iteration: 310100 loss: 0.0021 lr: 0.02\n",
      "iteration: 310110 loss: 0.0015 lr: 0.02\n",
      "iteration: 310120 loss: 0.0014 lr: 0.02\n",
      "iteration: 310130 loss: 0.0018 lr: 0.02\n",
      "iteration: 310140 loss: 0.0019 lr: 0.02\n",
      "iteration: 310150 loss: 0.0018 lr: 0.02\n",
      "iteration: 310160 loss: 0.0014 lr: 0.02\n",
      "iteration: 310170 loss: 0.0017 lr: 0.02\n",
      "iteration: 310180 loss: 0.0022 lr: 0.02\n",
      "iteration: 310190 loss: 0.0017 lr: 0.02\n",
      "iteration: 310200 loss: 0.0017 lr: 0.02\n",
      "iteration: 310210 loss: 0.0017 lr: 0.02\n",
      "iteration: 310220 loss: 0.0017 lr: 0.02\n",
      "iteration: 310230 loss: 0.0019 lr: 0.02\n",
      "iteration: 310240 loss: 0.0022 lr: 0.02\n",
      "iteration: 310250 loss: 0.0026 lr: 0.02\n",
      "iteration: 310260 loss: 0.0015 lr: 0.02\n",
      "iteration: 310270 loss: 0.0015 lr: 0.02\n",
      "iteration: 310280 loss: 0.0013 lr: 0.02\n",
      "iteration: 310290 loss: 0.0014 lr: 0.02\n",
      "iteration: 310300 loss: 0.0016 lr: 0.02\n",
      "iteration: 310310 loss: 0.0016 lr: 0.02\n",
      "iteration: 310320 loss: 0.0016 lr: 0.02\n",
      "iteration: 310330 loss: 0.0017 lr: 0.02\n",
      "iteration: 310340 loss: 0.0017 lr: 0.02\n",
      "iteration: 310350 loss: 0.0012 lr: 0.02\n",
      "iteration: 310360 loss: 0.0021 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 310370 loss: 0.0017 lr: 0.02\n",
      "iteration: 310380 loss: 0.0020 lr: 0.02\n",
      "iteration: 310390 loss: 0.0016 lr: 0.02\n",
      "iteration: 310400 loss: 0.0016 lr: 0.02\n",
      "iteration: 310410 loss: 0.0019 lr: 0.02\n",
      "iteration: 310420 loss: 0.0015 lr: 0.02\n",
      "iteration: 310430 loss: 0.0018 lr: 0.02\n",
      "iteration: 310440 loss: 0.0017 lr: 0.02\n",
      "iteration: 310450 loss: 0.0013 lr: 0.02\n",
      "iteration: 310460 loss: 0.0014 lr: 0.02\n",
      "iteration: 310470 loss: 0.0015 lr: 0.02\n",
      "iteration: 310480 loss: 0.0025 lr: 0.02\n",
      "iteration: 310490 loss: 0.0016 lr: 0.02\n",
      "iteration: 310500 loss: 0.0016 lr: 0.02\n",
      "iteration: 310510 loss: 0.0017 lr: 0.02\n",
      "iteration: 310520 loss: 0.0016 lr: 0.02\n",
      "iteration: 310530 loss: 0.0010 lr: 0.02\n",
      "iteration: 310540 loss: 0.0013 lr: 0.02\n",
      "iteration: 310550 loss: 0.0020 lr: 0.02\n",
      "iteration: 310560 loss: 0.0018 lr: 0.02\n",
      "iteration: 310570 loss: 0.0017 lr: 0.02\n",
      "iteration: 310580 loss: 0.0019 lr: 0.02\n",
      "iteration: 310590 loss: 0.0020 lr: 0.02\n",
      "iteration: 310600 loss: 0.0014 lr: 0.02\n",
      "iteration: 310610 loss: 0.0021 lr: 0.02\n",
      "iteration: 310620 loss: 0.0021 lr: 0.02\n",
      "iteration: 310630 loss: 0.0019 lr: 0.02\n",
      "iteration: 310640 loss: 0.0019 lr: 0.02\n",
      "iteration: 310650 loss: 0.0022 lr: 0.02\n",
      "iteration: 310660 loss: 0.0014 lr: 0.02\n",
      "iteration: 310670 loss: 0.0020 lr: 0.02\n",
      "iteration: 310680 loss: 0.0020 lr: 0.02\n",
      "iteration: 310690 loss: 0.0023 lr: 0.02\n",
      "iteration: 310700 loss: 0.0018 lr: 0.02\n",
      "iteration: 310710 loss: 0.0015 lr: 0.02\n",
      "iteration: 310720 loss: 0.0016 lr: 0.02\n",
      "iteration: 310730 loss: 0.0012 lr: 0.02\n",
      "iteration: 310740 loss: 0.0014 lr: 0.02\n",
      "iteration: 310750 loss: 0.0022 lr: 0.02\n",
      "iteration: 310760 loss: 0.0021 lr: 0.02\n",
      "iteration: 310770 loss: 0.0022 lr: 0.02\n",
      "iteration: 310780 loss: 0.0016 lr: 0.02\n",
      "iteration: 310790 loss: 0.0022 lr: 0.02\n",
      "iteration: 310800 loss: 0.0019 lr: 0.02\n",
      "iteration: 310810 loss: 0.0017 lr: 0.02\n",
      "iteration: 310820 loss: 0.0017 lr: 0.02\n",
      "iteration: 310830 loss: 0.0014 lr: 0.02\n",
      "iteration: 310840 loss: 0.0015 lr: 0.02\n",
      "iteration: 310850 loss: 0.0016 lr: 0.02\n",
      "iteration: 310860 loss: 0.0013 lr: 0.02\n",
      "iteration: 310870 loss: 0.0018 lr: 0.02\n",
      "iteration: 310880 loss: 0.0015 lr: 0.02\n",
      "iteration: 310890 loss: 0.0015 lr: 0.02\n",
      "iteration: 310900 loss: 0.0011 lr: 0.02\n",
      "iteration: 310910 loss: 0.0025 lr: 0.02\n",
      "iteration: 310920 loss: 0.0020 lr: 0.02\n",
      "iteration: 310930 loss: 0.0019 lr: 0.02\n",
      "iteration: 310940 loss: 0.0019 lr: 0.02\n",
      "iteration: 310950 loss: 0.0021 lr: 0.02\n",
      "iteration: 310960 loss: 0.0013 lr: 0.02\n",
      "iteration: 310970 loss: 0.0018 lr: 0.02\n",
      "iteration: 310980 loss: 0.0014 lr: 0.02\n",
      "iteration: 310990 loss: 0.0020 lr: 0.02\n",
      "iteration: 311000 loss: 0.0014 lr: 0.02\n",
      "iteration: 311010 loss: 0.0027 lr: 0.02\n",
      "iteration: 311020 loss: 0.0020 lr: 0.02\n",
      "iteration: 311030 loss: 0.0016 lr: 0.02\n",
      "iteration: 311040 loss: 0.0019 lr: 0.02\n",
      "iteration: 311050 loss: 0.0022 lr: 0.02\n",
      "iteration: 311060 loss: 0.0021 lr: 0.02\n",
      "iteration: 311070 loss: 0.0016 lr: 0.02\n",
      "iteration: 311080 loss: 0.0020 lr: 0.02\n",
      "iteration: 311090 loss: 0.0020 lr: 0.02\n",
      "iteration: 311100 loss: 0.0015 lr: 0.02\n",
      "iteration: 311110 loss: 0.0018 lr: 0.02\n",
      "iteration: 311120 loss: 0.0022 lr: 0.02\n",
      "iteration: 311130 loss: 0.0015 lr: 0.02\n",
      "iteration: 311140 loss: 0.0043 lr: 0.02\n",
      "iteration: 311150 loss: 0.0017 lr: 0.02\n",
      "iteration: 311160 loss: 0.0023 lr: 0.02\n",
      "iteration: 311170 loss: 0.0014 lr: 0.02\n",
      "iteration: 311180 loss: 0.0014 lr: 0.02\n",
      "iteration: 311190 loss: 0.0015 lr: 0.02\n",
      "iteration: 311200 loss: 0.0015 lr: 0.02\n",
      "iteration: 311210 loss: 0.0017 lr: 0.02\n",
      "iteration: 311220 loss: 0.0015 lr: 0.02\n",
      "iteration: 311230 loss: 0.0016 lr: 0.02\n",
      "iteration: 311240 loss: 0.0016 lr: 0.02\n",
      "iteration: 311250 loss: 0.0014 lr: 0.02\n",
      "iteration: 311260 loss: 0.0020 lr: 0.02\n",
      "iteration: 311270 loss: 0.0017 lr: 0.02\n",
      "iteration: 311280 loss: 0.0013 lr: 0.02\n",
      "iteration: 311290 loss: 0.0020 lr: 0.02\n",
      "iteration: 311300 loss: 0.0016 lr: 0.02\n",
      "iteration: 311310 loss: 0.0013 lr: 0.02\n",
      "iteration: 311320 loss: 0.0017 lr: 0.02\n",
      "iteration: 311330 loss: 0.0017 lr: 0.02\n",
      "iteration: 311340 loss: 0.0015 lr: 0.02\n",
      "iteration: 311350 loss: 0.0014 lr: 0.02\n",
      "iteration: 311360 loss: 0.0011 lr: 0.02\n",
      "iteration: 311370 loss: 0.0019 lr: 0.02\n",
      "iteration: 311380 loss: 0.0014 lr: 0.02\n",
      "iteration: 311390 loss: 0.0018 lr: 0.02\n",
      "iteration: 311400 loss: 0.0015 lr: 0.02\n",
      "iteration: 311410 loss: 0.0025 lr: 0.02\n",
      "iteration: 311420 loss: 0.0021 lr: 0.02\n",
      "iteration: 311430 loss: 0.0018 lr: 0.02\n",
      "iteration: 311440 loss: 0.0013 lr: 0.02\n",
      "iteration: 311450 loss: 0.0021 lr: 0.02\n",
      "iteration: 311460 loss: 0.0019 lr: 0.02\n",
      "iteration: 311470 loss: 0.0016 lr: 0.02\n",
      "iteration: 311480 loss: 0.0014 lr: 0.02\n",
      "iteration: 311490 loss: 0.0020 lr: 0.02\n",
      "iteration: 311500 loss: 0.0024 lr: 0.02\n",
      "iteration: 311510 loss: 0.0015 lr: 0.02\n",
      "iteration: 311520 loss: 0.0022 lr: 0.02\n",
      "iteration: 311530 loss: 0.0015 lr: 0.02\n",
      "iteration: 311540 loss: 0.0016 lr: 0.02\n",
      "iteration: 311550 loss: 0.0020 lr: 0.02\n",
      "iteration: 311560 loss: 0.0028 lr: 0.02\n",
      "iteration: 311570 loss: 0.0018 lr: 0.02\n",
      "iteration: 311580 loss: 0.0021 lr: 0.02\n",
      "iteration: 311590 loss: 0.0013 lr: 0.02\n",
      "iteration: 311600 loss: 0.0021 lr: 0.02\n",
      "iteration: 311610 loss: 0.0021 lr: 0.02\n",
      "iteration: 311620 loss: 0.0021 lr: 0.02\n",
      "iteration: 311630 loss: 0.0013 lr: 0.02\n",
      "iteration: 311640 loss: 0.0020 lr: 0.02\n",
      "iteration: 311650 loss: 0.0022 lr: 0.02\n",
      "iteration: 311660 loss: 0.0024 lr: 0.02\n",
      "iteration: 311670 loss: 0.0015 lr: 0.02\n",
      "iteration: 311680 loss: 0.0022 lr: 0.02\n",
      "iteration: 311690 loss: 0.0014 lr: 0.02\n",
      "iteration: 311700 loss: 0.0022 lr: 0.02\n",
      "iteration: 311710 loss: 0.0013 lr: 0.02\n",
      "iteration: 311720 loss: 0.0017 lr: 0.02\n",
      "iteration: 311730 loss: 0.0016 lr: 0.02\n",
      "iteration: 311740 loss: 0.0016 lr: 0.02\n",
      "iteration: 311750 loss: 0.0022 lr: 0.02\n",
      "iteration: 311760 loss: 0.0019 lr: 0.02\n",
      "iteration: 311770 loss: 0.0018 lr: 0.02\n",
      "iteration: 311780 loss: 0.0015 lr: 0.02\n",
      "iteration: 311790 loss: 0.0017 lr: 0.02\n",
      "iteration: 311800 loss: 0.0015 lr: 0.02\n",
      "iteration: 311810 loss: 0.0014 lr: 0.02\n",
      "iteration: 311820 loss: 0.0012 lr: 0.02\n",
      "iteration: 311830 loss: 0.0015 lr: 0.02\n",
      "iteration: 311840 loss: 0.0015 lr: 0.02\n",
      "iteration: 311850 loss: 0.0020 lr: 0.02\n",
      "iteration: 311860 loss: 0.0016 lr: 0.02\n",
      "iteration: 311870 loss: 0.0016 lr: 0.02\n",
      "iteration: 311880 loss: 0.0009 lr: 0.02\n",
      "iteration: 311890 loss: 0.0018 lr: 0.02\n",
      "iteration: 311900 loss: 0.0020 lr: 0.02\n",
      "iteration: 311910 loss: 0.0021 lr: 0.02\n",
      "iteration: 311920 loss: 0.0013 lr: 0.02\n",
      "iteration: 311930 loss: 0.0016 lr: 0.02\n",
      "iteration: 311940 loss: 0.0016 lr: 0.02\n",
      "iteration: 311950 loss: 0.0014 lr: 0.02\n",
      "iteration: 311960 loss: 0.0014 lr: 0.02\n",
      "iteration: 311970 loss: 0.0019 lr: 0.02\n",
      "iteration: 311980 loss: 0.0014 lr: 0.02\n",
      "iteration: 311990 loss: 0.0018 lr: 0.02\n",
      "iteration: 312000 loss: 0.0019 lr: 0.02\n",
      "iteration: 312010 loss: 0.0017 lr: 0.02\n",
      "iteration: 312020 loss: 0.0015 lr: 0.02\n",
      "iteration: 312030 loss: 0.0016 lr: 0.02\n",
      "iteration: 312040 loss: 0.0017 lr: 0.02\n",
      "iteration: 312050 loss: 0.0017 lr: 0.02\n",
      "iteration: 312060 loss: 0.0018 lr: 0.02\n",
      "iteration: 312070 loss: 0.0021 lr: 0.02\n",
      "iteration: 312080 loss: 0.0018 lr: 0.02\n",
      "iteration: 312090 loss: 0.0012 lr: 0.02\n",
      "iteration: 312100 loss: 0.0018 lr: 0.02\n",
      "iteration: 312110 loss: 0.0013 lr: 0.02\n",
      "iteration: 312120 loss: 0.0015 lr: 0.02\n",
      "iteration: 312130 loss: 0.0020 lr: 0.02\n",
      "iteration: 312140 loss: 0.0012 lr: 0.02\n",
      "iteration: 312150 loss: 0.0019 lr: 0.02\n",
      "iteration: 312160 loss: 0.0011 lr: 0.02\n",
      "iteration: 312170 loss: 0.0024 lr: 0.02\n",
      "iteration: 312180 loss: 0.0017 lr: 0.02\n",
      "iteration: 312190 loss: 0.0017 lr: 0.02\n",
      "iteration: 312200 loss: 0.0021 lr: 0.02\n",
      "iteration: 312210 loss: 0.0026 lr: 0.02\n",
      "iteration: 312220 loss: 0.0021 lr: 0.02\n",
      "iteration: 312230 loss: 0.0024 lr: 0.02\n",
      "iteration: 312240 loss: 0.0019 lr: 0.02\n",
      "iteration: 312250 loss: 0.0017 lr: 0.02\n",
      "iteration: 312260 loss: 0.0017 lr: 0.02\n",
      "iteration: 312270 loss: 0.0024 lr: 0.02\n",
      "iteration: 312280 loss: 0.0020 lr: 0.02\n",
      "iteration: 312290 loss: 0.0025 lr: 0.02\n",
      "iteration: 312300 loss: 0.0016 lr: 0.02\n",
      "iteration: 312310 loss: 0.0023 lr: 0.02\n",
      "iteration: 312320 loss: 0.0015 lr: 0.02\n",
      "iteration: 312330 loss: 0.0016 lr: 0.02\n",
      "iteration: 312340 loss: 0.0014 lr: 0.02\n",
      "iteration: 312350 loss: 0.0023 lr: 0.02\n",
      "iteration: 312360 loss: 0.0016 lr: 0.02\n",
      "iteration: 312370 loss: 0.0016 lr: 0.02\n",
      "iteration: 312380 loss: 0.0018 lr: 0.02\n",
      "iteration: 312390 loss: 0.0021 lr: 0.02\n",
      "iteration: 312400 loss: 0.0013 lr: 0.02\n",
      "iteration: 312410 loss: 0.0014 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 312420 loss: 0.0016 lr: 0.02\n",
      "iteration: 312430 loss: 0.0016 lr: 0.02\n",
      "iteration: 312440 loss: 0.0014 lr: 0.02\n",
      "iteration: 312450 loss: 0.0012 lr: 0.02\n",
      "iteration: 312460 loss: 0.0020 lr: 0.02\n",
      "iteration: 312470 loss: 0.0013 lr: 0.02\n",
      "iteration: 312480 loss: 0.0012 lr: 0.02\n",
      "iteration: 312490 loss: 0.0013 lr: 0.02\n",
      "iteration: 312500 loss: 0.0021 lr: 0.02\n",
      "iteration: 312510 loss: 0.0013 lr: 0.02\n",
      "iteration: 312520 loss: 0.0016 lr: 0.02\n",
      "iteration: 312530 loss: 0.0011 lr: 0.02\n",
      "iteration: 312540 loss: 0.0011 lr: 0.02\n",
      "iteration: 312550 loss: 0.0017 lr: 0.02\n",
      "iteration: 312560 loss: 0.0020 lr: 0.02\n",
      "iteration: 312570 loss: 0.0019 lr: 0.02\n",
      "iteration: 312580 loss: 0.0019 lr: 0.02\n",
      "iteration: 312590 loss: 0.0014 lr: 0.02\n",
      "iteration: 312600 loss: 0.0011 lr: 0.02\n",
      "iteration: 312610 loss: 0.0016 lr: 0.02\n",
      "iteration: 312620 loss: 0.0015 lr: 0.02\n",
      "iteration: 312630 loss: 0.0018 lr: 0.02\n",
      "iteration: 312640 loss: 0.0016 lr: 0.02\n",
      "iteration: 312650 loss: 0.0016 lr: 0.02\n",
      "iteration: 312660 loss: 0.0012 lr: 0.02\n",
      "iteration: 312670 loss: 0.0014 lr: 0.02\n",
      "iteration: 312680 loss: 0.0019 lr: 0.02\n",
      "iteration: 312690 loss: 0.0029 lr: 0.02\n",
      "iteration: 312700 loss: 0.0015 lr: 0.02\n",
      "iteration: 312710 loss: 0.0017 lr: 0.02\n",
      "iteration: 312720 loss: 0.0014 lr: 0.02\n",
      "iteration: 312730 loss: 0.0017 lr: 0.02\n",
      "iteration: 312740 loss: 0.0016 lr: 0.02\n",
      "iteration: 312750 loss: 0.0018 lr: 0.02\n",
      "iteration: 312760 loss: 0.0021 lr: 0.02\n",
      "iteration: 312770 loss: 0.0013 lr: 0.02\n",
      "iteration: 312780 loss: 0.0015 lr: 0.02\n",
      "iteration: 312790 loss: 0.0016 lr: 0.02\n",
      "iteration: 312800 loss: 0.0012 lr: 0.02\n",
      "iteration: 312810 loss: 0.0012 lr: 0.02\n",
      "iteration: 312820 loss: 0.0020 lr: 0.02\n",
      "iteration: 312830 loss: 0.0015 lr: 0.02\n",
      "iteration: 312840 loss: 0.0012 lr: 0.02\n",
      "iteration: 312850 loss: 0.0017 lr: 0.02\n",
      "iteration: 312860 loss: 0.0015 lr: 0.02\n",
      "iteration: 312870 loss: 0.0015 lr: 0.02\n",
      "iteration: 312880 loss: 0.0013 lr: 0.02\n",
      "iteration: 312890 loss: 0.0031 lr: 0.02\n",
      "iteration: 312900 loss: 0.0013 lr: 0.02\n",
      "iteration: 312910 loss: 0.0026 lr: 0.02\n",
      "iteration: 312920 loss: 0.0023 lr: 0.02\n",
      "iteration: 312930 loss: 0.0033 lr: 0.02\n",
      "iteration: 312940 loss: 0.0014 lr: 0.02\n",
      "iteration: 312950 loss: 0.0025 lr: 0.02\n",
      "iteration: 312960 loss: 0.0017 lr: 0.02\n",
      "iteration: 312970 loss: 0.0022 lr: 0.02\n",
      "iteration: 312980 loss: 0.0022 lr: 0.02\n",
      "iteration: 312990 loss: 0.0018 lr: 0.02\n",
      "iteration: 313000 loss: 0.0018 lr: 0.02\n",
      "iteration: 313010 loss: 0.0019 lr: 0.02\n",
      "iteration: 313020 loss: 0.0017 lr: 0.02\n",
      "iteration: 313030 loss: 0.0020 lr: 0.02\n",
      "iteration: 313040 loss: 0.0015 lr: 0.02\n",
      "iteration: 313050 loss: 0.0022 lr: 0.02\n",
      "iteration: 313060 loss: 0.0021 lr: 0.02\n",
      "iteration: 313070 loss: 0.0022 lr: 0.02\n",
      "iteration: 313080 loss: 0.0016 lr: 0.02\n",
      "iteration: 313090 loss: 0.0020 lr: 0.02\n",
      "iteration: 313100 loss: 0.0020 lr: 0.02\n",
      "iteration: 313110 loss: 0.0012 lr: 0.02\n",
      "iteration: 313120 loss: 0.0021 lr: 0.02\n",
      "iteration: 313130 loss: 0.0015 lr: 0.02\n",
      "iteration: 313140 loss: 0.0016 lr: 0.02\n",
      "iteration: 313150 loss: 0.0021 lr: 0.02\n",
      "iteration: 313160 loss: 0.0015 lr: 0.02\n",
      "iteration: 313170 loss: 0.0014 lr: 0.02\n",
      "iteration: 313180 loss: 0.0020 lr: 0.02\n",
      "iteration: 313190 loss: 0.0012 lr: 0.02\n",
      "iteration: 313200 loss: 0.0015 lr: 0.02\n",
      "iteration: 313210 loss: 0.0011 lr: 0.02\n",
      "iteration: 313220 loss: 0.0025 lr: 0.02\n",
      "iteration: 313230 loss: 0.0014 lr: 0.02\n",
      "iteration: 313240 loss: 0.0020 lr: 0.02\n",
      "iteration: 313250 loss: 0.0015 lr: 0.02\n",
      "iteration: 313260 loss: 0.0013 lr: 0.02\n",
      "iteration: 313270 loss: 0.0015 lr: 0.02\n",
      "iteration: 313280 loss: 0.0021 lr: 0.02\n",
      "iteration: 313290 loss: 0.0016 lr: 0.02\n",
      "iteration: 313300 loss: 0.0021 lr: 0.02\n",
      "iteration: 313310 loss: 0.0012 lr: 0.02\n",
      "iteration: 313320 loss: 0.0015 lr: 0.02\n",
      "iteration: 313330 loss: 0.0015 lr: 0.02\n",
      "iteration: 313340 loss: 0.0013 lr: 0.02\n",
      "iteration: 313350 loss: 0.0020 lr: 0.02\n",
      "iteration: 313360 loss: 0.0016 lr: 0.02\n",
      "iteration: 313370 loss: 0.0025 lr: 0.02\n",
      "iteration: 313380 loss: 0.0015 lr: 0.02\n",
      "iteration: 313390 loss: 0.0013 lr: 0.02\n",
      "iteration: 313400 loss: 0.0017 lr: 0.02\n",
      "iteration: 313410 loss: 0.0018 lr: 0.02\n",
      "iteration: 313420 loss: 0.0011 lr: 0.02\n",
      "iteration: 313430 loss: 0.0016 lr: 0.02\n",
      "iteration: 313440 loss: 0.0014 lr: 0.02\n",
      "iteration: 313450 loss: 0.0023 lr: 0.02\n",
      "iteration: 313460 loss: 0.0013 lr: 0.02\n",
      "iteration: 313470 loss: 0.0016 lr: 0.02\n",
      "iteration: 313480 loss: 0.0017 lr: 0.02\n",
      "iteration: 313490 loss: 0.0016 lr: 0.02\n",
      "iteration: 313500 loss: 0.0019 lr: 0.02\n",
      "iteration: 313510 loss: 0.0017 lr: 0.02\n",
      "iteration: 313520 loss: 0.0019 lr: 0.02\n",
      "iteration: 313530 loss: 0.0020 lr: 0.02\n",
      "iteration: 313540 loss: 0.0015 lr: 0.02\n",
      "iteration: 313550 loss: 0.0015 lr: 0.02\n",
      "iteration: 313560 loss: 0.0012 lr: 0.02\n",
      "iteration: 313570 loss: 0.0014 lr: 0.02\n",
      "iteration: 313580 loss: 0.0016 lr: 0.02\n",
      "iteration: 313590 loss: 0.0021 lr: 0.02\n",
      "iteration: 313600 loss: 0.0020 lr: 0.02\n",
      "iteration: 313610 loss: 0.0010 lr: 0.02\n",
      "iteration: 313620 loss: 0.0014 lr: 0.02\n",
      "iteration: 313630 loss: 0.0019 lr: 0.02\n",
      "iteration: 313640 loss: 0.0010 lr: 0.02\n",
      "iteration: 313650 loss: 0.0018 lr: 0.02\n",
      "iteration: 313660 loss: 0.0012 lr: 0.02\n",
      "iteration: 313670 loss: 0.0016 lr: 0.02\n",
      "iteration: 313680 loss: 0.0016 lr: 0.02\n",
      "iteration: 313690 loss: 0.0042 lr: 0.02\n",
      "iteration: 313700 loss: 0.0019 lr: 0.02\n",
      "iteration: 313710 loss: 0.0014 lr: 0.02\n",
      "iteration: 313720 loss: 0.0017 lr: 0.02\n",
      "iteration: 313730 loss: 0.0015 lr: 0.02\n",
      "iteration: 313740 loss: 0.0023 lr: 0.02\n",
      "iteration: 313750 loss: 0.0013 lr: 0.02\n",
      "iteration: 313760 loss: 0.0020 lr: 0.02\n",
      "iteration: 313770 loss: 0.0016 lr: 0.02\n",
      "iteration: 313780 loss: 0.0017 lr: 0.02\n",
      "iteration: 313790 loss: 0.0021 lr: 0.02\n",
      "iteration: 313800 loss: 0.0012 lr: 0.02\n",
      "iteration: 313810 loss: 0.0018 lr: 0.02\n",
      "iteration: 313820 loss: 0.0018 lr: 0.02\n",
      "iteration: 313830 loss: 0.0017 lr: 0.02\n",
      "iteration: 313840 loss: 0.0019 lr: 0.02\n",
      "iteration: 313850 loss: 0.0018 lr: 0.02\n",
      "iteration: 313860 loss: 0.0018 lr: 0.02\n",
      "iteration: 313870 loss: 0.0015 lr: 0.02\n",
      "iteration: 313880 loss: 0.0026 lr: 0.02\n",
      "iteration: 313890 loss: 0.0022 lr: 0.02\n",
      "iteration: 313900 loss: 0.0018 lr: 0.02\n",
      "iteration: 313910 loss: 0.0013 lr: 0.02\n",
      "iteration: 313920 loss: 0.0015 lr: 0.02\n",
      "iteration: 313930 loss: 0.0012 lr: 0.02\n",
      "iteration: 313940 loss: 0.0017 lr: 0.02\n",
      "iteration: 313950 loss: 0.0024 lr: 0.02\n",
      "iteration: 313960 loss: 0.0021 lr: 0.02\n",
      "iteration: 313970 loss: 0.0017 lr: 0.02\n",
      "iteration: 313980 loss: 0.0019 lr: 0.02\n",
      "iteration: 313990 loss: 0.0024 lr: 0.02\n",
      "iteration: 314000 loss: 0.0018 lr: 0.02\n",
      "iteration: 314010 loss: 0.0024 lr: 0.02\n",
      "iteration: 314020 loss: 0.0017 lr: 0.02\n",
      "iteration: 314030 loss: 0.0016 lr: 0.02\n",
      "iteration: 314040 loss: 0.0013 lr: 0.02\n",
      "iteration: 314050 loss: 0.0049 lr: 0.02\n",
      "iteration: 314060 loss: 0.0018 lr: 0.02\n",
      "iteration: 314070 loss: 0.0017 lr: 0.02\n",
      "iteration: 314080 loss: 0.0023 lr: 0.02\n",
      "iteration: 314090 loss: 0.0013 lr: 0.02\n",
      "iteration: 314100 loss: 0.0015 lr: 0.02\n",
      "iteration: 314110 loss: 0.0012 lr: 0.02\n",
      "iteration: 314120 loss: 0.0014 lr: 0.02\n",
      "iteration: 314130 loss: 0.0019 lr: 0.02\n",
      "iteration: 314140 loss: 0.0015 lr: 0.02\n",
      "iteration: 314150 loss: 0.0020 lr: 0.02\n",
      "iteration: 314160 loss: 0.0016 lr: 0.02\n",
      "iteration: 314170 loss: 0.0023 lr: 0.02\n",
      "iteration: 314180 loss: 0.0019 lr: 0.02\n",
      "iteration: 314190 loss: 0.0018 lr: 0.02\n",
      "iteration: 314200 loss: 0.0019 lr: 0.02\n",
      "iteration: 314210 loss: 0.0014 lr: 0.02\n",
      "iteration: 314220 loss: 0.0015 lr: 0.02\n",
      "iteration: 314230 loss: 0.0014 lr: 0.02\n",
      "iteration: 314240 loss: 0.0015 lr: 0.02\n",
      "iteration: 314250 loss: 0.0013 lr: 0.02\n",
      "iteration: 314260 loss: 0.0025 lr: 0.02\n",
      "iteration: 314270 loss: 0.0022 lr: 0.02\n",
      "iteration: 314280 loss: 0.0021 lr: 0.02\n",
      "iteration: 314290 loss: 0.0023 lr: 0.02\n",
      "iteration: 314300 loss: 0.0029 lr: 0.02\n",
      "iteration: 314310 loss: 0.0016 lr: 0.02\n",
      "iteration: 314320 loss: 0.0020 lr: 0.02\n",
      "iteration: 314330 loss: 0.0018 lr: 0.02\n",
      "iteration: 314340 loss: 0.0020 lr: 0.02\n",
      "iteration: 314350 loss: 0.0011 lr: 0.02\n",
      "iteration: 314360 loss: 0.0016 lr: 0.02\n",
      "iteration: 314370 loss: 0.0014 lr: 0.02\n",
      "iteration: 314380 loss: 0.0018 lr: 0.02\n",
      "iteration: 314390 loss: 0.0014 lr: 0.02\n",
      "iteration: 314400 loss: 0.0025 lr: 0.02\n",
      "iteration: 314410 loss: 0.0012 lr: 0.02\n",
      "iteration: 314420 loss: 0.0013 lr: 0.02\n",
      "iteration: 314430 loss: 0.0021 lr: 0.02\n",
      "iteration: 314440 loss: 0.0018 lr: 0.02\n",
      "iteration: 314450 loss: 0.0016 lr: 0.02\n",
      "iteration: 314460 loss: 0.0016 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 314470 loss: 0.0015 lr: 0.02\n",
      "iteration: 314480 loss: 0.0015 lr: 0.02\n",
      "iteration: 314490 loss: 0.0026 lr: 0.02\n",
      "iteration: 314500 loss: 0.0012 lr: 0.02\n",
      "iteration: 314510 loss: 0.0013 lr: 0.02\n",
      "iteration: 314520 loss: 0.0014 lr: 0.02\n",
      "iteration: 314530 loss: 0.0019 lr: 0.02\n",
      "iteration: 314540 loss: 0.0026 lr: 0.02\n",
      "iteration: 314550 loss: 0.0019 lr: 0.02\n",
      "iteration: 314560 loss: 0.0015 lr: 0.02\n",
      "iteration: 314570 loss: 0.0014 lr: 0.02\n",
      "iteration: 314580 loss: 0.0021 lr: 0.02\n",
      "iteration: 314590 loss: 0.0021 lr: 0.02\n",
      "iteration: 314600 loss: 0.0017 lr: 0.02\n",
      "iteration: 314610 loss: 0.0019 lr: 0.02\n",
      "iteration: 314620 loss: 0.0017 lr: 0.02\n",
      "iteration: 314630 loss: 0.0011 lr: 0.02\n",
      "iteration: 314640 loss: 0.0019 lr: 0.02\n",
      "iteration: 314650 loss: 0.0014 lr: 0.02\n",
      "iteration: 314660 loss: 0.0012 lr: 0.02\n",
      "iteration: 314670 loss: 0.0017 lr: 0.02\n",
      "iteration: 314680 loss: 0.0015 lr: 0.02\n",
      "iteration: 314690 loss: 0.0019 lr: 0.02\n",
      "iteration: 314700 loss: 0.0024 lr: 0.02\n",
      "iteration: 314710 loss: 0.0018 lr: 0.02\n",
      "iteration: 314720 loss: 0.0018 lr: 0.02\n",
      "iteration: 314730 loss: 0.0018 lr: 0.02\n",
      "iteration: 314740 loss: 0.0020 lr: 0.02\n",
      "iteration: 314750 loss: 0.0016 lr: 0.02\n",
      "iteration: 314760 loss: 0.0013 lr: 0.02\n",
      "iteration: 314770 loss: 0.0019 lr: 0.02\n",
      "iteration: 314780 loss: 0.0018 lr: 0.02\n",
      "iteration: 314790 loss: 0.0015 lr: 0.02\n",
      "iteration: 314800 loss: 0.0019 lr: 0.02\n",
      "iteration: 314810 loss: 0.0014 lr: 0.02\n",
      "iteration: 314820 loss: 0.0019 lr: 0.02\n",
      "iteration: 314830 loss: 0.0015 lr: 0.02\n",
      "iteration: 314840 loss: 0.0016 lr: 0.02\n",
      "iteration: 314850 loss: 0.0017 lr: 0.02\n",
      "iteration: 314860 loss: 0.0017 lr: 0.02\n",
      "iteration: 314870 loss: 0.0023 lr: 0.02\n",
      "iteration: 314880 loss: 0.0013 lr: 0.02\n",
      "iteration: 314890 loss: 0.0018 lr: 0.02\n",
      "iteration: 314900 loss: 0.0016 lr: 0.02\n",
      "iteration: 314910 loss: 0.0014 lr: 0.02\n",
      "iteration: 314920 loss: 0.0024 lr: 0.02\n",
      "iteration: 314930 loss: 0.0022 lr: 0.02\n",
      "iteration: 314940 loss: 0.0019 lr: 0.02\n",
      "iteration: 314950 loss: 0.0014 lr: 0.02\n",
      "iteration: 314960 loss: 0.0020 lr: 0.02\n",
      "iteration: 314970 loss: 0.0020 lr: 0.02\n",
      "iteration: 314980 loss: 0.0024 lr: 0.02\n",
      "iteration: 314990 loss: 0.0021 lr: 0.02\n",
      "iteration: 315000 loss: 0.0022 lr: 0.02\n",
      "iteration: 315010 loss: 0.0024 lr: 0.02\n",
      "iteration: 315020 loss: 0.0020 lr: 0.02\n",
      "iteration: 315030 loss: 0.0026 lr: 0.02\n",
      "iteration: 315040 loss: 0.0013 lr: 0.02\n",
      "iteration: 315050 loss: 0.0023 lr: 0.02\n",
      "iteration: 315060 loss: 0.0019 lr: 0.02\n",
      "iteration: 315070 loss: 0.0023 lr: 0.02\n",
      "iteration: 315080 loss: 0.0026 lr: 0.02\n",
      "iteration: 315090 loss: 0.0018 lr: 0.02\n",
      "iteration: 315100 loss: 0.0025 lr: 0.02\n",
      "iteration: 315110 loss: 0.0020 lr: 0.02\n",
      "iteration: 315120 loss: 0.0016 lr: 0.02\n",
      "iteration: 315130 loss: 0.0015 lr: 0.02\n",
      "iteration: 315140 loss: 0.0023 lr: 0.02\n",
      "iteration: 315150 loss: 0.0027 lr: 0.02\n",
      "iteration: 315160 loss: 0.0019 lr: 0.02\n",
      "iteration: 315170 loss: 0.0017 lr: 0.02\n",
      "iteration: 315180 loss: 0.0028 lr: 0.02\n",
      "iteration: 315190 loss: 0.0016 lr: 0.02\n",
      "iteration: 315200 loss: 0.0022 lr: 0.02\n",
      "iteration: 315210 loss: 0.0018 lr: 0.02\n",
      "iteration: 315220 loss: 0.0020 lr: 0.02\n",
      "iteration: 315230 loss: 0.0016 lr: 0.02\n",
      "iteration: 315240 loss: 0.0019 lr: 0.02\n",
      "iteration: 315250 loss: 0.0020 lr: 0.02\n",
      "iteration: 315260 loss: 0.0021 lr: 0.02\n",
      "iteration: 315270 loss: 0.0016 lr: 0.02\n",
      "iteration: 315280 loss: 0.0017 lr: 0.02\n",
      "iteration: 315290 loss: 0.0014 lr: 0.02\n",
      "iteration: 315300 loss: 0.0015 lr: 0.02\n",
      "iteration: 315310 loss: 0.0020 lr: 0.02\n",
      "iteration: 315320 loss: 0.0017 lr: 0.02\n",
      "iteration: 315330 loss: 0.0013 lr: 0.02\n",
      "iteration: 315340 loss: 0.0018 lr: 0.02\n",
      "iteration: 315350 loss: 0.0014 lr: 0.02\n",
      "iteration: 315360 loss: 0.0017 lr: 0.02\n",
      "iteration: 315370 loss: 0.0017 lr: 0.02\n",
      "iteration: 315380 loss: 0.0018 lr: 0.02\n",
      "iteration: 315390 loss: 0.0020 lr: 0.02\n",
      "iteration: 315400 loss: 0.0012 lr: 0.02\n",
      "iteration: 315410 loss: 0.0020 lr: 0.02\n",
      "iteration: 315420 loss: 0.0020 lr: 0.02\n",
      "iteration: 315430 loss: 0.0020 lr: 0.02\n",
      "iteration: 315440 loss: 0.0025 lr: 0.02\n",
      "iteration: 315450 loss: 0.0017 lr: 0.02\n",
      "iteration: 315460 loss: 0.0014 lr: 0.02\n",
      "iteration: 315470 loss: 0.0016 lr: 0.02\n",
      "iteration: 315480 loss: 0.0021 lr: 0.02\n",
      "iteration: 315490 loss: 0.0021 lr: 0.02\n",
      "iteration: 315500 loss: 0.0024 lr: 0.02\n",
      "iteration: 315510 loss: 0.0016 lr: 0.02\n",
      "iteration: 315520 loss: 0.0020 lr: 0.02\n",
      "iteration: 315530 loss: 0.0019 lr: 0.02\n",
      "iteration: 315540 loss: 0.0023 lr: 0.02\n",
      "iteration: 315550 loss: 0.0015 lr: 0.02\n",
      "iteration: 315560 loss: 0.0018 lr: 0.02\n",
      "iteration: 315570 loss: 0.0025 lr: 0.02\n",
      "iteration: 315580 loss: 0.0022 lr: 0.02\n",
      "iteration: 315590 loss: 0.0015 lr: 0.02\n",
      "iteration: 315600 loss: 0.0020 lr: 0.02\n",
      "iteration: 315610 loss: 0.0015 lr: 0.02\n",
      "iteration: 315620 loss: 0.0013 lr: 0.02\n",
      "iteration: 315630 loss: 0.0011 lr: 0.02\n",
      "iteration: 315640 loss: 0.0021 lr: 0.02\n",
      "iteration: 315650 loss: 0.0016 lr: 0.02\n",
      "iteration: 315660 loss: 0.0023 lr: 0.02\n",
      "iteration: 315670 loss: 0.0018 lr: 0.02\n",
      "iteration: 315680 loss: 0.0013 lr: 0.02\n",
      "iteration: 315690 loss: 0.0015 lr: 0.02\n",
      "iteration: 315700 loss: 0.0017 lr: 0.02\n",
      "iteration: 315710 loss: 0.0019 lr: 0.02\n",
      "iteration: 315720 loss: 0.0026 lr: 0.02\n",
      "iteration: 315730 loss: 0.0014 lr: 0.02\n",
      "iteration: 315740 loss: 0.0018 lr: 0.02\n",
      "iteration: 315750 loss: 0.0026 lr: 0.02\n",
      "iteration: 315760 loss: 0.0019 lr: 0.02\n",
      "iteration: 315770 loss: 0.0018 lr: 0.02\n",
      "iteration: 315780 loss: 0.0023 lr: 0.02\n",
      "iteration: 315790 loss: 0.0021 lr: 0.02\n",
      "iteration: 315800 loss: 0.0015 lr: 0.02\n",
      "iteration: 315810 loss: 0.0013 lr: 0.02\n",
      "iteration: 315820 loss: 0.0020 lr: 0.02\n",
      "iteration: 315830 loss: 0.0018 lr: 0.02\n",
      "iteration: 315840 loss: 0.0015 lr: 0.02\n",
      "iteration: 315850 loss: 0.0017 lr: 0.02\n",
      "iteration: 315860 loss: 0.0027 lr: 0.02\n",
      "iteration: 315870 loss: 0.0025 lr: 0.02\n",
      "iteration: 315880 loss: 0.0020 lr: 0.02\n",
      "iteration: 315890 loss: 0.0025 lr: 0.02\n",
      "iteration: 315900 loss: 0.0016 lr: 0.02\n",
      "iteration: 315910 loss: 0.0014 lr: 0.02\n",
      "iteration: 315920 loss: 0.0012 lr: 0.02\n",
      "iteration: 315930 loss: 0.0018 lr: 0.02\n",
      "iteration: 315940 loss: 0.0015 lr: 0.02\n",
      "iteration: 315950 loss: 0.0023 lr: 0.02\n",
      "iteration: 315960 loss: 0.0019 lr: 0.02\n",
      "iteration: 315970 loss: 0.0020 lr: 0.02\n",
      "iteration: 315980 loss: 0.0026 lr: 0.02\n",
      "iteration: 315990 loss: 0.0023 lr: 0.02\n",
      "iteration: 316000 loss: 0.0016 lr: 0.02\n",
      "iteration: 316010 loss: 0.0018 lr: 0.02\n",
      "iteration: 316020 loss: 0.0018 lr: 0.02\n",
      "iteration: 316030 loss: 0.0015 lr: 0.02\n",
      "iteration: 316040 loss: 0.0011 lr: 0.02\n",
      "iteration: 316050 loss: 0.0023 lr: 0.02\n",
      "iteration: 316060 loss: 0.0015 lr: 0.02\n",
      "iteration: 316070 loss: 0.0017 lr: 0.02\n",
      "iteration: 316080 loss: 0.0014 lr: 0.02\n",
      "iteration: 316090 loss: 0.0020 lr: 0.02\n",
      "iteration: 316100 loss: 0.0020 lr: 0.02\n",
      "iteration: 316110 loss: 0.0014 lr: 0.02\n",
      "iteration: 316120 loss: 0.0014 lr: 0.02\n",
      "iteration: 316130 loss: 0.0023 lr: 0.02\n",
      "iteration: 316140 loss: 0.0024 lr: 0.02\n",
      "iteration: 316150 loss: 0.0025 lr: 0.02\n",
      "iteration: 316160 loss: 0.0021 lr: 0.02\n",
      "iteration: 316170 loss: 0.0015 lr: 0.02\n",
      "iteration: 316180 loss: 0.0014 lr: 0.02\n",
      "iteration: 316190 loss: 0.0012 lr: 0.02\n",
      "iteration: 316200 loss: 0.0019 lr: 0.02\n",
      "iteration: 316210 loss: 0.0020 lr: 0.02\n",
      "iteration: 316220 loss: 0.0017 lr: 0.02\n",
      "iteration: 316230 loss: 0.0017 lr: 0.02\n",
      "iteration: 316240 loss: 0.0015 lr: 0.02\n",
      "iteration: 316250 loss: 0.0016 lr: 0.02\n",
      "iteration: 316260 loss: 0.0019 lr: 0.02\n",
      "iteration: 316270 loss: 0.0014 lr: 0.02\n",
      "iteration: 316280 loss: 0.0015 lr: 0.02\n",
      "iteration: 316290 loss: 0.0018 lr: 0.02\n",
      "iteration: 316300 loss: 0.0015 lr: 0.02\n",
      "iteration: 316310 loss: 0.0019 lr: 0.02\n",
      "iteration: 316320 loss: 0.0015 lr: 0.02\n",
      "iteration: 316330 loss: 0.0018 lr: 0.02\n",
      "iteration: 316340 loss: 0.0015 lr: 0.02\n",
      "iteration: 316350 loss: 0.0022 lr: 0.02\n",
      "iteration: 316360 loss: 0.0014 lr: 0.02\n",
      "iteration: 316370 loss: 0.0023 lr: 0.02\n",
      "iteration: 316380 loss: 0.0020 lr: 0.02\n",
      "iteration: 316390 loss: 0.0018 lr: 0.02\n",
      "iteration: 316400 loss: 0.0029 lr: 0.02\n",
      "iteration: 316410 loss: 0.0015 lr: 0.02\n",
      "iteration: 316420 loss: 0.0018 lr: 0.02\n",
      "iteration: 316430 loss: 0.0026 lr: 0.02\n",
      "iteration: 316440 loss: 0.0021 lr: 0.02\n",
      "iteration: 316450 loss: 0.0018 lr: 0.02\n",
      "iteration: 316460 loss: 0.0013 lr: 0.02\n",
      "iteration: 316470 loss: 0.0020 lr: 0.02\n",
      "iteration: 316480 loss: 0.0012 lr: 0.02\n",
      "iteration: 316490 loss: 0.0020 lr: 0.02\n",
      "iteration: 316500 loss: 0.0012 lr: 0.02\n",
      "iteration: 316510 loss: 0.0013 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 316520 loss: 0.0018 lr: 0.02\n",
      "iteration: 316530 loss: 0.0011 lr: 0.02\n",
      "iteration: 316540 loss: 0.0017 lr: 0.02\n",
      "iteration: 316550 loss: 0.0023 lr: 0.02\n",
      "iteration: 316560 loss: 0.0019 lr: 0.02\n",
      "iteration: 316570 loss: 0.0013 lr: 0.02\n",
      "iteration: 316580 loss: 0.0014 lr: 0.02\n",
      "iteration: 316590 loss: 0.0023 lr: 0.02\n",
      "iteration: 316600 loss: 0.0016 lr: 0.02\n",
      "iteration: 316610 loss: 0.0013 lr: 0.02\n",
      "iteration: 316620 loss: 0.0017 lr: 0.02\n",
      "iteration: 316630 loss: 0.0020 lr: 0.02\n",
      "iteration: 316640 loss: 0.0018 lr: 0.02\n",
      "iteration: 316650 loss: 0.0019 lr: 0.02\n",
      "iteration: 316660 loss: 0.0017 lr: 0.02\n",
      "iteration: 316670 loss: 0.0018 lr: 0.02\n",
      "iteration: 316680 loss: 0.0014 lr: 0.02\n",
      "iteration: 316690 loss: 0.0015 lr: 0.02\n",
      "iteration: 316700 loss: 0.0018 lr: 0.02\n",
      "iteration: 316710 loss: 0.0016 lr: 0.02\n",
      "iteration: 316720 loss: 0.0020 lr: 0.02\n",
      "iteration: 316730 loss: 0.0019 lr: 0.02\n",
      "iteration: 316740 loss: 0.0013 lr: 0.02\n",
      "iteration: 316750 loss: 0.0014 lr: 0.02\n",
      "iteration: 316760 loss: 0.0014 lr: 0.02\n",
      "iteration: 316770 loss: 0.0012 lr: 0.02\n",
      "iteration: 316780 loss: 0.0018 lr: 0.02\n",
      "iteration: 316790 loss: 0.0012 lr: 0.02\n",
      "iteration: 316800 loss: 0.0027 lr: 0.02\n",
      "iteration: 316810 loss: 0.0015 lr: 0.02\n",
      "iteration: 316820 loss: 0.0019 lr: 0.02\n",
      "iteration: 316830 loss: 0.0024 lr: 0.02\n",
      "iteration: 316840 loss: 0.0023 lr: 0.02\n",
      "iteration: 316850 loss: 0.0020 lr: 0.02\n",
      "iteration: 316860 loss: 0.0015 lr: 0.02\n",
      "iteration: 316870 loss: 0.0019 lr: 0.02\n",
      "iteration: 316880 loss: 0.0020 lr: 0.02\n",
      "iteration: 316890 loss: 0.0015 lr: 0.02\n",
      "iteration: 316900 loss: 0.0021 lr: 0.02\n",
      "iteration: 316910 loss: 0.0016 lr: 0.02\n",
      "iteration: 316920 loss: 0.0018 lr: 0.02\n",
      "iteration: 316930 loss: 0.0016 lr: 0.02\n",
      "iteration: 316940 loss: 0.0021 lr: 0.02\n",
      "iteration: 316950 loss: 0.0020 lr: 0.02\n",
      "iteration: 316960 loss: 0.0021 lr: 0.02\n",
      "iteration: 316970 loss: 0.0016 lr: 0.02\n",
      "iteration: 316980 loss: 0.0013 lr: 0.02\n",
      "iteration: 316990 loss: 0.0025 lr: 0.02\n",
      "iteration: 317000 loss: 0.0023 lr: 0.02\n",
      "iteration: 317010 loss: 0.0023 lr: 0.02\n",
      "iteration: 317020 loss: 0.0022 lr: 0.02\n",
      "iteration: 317030 loss: 0.0016 lr: 0.02\n",
      "iteration: 317040 loss: 0.0016 lr: 0.02\n",
      "iteration: 317050 loss: 0.0014 lr: 0.02\n",
      "iteration: 317060 loss: 0.0019 lr: 0.02\n",
      "iteration: 317070 loss: 0.0023 lr: 0.02\n",
      "iteration: 317080 loss: 0.0017 lr: 0.02\n",
      "iteration: 317090 loss: 0.0012 lr: 0.02\n",
      "iteration: 317100 loss: 0.0016 lr: 0.02\n",
      "iteration: 317110 loss: 0.0017 lr: 0.02\n",
      "iteration: 317120 loss: 0.0018 lr: 0.02\n",
      "iteration: 317130 loss: 0.0024 lr: 0.02\n",
      "iteration: 317140 loss: 0.0021 lr: 0.02\n",
      "iteration: 317150 loss: 0.0019 lr: 0.02\n",
      "iteration: 317160 loss: 0.0016 lr: 0.02\n",
      "iteration: 317170 loss: 0.0016 lr: 0.02\n",
      "iteration: 317180 loss: 0.0017 lr: 0.02\n",
      "iteration: 317190 loss: 0.0020 lr: 0.02\n",
      "iteration: 317200 loss: 0.0015 lr: 0.02\n",
      "iteration: 317210 loss: 0.0015 lr: 0.02\n",
      "iteration: 317220 loss: 0.0019 lr: 0.02\n",
      "iteration: 317230 loss: 0.0022 lr: 0.02\n",
      "iteration: 317240 loss: 0.0021 lr: 0.02\n",
      "iteration: 317250 loss: 0.0015 lr: 0.02\n",
      "iteration: 317260 loss: 0.0032 lr: 0.02\n",
      "iteration: 317270 loss: 0.0021 lr: 0.02\n",
      "iteration: 317280 loss: 0.0021 lr: 0.02\n",
      "iteration: 317290 loss: 0.0014 lr: 0.02\n",
      "iteration: 317300 loss: 0.0014 lr: 0.02\n",
      "iteration: 317310 loss: 0.0018 lr: 0.02\n",
      "iteration: 317320 loss: 0.0018 lr: 0.02\n",
      "iteration: 317330 loss: 0.0023 lr: 0.02\n",
      "iteration: 317340 loss: 0.0020 lr: 0.02\n",
      "iteration: 317350 loss: 0.0013 lr: 0.02\n",
      "iteration: 317360 loss: 0.0023 lr: 0.02\n",
      "iteration: 317370 loss: 0.0018 lr: 0.02\n",
      "iteration: 317380 loss: 0.0016 lr: 0.02\n",
      "iteration: 317390 loss: 0.0026 lr: 0.02\n",
      "iteration: 317400 loss: 0.0022 lr: 0.02\n",
      "iteration: 317410 loss: 0.0020 lr: 0.02\n",
      "iteration: 317420 loss: 0.0011 lr: 0.02\n",
      "iteration: 317430 loss: 0.0019 lr: 0.02\n",
      "iteration: 317440 loss: 0.0019 lr: 0.02\n",
      "iteration: 317450 loss: 0.0015 lr: 0.02\n",
      "iteration: 317460 loss: 0.0016 lr: 0.02\n",
      "iteration: 317470 loss: 0.0014 lr: 0.02\n",
      "iteration: 317480 loss: 0.0033 lr: 0.02\n",
      "iteration: 317490 loss: 0.0014 lr: 0.02\n",
      "iteration: 317500 loss: 0.0017 lr: 0.02\n",
      "iteration: 317510 loss: 0.0018 lr: 0.02\n",
      "iteration: 317520 loss: 0.0015 lr: 0.02\n",
      "iteration: 317530 loss: 0.0020 lr: 0.02\n",
      "iteration: 317540 loss: 0.0020 lr: 0.02\n",
      "iteration: 317550 loss: 0.0012 lr: 0.02\n",
      "iteration: 317560 loss: 0.0022 lr: 0.02\n",
      "iteration: 317570 loss: 0.0016 lr: 0.02\n",
      "iteration: 317580 loss: 0.0015 lr: 0.02\n",
      "iteration: 317590 loss: 0.0012 lr: 0.02\n",
      "iteration: 317600 loss: 0.0014 lr: 0.02\n",
      "iteration: 317610 loss: 0.0012 lr: 0.02\n",
      "iteration: 317620 loss: 0.0017 lr: 0.02\n",
      "iteration: 317630 loss: 0.0016 lr: 0.02\n",
      "iteration: 317640 loss: 0.0018 lr: 0.02\n",
      "iteration: 317650 loss: 0.0019 lr: 0.02\n",
      "iteration: 317660 loss: 0.0015 lr: 0.02\n",
      "iteration: 317670 loss: 0.0016 lr: 0.02\n",
      "iteration: 317680 loss: 0.0020 lr: 0.02\n",
      "iteration: 317690 loss: 0.0018 lr: 0.02\n",
      "iteration: 317700 loss: 0.0017 lr: 0.02\n",
      "iteration: 317710 loss: 0.0023 lr: 0.02\n",
      "iteration: 317720 loss: 0.0018 lr: 0.02\n",
      "iteration: 317730 loss: 0.0021 lr: 0.02\n",
      "iteration: 317740 loss: 0.0015 lr: 0.02\n",
      "iteration: 317750 loss: 0.0022 lr: 0.02\n",
      "iteration: 317760 loss: 0.0016 lr: 0.02\n",
      "iteration: 317770 loss: 0.0021 lr: 0.02\n",
      "iteration: 317780 loss: 0.0018 lr: 0.02\n",
      "iteration: 317790 loss: 0.0022 lr: 0.02\n",
      "iteration: 317800 loss: 0.0021 lr: 0.02\n",
      "iteration: 317810 loss: 0.0021 lr: 0.02\n",
      "iteration: 317820 loss: 0.0018 lr: 0.02\n",
      "iteration: 317830 loss: 0.0013 lr: 0.02\n",
      "iteration: 317840 loss: 0.0020 lr: 0.02\n",
      "iteration: 317850 loss: 0.0015 lr: 0.02\n",
      "iteration: 317860 loss: 0.0014 lr: 0.02\n",
      "iteration: 317870 loss: 0.0019 lr: 0.02\n",
      "iteration: 317880 loss: 0.0030 lr: 0.02\n",
      "iteration: 317890 loss: 0.0017 lr: 0.02\n",
      "iteration: 317900 loss: 0.0024 lr: 0.02\n",
      "iteration: 317910 loss: 0.0016 lr: 0.02\n",
      "iteration: 317920 loss: 0.0023 lr: 0.02\n",
      "iteration: 317930 loss: 0.0015 lr: 0.02\n",
      "iteration: 317940 loss: 0.0015 lr: 0.02\n",
      "iteration: 317950 loss: 0.0010 lr: 0.02\n",
      "iteration: 317960 loss: 0.0015 lr: 0.02\n",
      "iteration: 317970 loss: 0.0014 lr: 0.02\n",
      "iteration: 317980 loss: 0.0013 lr: 0.02\n",
      "iteration: 317990 loss: 0.0013 lr: 0.02\n",
      "iteration: 318000 loss: 0.0021 lr: 0.02\n",
      "iteration: 318010 loss: 0.0016 lr: 0.02\n",
      "iteration: 318020 loss: 0.0020 lr: 0.02\n",
      "iteration: 318030 loss: 0.0018 lr: 0.02\n",
      "iteration: 318040 loss: 0.0026 lr: 0.02\n",
      "iteration: 318050 loss: 0.0017 lr: 0.02\n",
      "iteration: 318060 loss: 0.0017 lr: 0.02\n",
      "iteration: 318070 loss: 0.0019 lr: 0.02\n",
      "iteration: 318080 loss: 0.0012 lr: 0.02\n",
      "iteration: 318090 loss: 0.0014 lr: 0.02\n",
      "iteration: 318100 loss: 0.0014 lr: 0.02\n",
      "iteration: 318110 loss: 0.0012 lr: 0.02\n",
      "iteration: 318120 loss: 0.0022 lr: 0.02\n",
      "iteration: 318130 loss: 0.0012 lr: 0.02\n",
      "iteration: 318140 loss: 0.0020 lr: 0.02\n",
      "iteration: 318150 loss: 0.0019 lr: 0.02\n",
      "iteration: 318160 loss: 0.0013 lr: 0.02\n",
      "iteration: 318170 loss: 0.0014 lr: 0.02\n",
      "iteration: 318180 loss: 0.0014 lr: 0.02\n",
      "iteration: 318190 loss: 0.0012 lr: 0.02\n",
      "iteration: 318200 loss: 0.0026 lr: 0.02\n",
      "iteration: 318210 loss: 0.0014 lr: 0.02\n",
      "iteration: 318220 loss: 0.0016 lr: 0.02\n",
      "iteration: 318230 loss: 0.0018 lr: 0.02\n",
      "iteration: 318240 loss: 0.0013 lr: 0.02\n",
      "iteration: 318250 loss: 0.0017 lr: 0.02\n",
      "iteration: 318260 loss: 0.0015 lr: 0.02\n",
      "iteration: 318270 loss: 0.0023 lr: 0.02\n",
      "iteration: 318280 loss: 0.0026 lr: 0.02\n",
      "iteration: 318290 loss: 0.0014 lr: 0.02\n",
      "iteration: 318300 loss: 0.0016 lr: 0.02\n",
      "iteration: 318310 loss: 0.0016 lr: 0.02\n",
      "iteration: 318320 loss: 0.0016 lr: 0.02\n",
      "iteration: 318330 loss: 0.0015 lr: 0.02\n",
      "iteration: 318340 loss: 0.0017 lr: 0.02\n",
      "iteration: 318350 loss: 0.0014 lr: 0.02\n",
      "iteration: 318360 loss: 0.0017 lr: 0.02\n",
      "iteration: 318370 loss: 0.0020 lr: 0.02\n",
      "iteration: 318380 loss: 0.0017 lr: 0.02\n",
      "iteration: 318390 loss: 0.0023 lr: 0.02\n",
      "iteration: 318400 loss: 0.0013 lr: 0.02\n",
      "iteration: 318410 loss: 0.0017 lr: 0.02\n",
      "iteration: 318420 loss: 0.0014 lr: 0.02\n",
      "iteration: 318430 loss: 0.0021 lr: 0.02\n",
      "iteration: 318440 loss: 0.0023 lr: 0.02\n",
      "iteration: 318450 loss: 0.0019 lr: 0.02\n",
      "iteration: 318460 loss: 0.0018 lr: 0.02\n",
      "iteration: 318470 loss: 0.0016 lr: 0.02\n",
      "iteration: 318480 loss: 0.0024 lr: 0.02\n",
      "iteration: 318490 loss: 0.0020 lr: 0.02\n",
      "iteration: 318500 loss: 0.0013 lr: 0.02\n",
      "iteration: 318510 loss: 0.0017 lr: 0.02\n",
      "iteration: 318520 loss: 0.0017 lr: 0.02\n",
      "iteration: 318530 loss: 0.0019 lr: 0.02\n",
      "iteration: 318540 loss: 0.0011 lr: 0.02\n",
      "iteration: 318550 loss: 0.0017 lr: 0.02\n",
      "iteration: 318560 loss: 0.0016 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 318570 loss: 0.0020 lr: 0.02\n",
      "iteration: 318580 loss: 0.0012 lr: 0.02\n",
      "iteration: 318590 loss: 0.0025 lr: 0.02\n",
      "iteration: 318600 loss: 0.0014 lr: 0.02\n",
      "iteration: 318610 loss: 0.0016 lr: 0.02\n",
      "iteration: 318620 loss: 0.0014 lr: 0.02\n",
      "iteration: 318630 loss: 0.0019 lr: 0.02\n",
      "iteration: 318640 loss: 0.0021 lr: 0.02\n",
      "iteration: 318650 loss: 0.0012 lr: 0.02\n",
      "iteration: 318660 loss: 0.0013 lr: 0.02\n",
      "iteration: 318670 loss: 0.0022 lr: 0.02\n",
      "iteration: 318680 loss: 0.0017 lr: 0.02\n",
      "iteration: 318690 loss: 0.0012 lr: 0.02\n",
      "iteration: 318700 loss: 0.0026 lr: 0.02\n",
      "iteration: 318710 loss: 0.0018 lr: 0.02\n",
      "iteration: 318720 loss: 0.0015 lr: 0.02\n",
      "iteration: 318730 loss: 0.0012 lr: 0.02\n",
      "iteration: 318740 loss: 0.0013 lr: 0.02\n",
      "iteration: 318750 loss: 0.0019 lr: 0.02\n",
      "iteration: 318760 loss: 0.0016 lr: 0.02\n",
      "iteration: 318770 loss: 0.0020 lr: 0.02\n",
      "iteration: 318780 loss: 0.0014 lr: 0.02\n",
      "iteration: 318790 loss: 0.0020 lr: 0.02\n",
      "iteration: 318800 loss: 0.0017 lr: 0.02\n",
      "iteration: 318810 loss: 0.0023 lr: 0.02\n",
      "iteration: 318820 loss: 0.0015 lr: 0.02\n",
      "iteration: 318830 loss: 0.0013 lr: 0.02\n",
      "iteration: 318840 loss: 0.0020 lr: 0.02\n",
      "iteration: 318850 loss: 0.0015 lr: 0.02\n",
      "iteration: 318860 loss: 0.0015 lr: 0.02\n",
      "iteration: 318870 loss: 0.0013 lr: 0.02\n",
      "iteration: 318880 loss: 0.0021 lr: 0.02\n",
      "iteration: 318890 loss: 0.0014 lr: 0.02\n",
      "iteration: 318900 loss: 0.0014 lr: 0.02\n",
      "iteration: 318910 loss: 0.0013 lr: 0.02\n",
      "iteration: 318920 loss: 0.0020 lr: 0.02\n",
      "iteration: 318930 loss: 0.0025 lr: 0.02\n",
      "iteration: 318940 loss: 0.0015 lr: 0.02\n",
      "iteration: 318950 loss: 0.0013 lr: 0.02\n",
      "iteration: 318960 loss: 0.0018 lr: 0.02\n",
      "iteration: 318970 loss: 0.0016 lr: 0.02\n",
      "iteration: 318980 loss: 0.0030 lr: 0.02\n",
      "iteration: 318990 loss: 0.0020 lr: 0.02\n",
      "iteration: 319000 loss: 0.0011 lr: 0.02\n",
      "iteration: 319010 loss: 0.0016 lr: 0.02\n",
      "iteration: 319020 loss: 0.0013 lr: 0.02\n",
      "iteration: 319030 loss: 0.0019 lr: 0.02\n",
      "iteration: 319040 loss: 0.0010 lr: 0.02\n",
      "iteration: 319050 loss: 0.0014 lr: 0.02\n",
      "iteration: 319060 loss: 0.0016 lr: 0.02\n",
      "iteration: 319070 loss: 0.0019 lr: 0.02\n",
      "iteration: 319080 loss: 0.0015 lr: 0.02\n",
      "iteration: 319090 loss: 0.0013 lr: 0.02\n",
      "iteration: 319100 loss: 0.0013 lr: 0.02\n",
      "iteration: 319110 loss: 0.0018 lr: 0.02\n",
      "iteration: 319120 loss: 0.0011 lr: 0.02\n",
      "iteration: 319130 loss: 0.0018 lr: 0.02\n",
      "iteration: 319140 loss: 0.0017 lr: 0.02\n",
      "iteration: 319150 loss: 0.0018 lr: 0.02\n",
      "iteration: 319160 loss: 0.0013 lr: 0.02\n",
      "iteration: 319170 loss: 0.0015 lr: 0.02\n",
      "iteration: 319180 loss: 0.0015 lr: 0.02\n",
      "iteration: 319190 loss: 0.0022 lr: 0.02\n",
      "iteration: 319200 loss: 0.0015 lr: 0.02\n",
      "iteration: 319210 loss: 0.0014 lr: 0.02\n",
      "iteration: 319220 loss: 0.0027 lr: 0.02\n",
      "iteration: 319230 loss: 0.0008 lr: 0.02\n",
      "iteration: 319240 loss: 0.0017 lr: 0.02\n",
      "iteration: 319250 loss: 0.0018 lr: 0.02\n",
      "iteration: 319260 loss: 0.0022 lr: 0.02\n",
      "iteration: 319270 loss: 0.0015 lr: 0.02\n",
      "iteration: 319280 loss: 0.0021 lr: 0.02\n",
      "iteration: 319290 loss: 0.0016 lr: 0.02\n",
      "iteration: 319300 loss: 0.0021 lr: 0.02\n",
      "iteration: 319310 loss: 0.0021 lr: 0.02\n",
      "iteration: 319320 loss: 0.0013 lr: 0.02\n",
      "iteration: 319330 loss: 0.0018 lr: 0.02\n",
      "iteration: 319340 loss: 0.0015 lr: 0.02\n",
      "iteration: 319350 loss: 0.0022 lr: 0.02\n",
      "iteration: 319360 loss: 0.0020 lr: 0.02\n",
      "iteration: 319370 loss: 0.0016 lr: 0.02\n",
      "iteration: 319380 loss: 0.0015 lr: 0.02\n",
      "iteration: 319390 loss: 0.0013 lr: 0.02\n",
      "iteration: 319400 loss: 0.0014 lr: 0.02\n",
      "iteration: 319410 loss: 0.0020 lr: 0.02\n",
      "iteration: 319420 loss: 0.0017 lr: 0.02\n",
      "iteration: 319430 loss: 0.0017 lr: 0.02\n",
      "iteration: 319440 loss: 0.0020 lr: 0.02\n",
      "iteration: 319450 loss: 0.0018 lr: 0.02\n",
      "iteration: 319460 loss: 0.0018 lr: 0.02\n",
      "iteration: 319470 loss: 0.0018 lr: 0.02\n",
      "iteration: 319480 loss: 0.0015 lr: 0.02\n",
      "iteration: 319490 loss: 0.0018 lr: 0.02\n",
      "iteration: 319500 loss: 0.0022 lr: 0.02\n",
      "iteration: 319510 loss: 0.0015 lr: 0.02\n",
      "iteration: 319520 loss: 0.0016 lr: 0.02\n",
      "iteration: 319530 loss: 0.0016 lr: 0.02\n",
      "iteration: 319540 loss: 0.0014 lr: 0.02\n",
      "iteration: 319550 loss: 0.0017 lr: 0.02\n",
      "iteration: 319560 loss: 0.0017 lr: 0.02\n",
      "iteration: 319570 loss: 0.0010 lr: 0.02\n",
      "iteration: 319580 loss: 0.0019 lr: 0.02\n",
      "iteration: 319590 loss: 0.0015 lr: 0.02\n",
      "iteration: 319600 loss: 0.0016 lr: 0.02\n",
      "iteration: 319610 loss: 0.0021 lr: 0.02\n",
      "iteration: 319620 loss: 0.0023 lr: 0.02\n",
      "iteration: 319630 loss: 0.0014 lr: 0.02\n",
      "iteration: 319640 loss: 0.0020 lr: 0.02\n",
      "iteration: 319650 loss: 0.0016 lr: 0.02\n",
      "iteration: 319660 loss: 0.0026 lr: 0.02\n",
      "iteration: 319670 loss: 0.0019 lr: 0.02\n",
      "iteration: 319680 loss: 0.0017 lr: 0.02\n",
      "iteration: 319690 loss: 0.0015 lr: 0.02\n",
      "iteration: 319700 loss: 0.0020 lr: 0.02\n",
      "iteration: 319710 loss: 0.0014 lr: 0.02\n",
      "iteration: 319720 loss: 0.0021 lr: 0.02\n",
      "iteration: 319730 loss: 0.0015 lr: 0.02\n",
      "iteration: 319740 loss: 0.0015 lr: 0.02\n",
      "iteration: 319750 loss: 0.0020 lr: 0.02\n",
      "iteration: 319760 loss: 0.0016 lr: 0.02\n",
      "iteration: 319770 loss: 0.0017 lr: 0.02\n",
      "iteration: 319780 loss: 0.0014 lr: 0.02\n",
      "iteration: 319790 loss: 0.0020 lr: 0.02\n",
      "iteration: 319800 loss: 0.0015 lr: 0.02\n",
      "iteration: 319810 loss: 0.0023 lr: 0.02\n",
      "iteration: 319820 loss: 0.0015 lr: 0.02\n",
      "iteration: 319830 loss: 0.0018 lr: 0.02\n",
      "iteration: 319840 loss: 0.0017 lr: 0.02\n",
      "iteration: 319850 loss: 0.0011 lr: 0.02\n",
      "iteration: 319860 loss: 0.0017 lr: 0.02\n",
      "iteration: 319870 loss: 0.0019 lr: 0.02\n",
      "iteration: 319880 loss: 0.0013 lr: 0.02\n",
      "iteration: 319890 loss: 0.0017 lr: 0.02\n",
      "iteration: 319900 loss: 0.0019 lr: 0.02\n",
      "iteration: 319910 loss: 0.0014 lr: 0.02\n",
      "iteration: 319920 loss: 0.0014 lr: 0.02\n",
      "iteration: 319930 loss: 0.0019 lr: 0.02\n",
      "iteration: 319940 loss: 0.0021 lr: 0.02\n",
      "iteration: 319950 loss: 0.0014 lr: 0.02\n",
      "iteration: 319960 loss: 0.0013 lr: 0.02\n",
      "iteration: 319970 loss: 0.0017 lr: 0.02\n",
      "iteration: 319980 loss: 0.0026 lr: 0.02\n",
      "iteration: 319990 loss: 0.0019 lr: 0.02\n",
      "iteration: 320000 loss: 0.0024 lr: 0.02\n",
      "iteration: 320010 loss: 0.0017 lr: 0.02\n",
      "iteration: 320020 loss: 0.0018 lr: 0.02\n",
      "iteration: 320030 loss: 0.0019 lr: 0.02\n",
      "iteration: 320040 loss: 0.0018 lr: 0.02\n",
      "iteration: 320050 loss: 0.0018 lr: 0.02\n",
      "iteration: 320060 loss: 0.0013 lr: 0.02\n",
      "iteration: 320070 loss: 0.0013 lr: 0.02\n",
      "iteration: 320080 loss: 0.0015 lr: 0.02\n",
      "iteration: 320090 loss: 0.0011 lr: 0.02\n",
      "iteration: 320100 loss: 0.0024 lr: 0.02\n",
      "iteration: 320110 loss: 0.0016 lr: 0.02\n",
      "iteration: 320120 loss: 0.0037 lr: 0.02\n",
      "iteration: 320130 loss: 0.0026 lr: 0.02\n",
      "iteration: 320140 loss: 0.0019 lr: 0.02\n",
      "iteration: 320150 loss: 0.0015 lr: 0.02\n",
      "iteration: 320160 loss: 0.0019 lr: 0.02\n",
      "iteration: 320170 loss: 0.0024 lr: 0.02\n",
      "iteration: 320180 loss: 0.0018 lr: 0.02\n",
      "iteration: 320190 loss: 0.0016 lr: 0.02\n",
      "iteration: 320200 loss: 0.0034 lr: 0.02\n",
      "iteration: 320210 loss: 0.0019 lr: 0.02\n",
      "iteration: 320220 loss: 0.0020 lr: 0.02\n",
      "iteration: 320230 loss: 0.0017 lr: 0.02\n",
      "iteration: 320240 loss: 0.0018 lr: 0.02\n",
      "iteration: 320250 loss: 0.0016 lr: 0.02\n",
      "iteration: 320260 loss: 0.0019 lr: 0.02\n",
      "iteration: 320270 loss: 0.0017 lr: 0.02\n",
      "iteration: 320280 loss: 0.0016 lr: 0.02\n",
      "iteration: 320290 loss: 0.0014 lr: 0.02\n",
      "iteration: 320300 loss: 0.0019 lr: 0.02\n",
      "iteration: 320310 loss: 0.0011 lr: 0.02\n",
      "iteration: 320320 loss: 0.0016 lr: 0.02\n",
      "iteration: 320330 loss: 0.0021 lr: 0.02\n",
      "iteration: 320340 loss: 0.0024 lr: 0.02\n",
      "iteration: 320350 loss: 0.0016 lr: 0.02\n",
      "iteration: 320360 loss: 0.0018 lr: 0.02\n",
      "iteration: 320370 loss: 0.0014 lr: 0.02\n",
      "iteration: 320380 loss: 0.0015 lr: 0.02\n",
      "iteration: 320390 loss: 0.0013 lr: 0.02\n",
      "iteration: 320400 loss: 0.0022 lr: 0.02\n",
      "iteration: 320410 loss: 0.0018 lr: 0.02\n",
      "iteration: 320420 loss: 0.0011 lr: 0.02\n",
      "iteration: 320430 loss: 0.0015 lr: 0.02\n",
      "iteration: 320440 loss: 0.0014 lr: 0.02\n",
      "iteration: 320450 loss: 0.0025 lr: 0.02\n",
      "iteration: 320460 loss: 0.0015 lr: 0.02\n",
      "iteration: 320470 loss: 0.0014 lr: 0.02\n",
      "iteration: 320480 loss: 0.0016 lr: 0.02\n",
      "iteration: 320490 loss: 0.0015 lr: 0.02\n",
      "iteration: 320500 loss: 0.0014 lr: 0.02\n",
      "iteration: 320510 loss: 0.0012 lr: 0.02\n",
      "iteration: 320520 loss: 0.0017 lr: 0.02\n",
      "iteration: 320530 loss: 0.0015 lr: 0.02\n",
      "iteration: 320540 loss: 0.0017 lr: 0.02\n",
      "iteration: 320550 loss: 0.0018 lr: 0.02\n",
      "iteration: 320560 loss: 0.0021 lr: 0.02\n",
      "iteration: 320570 loss: 0.0015 lr: 0.02\n",
      "iteration: 320580 loss: 0.0014 lr: 0.02\n",
      "iteration: 320590 loss: 0.0018 lr: 0.02\n",
      "iteration: 320600 loss: 0.0016 lr: 0.02\n",
      "iteration: 320610 loss: 0.0012 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 320620 loss: 0.0013 lr: 0.02\n",
      "iteration: 320630 loss: 0.0019 lr: 0.02\n",
      "iteration: 320640 loss: 0.0014 lr: 0.02\n",
      "iteration: 320650 loss: 0.0013 lr: 0.02\n",
      "iteration: 320660 loss: 0.0018 lr: 0.02\n",
      "iteration: 320670 loss: 0.0018 lr: 0.02\n",
      "iteration: 320680 loss: 0.0023 lr: 0.02\n",
      "iteration: 320690 loss: 0.0015 lr: 0.02\n",
      "iteration: 320700 loss: 0.0016 lr: 0.02\n",
      "iteration: 320710 loss: 0.0030 lr: 0.02\n",
      "iteration: 320720 loss: 0.0020 lr: 0.02\n",
      "iteration: 320730 loss: 0.0017 lr: 0.02\n",
      "iteration: 320740 loss: 0.0015 lr: 0.02\n",
      "iteration: 320750 loss: 0.0021 lr: 0.02\n",
      "iteration: 320760 loss: 0.0016 lr: 0.02\n",
      "iteration: 320770 loss: 0.0015 lr: 0.02\n",
      "iteration: 320780 loss: 0.0016 lr: 0.02\n",
      "iteration: 320790 loss: 0.0017 lr: 0.02\n",
      "iteration: 320800 loss: 0.0013 lr: 0.02\n",
      "iteration: 320810 loss: 0.0018 lr: 0.02\n",
      "iteration: 320820 loss: 0.0022 lr: 0.02\n",
      "iteration: 320830 loss: 0.0016 lr: 0.02\n",
      "iteration: 320840 loss: 0.0018 lr: 0.02\n",
      "iteration: 320850 loss: 0.0018 lr: 0.02\n",
      "iteration: 320860 loss: 0.0015 lr: 0.02\n",
      "iteration: 320870 loss: 0.0012 lr: 0.02\n",
      "iteration: 320880 loss: 0.0014 lr: 0.02\n",
      "iteration: 320890 loss: 0.0014 lr: 0.02\n",
      "iteration: 320900 loss: 0.0027 lr: 0.02\n",
      "iteration: 320910 loss: 0.0014 lr: 0.02\n",
      "iteration: 320920 loss: 0.0016 lr: 0.02\n",
      "iteration: 320930 loss: 0.0016 lr: 0.02\n",
      "iteration: 320940 loss: 0.0019 lr: 0.02\n",
      "iteration: 320950 loss: 0.0016 lr: 0.02\n",
      "iteration: 320960 loss: 0.0015 lr: 0.02\n",
      "iteration: 320970 loss: 0.0018 lr: 0.02\n",
      "iteration: 320980 loss: 0.0015 lr: 0.02\n",
      "iteration: 320990 loss: 0.0013 lr: 0.02\n",
      "iteration: 321000 loss: 0.0023 lr: 0.02\n",
      "iteration: 321010 loss: 0.0018 lr: 0.02\n",
      "iteration: 321020 loss: 0.0017 lr: 0.02\n",
      "iteration: 321030 loss: 0.0015 lr: 0.02\n",
      "iteration: 321040 loss: 0.0015 lr: 0.02\n",
      "iteration: 321050 loss: 0.0018 lr: 0.02\n",
      "iteration: 321060 loss: 0.0017 lr: 0.02\n",
      "iteration: 321070 loss: 0.0016 lr: 0.02\n",
      "iteration: 321080 loss: 0.0010 lr: 0.02\n",
      "iteration: 321090 loss: 0.0017 lr: 0.02\n",
      "iteration: 321100 loss: 0.0013 lr: 0.02\n",
      "iteration: 321110 loss: 0.0012 lr: 0.02\n",
      "iteration: 321120 loss: 0.0015 lr: 0.02\n",
      "iteration: 321130 loss: 0.0015 lr: 0.02\n",
      "iteration: 321140 loss: 0.0027 lr: 0.02\n",
      "iteration: 321150 loss: 0.0014 lr: 0.02\n",
      "iteration: 321160 loss: 0.0011 lr: 0.02\n",
      "iteration: 321170 loss: 0.0021 lr: 0.02\n",
      "iteration: 321180 loss: 0.0013 lr: 0.02\n",
      "iteration: 321190 loss: 0.0024 lr: 0.02\n",
      "iteration: 321200 loss: 0.0012 lr: 0.02\n",
      "iteration: 321210 loss: 0.0017 lr: 0.02\n",
      "iteration: 321220 loss: 0.0021 lr: 0.02\n",
      "iteration: 321230 loss: 0.0014 lr: 0.02\n",
      "iteration: 321240 loss: 0.0017 lr: 0.02\n",
      "iteration: 321250 loss: 0.0013 lr: 0.02\n",
      "iteration: 321260 loss: 0.0018 lr: 0.02\n",
      "iteration: 321270 loss: 0.0014 lr: 0.02\n",
      "iteration: 321280 loss: 0.0016 lr: 0.02\n",
      "iteration: 321290 loss: 0.0014 lr: 0.02\n",
      "iteration: 321300 loss: 0.0024 lr: 0.02\n",
      "iteration: 321310 loss: 0.0019 lr: 0.02\n",
      "iteration: 321320 loss: 0.0017 lr: 0.02\n",
      "iteration: 321330 loss: 0.0027 lr: 0.02\n",
      "iteration: 321340 loss: 0.0015 lr: 0.02\n",
      "iteration: 321350 loss: 0.0014 lr: 0.02\n",
      "iteration: 321360 loss: 0.0014 lr: 0.02\n",
      "iteration: 321370 loss: 0.0015 lr: 0.02\n",
      "iteration: 321380 loss: 0.0023 lr: 0.02\n",
      "iteration: 321390 loss: 0.0011 lr: 0.02\n",
      "iteration: 321400 loss: 0.0016 lr: 0.02\n",
      "iteration: 321410 loss: 0.0012 lr: 0.02\n",
      "iteration: 321420 loss: 0.0022 lr: 0.02\n",
      "iteration: 321430 loss: 0.0021 lr: 0.02\n",
      "iteration: 321440 loss: 0.0019 lr: 0.02\n",
      "iteration: 321450 loss: 0.0024 lr: 0.02\n",
      "iteration: 321460 loss: 0.0016 lr: 0.02\n",
      "iteration: 321470 loss: 0.0020 lr: 0.02\n",
      "iteration: 321480 loss: 0.0018 lr: 0.02\n",
      "iteration: 321490 loss: 0.0020 lr: 0.02\n",
      "iteration: 321500 loss: 0.0021 lr: 0.02\n",
      "iteration: 321510 loss: 0.0018 lr: 0.02\n",
      "iteration: 321520 loss: 0.0021 lr: 0.02\n",
      "iteration: 321530 loss: 0.0016 lr: 0.02\n",
      "iteration: 321540 loss: 0.0017 lr: 0.02\n",
      "iteration: 321550 loss: 0.0016 lr: 0.02\n",
      "iteration: 321560 loss: 0.0013 lr: 0.02\n",
      "iteration: 321570 loss: 0.0018 lr: 0.02\n",
      "iteration: 321580 loss: 0.0022 lr: 0.02\n",
      "iteration: 321590 loss: 0.0015 lr: 0.02\n",
      "iteration: 321600 loss: 0.0022 lr: 0.02\n",
      "iteration: 321610 loss: 0.0014 lr: 0.02\n",
      "iteration: 321620 loss: 0.0021 lr: 0.02\n",
      "iteration: 321630 loss: 0.0016 lr: 0.02\n",
      "iteration: 321640 loss: 0.0018 lr: 0.02\n",
      "iteration: 321650 loss: 0.0015 lr: 0.02\n",
      "iteration: 321660 loss: 0.0019 lr: 0.02\n",
      "iteration: 321670 loss: 0.0017 lr: 0.02\n",
      "iteration: 321680 loss: 0.0022 lr: 0.02\n",
      "iteration: 321690 loss: 0.0030 lr: 0.02\n",
      "iteration: 321700 loss: 0.0019 lr: 0.02\n",
      "iteration: 321710 loss: 0.0015 lr: 0.02\n",
      "iteration: 321720 loss: 0.0016 lr: 0.02\n",
      "iteration: 321730 loss: 0.0021 lr: 0.02\n",
      "iteration: 321740 loss: 0.0015 lr: 0.02\n",
      "iteration: 321750 loss: 0.0019 lr: 0.02\n",
      "iteration: 321760 loss: 0.0016 lr: 0.02\n",
      "iteration: 321770 loss: 0.0017 lr: 0.02\n",
      "iteration: 321780 loss: 0.0015 lr: 0.02\n",
      "iteration: 321790 loss: 0.0017 lr: 0.02\n",
      "iteration: 321800 loss: 0.0019 lr: 0.02\n",
      "iteration: 321810 loss: 0.0016 lr: 0.02\n",
      "iteration: 321820 loss: 0.0017 lr: 0.02\n",
      "iteration: 321830 loss: 0.0019 lr: 0.02\n",
      "iteration: 321840 loss: 0.0012 lr: 0.02\n",
      "iteration: 321850 loss: 0.0014 lr: 0.02\n",
      "iteration: 321860 loss: 0.0015 lr: 0.02\n",
      "iteration: 321870 loss: 0.0013 lr: 0.02\n",
      "iteration: 321880 loss: 0.0015 lr: 0.02\n",
      "iteration: 321890 loss: 0.0017 lr: 0.02\n",
      "iteration: 321900 loss: 0.0015 lr: 0.02\n",
      "iteration: 321910 loss: 0.0020 lr: 0.02\n",
      "iteration: 321920 loss: 0.0018 lr: 0.02\n",
      "iteration: 321930 loss: 0.0017 lr: 0.02\n",
      "iteration: 321940 loss: 0.0024 lr: 0.02\n",
      "iteration: 321950 loss: 0.0017 lr: 0.02\n",
      "iteration: 321960 loss: 0.0017 lr: 0.02\n",
      "iteration: 321970 loss: 0.0022 lr: 0.02\n",
      "iteration: 321980 loss: 0.0023 lr: 0.02\n",
      "iteration: 321990 loss: 0.0019 lr: 0.02\n",
      "iteration: 322000 loss: 0.0021 lr: 0.02\n",
      "iteration: 322010 loss: 0.0012 lr: 0.02\n",
      "iteration: 322020 loss: 0.0018 lr: 0.02\n",
      "iteration: 322030 loss: 0.0016 lr: 0.02\n",
      "iteration: 322040 loss: 0.0016 lr: 0.02\n",
      "iteration: 322050 loss: 0.0018 lr: 0.02\n",
      "iteration: 322060 loss: 0.0016 lr: 0.02\n",
      "iteration: 322070 loss: 0.0017 lr: 0.02\n",
      "iteration: 322080 loss: 0.0015 lr: 0.02\n",
      "iteration: 322090 loss: 0.0013 lr: 0.02\n",
      "iteration: 322100 loss: 0.0017 lr: 0.02\n",
      "iteration: 322110 loss: 0.0018 lr: 0.02\n",
      "iteration: 322120 loss: 0.0016 lr: 0.02\n",
      "iteration: 322130 loss: 0.0017 lr: 0.02\n",
      "iteration: 322140 loss: 0.0017 lr: 0.02\n",
      "iteration: 322150 loss: 0.0019 lr: 0.02\n",
      "iteration: 322160 loss: 0.0016 lr: 0.02\n",
      "iteration: 322170 loss: 0.0021 lr: 0.02\n",
      "iteration: 322180 loss: 0.0013 lr: 0.02\n",
      "iteration: 322190 loss: 0.0018 lr: 0.02\n",
      "iteration: 322200 loss: 0.0012 lr: 0.02\n",
      "iteration: 322210 loss: 0.0017 lr: 0.02\n",
      "iteration: 322220 loss: 0.0013 lr: 0.02\n",
      "iteration: 322230 loss: 0.0016 lr: 0.02\n",
      "iteration: 322240 loss: 0.0018 lr: 0.02\n",
      "iteration: 322250 loss: 0.0010 lr: 0.02\n",
      "iteration: 322260 loss: 0.0015 lr: 0.02\n",
      "iteration: 322270 loss: 0.0015 lr: 0.02\n",
      "iteration: 322280 loss: 0.0012 lr: 0.02\n",
      "iteration: 322290 loss: 0.0009 lr: 0.02\n",
      "iteration: 322300 loss: 0.0019 lr: 0.02\n",
      "iteration: 322310 loss: 0.0019 lr: 0.02\n",
      "iteration: 322320 loss: 0.0014 lr: 0.02\n",
      "iteration: 322330 loss: 0.0018 lr: 0.02\n",
      "iteration: 322340 loss: 0.0021 lr: 0.02\n",
      "iteration: 322350 loss: 0.0018 lr: 0.02\n",
      "iteration: 322360 loss: 0.0012 lr: 0.02\n",
      "iteration: 322370 loss: 0.0012 lr: 0.02\n",
      "iteration: 322380 loss: 0.0019 lr: 0.02\n",
      "iteration: 322390 loss: 0.0012 lr: 0.02\n",
      "iteration: 322400 loss: 0.0020 lr: 0.02\n",
      "iteration: 322410 loss: 0.0015 lr: 0.02\n",
      "iteration: 322420 loss: 0.0019 lr: 0.02\n",
      "iteration: 322430 loss: 0.0013 lr: 0.02\n",
      "iteration: 322440 loss: 0.0016 lr: 0.02\n",
      "iteration: 322450 loss: 0.0013 lr: 0.02\n",
      "iteration: 322460 loss: 0.0013 lr: 0.02\n",
      "iteration: 322470 loss: 0.0016 lr: 0.02\n",
      "iteration: 322480 loss: 0.0015 lr: 0.02\n",
      "iteration: 322490 loss: 0.0018 lr: 0.02\n",
      "iteration: 322500 loss: 0.0017 lr: 0.02\n",
      "iteration: 322510 loss: 0.0022 lr: 0.02\n",
      "iteration: 322520 loss: 0.0017 lr: 0.02\n",
      "iteration: 322530 loss: 0.0018 lr: 0.02\n",
      "iteration: 322540 loss: 0.0025 lr: 0.02\n",
      "iteration: 322550 loss: 0.0021 lr: 0.02\n",
      "iteration: 322560 loss: 0.0014 lr: 0.02\n",
      "iteration: 322570 loss: 0.0011 lr: 0.02\n",
      "iteration: 322580 loss: 0.0018 lr: 0.02\n",
      "iteration: 322590 loss: 0.0012 lr: 0.02\n",
      "iteration: 322600 loss: 0.0017 lr: 0.02\n",
      "iteration: 322610 loss: 0.0024 lr: 0.02\n",
      "iteration: 322620 loss: 0.0017 lr: 0.02\n",
      "iteration: 322630 loss: 0.0018 lr: 0.02\n",
      "iteration: 322640 loss: 0.0027 lr: 0.02\n",
      "iteration: 322650 loss: 0.0021 lr: 0.02\n",
      "iteration: 322660 loss: 0.0014 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 322670 loss: 0.0012 lr: 0.02\n",
      "iteration: 322680 loss: 0.0019 lr: 0.02\n",
      "iteration: 322690 loss: 0.0013 lr: 0.02\n",
      "iteration: 322700 loss: 0.0015 lr: 0.02\n",
      "iteration: 322710 loss: 0.0017 lr: 0.02\n",
      "iteration: 322720 loss: 0.0019 lr: 0.02\n",
      "iteration: 322730 loss: 0.0037 lr: 0.02\n",
      "iteration: 322740 loss: 0.0025 lr: 0.02\n",
      "iteration: 322750 loss: 0.0024 lr: 0.02\n",
      "iteration: 322760 loss: 0.0017 lr: 0.02\n",
      "iteration: 322770 loss: 0.0017 lr: 0.02\n",
      "iteration: 322780 loss: 0.0018 lr: 0.02\n",
      "iteration: 322790 loss: 0.0021 lr: 0.02\n",
      "iteration: 322800 loss: 0.0021 lr: 0.02\n",
      "iteration: 322810 loss: 0.0013 lr: 0.02\n",
      "iteration: 322820 loss: 0.0019 lr: 0.02\n",
      "iteration: 322830 loss: 0.0014 lr: 0.02\n",
      "iteration: 322840 loss: 0.0025 lr: 0.02\n",
      "iteration: 322850 loss: 0.0020 lr: 0.02\n",
      "iteration: 322860 loss: 0.0013 lr: 0.02\n",
      "iteration: 322870 loss: 0.0013 lr: 0.02\n",
      "iteration: 322880 loss: 0.0015 lr: 0.02\n",
      "iteration: 322890 loss: 0.0025 lr: 0.02\n",
      "iteration: 322900 loss: 0.0026 lr: 0.02\n",
      "iteration: 322910 loss: 0.0021 lr: 0.02\n",
      "iteration: 322920 loss: 0.0016 lr: 0.02\n",
      "iteration: 322930 loss: 0.0013 lr: 0.02\n",
      "iteration: 322940 loss: 0.0018 lr: 0.02\n",
      "iteration: 322950 loss: 0.0017 lr: 0.02\n",
      "iteration: 322960 loss: 0.0015 lr: 0.02\n",
      "iteration: 322970 loss: 0.0015 lr: 0.02\n",
      "iteration: 322980 loss: 0.0016 lr: 0.02\n",
      "iteration: 322990 loss: 0.0017 lr: 0.02\n",
      "iteration: 323000 loss: 0.0021 lr: 0.02\n",
      "iteration: 323010 loss: 0.0011 lr: 0.02\n",
      "iteration: 323020 loss: 0.0022 lr: 0.02\n",
      "iteration: 323030 loss: 0.0013 lr: 0.02\n",
      "iteration: 323040 loss: 0.0024 lr: 0.02\n",
      "iteration: 323050 loss: 0.0015 lr: 0.02\n",
      "iteration: 323060 loss: 0.0018 lr: 0.02\n",
      "iteration: 323070 loss: 0.0012 lr: 0.02\n",
      "iteration: 323080 loss: 0.0021 lr: 0.02\n",
      "iteration: 323090 loss: 0.0050 lr: 0.02\n",
      "iteration: 323100 loss: 0.0020 lr: 0.02\n",
      "iteration: 323110 loss: 0.0017 lr: 0.02\n",
      "iteration: 323120 loss: 0.0023 lr: 0.02\n",
      "iteration: 323130 loss: 0.0023 lr: 0.02\n",
      "iteration: 323140 loss: 0.0020 lr: 0.02\n",
      "iteration: 323150 loss: 0.0017 lr: 0.02\n",
      "iteration: 323160 loss: 0.0023 lr: 0.02\n",
      "iteration: 323170 loss: 0.0021 lr: 0.02\n",
      "iteration: 323180 loss: 0.0017 lr: 0.02\n",
      "iteration: 323190 loss: 0.0017 lr: 0.02\n",
      "iteration: 323200 loss: 0.0017 lr: 0.02\n",
      "iteration: 323210 loss: 0.0021 lr: 0.02\n",
      "iteration: 323220 loss: 0.0019 lr: 0.02\n",
      "iteration: 323230 loss: 0.0014 lr: 0.02\n",
      "iteration: 323240 loss: 0.0013 lr: 0.02\n",
      "iteration: 323250 loss: 0.0021 lr: 0.02\n",
      "iteration: 323260 loss: 0.0014 lr: 0.02\n",
      "iteration: 323270 loss: 0.0011 lr: 0.02\n",
      "iteration: 323280 loss: 0.0020 lr: 0.02\n",
      "iteration: 323290 loss: 0.0015 lr: 0.02\n",
      "iteration: 323300 loss: 0.0011 lr: 0.02\n",
      "iteration: 323310 loss: 0.0014 lr: 0.02\n",
      "iteration: 323320 loss: 0.0019 lr: 0.02\n",
      "iteration: 323330 loss: 0.0019 lr: 0.02\n",
      "iteration: 323340 loss: 0.0012 lr: 0.02\n",
      "iteration: 323350 loss: 0.0020 lr: 0.02\n",
      "iteration: 323360 loss: 0.0014 lr: 0.02\n",
      "iteration: 323370 loss: 0.0016 lr: 0.02\n",
      "iteration: 323380 loss: 0.0015 lr: 0.02\n",
      "iteration: 323390 loss: 0.0020 lr: 0.02\n",
      "iteration: 323400 loss: 0.0016 lr: 0.02\n",
      "iteration: 323410 loss: 0.0016 lr: 0.02\n",
      "iteration: 323420 loss: 0.0018 lr: 0.02\n",
      "iteration: 323430 loss: 0.0014 lr: 0.02\n",
      "iteration: 323440 loss: 0.0012 lr: 0.02\n",
      "iteration: 323450 loss: 0.0021 lr: 0.02\n",
      "iteration: 323460 loss: 0.0024 lr: 0.02\n",
      "iteration: 323470 loss: 0.0015 lr: 0.02\n",
      "iteration: 323480 loss: 0.0018 lr: 0.02\n",
      "iteration: 323490 loss: 0.0019 lr: 0.02\n",
      "iteration: 323500 loss: 0.0016 lr: 0.02\n",
      "iteration: 323510 loss: 0.0014 lr: 0.02\n",
      "iteration: 323520 loss: 0.0015 lr: 0.02\n",
      "iteration: 323530 loss: 0.0022 lr: 0.02\n",
      "iteration: 323540 loss: 0.0019 lr: 0.02\n",
      "iteration: 323550 loss: 0.0021 lr: 0.02\n",
      "iteration: 323560 loss: 0.0013 lr: 0.02\n",
      "iteration: 323570 loss: 0.0019 lr: 0.02\n",
      "iteration: 323580 loss: 0.0020 lr: 0.02\n",
      "iteration: 323590 loss: 0.0027 lr: 0.02\n",
      "iteration: 323600 loss: 0.0021 lr: 0.02\n",
      "iteration: 323610 loss: 0.0014 lr: 0.02\n",
      "iteration: 323620 loss: 0.0017 lr: 0.02\n",
      "iteration: 323630 loss: 0.0013 lr: 0.02\n",
      "iteration: 323640 loss: 0.0015 lr: 0.02\n",
      "iteration: 323650 loss: 0.0015 lr: 0.02\n",
      "iteration: 323660 loss: 0.0021 lr: 0.02\n",
      "iteration: 323670 loss: 0.0020 lr: 0.02\n",
      "iteration: 323680 loss: 0.0024 lr: 0.02\n",
      "iteration: 323690 loss: 0.0018 lr: 0.02\n",
      "iteration: 323700 loss: 0.0015 lr: 0.02\n",
      "iteration: 323710 loss: 0.0016 lr: 0.02\n",
      "iteration: 323720 loss: 0.0021 lr: 0.02\n",
      "iteration: 323730 loss: 0.0018 lr: 0.02\n",
      "iteration: 323740 loss: 0.0019 lr: 0.02\n",
      "iteration: 323750 loss: 0.0020 lr: 0.02\n",
      "iteration: 323760 loss: 0.0014 lr: 0.02\n",
      "iteration: 323770 loss: 0.0014 lr: 0.02\n",
      "iteration: 323780 loss: 0.0012 lr: 0.02\n",
      "iteration: 323790 loss: 0.0017 lr: 0.02\n",
      "iteration: 323800 loss: 0.0022 lr: 0.02\n",
      "iteration: 323810 loss: 0.0022 lr: 0.02\n",
      "iteration: 323820 loss: 0.0016 lr: 0.02\n",
      "iteration: 323830 loss: 0.0017 lr: 0.02\n",
      "iteration: 323840 loss: 0.0014 lr: 0.02\n",
      "iteration: 323850 loss: 0.0014 lr: 0.02\n",
      "iteration: 323860 loss: 0.0016 lr: 0.02\n",
      "iteration: 323870 loss: 0.0035 lr: 0.02\n",
      "iteration: 323880 loss: 0.0018 lr: 0.02\n",
      "iteration: 323890 loss: 0.0013 lr: 0.02\n",
      "iteration: 323900 loss: 0.0022 lr: 0.02\n",
      "iteration: 323910 loss: 0.0016 lr: 0.02\n",
      "iteration: 323920 loss: 0.0033 lr: 0.02\n",
      "iteration: 323930 loss: 0.0016 lr: 0.02\n",
      "iteration: 323940 loss: 0.0017 lr: 0.02\n",
      "iteration: 323950 loss: 0.0016 lr: 0.02\n",
      "iteration: 323960 loss: 0.0016 lr: 0.02\n",
      "iteration: 323970 loss: 0.0021 lr: 0.02\n",
      "iteration: 323980 loss: 0.0023 lr: 0.02\n",
      "iteration: 323990 loss: 0.0018 lr: 0.02\n",
      "iteration: 324000 loss: 0.0015 lr: 0.02\n",
      "iteration: 324010 loss: 0.0014 lr: 0.02\n",
      "iteration: 324020 loss: 0.0013 lr: 0.02\n",
      "iteration: 324030 loss: 0.0014 lr: 0.02\n",
      "iteration: 324040 loss: 0.0019 lr: 0.02\n",
      "iteration: 324050 loss: 0.0016 lr: 0.02\n",
      "iteration: 324060 loss: 0.0015 lr: 0.02\n",
      "iteration: 324070 loss: 0.0014 lr: 0.02\n",
      "iteration: 324080 loss: 0.0014 lr: 0.02\n",
      "iteration: 324090 loss: 0.0018 lr: 0.02\n",
      "iteration: 324100 loss: 0.0019 lr: 0.02\n",
      "iteration: 324110 loss: 0.0012 lr: 0.02\n",
      "iteration: 324120 loss: 0.0017 lr: 0.02\n",
      "iteration: 324130 loss: 0.0017 lr: 0.02\n",
      "iteration: 324140 loss: 0.0022 lr: 0.02\n",
      "iteration: 324150 loss: 0.0018 lr: 0.02\n",
      "iteration: 324160 loss: 0.0016 lr: 0.02\n",
      "iteration: 324170 loss: 0.0016 lr: 0.02\n",
      "iteration: 324180 loss: 0.0014 lr: 0.02\n",
      "iteration: 324190 loss: 0.0018 lr: 0.02\n",
      "iteration: 324200 loss: 0.0022 lr: 0.02\n",
      "iteration: 324210 loss: 0.0018 lr: 0.02\n",
      "iteration: 324220 loss: 0.0021 lr: 0.02\n",
      "iteration: 324230 loss: 0.0018 lr: 0.02\n",
      "iteration: 324240 loss: 0.0016 lr: 0.02\n",
      "iteration: 324250 loss: 0.0017 lr: 0.02\n",
      "iteration: 324260 loss: 0.0017 lr: 0.02\n",
      "iteration: 324270 loss: 0.0025 lr: 0.02\n",
      "iteration: 324280 loss: 0.0020 lr: 0.02\n",
      "iteration: 324290 loss: 0.0018 lr: 0.02\n",
      "iteration: 324300 loss: 0.0018 lr: 0.02\n",
      "iteration: 324310 loss: 0.0020 lr: 0.02\n",
      "iteration: 324320 loss: 0.0021 lr: 0.02\n",
      "iteration: 324330 loss: 0.0021 lr: 0.02\n",
      "iteration: 324340 loss: 0.0021 lr: 0.02\n",
      "iteration: 324350 loss: 0.0017 lr: 0.02\n",
      "iteration: 324360 loss: 0.0021 lr: 0.02\n",
      "iteration: 324370 loss: 0.0015 lr: 0.02\n",
      "iteration: 324380 loss: 0.0020 lr: 0.02\n",
      "iteration: 324390 loss: 0.0017 lr: 0.02\n",
      "iteration: 324400 loss: 0.0016 lr: 0.02\n",
      "iteration: 324410 loss: 0.0020 lr: 0.02\n",
      "iteration: 324420 loss: 0.0021 lr: 0.02\n",
      "iteration: 324430 loss: 0.0020 lr: 0.02\n",
      "iteration: 324440 loss: 0.0013 lr: 0.02\n",
      "iteration: 324450 loss: 0.0012 lr: 0.02\n",
      "iteration: 324460 loss: 0.0016 lr: 0.02\n",
      "iteration: 324470 loss: 0.0016 lr: 0.02\n",
      "iteration: 324480 loss: 0.0014 lr: 0.02\n",
      "iteration: 324490 loss: 0.0023 lr: 0.02\n",
      "iteration: 324500 loss: 0.0011 lr: 0.02\n",
      "iteration: 324510 loss: 0.0018 lr: 0.02\n",
      "iteration: 324520 loss: 0.0024 lr: 0.02\n",
      "iteration: 324530 loss: 0.0016 lr: 0.02\n",
      "iteration: 324540 loss: 0.0015 lr: 0.02\n",
      "iteration: 324550 loss: 0.0013 lr: 0.02\n",
      "iteration: 324560 loss: 0.0010 lr: 0.02\n",
      "iteration: 324570 loss: 0.0015 lr: 0.02\n",
      "iteration: 324580 loss: 0.0013 lr: 0.02\n",
      "iteration: 324590 loss: 0.0016 lr: 0.02\n",
      "iteration: 324600 loss: 0.0014 lr: 0.02\n",
      "iteration: 324610 loss: 0.0018 lr: 0.02\n",
      "iteration: 324620 loss: 0.0027 lr: 0.02\n",
      "iteration: 324630 loss: 0.0023 lr: 0.02\n",
      "iteration: 324640 loss: 0.0022 lr: 0.02\n",
      "iteration: 324650 loss: 0.0015 lr: 0.02\n",
      "iteration: 324660 loss: 0.0020 lr: 0.02\n",
      "iteration: 324670 loss: 0.0020 lr: 0.02\n",
      "iteration: 324680 loss: 0.0011 lr: 0.02\n",
      "iteration: 324690 loss: 0.0013 lr: 0.02\n",
      "iteration: 324700 loss: 0.0026 lr: 0.02\n",
      "iteration: 324710 loss: 0.0018 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 324720 loss: 0.0014 lr: 0.02\n",
      "iteration: 324730 loss: 0.0025 lr: 0.02\n",
      "iteration: 324740 loss: 0.0022 lr: 0.02\n",
      "iteration: 324750 loss: 0.0015 lr: 0.02\n",
      "iteration: 324760 loss: 0.0013 lr: 0.02\n",
      "iteration: 324770 loss: 0.0042 lr: 0.02\n",
      "iteration: 324780 loss: 0.0017 lr: 0.02\n",
      "iteration: 324790 loss: 0.0017 lr: 0.02\n",
      "iteration: 324800 loss: 0.0017 lr: 0.02\n",
      "iteration: 324810 loss: 0.0022 lr: 0.02\n",
      "iteration: 324820 loss: 0.0018 lr: 0.02\n",
      "iteration: 324830 loss: 0.0016 lr: 0.02\n",
      "iteration: 324840 loss: 0.0026 lr: 0.02\n",
      "iteration: 324850 loss: 0.0019 lr: 0.02\n",
      "iteration: 324860 loss: 0.0014 lr: 0.02\n",
      "iteration: 324870 loss: 0.0012 lr: 0.02\n",
      "iteration: 324880 loss: 0.0015 lr: 0.02\n",
      "iteration: 324890 loss: 0.0019 lr: 0.02\n",
      "iteration: 324900 loss: 0.0016 lr: 0.02\n",
      "iteration: 324910 loss: 0.0015 lr: 0.02\n",
      "iteration: 324920 loss: 0.0014 lr: 0.02\n",
      "iteration: 324930 loss: 0.0016 lr: 0.02\n",
      "iteration: 324940 loss: 0.0022 lr: 0.02\n",
      "iteration: 324950 loss: 0.0016 lr: 0.02\n",
      "iteration: 324960 loss: 0.0013 lr: 0.02\n",
      "iteration: 324970 loss: 0.0012 lr: 0.02\n",
      "iteration: 324980 loss: 0.0021 lr: 0.02\n",
      "iteration: 324990 loss: 0.0014 lr: 0.02\n",
      "iteration: 325000 loss: 0.0012 lr: 0.02\n",
      "iteration: 325010 loss: 0.0015 lr: 0.02\n",
      "iteration: 325020 loss: 0.0013 lr: 0.02\n",
      "iteration: 325030 loss: 0.0015 lr: 0.02\n",
      "iteration: 325040 loss: 0.0024 lr: 0.02\n",
      "iteration: 325050 loss: 0.0019 lr: 0.02\n",
      "iteration: 325060 loss: 0.0015 lr: 0.02\n",
      "iteration: 325070 loss: 0.0013 lr: 0.02\n",
      "iteration: 325080 loss: 0.0022 lr: 0.02\n",
      "iteration: 325090 loss: 0.0016 lr: 0.02\n",
      "iteration: 325100 loss: 0.0014 lr: 0.02\n",
      "iteration: 325110 loss: 0.0022 lr: 0.02\n",
      "iteration: 325120 loss: 0.0015 lr: 0.02\n",
      "iteration: 325130 loss: 0.0019 lr: 0.02\n",
      "iteration: 325140 loss: 0.0020 lr: 0.02\n",
      "iteration: 325150 loss: 0.0015 lr: 0.02\n",
      "iteration: 325160 loss: 0.0021 lr: 0.02\n",
      "iteration: 325170 loss: 0.0020 lr: 0.02\n",
      "iteration: 325180 loss: 0.0020 lr: 0.02\n",
      "iteration: 325190 loss: 0.0014 lr: 0.02\n",
      "iteration: 325200 loss: 0.0017 lr: 0.02\n",
      "iteration: 325210 loss: 0.0018 lr: 0.02\n",
      "iteration: 325220 loss: 0.0025 lr: 0.02\n",
      "iteration: 325230 loss: 0.0011 lr: 0.02\n",
      "iteration: 325240 loss: 0.0014 lr: 0.02\n",
      "iteration: 325250 loss: 0.0023 lr: 0.02\n",
      "iteration: 325260 loss: 0.0021 lr: 0.02\n",
      "iteration: 325270 loss: 0.0016 lr: 0.02\n",
      "iteration: 325280 loss: 0.0021 lr: 0.02\n",
      "iteration: 325290 loss: 0.0042 lr: 0.02\n",
      "iteration: 325300 loss: 0.0030 lr: 0.02\n",
      "iteration: 325310 loss: 0.0016 lr: 0.02\n",
      "iteration: 325320 loss: 0.0025 lr: 0.02\n",
      "iteration: 325330 loss: 0.0016 lr: 0.02\n",
      "iteration: 325340 loss: 0.0016 lr: 0.02\n",
      "iteration: 325350 loss: 0.0014 lr: 0.02\n",
      "iteration: 325360 loss: 0.0021 lr: 0.02\n",
      "iteration: 325370 loss: 0.0010 lr: 0.02\n",
      "iteration: 325380 loss: 0.0019 lr: 0.02\n",
      "iteration: 325390 loss: 0.0010 lr: 0.02\n",
      "iteration: 325400 loss: 0.0015 lr: 0.02\n",
      "iteration: 325410 loss: 0.0013 lr: 0.02\n",
      "iteration: 325420 loss: 0.0030 lr: 0.02\n",
      "iteration: 325430 loss: 0.0015 lr: 0.02\n",
      "iteration: 325440 loss: 0.0021 lr: 0.02\n",
      "iteration: 325450 loss: 0.0017 lr: 0.02\n",
      "iteration: 325460 loss: 0.0018 lr: 0.02\n",
      "iteration: 325470 loss: 0.0013 lr: 0.02\n",
      "iteration: 325480 loss: 0.0019 lr: 0.02\n",
      "iteration: 325490 loss: 0.0018 lr: 0.02\n",
      "iteration: 325500 loss: 0.0017 lr: 0.02\n",
      "iteration: 325510 loss: 0.0020 lr: 0.02\n",
      "iteration: 325520 loss: 0.0018 lr: 0.02\n",
      "iteration: 325530 loss: 0.0021 lr: 0.02\n",
      "iteration: 325540 loss: 0.0014 lr: 0.02\n",
      "iteration: 325550 loss: 0.0015 lr: 0.02\n",
      "iteration: 325560 loss: 0.0016 lr: 0.02\n",
      "iteration: 325570 loss: 0.0026 lr: 0.02\n",
      "iteration: 325580 loss: 0.0014 lr: 0.02\n",
      "iteration: 325590 loss: 0.0016 lr: 0.02\n",
      "iteration: 325600 loss: 0.0016 lr: 0.02\n",
      "iteration: 325610 loss: 0.0016 lr: 0.02\n",
      "iteration: 325620 loss: 0.0018 lr: 0.02\n",
      "iteration: 325630 loss: 0.0026 lr: 0.02\n",
      "iteration: 325640 loss: 0.0021 lr: 0.02\n",
      "iteration: 325650 loss: 0.0021 lr: 0.02\n",
      "iteration: 325660 loss: 0.0016 lr: 0.02\n",
      "iteration: 325670 loss: 0.0014 lr: 0.02\n",
      "iteration: 325680 loss: 0.0022 lr: 0.02\n",
      "iteration: 325690 loss: 0.0011 lr: 0.02\n",
      "iteration: 325700 loss: 0.0020 lr: 0.02\n",
      "iteration: 325710 loss: 0.0019 lr: 0.02\n",
      "iteration: 325720 loss: 0.0013 lr: 0.02\n",
      "iteration: 325730 loss: 0.0024 lr: 0.02\n",
      "iteration: 325740 loss: 0.0014 lr: 0.02\n",
      "iteration: 325750 loss: 0.0019 lr: 0.02\n",
      "iteration: 325760 loss: 0.0018 lr: 0.02\n",
      "iteration: 325770 loss: 0.0014 lr: 0.02\n",
      "iteration: 325780 loss: 0.0018 lr: 0.02\n",
      "iteration: 325790 loss: 0.0016 lr: 0.02\n",
      "iteration: 325800 loss: 0.0015 lr: 0.02\n",
      "iteration: 325810 loss: 0.0025 lr: 0.02\n",
      "iteration: 325820 loss: 0.0030 lr: 0.02\n",
      "iteration: 325830 loss: 0.0013 lr: 0.02\n",
      "iteration: 325840 loss: 0.0030 lr: 0.02\n",
      "iteration: 325850 loss: 0.0021 lr: 0.02\n",
      "iteration: 325860 loss: 0.0028 lr: 0.02\n",
      "iteration: 325870 loss: 0.0019 lr: 0.02\n",
      "iteration: 325880 loss: 0.0016 lr: 0.02\n",
      "iteration: 325890 loss: 0.0019 lr: 0.02\n",
      "iteration: 325900 loss: 0.0026 lr: 0.02\n",
      "iteration: 325910 loss: 0.0016 lr: 0.02\n",
      "iteration: 325920 loss: 0.0015 lr: 0.02\n",
      "iteration: 325930 loss: 0.0016 lr: 0.02\n",
      "iteration: 325940 loss: 0.0041 lr: 0.02\n",
      "iteration: 325950 loss: 0.0021 lr: 0.02\n",
      "iteration: 325960 loss: 0.0013 lr: 0.02\n",
      "iteration: 325970 loss: 0.0021 lr: 0.02\n",
      "iteration: 325980 loss: 0.0018 lr: 0.02\n",
      "iteration: 325990 loss: 0.0028 lr: 0.02\n",
      "iteration: 326000 loss: 0.0019 lr: 0.02\n",
      "iteration: 326010 loss: 0.0028 lr: 0.02\n",
      "iteration: 326020 loss: 0.0025 lr: 0.02\n",
      "iteration: 326030 loss: 0.0022 lr: 0.02\n",
      "iteration: 326040 loss: 0.0016 lr: 0.02\n",
      "iteration: 326050 loss: 0.0017 lr: 0.02\n",
      "iteration: 326060 loss: 0.0020 lr: 0.02\n",
      "iteration: 326070 loss: 0.0018 lr: 0.02\n",
      "iteration: 326080 loss: 0.0014 lr: 0.02\n",
      "iteration: 326090 loss: 0.0018 lr: 0.02\n",
      "iteration: 326100 loss: 0.0020 lr: 0.02\n",
      "iteration: 326110 loss: 0.0015 lr: 0.02\n",
      "iteration: 326120 loss: 0.0016 lr: 0.02\n",
      "iteration: 326130 loss: 0.0023 lr: 0.02\n",
      "iteration: 326140 loss: 0.0020 lr: 0.02\n",
      "iteration: 326150 loss: 0.0027 lr: 0.02\n",
      "iteration: 326160 loss: 0.0014 lr: 0.02\n",
      "iteration: 326170 loss: 0.0016 lr: 0.02\n",
      "iteration: 326180 loss: 0.0015 lr: 0.02\n",
      "iteration: 326190 loss: 0.0017 lr: 0.02\n",
      "iteration: 326200 loss: 0.0021 lr: 0.02\n",
      "iteration: 326210 loss: 0.0017 lr: 0.02\n",
      "iteration: 326220 loss: 0.0022 lr: 0.02\n",
      "iteration: 326230 loss: 0.0018 lr: 0.02\n",
      "iteration: 326240 loss: 0.0016 lr: 0.02\n",
      "iteration: 326250 loss: 0.0013 lr: 0.02\n",
      "iteration: 326260 loss: 0.0014 lr: 0.02\n",
      "iteration: 326270 loss: 0.0013 lr: 0.02\n",
      "iteration: 326280 loss: 0.0017 lr: 0.02\n",
      "iteration: 326290 loss: 0.0014 lr: 0.02\n",
      "iteration: 326300 loss: 0.0012 lr: 0.02\n",
      "iteration: 326310 loss: 0.0014 lr: 0.02\n",
      "iteration: 326320 loss: 0.0021 lr: 0.02\n",
      "iteration: 326330 loss: 0.0023 lr: 0.02\n",
      "iteration: 326340 loss: 0.0018 lr: 0.02\n",
      "iteration: 326350 loss: 0.0013 lr: 0.02\n",
      "iteration: 326360 loss: 0.0020 lr: 0.02\n",
      "iteration: 326370 loss: 0.0016 lr: 0.02\n",
      "iteration: 326380 loss: 0.0012 lr: 0.02\n",
      "iteration: 326390 loss: 0.0015 lr: 0.02\n",
      "iteration: 326400 loss: 0.0019 lr: 0.02\n",
      "iteration: 326410 loss: 0.0015 lr: 0.02\n",
      "iteration: 326420 loss: 0.0021 lr: 0.02\n",
      "iteration: 326430 loss: 0.0018 lr: 0.02\n",
      "iteration: 326440 loss: 0.0013 lr: 0.02\n",
      "iteration: 326450 loss: 0.0017 lr: 0.02\n",
      "iteration: 326460 loss: 0.0017 lr: 0.02\n",
      "iteration: 326470 loss: 0.0015 lr: 0.02\n",
      "iteration: 326480 loss: 0.0014 lr: 0.02\n",
      "iteration: 326490 loss: 0.0017 lr: 0.02\n",
      "iteration: 326500 loss: 0.0012 lr: 0.02\n",
      "iteration: 326510 loss: 0.0015 lr: 0.02\n",
      "iteration: 326520 loss: 0.0013 lr: 0.02\n",
      "iteration: 326530 loss: 0.0017 lr: 0.02\n",
      "iteration: 326540 loss: 0.0015 lr: 0.02\n",
      "iteration: 326550 loss: 0.0016 lr: 0.02\n",
      "iteration: 326560 loss: 0.0010 lr: 0.02\n",
      "iteration: 326570 loss: 0.0020 lr: 0.02\n",
      "iteration: 326580 loss: 0.0017 lr: 0.02\n",
      "iteration: 326590 loss: 0.0013 lr: 0.02\n",
      "iteration: 326600 loss: 0.0016 lr: 0.02\n",
      "iteration: 326610 loss: 0.0010 lr: 0.02\n",
      "iteration: 326620 loss: 0.0014 lr: 0.02\n",
      "iteration: 326630 loss: 0.0015 lr: 0.02\n",
      "iteration: 326640 loss: 0.0010 lr: 0.02\n",
      "iteration: 326650 loss: 0.0016 lr: 0.02\n",
      "iteration: 326660 loss: 0.0015 lr: 0.02\n",
      "iteration: 326670 loss: 0.0015 lr: 0.02\n",
      "iteration: 326680 loss: 0.0017 lr: 0.02\n",
      "iteration: 326690 loss: 0.0015 lr: 0.02\n",
      "iteration: 326700 loss: 0.0012 lr: 0.02\n",
      "iteration: 326710 loss: 0.0021 lr: 0.02\n",
      "iteration: 326720 loss: 0.0018 lr: 0.02\n",
      "iteration: 326730 loss: 0.0020 lr: 0.02\n",
      "iteration: 326740 loss: 0.0021 lr: 0.02\n",
      "iteration: 326750 loss: 0.0015 lr: 0.02\n",
      "iteration: 326760 loss: 0.0018 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 326770 loss: 0.0015 lr: 0.02\n",
      "iteration: 326780 loss: 0.0013 lr: 0.02\n",
      "iteration: 326790 loss: 0.0015 lr: 0.02\n",
      "iteration: 326800 loss: 0.0018 lr: 0.02\n",
      "iteration: 326810 loss: 0.0016 lr: 0.02\n",
      "iteration: 326820 loss: 0.0019 lr: 0.02\n",
      "iteration: 326830 loss: 0.0025 lr: 0.02\n",
      "iteration: 326840 loss: 0.0029 lr: 0.02\n",
      "iteration: 326850 loss: 0.0016 lr: 0.02\n",
      "iteration: 326860 loss: 0.0017 lr: 0.02\n",
      "iteration: 326870 loss: 0.0021 lr: 0.02\n",
      "iteration: 326880 loss: 0.0011 lr: 0.02\n",
      "iteration: 326890 loss: 0.0012 lr: 0.02\n",
      "iteration: 326900 loss: 0.0012 lr: 0.02\n",
      "iteration: 326910 loss: 0.0016 lr: 0.02\n",
      "iteration: 326920 loss: 0.0017 lr: 0.02\n",
      "iteration: 326930 loss: 0.0021 lr: 0.02\n",
      "iteration: 326940 loss: 0.0021 lr: 0.02\n",
      "iteration: 326950 loss: 0.0020 lr: 0.02\n",
      "iteration: 326960 loss: 0.0024 lr: 0.02\n",
      "iteration: 326970 loss: 0.0022 lr: 0.02\n",
      "iteration: 326980 loss: 0.0014 lr: 0.02\n",
      "iteration: 326990 loss: 0.0021 lr: 0.02\n",
      "iteration: 327000 loss: 0.0017 lr: 0.02\n",
      "iteration: 327010 loss: 0.0015 lr: 0.02\n",
      "iteration: 327020 loss: 0.0016 lr: 0.02\n",
      "iteration: 327030 loss: 0.0015 lr: 0.02\n",
      "iteration: 327040 loss: 0.0019 lr: 0.02\n",
      "iteration: 327050 loss: 0.0021 lr: 0.02\n",
      "iteration: 327060 loss: 0.0016 lr: 0.02\n",
      "iteration: 327070 loss: 0.0028 lr: 0.02\n",
      "iteration: 327080 loss: 0.0040 lr: 0.02\n",
      "iteration: 327090 loss: 0.0015 lr: 0.02\n",
      "iteration: 327100 loss: 0.0012 lr: 0.02\n",
      "iteration: 327110 loss: 0.0012 lr: 0.02\n",
      "iteration: 327120 loss: 0.0010 lr: 0.02\n",
      "iteration: 327130 loss: 0.0019 lr: 0.02\n",
      "iteration: 327140 loss: 0.0021 lr: 0.02\n",
      "iteration: 327150 loss: 0.0014 lr: 0.02\n",
      "iteration: 327160 loss: 0.0023 lr: 0.02\n",
      "iteration: 327170 loss: 0.0026 lr: 0.02\n",
      "iteration: 327180 loss: 0.0022 lr: 0.02\n",
      "iteration: 327190 loss: 0.0017 lr: 0.02\n",
      "iteration: 327200 loss: 0.0023 lr: 0.02\n",
      "iteration: 327210 loss: 0.0016 lr: 0.02\n",
      "iteration: 327220 loss: 0.0019 lr: 0.02\n",
      "iteration: 327230 loss: 0.0016 lr: 0.02\n",
      "iteration: 327240 loss: 0.0014 lr: 0.02\n",
      "iteration: 327250 loss: 0.0017 lr: 0.02\n",
      "iteration: 327260 loss: 0.0013 lr: 0.02\n",
      "iteration: 327270 loss: 0.0019 lr: 0.02\n",
      "iteration: 327280 loss: 0.0014 lr: 0.02\n",
      "iteration: 327290 loss: 0.0014 lr: 0.02\n",
      "iteration: 327300 loss: 0.0018 lr: 0.02\n",
      "iteration: 327310 loss: 0.0019 lr: 0.02\n",
      "iteration: 327320 loss: 0.0021 lr: 0.02\n",
      "iteration: 327330 loss: 0.0019 lr: 0.02\n",
      "iteration: 327340 loss: 0.0037 lr: 0.02\n",
      "iteration: 327350 loss: 0.0021 lr: 0.02\n",
      "iteration: 327360 loss: 0.0015 lr: 0.02\n",
      "iteration: 327370 loss: 0.0014 lr: 0.02\n",
      "iteration: 327380 loss: 0.0019 lr: 0.02\n",
      "iteration: 327390 loss: 0.0014 lr: 0.02\n",
      "iteration: 327400 loss: 0.0015 lr: 0.02\n",
      "iteration: 327410 loss: 0.0017 lr: 0.02\n",
      "iteration: 327420 loss: 0.0024 lr: 0.02\n",
      "iteration: 327430 loss: 0.0015 lr: 0.02\n",
      "iteration: 327440 loss: 0.0012 lr: 0.02\n",
      "iteration: 327450 loss: 0.0018 lr: 0.02\n",
      "iteration: 327460 loss: 0.0014 lr: 0.02\n",
      "iteration: 327470 loss: 0.0023 lr: 0.02\n",
      "iteration: 327480 loss: 0.0016 lr: 0.02\n",
      "iteration: 327490 loss: 0.0020 lr: 0.02\n",
      "iteration: 327500 loss: 0.0022 lr: 0.02\n",
      "iteration: 327510 loss: 0.0013 lr: 0.02\n",
      "iteration: 327520 loss: 0.0023 lr: 0.02\n",
      "iteration: 327530 loss: 0.0013 lr: 0.02\n",
      "iteration: 327540 loss: 0.0016 lr: 0.02\n",
      "iteration: 327550 loss: 0.0016 lr: 0.02\n",
      "iteration: 327560 loss: 0.0020 lr: 0.02\n",
      "iteration: 327570 loss: 0.0021 lr: 0.02\n",
      "iteration: 327580 loss: 0.0019 lr: 0.02\n",
      "iteration: 327590 loss: 0.0027 lr: 0.02\n",
      "iteration: 327600 loss: 0.0016 lr: 0.02\n",
      "iteration: 327610 loss: 0.0015 lr: 0.02\n",
      "iteration: 327620 loss: 0.0013 lr: 0.02\n",
      "iteration: 327630 loss: 0.0015 lr: 0.02\n",
      "iteration: 327640 loss: 0.0016 lr: 0.02\n",
      "iteration: 327650 loss: 0.0019 lr: 0.02\n",
      "iteration: 327660 loss: 0.0018 lr: 0.02\n",
      "iteration: 327670 loss: 0.0019 lr: 0.02\n",
      "iteration: 327680 loss: 0.0029 lr: 0.02\n",
      "iteration: 327690 loss: 0.0015 lr: 0.02\n",
      "iteration: 327700 loss: 0.0016 lr: 0.02\n",
      "iteration: 327710 loss: 0.0020 lr: 0.02\n",
      "iteration: 327720 loss: 0.0021 lr: 0.02\n",
      "iteration: 327730 loss: 0.0017 lr: 0.02\n",
      "iteration: 327740 loss: 0.0020 lr: 0.02\n",
      "iteration: 327750 loss: 0.0016 lr: 0.02\n",
      "iteration: 327760 loss: 0.0020 lr: 0.02\n",
      "iteration: 327770 loss: 0.0013 lr: 0.02\n",
      "iteration: 327780 loss: 0.0012 lr: 0.02\n",
      "iteration: 327790 loss: 0.0015 lr: 0.02\n",
      "iteration: 327800 loss: 0.0010 lr: 0.02\n",
      "iteration: 327810 loss: 0.0018 lr: 0.02\n",
      "iteration: 327820 loss: 0.0018 lr: 0.02\n",
      "iteration: 327830 loss: 0.0023 lr: 0.02\n",
      "iteration: 327840 loss: 0.0023 lr: 0.02\n",
      "iteration: 327850 loss: 0.0013 lr: 0.02\n",
      "iteration: 327860 loss: 0.0021 lr: 0.02\n",
      "iteration: 327870 loss: 0.0018 lr: 0.02\n",
      "iteration: 327880 loss: 0.0020 lr: 0.02\n",
      "iteration: 327890 loss: 0.0021 lr: 0.02\n",
      "iteration: 327900 loss: 0.0016 lr: 0.02\n",
      "iteration: 327910 loss: 0.0019 lr: 0.02\n",
      "iteration: 327920 loss: 0.0025 lr: 0.02\n",
      "iteration: 327930 loss: 0.0020 lr: 0.02\n",
      "iteration: 327940 loss: 0.0017 lr: 0.02\n",
      "iteration: 327950 loss: 0.0017 lr: 0.02\n",
      "iteration: 327960 loss: 0.0013 lr: 0.02\n",
      "iteration: 327970 loss: 0.0018 lr: 0.02\n",
      "iteration: 327980 loss: 0.0016 lr: 0.02\n",
      "iteration: 327990 loss: 0.0017 lr: 0.02\n",
      "iteration: 328000 loss: 0.0015 lr: 0.02\n",
      "iteration: 328010 loss: 0.0014 lr: 0.02\n",
      "iteration: 328020 loss: 0.0015 lr: 0.02\n",
      "iteration: 328030 loss: 0.0010 lr: 0.02\n",
      "iteration: 328040 loss: 0.0013 lr: 0.02\n",
      "iteration: 328050 loss: 0.0015 lr: 0.02\n",
      "iteration: 328060 loss: 0.0015 lr: 0.02\n",
      "iteration: 328070 loss: 0.0018 lr: 0.02\n",
      "iteration: 328080 loss: 0.0009 lr: 0.02\n",
      "iteration: 328090 loss: 0.0014 lr: 0.02\n",
      "iteration: 328100 loss: 0.0042 lr: 0.02\n",
      "iteration: 328110 loss: 0.0014 lr: 0.02\n",
      "iteration: 328120 loss: 0.0019 lr: 0.02\n",
      "iteration: 328130 loss: 0.0014 lr: 0.02\n",
      "iteration: 328140 loss: 0.0012 lr: 0.02\n",
      "iteration: 328150 loss: 0.0017 lr: 0.02\n",
      "iteration: 328160 loss: 0.0013 lr: 0.02\n",
      "iteration: 328170 loss: 0.0011 lr: 0.02\n",
      "iteration: 328180 loss: 0.0013 lr: 0.02\n",
      "iteration: 328190 loss: 0.0013 lr: 0.02\n",
      "iteration: 328200 loss: 0.0013 lr: 0.02\n",
      "iteration: 328210 loss: 0.0013 lr: 0.02\n",
      "iteration: 328220 loss: 0.0020 lr: 0.02\n",
      "iteration: 328230 loss: 0.0019 lr: 0.02\n",
      "iteration: 328240 loss: 0.0019 lr: 0.02\n",
      "iteration: 328250 loss: 0.0019 lr: 0.02\n",
      "iteration: 328260 loss: 0.0021 lr: 0.02\n",
      "iteration: 328270 loss: 0.0020 lr: 0.02\n",
      "iteration: 328280 loss: 0.0026 lr: 0.02\n",
      "iteration: 328290 loss: 0.0014 lr: 0.02\n",
      "iteration: 328300 loss: 0.0014 lr: 0.02\n",
      "iteration: 328310 loss: 0.0015 lr: 0.02\n",
      "iteration: 328320 loss: 0.0014 lr: 0.02\n",
      "iteration: 328330 loss: 0.0019 lr: 0.02\n",
      "iteration: 328340 loss: 0.0017 lr: 0.02\n",
      "iteration: 328350 loss: 0.0016 lr: 0.02\n",
      "iteration: 328360 loss: 0.0010 lr: 0.02\n",
      "iteration: 328370 loss: 0.0025 lr: 0.02\n",
      "iteration: 328380 loss: 0.0020 lr: 0.02\n",
      "iteration: 328390 loss: 0.0020 lr: 0.02\n",
      "iteration: 328400 loss: 0.0021 lr: 0.02\n",
      "iteration: 328410 loss: 0.0013 lr: 0.02\n",
      "iteration: 328420 loss: 0.0015 lr: 0.02\n",
      "iteration: 328430 loss: 0.0014 lr: 0.02\n",
      "iteration: 328440 loss: 0.0018 lr: 0.02\n",
      "iteration: 328450 loss: 0.0018 lr: 0.02\n",
      "iteration: 328460 loss: 0.0014 lr: 0.02\n",
      "iteration: 328470 loss: 0.0016 lr: 0.02\n",
      "iteration: 328480 loss: 0.0013 lr: 0.02\n",
      "iteration: 328490 loss: 0.0011 lr: 0.02\n",
      "iteration: 328500 loss: 0.0014 lr: 0.02\n",
      "iteration: 328510 loss: 0.0015 lr: 0.02\n",
      "iteration: 328520 loss: 0.0013 lr: 0.02\n",
      "iteration: 328530 loss: 0.0015 lr: 0.02\n",
      "iteration: 328540 loss: 0.0017 lr: 0.02\n",
      "iteration: 328550 loss: 0.0015 lr: 0.02\n",
      "iteration: 328560 loss: 0.0015 lr: 0.02\n",
      "iteration: 328570 loss: 0.0011 lr: 0.02\n",
      "iteration: 328580 loss: 0.0015 lr: 0.02\n",
      "iteration: 328590 loss: 0.0013 lr: 0.02\n",
      "iteration: 328600 loss: 0.0016 lr: 0.02\n",
      "iteration: 328610 loss: 0.0016 lr: 0.02\n",
      "iteration: 328620 loss: 0.0014 lr: 0.02\n",
      "iteration: 328630 loss: 0.0011 lr: 0.02\n",
      "iteration: 328640 loss: 0.0014 lr: 0.02\n",
      "iteration: 328650 loss: 0.0013 lr: 0.02\n",
      "iteration: 328660 loss: 0.0012 lr: 0.02\n",
      "iteration: 328670 loss: 0.0016 lr: 0.02\n",
      "iteration: 328680 loss: 0.0014 lr: 0.02\n",
      "iteration: 328690 loss: 0.0016 lr: 0.02\n",
      "iteration: 328700 loss: 0.0016 lr: 0.02\n",
      "iteration: 328710 loss: 0.0030 lr: 0.02\n",
      "iteration: 328720 loss: 0.0014 lr: 0.02\n",
      "iteration: 328730 loss: 0.0016 lr: 0.02\n",
      "iteration: 328740 loss: 0.0018 lr: 0.02\n",
      "iteration: 328750 loss: 0.0015 lr: 0.02\n",
      "iteration: 328760 loss: 0.0014 lr: 0.02\n",
      "iteration: 328770 loss: 0.0015 lr: 0.02\n",
      "iteration: 328780 loss: 0.0020 lr: 0.02\n",
      "iteration: 328790 loss: 0.0016 lr: 0.02\n",
      "iteration: 328800 loss: 0.0016 lr: 0.02\n",
      "iteration: 328810 loss: 0.0017 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 328820 loss: 0.0019 lr: 0.02\n",
      "iteration: 328830 loss: 0.0015 lr: 0.02\n",
      "iteration: 328840 loss: 0.0013 lr: 0.02\n",
      "iteration: 328850 loss: 0.0022 lr: 0.02\n",
      "iteration: 328860 loss: 0.0016 lr: 0.02\n",
      "iteration: 328870 loss: 0.0019 lr: 0.02\n",
      "iteration: 328880 loss: 0.0020 lr: 0.02\n",
      "iteration: 328890 loss: 0.0019 lr: 0.02\n",
      "iteration: 328900 loss: 0.0015 lr: 0.02\n",
      "iteration: 328910 loss: 0.0014 lr: 0.02\n",
      "iteration: 328920 loss: 0.0019 lr: 0.02\n",
      "iteration: 328930 loss: 0.0013 lr: 0.02\n",
      "iteration: 328940 loss: 0.0018 lr: 0.02\n",
      "iteration: 328950 loss: 0.0019 lr: 0.02\n",
      "iteration: 328960 loss: 0.0016 lr: 0.02\n",
      "iteration: 328970 loss: 0.0022 lr: 0.02\n",
      "iteration: 328980 loss: 0.0019 lr: 0.02\n",
      "iteration: 328990 loss: 0.0012 lr: 0.02\n",
      "iteration: 329000 loss: 0.0017 lr: 0.02\n",
      "iteration: 329010 loss: 0.0016 lr: 0.02\n",
      "iteration: 329020 loss: 0.0013 lr: 0.02\n",
      "iteration: 329030 loss: 0.0013 lr: 0.02\n",
      "iteration: 329040 loss: 0.0017 lr: 0.02\n",
      "iteration: 329050 loss: 0.0012 lr: 0.02\n",
      "iteration: 329060 loss: 0.0015 lr: 0.02\n",
      "iteration: 329070 loss: 0.0013 lr: 0.02\n",
      "iteration: 329080 loss: 0.0016 lr: 0.02\n",
      "iteration: 329090 loss: 0.0016 lr: 0.02\n",
      "iteration: 329100 loss: 0.0014 lr: 0.02\n",
      "iteration: 329110 loss: 0.0016 lr: 0.02\n",
      "iteration: 329120 loss: 0.0018 lr: 0.02\n",
      "iteration: 329130 loss: 0.0016 lr: 0.02\n",
      "iteration: 329140 loss: 0.0014 lr: 0.02\n",
      "iteration: 329150 loss: 0.0015 lr: 0.02\n",
      "iteration: 329160 loss: 0.0019 lr: 0.02\n",
      "iteration: 329170 loss: 0.0017 lr: 0.02\n",
      "iteration: 329180 loss: 0.0031 lr: 0.02\n",
      "iteration: 329190 loss: 0.0017 lr: 0.02\n",
      "iteration: 329200 loss: 0.0017 lr: 0.02\n",
      "iteration: 329210 loss: 0.0014 lr: 0.02\n",
      "iteration: 329220 loss: 0.0012 lr: 0.02\n",
      "iteration: 329230 loss: 0.0019 lr: 0.02\n",
      "iteration: 329240 loss: 0.0015 lr: 0.02\n",
      "iteration: 329250 loss: 0.0018 lr: 0.02\n",
      "iteration: 329260 loss: 0.0016 lr: 0.02\n",
      "iteration: 329270 loss: 0.0016 lr: 0.02\n",
      "iteration: 329280 loss: 0.0014 lr: 0.02\n",
      "iteration: 329290 loss: 0.0017 lr: 0.02\n",
      "iteration: 329300 loss: 0.0014 lr: 0.02\n",
      "iteration: 329310 loss: 0.0014 lr: 0.02\n",
      "iteration: 329320 loss: 0.0024 lr: 0.02\n",
      "iteration: 329330 loss: 0.0020 lr: 0.02\n",
      "iteration: 329340 loss: 0.0011 lr: 0.02\n",
      "iteration: 329350 loss: 0.0015 lr: 0.02\n",
      "iteration: 329360 loss: 0.0022 lr: 0.02\n",
      "iteration: 329370 loss: 0.0015 lr: 0.02\n",
      "iteration: 329380 loss: 0.0015 lr: 0.02\n",
      "iteration: 329390 loss: 0.0019 lr: 0.02\n",
      "iteration: 329400 loss: 0.0016 lr: 0.02\n",
      "iteration: 329410 loss: 0.0026 lr: 0.02\n",
      "iteration: 329420 loss: 0.0013 lr: 0.02\n",
      "iteration: 329430 loss: 0.0018 lr: 0.02\n",
      "iteration: 329440 loss: 0.0017 lr: 0.02\n",
      "iteration: 329450 loss: 0.0029 lr: 0.02\n",
      "iteration: 329460 loss: 0.0016 lr: 0.02\n",
      "iteration: 329470 loss: 0.0027 lr: 0.02\n",
      "iteration: 329480 loss: 0.0023 lr: 0.02\n",
      "iteration: 329490 loss: 0.0017 lr: 0.02\n",
      "iteration: 329500 loss: 0.0017 lr: 0.02\n",
      "iteration: 329510 loss: 0.0010 lr: 0.02\n",
      "iteration: 329520 loss: 0.0017 lr: 0.02\n",
      "iteration: 329530 loss: 0.0013 lr: 0.02\n",
      "iteration: 329540 loss: 0.0019 lr: 0.02\n",
      "iteration: 329550 loss: 0.0014 lr: 0.02\n",
      "iteration: 329560 loss: 0.0013 lr: 0.02\n",
      "iteration: 329570 loss: 0.0017 lr: 0.02\n",
      "iteration: 329580 loss: 0.0020 lr: 0.02\n",
      "iteration: 329590 loss: 0.0024 lr: 0.02\n",
      "iteration: 329600 loss: 0.0019 lr: 0.02\n",
      "iteration: 329610 loss: 0.0018 lr: 0.02\n",
      "iteration: 329620 loss: 0.0015 lr: 0.02\n",
      "iteration: 329630 loss: 0.0024 lr: 0.02\n",
      "iteration: 329640 loss: 0.0016 lr: 0.02\n",
      "iteration: 329650 loss: 0.0023 lr: 0.02\n",
      "iteration: 329660 loss: 0.0015 lr: 0.02\n",
      "iteration: 329670 loss: 0.0025 lr: 0.02\n",
      "iteration: 329680 loss: 0.0012 lr: 0.02\n",
      "iteration: 329690 loss: 0.0019 lr: 0.02\n",
      "iteration: 329700 loss: 0.0020 lr: 0.02\n",
      "iteration: 329710 loss: 0.0015 lr: 0.02\n",
      "iteration: 329720 loss: 0.0014 lr: 0.02\n",
      "iteration: 329730 loss: 0.0019 lr: 0.02\n",
      "iteration: 329740 loss: 0.0016 lr: 0.02\n",
      "iteration: 329750 loss: 0.0017 lr: 0.02\n",
      "iteration: 329760 loss: 0.0016 lr: 0.02\n",
      "iteration: 329770 loss: 0.0014 lr: 0.02\n",
      "iteration: 329780 loss: 0.0014 lr: 0.02\n",
      "iteration: 329790 loss: 0.0026 lr: 0.02\n",
      "iteration: 329800 loss: 0.0020 lr: 0.02\n",
      "iteration: 329810 loss: 0.0015 lr: 0.02\n",
      "iteration: 329820 loss: 0.0020 lr: 0.02\n",
      "iteration: 329830 loss: 0.0020 lr: 0.02\n",
      "iteration: 329840 loss: 0.0014 lr: 0.02\n",
      "iteration: 329850 loss: 0.0017 lr: 0.02\n",
      "iteration: 329860 loss: 0.0017 lr: 0.02\n",
      "iteration: 329870 loss: 0.0028 lr: 0.02\n",
      "iteration: 329880 loss: 0.0020 lr: 0.02\n",
      "iteration: 329890 loss: 0.0022 lr: 0.02\n",
      "iteration: 329900 loss: 0.0020 lr: 0.02\n",
      "iteration: 329910 loss: 0.0025 lr: 0.02\n",
      "iteration: 329920 loss: 0.0020 lr: 0.02\n",
      "iteration: 329930 loss: 0.0026 lr: 0.02\n",
      "iteration: 329940 loss: 0.0014 lr: 0.02\n",
      "iteration: 329950 loss: 0.0020 lr: 0.02\n",
      "iteration: 329960 loss: 0.0023 lr: 0.02\n",
      "iteration: 329970 loss: 0.0016 lr: 0.02\n",
      "iteration: 329980 loss: 0.0013 lr: 0.02\n",
      "iteration: 329990 loss: 0.0028 lr: 0.02\n",
      "iteration: 330000 loss: 0.0015 lr: 0.02\n",
      "iteration: 330010 loss: 0.0030 lr: 0.02\n",
      "iteration: 330020 loss: 0.0017 lr: 0.02\n",
      "iteration: 330030 loss: 0.0019 lr: 0.02\n",
      "iteration: 330040 loss: 0.0018 lr: 0.02\n",
      "iteration: 330050 loss: 0.0019 lr: 0.02\n",
      "iteration: 330060 loss: 0.0016 lr: 0.02\n",
      "iteration: 330070 loss: 0.0017 lr: 0.02\n",
      "iteration: 330080 loss: 0.0021 lr: 0.02\n",
      "iteration: 330090 loss: 0.0015 lr: 0.02\n",
      "iteration: 330100 loss: 0.0014 lr: 0.02\n",
      "iteration: 330110 loss: 0.0019 lr: 0.02\n",
      "iteration: 330120 loss: 0.0024 lr: 0.02\n",
      "iteration: 330130 loss: 0.0015 lr: 0.02\n",
      "iteration: 330140 loss: 0.0010 lr: 0.02\n",
      "iteration: 330150 loss: 0.0020 lr: 0.02\n",
      "iteration: 330160 loss: 0.0014 lr: 0.02\n",
      "iteration: 330170 loss: 0.0018 lr: 0.02\n",
      "iteration: 330180 loss: 0.0018 lr: 0.02\n",
      "iteration: 330190 loss: 0.0016 lr: 0.02\n",
      "iteration: 330200 loss: 0.0017 lr: 0.02\n",
      "iteration: 330210 loss: 0.0013 lr: 0.02\n",
      "iteration: 330220 loss: 0.0020 lr: 0.02\n",
      "iteration: 330230 loss: 0.0009 lr: 0.02\n",
      "iteration: 330240 loss: 0.0021 lr: 0.02\n",
      "iteration: 330250 loss: 0.0016 lr: 0.02\n",
      "iteration: 330260 loss: 0.0012 lr: 0.02\n",
      "iteration: 330270 loss: 0.0015 lr: 0.02\n",
      "iteration: 330280 loss: 0.0017 lr: 0.02\n",
      "iteration: 330290 loss: 0.0021 lr: 0.02\n",
      "iteration: 330300 loss: 0.0024 lr: 0.02\n",
      "iteration: 330310 loss: 0.0027 lr: 0.02\n",
      "iteration: 330320 loss: 0.0019 lr: 0.02\n",
      "iteration: 330330 loss: 0.0015 lr: 0.02\n",
      "iteration: 330340 loss: 0.0019 lr: 0.02\n",
      "iteration: 330350 loss: 0.0013 lr: 0.02\n",
      "iteration: 330360 loss: 0.0017 lr: 0.02\n",
      "iteration: 330370 loss: 0.0025 lr: 0.02\n",
      "iteration: 330380 loss: 0.0020 lr: 0.02\n",
      "iteration: 330390 loss: 0.0021 lr: 0.02\n",
      "iteration: 330400 loss: 0.0014 lr: 0.02\n",
      "iteration: 330410 loss: 0.0017 lr: 0.02\n",
      "iteration: 330420 loss: 0.0015 lr: 0.02\n",
      "iteration: 330430 loss: 0.0023 lr: 0.02\n",
      "iteration: 330440 loss: 0.0025 lr: 0.02\n",
      "iteration: 330450 loss: 0.0013 lr: 0.02\n",
      "iteration: 330460 loss: 0.0022 lr: 0.02\n",
      "iteration: 330470 loss: 0.0014 lr: 0.02\n",
      "iteration: 330480 loss: 0.0015 lr: 0.02\n",
      "iteration: 330490 loss: 0.0020 lr: 0.02\n",
      "iteration: 330500 loss: 0.0015 lr: 0.02\n",
      "iteration: 330510 loss: 0.0019 lr: 0.02\n",
      "iteration: 330520 loss: 0.0019 lr: 0.02\n",
      "iteration: 330530 loss: 0.0019 lr: 0.02\n",
      "iteration: 330540 loss: 0.0018 lr: 0.02\n",
      "iteration: 330550 loss: 0.0015 lr: 0.02\n",
      "iteration: 330560 loss: 0.0036 lr: 0.02\n",
      "iteration: 330570 loss: 0.0025 lr: 0.02\n",
      "iteration: 330580 loss: 0.0014 lr: 0.02\n",
      "iteration: 330590 loss: 0.0015 lr: 0.02\n",
      "iteration: 330600 loss: 0.0016 lr: 0.02\n",
      "iteration: 330610 loss: 0.0021 lr: 0.02\n",
      "iteration: 330620 loss: 0.0014 lr: 0.02\n",
      "iteration: 330630 loss: 0.0015 lr: 0.02\n",
      "iteration: 330640 loss: 0.0016 lr: 0.02\n",
      "iteration: 330650 loss: 0.0020 lr: 0.02\n",
      "iteration: 330660 loss: 0.0023 lr: 0.02\n",
      "iteration: 330670 loss: 0.0015 lr: 0.02\n",
      "iteration: 330680 loss: 0.0018 lr: 0.02\n",
      "iteration: 330690 loss: 0.0015 lr: 0.02\n",
      "iteration: 330700 loss: 0.0015 lr: 0.02\n",
      "iteration: 330710 loss: 0.0020 lr: 0.02\n",
      "iteration: 330720 loss: 0.0020 lr: 0.02\n",
      "iteration: 330730 loss: 0.0018 lr: 0.02\n",
      "iteration: 330740 loss: 0.0018 lr: 0.02\n",
      "iteration: 330750 loss: 0.0014 lr: 0.02\n",
      "iteration: 330760 loss: 0.0014 lr: 0.02\n",
      "iteration: 330770 loss: 0.0017 lr: 0.02\n",
      "iteration: 330780 loss: 0.0014 lr: 0.02\n",
      "iteration: 330790 loss: 0.0018 lr: 0.02\n",
      "iteration: 330800 loss: 0.0013 lr: 0.02\n",
      "iteration: 330810 loss: 0.0026 lr: 0.02\n",
      "iteration: 330820 loss: 0.0016 lr: 0.02\n",
      "iteration: 330830 loss: 0.0015 lr: 0.02\n",
      "iteration: 330840 loss: 0.0018 lr: 0.02\n",
      "iteration: 330850 loss: 0.0018 lr: 0.02\n",
      "iteration: 330860 loss: 0.0015 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 330870 loss: 0.0024 lr: 0.02\n",
      "iteration: 330880 loss: 0.0018 lr: 0.02\n",
      "iteration: 330890 loss: 0.0018 lr: 0.02\n",
      "iteration: 330900 loss: 0.0015 lr: 0.02\n",
      "iteration: 330910 loss: 0.0019 lr: 0.02\n",
      "iteration: 330920 loss: 0.0013 lr: 0.02\n",
      "iteration: 330930 loss: 0.0019 lr: 0.02\n",
      "iteration: 330940 loss: 0.0019 lr: 0.02\n",
      "iteration: 330950 loss: 0.0010 lr: 0.02\n",
      "iteration: 330960 loss: 0.0012 lr: 0.02\n",
      "iteration: 330970 loss: 0.0016 lr: 0.02\n",
      "iteration: 330980 loss: 0.0015 lr: 0.02\n",
      "iteration: 330990 loss: 0.0020 lr: 0.02\n",
      "iteration: 331000 loss: 0.0028 lr: 0.02\n",
      "iteration: 331010 loss: 0.0016 lr: 0.02\n",
      "iteration: 331020 loss: 0.0015 lr: 0.02\n",
      "iteration: 331030 loss: 0.0023 lr: 0.02\n",
      "iteration: 331040 loss: 0.0016 lr: 0.02\n",
      "iteration: 331050 loss: 0.0017 lr: 0.02\n",
      "iteration: 331060 loss: 0.0019 lr: 0.02\n",
      "iteration: 331070 loss: 0.0016 lr: 0.02\n",
      "iteration: 331080 loss: 0.0017 lr: 0.02\n",
      "iteration: 331090 loss: 0.0020 lr: 0.02\n",
      "iteration: 331100 loss: 0.0019 lr: 0.02\n",
      "iteration: 331110 loss: 0.0015 lr: 0.02\n",
      "iteration: 331120 loss: 0.0017 lr: 0.02\n",
      "iteration: 331130 loss: 0.0013 lr: 0.02\n",
      "iteration: 331140 loss: 0.0039 lr: 0.02\n",
      "iteration: 331150 loss: 0.0016 lr: 0.02\n",
      "iteration: 331160 loss: 0.0022 lr: 0.02\n",
      "iteration: 331170 loss: 0.0019 lr: 0.02\n",
      "iteration: 331180 loss: 0.0014 lr: 0.02\n",
      "iteration: 331190 loss: 0.0017 lr: 0.02\n",
      "iteration: 331200 loss: 0.0013 lr: 0.02\n",
      "iteration: 331210 loss: 0.0016 lr: 0.02\n",
      "iteration: 331220 loss: 0.0022 lr: 0.02\n",
      "iteration: 331230 loss: 0.0020 lr: 0.02\n",
      "iteration: 331240 loss: 0.0018 lr: 0.02\n",
      "iteration: 331250 loss: 0.0012 lr: 0.02\n",
      "iteration: 331260 loss: 0.0014 lr: 0.02\n",
      "iteration: 331270 loss: 0.0031 lr: 0.02\n",
      "iteration: 331280 loss: 0.0015 lr: 0.02\n",
      "iteration: 331290 loss: 0.0018 lr: 0.02\n",
      "iteration: 331300 loss: 0.0021 lr: 0.02\n",
      "iteration: 331310 loss: 0.0014 lr: 0.02\n",
      "iteration: 331320 loss: 0.0018 lr: 0.02\n",
      "iteration: 331330 loss: 0.0012 lr: 0.02\n",
      "iteration: 331340 loss: 0.0015 lr: 0.02\n",
      "iteration: 331350 loss: 0.0020 lr: 0.02\n",
      "iteration: 331360 loss: 0.0013 lr: 0.02\n",
      "iteration: 331370 loss: 0.0016 lr: 0.02\n",
      "iteration: 331380 loss: 0.0017 lr: 0.02\n",
      "iteration: 331390 loss: 0.0018 lr: 0.02\n",
      "iteration: 331400 loss: 0.0015 lr: 0.02\n",
      "iteration: 331410 loss: 0.0013 lr: 0.02\n",
      "iteration: 331420 loss: 0.0016 lr: 0.02\n",
      "iteration: 331430 loss: 0.0013 lr: 0.02\n",
      "iteration: 331440 loss: 0.0012 lr: 0.02\n",
      "iteration: 331450 loss: 0.0016 lr: 0.02\n",
      "iteration: 331460 loss: 0.0013 lr: 0.02\n",
      "iteration: 331470 loss: 0.0021 lr: 0.02\n",
      "iteration: 331480 loss: 0.0019 lr: 0.02\n",
      "iteration: 331490 loss: 0.0010 lr: 0.02\n",
      "iteration: 331500 loss: 0.0014 lr: 0.02\n",
      "iteration: 331510 loss: 0.0018 lr: 0.02\n",
      "iteration: 331520 loss: 0.0020 lr: 0.02\n",
      "iteration: 331530 loss: 0.0015 lr: 0.02\n",
      "iteration: 331540 loss: 0.0013 lr: 0.02\n",
      "iteration: 331550 loss: 0.0020 lr: 0.02\n",
      "iteration: 331560 loss: 0.0016 lr: 0.02\n",
      "iteration: 331570 loss: 0.0019 lr: 0.02\n",
      "iteration: 331580 loss: 0.0023 lr: 0.02\n",
      "iteration: 331590 loss: 0.0017 lr: 0.02\n",
      "iteration: 331600 loss: 0.0012 lr: 0.02\n",
      "iteration: 331610 loss: 0.0015 lr: 0.02\n",
      "iteration: 331620 loss: 0.0018 lr: 0.02\n",
      "iteration: 331630 loss: 0.0017 lr: 0.02\n",
      "iteration: 331640 loss: 0.0018 lr: 0.02\n",
      "iteration: 331650 loss: 0.0016 lr: 0.02\n",
      "iteration: 331660 loss: 0.0019 lr: 0.02\n",
      "iteration: 331670 loss: 0.0023 lr: 0.02\n",
      "iteration: 331680 loss: 0.0020 lr: 0.02\n",
      "iteration: 331690 loss: 0.0016 lr: 0.02\n",
      "iteration: 331700 loss: 0.0012 lr: 0.02\n",
      "iteration: 331710 loss: 0.0015 lr: 0.02\n",
      "iteration: 331720 loss: 0.0014 lr: 0.02\n",
      "iteration: 331730 loss: 0.0015 lr: 0.02\n",
      "iteration: 331740 loss: 0.0016 lr: 0.02\n",
      "iteration: 331750 loss: 0.0023 lr: 0.02\n",
      "iteration: 331760 loss: 0.0017 lr: 0.02\n",
      "iteration: 331770 loss: 0.0016 lr: 0.02\n",
      "iteration: 331780 loss: 0.0013 lr: 0.02\n",
      "iteration: 331790 loss: 0.0022 lr: 0.02\n",
      "iteration: 331800 loss: 0.0017 lr: 0.02\n",
      "iteration: 331810 loss: 0.0020 lr: 0.02\n",
      "iteration: 331820 loss: 0.0018 lr: 0.02\n",
      "iteration: 331830 loss: 0.0024 lr: 0.02\n",
      "iteration: 331840 loss: 0.0024 lr: 0.02\n",
      "iteration: 331850 loss: 0.0022 lr: 0.02\n",
      "iteration: 331860 loss: 0.0022 lr: 0.02\n",
      "iteration: 331870 loss: 0.0014 lr: 0.02\n",
      "iteration: 331880 loss: 0.0026 lr: 0.02\n",
      "iteration: 331890 loss: 0.0018 lr: 0.02\n",
      "iteration: 331900 loss: 0.0017 lr: 0.02\n",
      "iteration: 331910 loss: 0.0012 lr: 0.02\n",
      "iteration: 331920 loss: 0.0017 lr: 0.02\n",
      "iteration: 331930 loss: 0.0015 lr: 0.02\n",
      "iteration: 331940 loss: 0.0017 lr: 0.02\n",
      "iteration: 331950 loss: 0.0010 lr: 0.02\n",
      "iteration: 331960 loss: 0.0019 lr: 0.02\n",
      "iteration: 331970 loss: 0.0013 lr: 0.02\n",
      "iteration: 331980 loss: 0.0014 lr: 0.02\n",
      "iteration: 331990 loss: 0.0015 lr: 0.02\n",
      "iteration: 332000 loss: 0.0034 lr: 0.02\n",
      "iteration: 332010 loss: 0.0018 lr: 0.02\n",
      "iteration: 332020 loss: 0.0012 lr: 0.02\n",
      "iteration: 332030 loss: 0.0016 lr: 0.02\n",
      "iteration: 332040 loss: 0.0016 lr: 0.02\n",
      "iteration: 332050 loss: 0.0011 lr: 0.02\n",
      "iteration: 332060 loss: 0.0019 lr: 0.02\n",
      "iteration: 332070 loss: 0.0018 lr: 0.02\n",
      "iteration: 332080 loss: 0.0022 lr: 0.02\n",
      "iteration: 332090 loss: 0.0027 lr: 0.02\n",
      "iteration: 332100 loss: 0.0015 lr: 0.02\n",
      "iteration: 332110 loss: 0.0014 lr: 0.02\n",
      "iteration: 332120 loss: 0.0016 lr: 0.02\n",
      "iteration: 332130 loss: 0.0015 lr: 0.02\n",
      "iteration: 332140 loss: 0.0021 lr: 0.02\n",
      "iteration: 332150 loss: 0.0010 lr: 0.02\n",
      "iteration: 332160 loss: 0.0015 lr: 0.02\n",
      "iteration: 332170 loss: 0.0016 lr: 0.02\n",
      "iteration: 332180 loss: 0.0016 lr: 0.02\n",
      "iteration: 332190 loss: 0.0023 lr: 0.02\n",
      "iteration: 332200 loss: 0.0025 lr: 0.02\n",
      "iteration: 332210 loss: 0.0016 lr: 0.02\n",
      "iteration: 332220 loss: 0.0017 lr: 0.02\n",
      "iteration: 332230 loss: 0.0019 lr: 0.02\n",
      "iteration: 332240 loss: 0.0016 lr: 0.02\n",
      "iteration: 332250 loss: 0.0020 lr: 0.02\n",
      "iteration: 332260 loss: 0.0014 lr: 0.02\n",
      "iteration: 332270 loss: 0.0016 lr: 0.02\n",
      "iteration: 332280 loss: 0.0013 lr: 0.02\n",
      "iteration: 332290 loss: 0.0017 lr: 0.02\n",
      "iteration: 332300 loss: 0.0024 lr: 0.02\n",
      "iteration: 332310 loss: 0.0019 lr: 0.02\n",
      "iteration: 332320 loss: 0.0020 lr: 0.02\n",
      "iteration: 332330 loss: 0.0010 lr: 0.02\n",
      "iteration: 332340 loss: 0.0018 lr: 0.02\n",
      "iteration: 332350 loss: 0.0022 lr: 0.02\n",
      "iteration: 332360 loss: 0.0019 lr: 0.02\n",
      "iteration: 332370 loss: 0.0017 lr: 0.02\n",
      "iteration: 332380 loss: 0.0020 lr: 0.02\n",
      "iteration: 332390 loss: 0.0022 lr: 0.02\n",
      "iteration: 332400 loss: 0.0018 lr: 0.02\n",
      "iteration: 332410 loss: 0.0024 lr: 0.02\n",
      "iteration: 332420 loss: 0.0014 lr: 0.02\n",
      "iteration: 332430 loss: 0.0025 lr: 0.02\n",
      "iteration: 332440 loss: 0.0018 lr: 0.02\n",
      "iteration: 332450 loss: 0.0023 lr: 0.02\n",
      "iteration: 332460 loss: 0.0017 lr: 0.02\n",
      "iteration: 332470 loss: 0.0018 lr: 0.02\n",
      "iteration: 332480 loss: 0.0018 lr: 0.02\n",
      "iteration: 332490 loss: 0.0013 lr: 0.02\n",
      "iteration: 332500 loss: 0.0021 lr: 0.02\n",
      "iteration: 332510 loss: 0.0022 lr: 0.02\n",
      "iteration: 332520 loss: 0.0019 lr: 0.02\n",
      "iteration: 332530 loss: 0.0017 lr: 0.02\n",
      "iteration: 332540 loss: 0.0017 lr: 0.02\n",
      "iteration: 332550 loss: 0.0014 lr: 0.02\n",
      "iteration: 332560 loss: 0.0019 lr: 0.02\n",
      "iteration: 332570 loss: 0.0020 lr: 0.02\n",
      "iteration: 332580 loss: 0.0025 lr: 0.02\n",
      "iteration: 332590 loss: 0.0025 lr: 0.02\n",
      "iteration: 332600 loss: 0.0020 lr: 0.02\n",
      "iteration: 332610 loss: 0.0015 lr: 0.02\n",
      "iteration: 332620 loss: 0.0017 lr: 0.02\n",
      "iteration: 332630 loss: 0.0016 lr: 0.02\n",
      "iteration: 332640 loss: 0.0026 lr: 0.02\n",
      "iteration: 332650 loss: 0.0019 lr: 0.02\n",
      "iteration: 332660 loss: 0.0014 lr: 0.02\n",
      "iteration: 332670 loss: 0.0015 lr: 0.02\n",
      "iteration: 332680 loss: 0.0014 lr: 0.02\n",
      "iteration: 332690 loss: 0.0011 lr: 0.02\n",
      "iteration: 332700 loss: 0.0011 lr: 0.02\n",
      "iteration: 332710 loss: 0.0014 lr: 0.02\n",
      "iteration: 332720 loss: 0.0014 lr: 0.02\n",
      "iteration: 332730 loss: 0.0024 lr: 0.02\n",
      "iteration: 332740 loss: 0.0025 lr: 0.02\n",
      "iteration: 332750 loss: 0.0018 lr: 0.02\n",
      "iteration: 332760 loss: 0.0013 lr: 0.02\n",
      "iteration: 332770 loss: 0.0014 lr: 0.02\n",
      "iteration: 332780 loss: 0.0015 lr: 0.02\n",
      "iteration: 332790 loss: 0.0016 lr: 0.02\n",
      "iteration: 332800 loss: 0.0018 lr: 0.02\n",
      "iteration: 332810 loss: 0.0018 lr: 0.02\n",
      "iteration: 332820 loss: 0.0018 lr: 0.02\n",
      "iteration: 332830 loss: 0.0016 lr: 0.02\n",
      "iteration: 332840 loss: 0.0014 lr: 0.02\n",
      "iteration: 332850 loss: 0.0017 lr: 0.02\n",
      "iteration: 332860 loss: 0.0016 lr: 0.02\n",
      "iteration: 332870 loss: 0.0020 lr: 0.02\n",
      "iteration: 332880 loss: 0.0024 lr: 0.02\n",
      "iteration: 332890 loss: 0.0014 lr: 0.02\n",
      "iteration: 332900 loss: 0.0016 lr: 0.02\n",
      "iteration: 332910 loss: 0.0018 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 332920 loss: 0.0012 lr: 0.02\n",
      "iteration: 332930 loss: 0.0019 lr: 0.02\n",
      "iteration: 332940 loss: 0.0012 lr: 0.02\n",
      "iteration: 332950 loss: 0.0016 lr: 0.02\n",
      "iteration: 332960 loss: 0.0014 lr: 0.02\n",
      "iteration: 332970 loss: 0.0017 lr: 0.02\n",
      "iteration: 332980 loss: 0.0018 lr: 0.02\n",
      "iteration: 332990 loss: 0.0010 lr: 0.02\n",
      "iteration: 333000 loss: 0.0013 lr: 0.02\n",
      "iteration: 333010 loss: 0.0014 lr: 0.02\n",
      "iteration: 333020 loss: 0.0025 lr: 0.02\n",
      "iteration: 333030 loss: 0.0025 lr: 0.02\n",
      "iteration: 333040 loss: 0.0013 lr: 0.02\n",
      "iteration: 333050 loss: 0.0013 lr: 0.02\n",
      "iteration: 333060 loss: 0.0015 lr: 0.02\n",
      "iteration: 333070 loss: 0.0017 lr: 0.02\n",
      "iteration: 333080 loss: 0.0015 lr: 0.02\n",
      "iteration: 333090 loss: 0.0014 lr: 0.02\n",
      "iteration: 333100 loss: 0.0018 lr: 0.02\n",
      "iteration: 333110 loss: 0.0013 lr: 0.02\n",
      "iteration: 333120 loss: 0.0018 lr: 0.02\n",
      "iteration: 333130 loss: 0.0019 lr: 0.02\n",
      "iteration: 333140 loss: 0.0015 lr: 0.02\n",
      "iteration: 333150 loss: 0.0022 lr: 0.02\n",
      "iteration: 333160 loss: 0.0014 lr: 0.02\n",
      "iteration: 333170 loss: 0.0015 lr: 0.02\n",
      "iteration: 333180 loss: 0.0014 lr: 0.02\n",
      "iteration: 333190 loss: 0.0020 lr: 0.02\n",
      "iteration: 333200 loss: 0.0020 lr: 0.02\n",
      "iteration: 333210 loss: 0.0018 lr: 0.02\n",
      "iteration: 333220 loss: 0.0015 lr: 0.02\n",
      "iteration: 333230 loss: 0.0017 lr: 0.02\n",
      "iteration: 333240 loss: 0.0016 lr: 0.02\n",
      "iteration: 333250 loss: 0.0017 lr: 0.02\n",
      "iteration: 333260 loss: 0.0018 lr: 0.02\n",
      "iteration: 333270 loss: 0.0016 lr: 0.02\n",
      "iteration: 333280 loss: 0.0016 lr: 0.02\n",
      "iteration: 333290 loss: 0.0021 lr: 0.02\n",
      "iteration: 333300 loss: 0.0013 lr: 0.02\n",
      "iteration: 333310 loss: 0.0020 lr: 0.02\n",
      "iteration: 333320 loss: 0.0022 lr: 0.02\n",
      "iteration: 333330 loss: 0.0016 lr: 0.02\n",
      "iteration: 333340 loss: 0.0014 lr: 0.02\n",
      "iteration: 333350 loss: 0.0015 lr: 0.02\n",
      "iteration: 333360 loss: 0.0018 lr: 0.02\n",
      "iteration: 333370 loss: 0.0023 lr: 0.02\n",
      "iteration: 333380 loss: 0.0017 lr: 0.02\n",
      "iteration: 333390 loss: 0.0016 lr: 0.02\n",
      "iteration: 333400 loss: 0.0017 lr: 0.02\n",
      "iteration: 333410 loss: 0.0018 lr: 0.02\n",
      "iteration: 333420 loss: 0.0015 lr: 0.02\n",
      "iteration: 333430 loss: 0.0014 lr: 0.02\n",
      "iteration: 333440 loss: 0.0015 lr: 0.02\n",
      "iteration: 333450 loss: 0.0015 lr: 0.02\n",
      "iteration: 333460 loss: 0.0014 lr: 0.02\n",
      "iteration: 333470 loss: 0.0025 lr: 0.02\n",
      "iteration: 333480 loss: 0.0018 lr: 0.02\n",
      "iteration: 333490 loss: 0.0014 lr: 0.02\n",
      "iteration: 333500 loss: 0.0015 lr: 0.02\n",
      "iteration: 333510 loss: 0.0013 lr: 0.02\n",
      "iteration: 333520 loss: 0.0017 lr: 0.02\n",
      "iteration: 333530 loss: 0.0016 lr: 0.02\n",
      "iteration: 333540 loss: 0.0020 lr: 0.02\n",
      "iteration: 333550 loss: 0.0021 lr: 0.02\n",
      "iteration: 333560 loss: 0.0014 lr: 0.02\n",
      "iteration: 333570 loss: 0.0018 lr: 0.02\n",
      "iteration: 333580 loss: 0.0022 lr: 0.02\n",
      "iteration: 333590 loss: 0.0012 lr: 0.02\n",
      "iteration: 333600 loss: 0.0011 lr: 0.02\n",
      "iteration: 333610 loss: 0.0024 lr: 0.02\n",
      "iteration: 333620 loss: 0.0014 lr: 0.02\n",
      "iteration: 333630 loss: 0.0014 lr: 0.02\n",
      "iteration: 333640 loss: 0.0015 lr: 0.02\n",
      "iteration: 333650 loss: 0.0014 lr: 0.02\n",
      "iteration: 333660 loss: 0.0022 lr: 0.02\n",
      "iteration: 333670 loss: 0.0024 lr: 0.02\n",
      "iteration: 333680 loss: 0.0015 lr: 0.02\n",
      "iteration: 333690 loss: 0.0016 lr: 0.02\n",
      "iteration: 333700 loss: 0.0014 lr: 0.02\n",
      "iteration: 333710 loss: 0.0022 lr: 0.02\n",
      "iteration: 333720 loss: 0.0019 lr: 0.02\n",
      "iteration: 333730 loss: 0.0014 lr: 0.02\n",
      "iteration: 333740 loss: 0.0022 lr: 0.02\n",
      "iteration: 333750 loss: 0.0011 lr: 0.02\n",
      "iteration: 333760 loss: 0.0025 lr: 0.02\n",
      "iteration: 333770 loss: 0.0015 lr: 0.02\n",
      "iteration: 333780 loss: 0.0020 lr: 0.02\n",
      "iteration: 333790 loss: 0.0012 lr: 0.02\n",
      "iteration: 333800 loss: 0.0033 lr: 0.02\n",
      "iteration: 333810 loss: 0.0015 lr: 0.02\n",
      "iteration: 333820 loss: 0.0019 lr: 0.02\n",
      "iteration: 333830 loss: 0.0015 lr: 0.02\n",
      "iteration: 333840 loss: 0.0026 lr: 0.02\n",
      "iteration: 333850 loss: 0.0021 lr: 0.02\n",
      "iteration: 333860 loss: 0.0017 lr: 0.02\n",
      "iteration: 333870 loss: 0.0022 lr: 0.02\n",
      "iteration: 333880 loss: 0.0014 lr: 0.02\n",
      "iteration: 333890 loss: 0.0027 lr: 0.02\n",
      "iteration: 333900 loss: 0.0014 lr: 0.02\n",
      "iteration: 333910 loss: 0.0018 lr: 0.02\n",
      "iteration: 333920 loss: 0.0033 lr: 0.02\n",
      "iteration: 333930 loss: 0.0026 lr: 0.02\n",
      "iteration: 333940 loss: 0.0016 lr: 0.02\n",
      "iteration: 333950 loss: 0.0019 lr: 0.02\n",
      "iteration: 333960 loss: 0.0016 lr: 0.02\n",
      "iteration: 333970 loss: 0.0015 lr: 0.02\n",
      "iteration: 333980 loss: 0.0017 lr: 0.02\n",
      "iteration: 333990 loss: 0.0016 lr: 0.02\n",
      "iteration: 334000 loss: 0.0017 lr: 0.02\n",
      "iteration: 334010 loss: 0.0016 lr: 0.02\n",
      "iteration: 334020 loss: 0.0020 lr: 0.02\n",
      "iteration: 334030 loss: 0.0009 lr: 0.02\n",
      "iteration: 334040 loss: 0.0015 lr: 0.02\n",
      "iteration: 334050 loss: 0.0016 lr: 0.02\n",
      "iteration: 334060 loss: 0.0020 lr: 0.02\n",
      "iteration: 334070 loss: 0.0015 lr: 0.02\n",
      "iteration: 334080 loss: 0.0018 lr: 0.02\n",
      "iteration: 334090 loss: 0.0015 lr: 0.02\n",
      "iteration: 334100 loss: 0.0024 lr: 0.02\n",
      "iteration: 334110 loss: 0.0020 lr: 0.02\n",
      "iteration: 334120 loss: 0.0011 lr: 0.02\n",
      "iteration: 334130 loss: 0.0018 lr: 0.02\n",
      "iteration: 334140 loss: 0.0018 lr: 0.02\n",
      "iteration: 334150 loss: 0.0012 lr: 0.02\n",
      "iteration: 334160 loss: 0.0014 lr: 0.02\n",
      "iteration: 334170 loss: 0.0018 lr: 0.02\n",
      "iteration: 334180 loss: 0.0015 lr: 0.02\n",
      "iteration: 334190 loss: 0.0012 lr: 0.02\n",
      "iteration: 334200 loss: 0.0014 lr: 0.02\n",
      "iteration: 334210 loss: 0.0016 lr: 0.02\n",
      "iteration: 334220 loss: 0.0014 lr: 0.02\n",
      "iteration: 334230 loss: 0.0022 lr: 0.02\n",
      "iteration: 334240 loss: 0.0016 lr: 0.02\n",
      "iteration: 334250 loss: 0.0014 lr: 0.02\n",
      "iteration: 334260 loss: 0.0018 lr: 0.02\n",
      "iteration: 334270 loss: 0.0018 lr: 0.02\n",
      "iteration: 334280 loss: 0.0022 lr: 0.02\n",
      "iteration: 334290 loss: 0.0023 lr: 0.02\n",
      "iteration: 334300 loss: 0.0016 lr: 0.02\n",
      "iteration: 334310 loss: 0.0021 lr: 0.02\n",
      "iteration: 334320 loss: 0.0015 lr: 0.02\n",
      "iteration: 334330 loss: 0.0021 lr: 0.02\n",
      "iteration: 334340 loss: 0.0015 lr: 0.02\n",
      "iteration: 334350 loss: 0.0011 lr: 0.02\n",
      "iteration: 334360 loss: 0.0025 lr: 0.02\n",
      "iteration: 334370 loss: 0.0013 lr: 0.02\n",
      "iteration: 334380 loss: 0.0022 lr: 0.02\n",
      "iteration: 334390 loss: 0.0026 lr: 0.02\n",
      "iteration: 334400 loss: 0.0018 lr: 0.02\n",
      "iteration: 334410 loss: 0.0020 lr: 0.02\n",
      "iteration: 334420 loss: 0.0012 lr: 0.02\n",
      "iteration: 334430 loss: 0.0018 lr: 0.02\n",
      "iteration: 334440 loss: 0.0015 lr: 0.02\n",
      "iteration: 334450 loss: 0.0020 lr: 0.02\n",
      "iteration: 334460 loss: 0.0015 lr: 0.02\n",
      "iteration: 334470 loss: 0.0017 lr: 0.02\n",
      "iteration: 334480 loss: 0.0014 lr: 0.02\n",
      "iteration: 334490 loss: 0.0022 lr: 0.02\n",
      "iteration: 334500 loss: 0.0011 lr: 0.02\n",
      "iteration: 334510 loss: 0.0016 lr: 0.02\n",
      "iteration: 334520 loss: 0.0015 lr: 0.02\n",
      "iteration: 334530 loss: 0.0053 lr: 0.02\n",
      "iteration: 334540 loss: 0.0017 lr: 0.02\n",
      "iteration: 334550 loss: 0.0019 lr: 0.02\n",
      "iteration: 334560 loss: 0.0020 lr: 0.02\n",
      "iteration: 334570 loss: 0.0021 lr: 0.02\n",
      "iteration: 334580 loss: 0.0027 lr: 0.02\n",
      "iteration: 334590 loss: 0.0020 lr: 0.02\n",
      "iteration: 334600 loss: 0.0017 lr: 0.02\n",
      "iteration: 334610 loss: 0.0017 lr: 0.02\n",
      "iteration: 334620 loss: 0.0021 lr: 0.02\n",
      "iteration: 334630 loss: 0.0019 lr: 0.02\n",
      "iteration: 334640 loss: 0.0024 lr: 0.02\n",
      "iteration: 334650 loss: 0.0022 lr: 0.02\n",
      "iteration: 334660 loss: 0.0018 lr: 0.02\n",
      "iteration: 334670 loss: 0.0017 lr: 0.02\n",
      "iteration: 334680 loss: 0.0021 lr: 0.02\n",
      "iteration: 334690 loss: 0.0017 lr: 0.02\n",
      "iteration: 334700 loss: 0.0023 lr: 0.02\n",
      "iteration: 334710 loss: 0.0023 lr: 0.02\n",
      "iteration: 334720 loss: 0.0020 lr: 0.02\n",
      "iteration: 334730 loss: 0.0016 lr: 0.02\n",
      "iteration: 334740 loss: 0.0014 lr: 0.02\n",
      "iteration: 334750 loss: 0.0026 lr: 0.02\n",
      "iteration: 334760 loss: 0.0024 lr: 0.02\n",
      "iteration: 334770 loss: 0.0017 lr: 0.02\n",
      "iteration: 334780 loss: 0.0011 lr: 0.02\n",
      "iteration: 334790 loss: 0.0019 lr: 0.02\n",
      "iteration: 334800 loss: 0.0019 lr: 0.02\n",
      "iteration: 334810 loss: 0.0013 lr: 0.02\n",
      "iteration: 334820 loss: 0.0020 lr: 0.02\n",
      "iteration: 334830 loss: 0.0021 lr: 0.02\n",
      "iteration: 334840 loss: 0.0013 lr: 0.02\n",
      "iteration: 334850 loss: 0.0016 lr: 0.02\n",
      "iteration: 334860 loss: 0.0018 lr: 0.02\n",
      "iteration: 334870 loss: 0.0022 lr: 0.02\n",
      "iteration: 334880 loss: 0.0011 lr: 0.02\n",
      "iteration: 334890 loss: 0.0021 lr: 0.02\n",
      "iteration: 334900 loss: 0.0012 lr: 0.02\n",
      "iteration: 334910 loss: 0.0017 lr: 0.02\n",
      "iteration: 334920 loss: 0.0012 lr: 0.02\n",
      "iteration: 334930 loss: 0.0011 lr: 0.02\n",
      "iteration: 334940 loss: 0.0011 lr: 0.02\n",
      "iteration: 334950 loss: 0.0014 lr: 0.02\n",
      "iteration: 334960 loss: 0.0016 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 334970 loss: 0.0020 lr: 0.02\n",
      "iteration: 334980 loss: 0.0014 lr: 0.02\n",
      "iteration: 334990 loss: 0.0026 lr: 0.02\n",
      "iteration: 335000 loss: 0.0020 lr: 0.02\n",
      "iteration: 335010 loss: 0.0030 lr: 0.02\n",
      "iteration: 335020 loss: 0.0018 lr: 0.02\n",
      "iteration: 335030 loss: 0.0019 lr: 0.02\n",
      "iteration: 335040 loss: 0.0018 lr: 0.02\n",
      "iteration: 335050 loss: 0.0015 lr: 0.02\n",
      "iteration: 335060 loss: 0.0018 lr: 0.02\n",
      "iteration: 335070 loss: 0.0018 lr: 0.02\n",
      "iteration: 335080 loss: 0.0020 lr: 0.02\n",
      "iteration: 335090 loss: 0.0019 lr: 0.02\n",
      "iteration: 335100 loss: 0.0018 lr: 0.02\n",
      "iteration: 335110 loss: 0.0020 lr: 0.02\n",
      "iteration: 335120 loss: 0.0016 lr: 0.02\n",
      "iteration: 335130 loss: 0.0026 lr: 0.02\n",
      "iteration: 335140 loss: 0.0018 lr: 0.02\n",
      "iteration: 335150 loss: 0.0019 lr: 0.02\n",
      "iteration: 335160 loss: 0.0015 lr: 0.02\n",
      "iteration: 335170 loss: 0.0021 lr: 0.02\n",
      "iteration: 335180 loss: 0.0016 lr: 0.02\n",
      "iteration: 335190 loss: 0.0017 lr: 0.02\n",
      "iteration: 335200 loss: 0.0016 lr: 0.02\n",
      "iteration: 335210 loss: 0.0018 lr: 0.02\n",
      "iteration: 335220 loss: 0.0017 lr: 0.02\n",
      "iteration: 335230 loss: 0.0022 lr: 0.02\n",
      "iteration: 335240 loss: 0.0013 lr: 0.02\n",
      "iteration: 335250 loss: 0.0024 lr: 0.02\n",
      "iteration: 335260 loss: 0.0017 lr: 0.02\n",
      "iteration: 335270 loss: 0.0020 lr: 0.02\n",
      "iteration: 335280 loss: 0.0013 lr: 0.02\n",
      "iteration: 335290 loss: 0.0015 lr: 0.02\n",
      "iteration: 335300 loss: 0.0012 lr: 0.02\n",
      "iteration: 335310 loss: 0.0014 lr: 0.02\n",
      "iteration: 335320 loss: 0.0016 lr: 0.02\n",
      "iteration: 335330 loss: 0.0037 lr: 0.02\n",
      "iteration: 335340 loss: 0.0020 lr: 0.02\n",
      "iteration: 335350 loss: 0.0016 lr: 0.02\n",
      "iteration: 335360 loss: 0.0016 lr: 0.02\n",
      "iteration: 335370 loss: 0.0015 lr: 0.02\n",
      "iteration: 335380 loss: 0.0016 lr: 0.02\n",
      "iteration: 335390 loss: 0.0019 lr: 0.02\n",
      "iteration: 335400 loss: 0.0017 lr: 0.02\n",
      "iteration: 335410 loss: 0.0013 lr: 0.02\n",
      "iteration: 335420 loss: 0.0022 lr: 0.02\n",
      "iteration: 335430 loss: 0.0020 lr: 0.02\n",
      "iteration: 335440 loss: 0.0022 lr: 0.02\n",
      "iteration: 335450 loss: 0.0020 lr: 0.02\n",
      "iteration: 335460 loss: 0.0014 lr: 0.02\n",
      "iteration: 335470 loss: 0.0019 lr: 0.02\n",
      "iteration: 335480 loss: 0.0015 lr: 0.02\n",
      "iteration: 335490 loss: 0.0019 lr: 0.02\n",
      "iteration: 335500 loss: 0.0017 lr: 0.02\n",
      "iteration: 335510 loss: 0.0018 lr: 0.02\n",
      "iteration: 335520 loss: 0.0018 lr: 0.02\n",
      "iteration: 335530 loss: 0.0014 lr: 0.02\n",
      "iteration: 335540 loss: 0.0016 lr: 0.02\n",
      "iteration: 335550 loss: 0.0020 lr: 0.02\n",
      "iteration: 335560 loss: 0.0020 lr: 0.02\n",
      "iteration: 335570 loss: 0.0016 lr: 0.02\n",
      "iteration: 335580 loss: 0.0015 lr: 0.02\n",
      "iteration: 335590 loss: 0.0019 lr: 0.02\n",
      "iteration: 335600 loss: 0.0014 lr: 0.02\n",
      "iteration: 335610 loss: 0.0015 lr: 0.02\n",
      "iteration: 335620 loss: 0.0012 lr: 0.02\n",
      "iteration: 335630 loss: 0.0030 lr: 0.02\n",
      "iteration: 335640 loss: 0.0021 lr: 0.02\n",
      "iteration: 335650 loss: 0.0016 lr: 0.02\n",
      "iteration: 335660 loss: 0.0016 lr: 0.02\n",
      "iteration: 335670 loss: 0.0014 lr: 0.02\n",
      "iteration: 335680 loss: 0.0014 lr: 0.02\n",
      "iteration: 335690 loss: 0.0018 lr: 0.02\n",
      "iteration: 335700 loss: 0.0017 lr: 0.02\n",
      "iteration: 335710 loss: 0.0022 lr: 0.02\n",
      "iteration: 335720 loss: 0.0020 lr: 0.02\n",
      "iteration: 335730 loss: 0.0026 lr: 0.02\n",
      "iteration: 335740 loss: 0.0015 lr: 0.02\n",
      "iteration: 335750 loss: 0.0012 lr: 0.02\n",
      "iteration: 335760 loss: 0.0023 lr: 0.02\n",
      "iteration: 335770 loss: 0.0023 lr: 0.02\n",
      "iteration: 335780 loss: 0.0015 lr: 0.02\n",
      "iteration: 335790 loss: 0.0015 lr: 0.02\n",
      "iteration: 335800 loss: 0.0018 lr: 0.02\n",
      "iteration: 335810 loss: 0.0013 lr: 0.02\n",
      "iteration: 335820 loss: 0.0017 lr: 0.02\n",
      "iteration: 335830 loss: 0.0020 lr: 0.02\n",
      "iteration: 335840 loss: 0.0014 lr: 0.02\n",
      "iteration: 335850 loss: 0.0012 lr: 0.02\n",
      "iteration: 335860 loss: 0.0018 lr: 0.02\n",
      "iteration: 335870 loss: 0.0014 lr: 0.02\n",
      "iteration: 335880 loss: 0.0012 lr: 0.02\n",
      "iteration: 335890 loss: 0.0022 lr: 0.02\n",
      "iteration: 335900 loss: 0.0014 lr: 0.02\n",
      "iteration: 335910 loss: 0.0028 lr: 0.02\n",
      "iteration: 335920 loss: 0.0020 lr: 0.02\n",
      "iteration: 335930 loss: 0.0023 lr: 0.02\n",
      "iteration: 335940 loss: 0.0013 lr: 0.02\n",
      "iteration: 335950 loss: 0.0016 lr: 0.02\n",
      "iteration: 335960 loss: 0.0015 lr: 0.02\n",
      "iteration: 335970 loss: 0.0019 lr: 0.02\n",
      "iteration: 335980 loss: 0.0022 lr: 0.02\n",
      "iteration: 335990 loss: 0.0018 lr: 0.02\n",
      "iteration: 336000 loss: 0.0017 lr: 0.02\n",
      "iteration: 336010 loss: 0.0018 lr: 0.02\n",
      "iteration: 336020 loss: 0.0018 lr: 0.02\n",
      "iteration: 336030 loss: 0.0027 lr: 0.02\n",
      "iteration: 336040 loss: 0.0031 lr: 0.02\n",
      "iteration: 336050 loss: 0.0020 lr: 0.02\n",
      "iteration: 336060 loss: 0.0018 lr: 0.02\n",
      "iteration: 336070 loss: 0.0018 lr: 0.02\n",
      "iteration: 336080 loss: 0.0013 lr: 0.02\n",
      "iteration: 336090 loss: 0.0018 lr: 0.02\n",
      "iteration: 336100 loss: 0.0017 lr: 0.02\n",
      "iteration: 336110 loss: 0.0018 lr: 0.02\n",
      "iteration: 336120 loss: 0.0021 lr: 0.02\n",
      "iteration: 336130 loss: 0.0020 lr: 0.02\n",
      "iteration: 336140 loss: 0.0018 lr: 0.02\n",
      "iteration: 336150 loss: 0.0023 lr: 0.02\n",
      "iteration: 336160 loss: 0.0017 lr: 0.02\n",
      "iteration: 336170 loss: 0.0018 lr: 0.02\n",
      "iteration: 336180 loss: 0.0014 lr: 0.02\n",
      "iteration: 336190 loss: 0.0027 lr: 0.02\n",
      "iteration: 336200 loss: 0.0014 lr: 0.02\n",
      "iteration: 336210 loss: 0.0017 lr: 0.02\n",
      "iteration: 336220 loss: 0.0017 lr: 0.02\n",
      "iteration: 336230 loss: 0.0016 lr: 0.02\n",
      "iteration: 336240 loss: 0.0015 lr: 0.02\n",
      "iteration: 336250 loss: 0.0026 lr: 0.02\n",
      "iteration: 336260 loss: 0.0018 lr: 0.02\n",
      "iteration: 336270 loss: 0.0013 lr: 0.02\n",
      "iteration: 336280 loss: 0.0013 lr: 0.02\n",
      "iteration: 336290 loss: 0.0016 lr: 0.02\n",
      "iteration: 336300 loss: 0.0013 lr: 0.02\n",
      "iteration: 336310 loss: 0.0015 lr: 0.02\n",
      "iteration: 336320 loss: 0.0011 lr: 0.02\n",
      "iteration: 336330 loss: 0.0018 lr: 0.02\n",
      "iteration: 336340 loss: 0.0017 lr: 0.02\n",
      "iteration: 336350 loss: 0.0021 lr: 0.02\n",
      "iteration: 336360 loss: 0.0014 lr: 0.02\n",
      "iteration: 336370 loss: 0.0015 lr: 0.02\n",
      "iteration: 336380 loss: 0.0018 lr: 0.02\n",
      "iteration: 336390 loss: 0.0015 lr: 0.02\n",
      "iteration: 336400 loss: 0.0018 lr: 0.02\n",
      "iteration: 336410 loss: 0.0014 lr: 0.02\n",
      "iteration: 336420 loss: 0.0017 lr: 0.02\n",
      "iteration: 336430 loss: 0.0014 lr: 0.02\n",
      "iteration: 336440 loss: 0.0014 lr: 0.02\n",
      "iteration: 336450 loss: 0.0014 lr: 0.02\n",
      "iteration: 336460 loss: 0.0015 lr: 0.02\n",
      "iteration: 336470 loss: 0.0020 lr: 0.02\n",
      "iteration: 336480 loss: 0.0037 lr: 0.02\n",
      "iteration: 336490 loss: 0.0016 lr: 0.02\n",
      "iteration: 336500 loss: 0.0017 lr: 0.02\n",
      "iteration: 336510 loss: 0.0022 lr: 0.02\n",
      "iteration: 336520 loss: 0.0025 lr: 0.02\n",
      "iteration: 336530 loss: 0.0021 lr: 0.02\n",
      "iteration: 336540 loss: 0.0013 lr: 0.02\n",
      "iteration: 336550 loss: 0.0015 lr: 0.02\n",
      "iteration: 336560 loss: 0.0030 lr: 0.02\n",
      "iteration: 336570 loss: 0.0016 lr: 0.02\n",
      "iteration: 336580 loss: 0.0031 lr: 0.02\n",
      "iteration: 336590 loss: 0.0020 lr: 0.02\n",
      "iteration: 336600 loss: 0.0011 lr: 0.02\n",
      "iteration: 336610 loss: 0.0019 lr: 0.02\n",
      "iteration: 336620 loss: 0.0022 lr: 0.02\n",
      "iteration: 336630 loss: 0.0019 lr: 0.02\n",
      "iteration: 336640 loss: 0.0013 lr: 0.02\n",
      "iteration: 336650 loss: 0.0018 lr: 0.02\n",
      "iteration: 336660 loss: 0.0013 lr: 0.02\n",
      "iteration: 336670 loss: 0.0021 lr: 0.02\n",
      "iteration: 336680 loss: 0.0018 lr: 0.02\n",
      "iteration: 336690 loss: 0.0012 lr: 0.02\n",
      "iteration: 336700 loss: 0.0015 lr: 0.02\n",
      "iteration: 336710 loss: 0.0017 lr: 0.02\n",
      "iteration: 336720 loss: 0.0016 lr: 0.02\n",
      "iteration: 336730 loss: 0.0017 lr: 0.02\n",
      "iteration: 336740 loss: 0.0014 lr: 0.02\n",
      "iteration: 336750 loss: 0.0014 lr: 0.02\n",
      "iteration: 336760 loss: 0.0014 lr: 0.02\n",
      "iteration: 336770 loss: 0.0009 lr: 0.02\n",
      "iteration: 336780 loss: 0.0011 lr: 0.02\n",
      "iteration: 336790 loss: 0.0014 lr: 0.02\n",
      "iteration: 336800 loss: 0.0018 lr: 0.02\n",
      "iteration: 336810 loss: 0.0018 lr: 0.02\n",
      "iteration: 336820 loss: 0.0013 lr: 0.02\n",
      "iteration: 336830 loss: 0.0023 lr: 0.02\n",
      "iteration: 336840 loss: 0.0013 lr: 0.02\n",
      "iteration: 336850 loss: 0.0015 lr: 0.02\n",
      "iteration: 336860 loss: 0.0017 lr: 0.02\n",
      "iteration: 336870 loss: 0.0014 lr: 0.02\n",
      "iteration: 336880 loss: 0.0013 lr: 0.02\n",
      "iteration: 336890 loss: 0.0016 lr: 0.02\n",
      "iteration: 336900 loss: 0.0017 lr: 0.02\n",
      "iteration: 336910 loss: 0.0013 lr: 0.02\n",
      "iteration: 336920 loss: 0.0020 lr: 0.02\n",
      "iteration: 336930 loss: 0.0017 lr: 0.02\n",
      "iteration: 336940 loss: 0.0014 lr: 0.02\n",
      "iteration: 336950 loss: 0.0030 lr: 0.02\n",
      "iteration: 336960 loss: 0.0010 lr: 0.02\n",
      "iteration: 336970 loss: 0.0021 lr: 0.02\n",
      "iteration: 336980 loss: 0.0021 lr: 0.02\n",
      "iteration: 336990 loss: 0.0016 lr: 0.02\n",
      "iteration: 337000 loss: 0.0012 lr: 0.02\n",
      "iteration: 337010 loss: 0.0012 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 337020 loss: 0.0018 lr: 0.02\n",
      "iteration: 337030 loss: 0.0017 lr: 0.02\n",
      "iteration: 337040 loss: 0.0015 lr: 0.02\n",
      "iteration: 337050 loss: 0.0014 lr: 0.02\n",
      "iteration: 337060 loss: 0.0017 lr: 0.02\n",
      "iteration: 337070 loss: 0.0011 lr: 0.02\n",
      "iteration: 337080 loss: 0.0021 lr: 0.02\n",
      "iteration: 337090 loss: 0.0018 lr: 0.02\n",
      "iteration: 337100 loss: 0.0017 lr: 0.02\n",
      "iteration: 337110 loss: 0.0017 lr: 0.02\n",
      "iteration: 337120 loss: 0.0015 lr: 0.02\n",
      "iteration: 337130 loss: 0.0018 lr: 0.02\n",
      "iteration: 337140 loss: 0.0018 lr: 0.02\n",
      "iteration: 337150 loss: 0.0018 lr: 0.02\n",
      "iteration: 337160 loss: 0.0015 lr: 0.02\n",
      "iteration: 337170 loss: 0.0024 lr: 0.02\n",
      "iteration: 337180 loss: 0.0019 lr: 0.02\n",
      "iteration: 337190 loss: 0.0018 lr: 0.02\n",
      "iteration: 337200 loss: 0.0014 lr: 0.02\n",
      "iteration: 337210 loss: 0.0015 lr: 0.02\n",
      "iteration: 337220 loss: 0.0013 lr: 0.02\n",
      "iteration: 337230 loss: 0.0014 lr: 0.02\n",
      "iteration: 337240 loss: 0.0017 lr: 0.02\n",
      "iteration: 337250 loss: 0.0020 lr: 0.02\n",
      "iteration: 337260 loss: 0.0021 lr: 0.02\n",
      "iteration: 337270 loss: 0.0014 lr: 0.02\n",
      "iteration: 337280 loss: 0.0016 lr: 0.02\n",
      "iteration: 337290 loss: 0.0018 lr: 0.02\n",
      "iteration: 337300 loss: 0.0021 lr: 0.02\n",
      "iteration: 337310 loss: 0.0020 lr: 0.02\n",
      "iteration: 337320 loss: 0.0015 lr: 0.02\n",
      "iteration: 337330 loss: 0.0030 lr: 0.02\n",
      "iteration: 337340 loss: 0.0014 lr: 0.02\n",
      "iteration: 337350 loss: 0.0022 lr: 0.02\n",
      "iteration: 337360 loss: 0.0013 lr: 0.02\n",
      "iteration: 337370 loss: 0.0014 lr: 0.02\n",
      "iteration: 337380 loss: 0.0014 lr: 0.02\n",
      "iteration: 337390 loss: 0.0013 lr: 0.02\n",
      "iteration: 337400 loss: 0.0013 lr: 0.02\n",
      "iteration: 337410 loss: 0.0018 lr: 0.02\n",
      "iteration: 337420 loss: 0.0022 lr: 0.02\n",
      "iteration: 337430 loss: 0.0025 lr: 0.02\n",
      "iteration: 337440 loss: 0.0019 lr: 0.02\n",
      "iteration: 337450 loss: 0.0018 lr: 0.02\n",
      "iteration: 337460 loss: 0.0011 lr: 0.02\n",
      "iteration: 337470 loss: 0.0022 lr: 0.02\n",
      "iteration: 337480 loss: 0.0013 lr: 0.02\n",
      "iteration: 337490 loss: 0.0014 lr: 0.02\n",
      "iteration: 337500 loss: 0.0016 lr: 0.02\n",
      "iteration: 337510 loss: 0.0017 lr: 0.02\n",
      "iteration: 337520 loss: 0.0013 lr: 0.02\n",
      "iteration: 337530 loss: 0.0029 lr: 0.02\n",
      "iteration: 337540 loss: 0.0017 lr: 0.02\n",
      "iteration: 337550 loss: 0.0023 lr: 0.02\n",
      "iteration: 337560 loss: 0.0017 lr: 0.02\n",
      "iteration: 337570 loss: 0.0013 lr: 0.02\n",
      "iteration: 337580 loss: 0.0014 lr: 0.02\n",
      "iteration: 337590 loss: 0.0020 lr: 0.02\n",
      "iteration: 337600 loss: 0.0016 lr: 0.02\n",
      "iteration: 337610 loss: 0.0015 lr: 0.02\n",
      "iteration: 337620 loss: 0.0013 lr: 0.02\n",
      "iteration: 337630 loss: 0.0019 lr: 0.02\n",
      "iteration: 337640 loss: 0.0018 lr: 0.02\n",
      "iteration: 337650 loss: 0.0013 lr: 0.02\n",
      "iteration: 337660 loss: 0.0014 lr: 0.02\n",
      "iteration: 337670 loss: 0.0025 lr: 0.02\n",
      "iteration: 337680 loss: 0.0023 lr: 0.02\n",
      "iteration: 337690 loss: 0.0013 lr: 0.02\n",
      "iteration: 337700 loss: 0.0023 lr: 0.02\n",
      "iteration: 337710 loss: 0.0022 lr: 0.02\n",
      "iteration: 337720 loss: 0.0022 lr: 0.02\n",
      "iteration: 337730 loss: 0.0017 lr: 0.02\n",
      "iteration: 337740 loss: 0.0018 lr: 0.02\n",
      "iteration: 337750 loss: 0.0019 lr: 0.02\n",
      "iteration: 337760 loss: 0.0014 lr: 0.02\n",
      "iteration: 337770 loss: 0.0018 lr: 0.02\n",
      "iteration: 337780 loss: 0.0017 lr: 0.02\n",
      "iteration: 337790 loss: 0.0013 lr: 0.02\n",
      "iteration: 337800 loss: 0.0018 lr: 0.02\n",
      "iteration: 337810 loss: 0.0017 lr: 0.02\n",
      "iteration: 337820 loss: 0.0020 lr: 0.02\n",
      "iteration: 337830 loss: 0.0014 lr: 0.02\n",
      "iteration: 337840 loss: 0.0016 lr: 0.02\n",
      "iteration: 337850 loss: 0.0010 lr: 0.02\n",
      "iteration: 337860 loss: 0.0019 lr: 0.02\n",
      "iteration: 337870 loss: 0.0032 lr: 0.02\n",
      "iteration: 337880 loss: 0.0014 lr: 0.02\n",
      "iteration: 337890 loss: 0.0024 lr: 0.02\n",
      "iteration: 337900 loss: 0.0018 lr: 0.02\n",
      "iteration: 337910 loss: 0.0019 lr: 0.02\n",
      "iteration: 337920 loss: 0.0016 lr: 0.02\n",
      "iteration: 337930 loss: 0.0012 lr: 0.02\n",
      "iteration: 337940 loss: 0.0016 lr: 0.02\n",
      "iteration: 337950 loss: 0.0014 lr: 0.02\n",
      "iteration: 337960 loss: 0.0015 lr: 0.02\n",
      "iteration: 337970 loss: 0.0025 lr: 0.02\n",
      "iteration: 337980 loss: 0.0015 lr: 0.02\n",
      "iteration: 337990 loss: 0.0016 lr: 0.02\n",
      "iteration: 338000 loss: 0.0022 lr: 0.02\n",
      "iteration: 338010 loss: 0.0015 lr: 0.02\n",
      "iteration: 338020 loss: 0.0015 lr: 0.02\n",
      "iteration: 338030 loss: 0.0014 lr: 0.02\n",
      "iteration: 338040 loss: 0.0019 lr: 0.02\n",
      "iteration: 338050 loss: 0.0016 lr: 0.02\n",
      "iteration: 338060 loss: 0.0018 lr: 0.02\n",
      "iteration: 338070 loss: 0.0014 lr: 0.02\n",
      "iteration: 338080 loss: 0.0025 lr: 0.02\n",
      "iteration: 338090 loss: 0.0032 lr: 0.02\n",
      "iteration: 338100 loss: 0.0022 lr: 0.02\n",
      "iteration: 338110 loss: 0.0018 lr: 0.02\n",
      "iteration: 338120 loss: 0.0023 lr: 0.02\n",
      "iteration: 338130 loss: 0.0016 lr: 0.02\n",
      "iteration: 338140 loss: 0.0024 lr: 0.02\n",
      "iteration: 338150 loss: 0.0013 lr: 0.02\n",
      "iteration: 338160 loss: 0.0022 lr: 0.02\n",
      "iteration: 338170 loss: 0.0016 lr: 0.02\n",
      "iteration: 338180 loss: 0.0018 lr: 0.02\n",
      "iteration: 338190 loss: 0.0018 lr: 0.02\n",
      "iteration: 338200 loss: 0.0018 lr: 0.02\n",
      "iteration: 338210 loss: 0.0016 lr: 0.02\n",
      "iteration: 338220 loss: 0.0020 lr: 0.02\n",
      "iteration: 338230 loss: 0.0014 lr: 0.02\n",
      "iteration: 338240 loss: 0.0011 lr: 0.02\n",
      "iteration: 338250 loss: 0.0014 lr: 0.02\n",
      "iteration: 338260 loss: 0.0024 lr: 0.02\n",
      "iteration: 338270 loss: 0.0017 lr: 0.02\n",
      "iteration: 338280 loss: 0.0015 lr: 0.02\n",
      "iteration: 338290 loss: 0.0025 lr: 0.02\n",
      "iteration: 338300 loss: 0.0013 lr: 0.02\n",
      "iteration: 338310 loss: 0.0023 lr: 0.02\n",
      "iteration: 338320 loss: 0.0018 lr: 0.02\n",
      "iteration: 338330 loss: 0.0019 lr: 0.02\n",
      "iteration: 338340 loss: 0.0018 lr: 0.02\n",
      "iteration: 338350 loss: 0.0018 lr: 0.02\n",
      "iteration: 338360 loss: 0.0016 lr: 0.02\n",
      "iteration: 338370 loss: 0.0021 lr: 0.02\n",
      "iteration: 338380 loss: 0.0013 lr: 0.02\n",
      "iteration: 338390 loss: 0.0016 lr: 0.02\n",
      "iteration: 338400 loss: 0.0016 lr: 0.02\n",
      "iteration: 338410 loss: 0.0021 lr: 0.02\n",
      "iteration: 338420 loss: 0.0013 lr: 0.02\n",
      "iteration: 338430 loss: 0.0010 lr: 0.02\n",
      "iteration: 338440 loss: 0.0016 lr: 0.02\n",
      "iteration: 338450 loss: 0.0015 lr: 0.02\n",
      "iteration: 338460 loss: 0.0012 lr: 0.02\n",
      "iteration: 338470 loss: 0.0016 lr: 0.02\n",
      "iteration: 338480 loss: 0.0017 lr: 0.02\n",
      "iteration: 338490 loss: 0.0016 lr: 0.02\n",
      "iteration: 338500 loss: 0.0016 lr: 0.02\n",
      "iteration: 338510 loss: 0.0016 lr: 0.02\n",
      "iteration: 338520 loss: 0.0011 lr: 0.02\n",
      "iteration: 338530 loss: 0.0012 lr: 0.02\n",
      "iteration: 338540 loss: 0.0017 lr: 0.02\n",
      "iteration: 338550 loss: 0.0011 lr: 0.02\n",
      "iteration: 338560 loss: 0.0017 lr: 0.02\n",
      "iteration: 338570 loss: 0.0012 lr: 0.02\n",
      "iteration: 338580 loss: 0.0013 lr: 0.02\n",
      "iteration: 338590 loss: 0.0013 lr: 0.02\n",
      "iteration: 338600 loss: 0.0018 lr: 0.02\n",
      "iteration: 338610 loss: 0.0022 lr: 0.02\n",
      "iteration: 338620 loss: 0.0019 lr: 0.02\n",
      "iteration: 338630 loss: 0.0018 lr: 0.02\n",
      "iteration: 338640 loss: 0.0016 lr: 0.02\n",
      "iteration: 338650 loss: 0.0016 lr: 0.02\n",
      "iteration: 338660 loss: 0.0017 lr: 0.02\n",
      "iteration: 338670 loss: 0.0010 lr: 0.02\n",
      "iteration: 338680 loss: 0.0013 lr: 0.02\n",
      "iteration: 338690 loss: 0.0016 lr: 0.02\n",
      "iteration: 338700 loss: 0.0016 lr: 0.02\n",
      "iteration: 338710 loss: 0.0016 lr: 0.02\n",
      "iteration: 338720 loss: 0.0013 lr: 0.02\n",
      "iteration: 338730 loss: 0.0017 lr: 0.02\n",
      "iteration: 338740 loss: 0.0026 lr: 0.02\n",
      "iteration: 338750 loss: 0.0017 lr: 0.02\n",
      "iteration: 338760 loss: 0.0016 lr: 0.02\n",
      "iteration: 338770 loss: 0.0018 lr: 0.02\n",
      "iteration: 338780 loss: 0.0025 lr: 0.02\n",
      "iteration: 338790 loss: 0.0015 lr: 0.02\n",
      "iteration: 338800 loss: 0.0018 lr: 0.02\n",
      "iteration: 338810 loss: 0.0013 lr: 0.02\n",
      "iteration: 338820 loss: 0.0019 lr: 0.02\n",
      "iteration: 338830 loss: 0.0013 lr: 0.02\n",
      "iteration: 338840 loss: 0.0015 lr: 0.02\n",
      "iteration: 338850 loss: 0.0018 lr: 0.02\n",
      "iteration: 338860 loss: 0.0020 lr: 0.02\n",
      "iteration: 338870 loss: 0.0011 lr: 0.02\n",
      "iteration: 338880 loss: 0.0015 lr: 0.02\n",
      "iteration: 338890 loss: 0.0014 lr: 0.02\n",
      "iteration: 338900 loss: 0.0015 lr: 0.02\n",
      "iteration: 338910 loss: 0.0016 lr: 0.02\n",
      "iteration: 338920 loss: 0.0016 lr: 0.02\n",
      "iteration: 338930 loss: 0.0015 lr: 0.02\n",
      "iteration: 338940 loss: 0.0019 lr: 0.02\n",
      "iteration: 338950 loss: 0.0017 lr: 0.02\n",
      "iteration: 338960 loss: 0.0019 lr: 0.02\n",
      "iteration: 338970 loss: 0.0015 lr: 0.02\n",
      "iteration: 338980 loss: 0.0030 lr: 0.02\n",
      "iteration: 338990 loss: 0.0018 lr: 0.02\n",
      "iteration: 339000 loss: 0.0014 lr: 0.02\n",
      "iteration: 339010 loss: 0.0024 lr: 0.02\n",
      "iteration: 339020 loss: 0.0014 lr: 0.02\n",
      "iteration: 339030 loss: 0.0013 lr: 0.02\n",
      "iteration: 339040 loss: 0.0018 lr: 0.02\n",
      "iteration: 339050 loss: 0.0013 lr: 0.02\n",
      "iteration: 339060 loss: 0.0031 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 339070 loss: 0.0022 lr: 0.02\n",
      "iteration: 339080 loss: 0.0018 lr: 0.02\n",
      "iteration: 339090 loss: 0.0020 lr: 0.02\n",
      "iteration: 339100 loss: 0.0013 lr: 0.02\n",
      "iteration: 339110 loss: 0.0023 lr: 0.02\n",
      "iteration: 339120 loss: 0.0019 lr: 0.02\n",
      "iteration: 339130 loss: 0.0022 lr: 0.02\n",
      "iteration: 339140 loss: 0.0016 lr: 0.02\n",
      "iteration: 339150 loss: 0.0016 lr: 0.02\n",
      "iteration: 339160 loss: 0.0024 lr: 0.02\n",
      "iteration: 339170 loss: 0.0015 lr: 0.02\n",
      "iteration: 339180 loss: 0.0015 lr: 0.02\n",
      "iteration: 339190 loss: 0.0013 lr: 0.02\n",
      "iteration: 339200 loss: 0.0014 lr: 0.02\n",
      "iteration: 339210 loss: 0.0014 lr: 0.02\n",
      "iteration: 339220 loss: 0.0021 lr: 0.02\n",
      "iteration: 339230 loss: 0.0010 lr: 0.02\n",
      "iteration: 339240 loss: 0.0015 lr: 0.02\n",
      "iteration: 339250 loss: 0.0014 lr: 0.02\n",
      "iteration: 339260 loss: 0.0012 lr: 0.02\n",
      "iteration: 339270 loss: 0.0027 lr: 0.02\n",
      "iteration: 339280 loss: 0.0017 lr: 0.02\n",
      "iteration: 339290 loss: 0.0015 lr: 0.02\n",
      "iteration: 339300 loss: 0.0013 lr: 0.02\n",
      "iteration: 339310 loss: 0.0013 lr: 0.02\n",
      "iteration: 339320 loss: 0.0013 lr: 0.02\n",
      "iteration: 339330 loss: 0.0023 lr: 0.02\n",
      "iteration: 339340 loss: 0.0018 lr: 0.02\n",
      "iteration: 339350 loss: 0.0015 lr: 0.02\n",
      "iteration: 339360 loss: 0.0016 lr: 0.02\n",
      "iteration: 339370 loss: 0.0017 lr: 0.02\n",
      "iteration: 339380 loss: 0.0015 lr: 0.02\n",
      "iteration: 339390 loss: 0.0022 lr: 0.02\n",
      "iteration: 339400 loss: 0.0013 lr: 0.02\n",
      "iteration: 339410 loss: 0.0015 lr: 0.02\n",
      "iteration: 339420 loss: 0.0029 lr: 0.02\n",
      "iteration: 339430 loss: 0.0012 lr: 0.02\n",
      "iteration: 339440 loss: 0.0019 lr: 0.02\n",
      "iteration: 339450 loss: 0.0020 lr: 0.02\n",
      "iteration: 339460 loss: 0.0017 lr: 0.02\n",
      "iteration: 339470 loss: 0.0018 lr: 0.02\n",
      "iteration: 339480 loss: 0.0017 lr: 0.02\n",
      "iteration: 339490 loss: 0.0022 lr: 0.02\n",
      "iteration: 339500 loss: 0.0020 lr: 0.02\n",
      "iteration: 339510 loss: 0.0014 lr: 0.02\n",
      "iteration: 339520 loss: 0.0015 lr: 0.02\n",
      "iteration: 339530 loss: 0.0020 lr: 0.02\n",
      "iteration: 339540 loss: 0.0017 lr: 0.02\n",
      "iteration: 339550 loss: 0.0011 lr: 0.02\n",
      "iteration: 339560 loss: 0.0027 lr: 0.02\n",
      "iteration: 339570 loss: 0.0016 lr: 0.02\n",
      "iteration: 339580 loss: 0.0024 lr: 0.02\n",
      "iteration: 339590 loss: 0.0021 lr: 0.02\n",
      "iteration: 339600 loss: 0.0030 lr: 0.02\n",
      "iteration: 339610 loss: 0.0012 lr: 0.02\n",
      "iteration: 339620 loss: 0.0017 lr: 0.02\n",
      "iteration: 339630 loss: 0.0020 lr: 0.02\n",
      "iteration: 339640 loss: 0.0016 lr: 0.02\n",
      "iteration: 339650 loss: 0.0022 lr: 0.02\n",
      "iteration: 339660 loss: 0.0020 lr: 0.02\n",
      "iteration: 339670 loss: 0.0015 lr: 0.02\n",
      "iteration: 339680 loss: 0.0010 lr: 0.02\n",
      "iteration: 339690 loss: 0.0020 lr: 0.02\n",
      "iteration: 339700 loss: 0.0019 lr: 0.02\n",
      "iteration: 339710 loss: 0.0014 lr: 0.02\n",
      "iteration: 339720 loss: 0.0020 lr: 0.02\n",
      "iteration: 339730 loss: 0.0021 lr: 0.02\n",
      "iteration: 339740 loss: 0.0018 lr: 0.02\n",
      "iteration: 339750 loss: 0.0020 lr: 0.02\n",
      "iteration: 339760 loss: 0.0013 lr: 0.02\n",
      "iteration: 339770 loss: 0.0018 lr: 0.02\n",
      "iteration: 339780 loss: 0.0014 lr: 0.02\n",
      "iteration: 339790 loss: 0.0013 lr: 0.02\n",
      "iteration: 339800 loss: 0.0019 lr: 0.02\n",
      "iteration: 339810 loss: 0.0015 lr: 0.02\n",
      "iteration: 339820 loss: 0.0018 lr: 0.02\n",
      "iteration: 339830 loss: 0.0016 lr: 0.02\n",
      "iteration: 339840 loss: 0.0021 lr: 0.02\n",
      "iteration: 339850 loss: 0.0030 lr: 0.02\n",
      "iteration: 339860 loss: 0.0016 lr: 0.02\n",
      "iteration: 339870 loss: 0.0019 lr: 0.02\n",
      "iteration: 339880 loss: 0.0017 lr: 0.02\n",
      "iteration: 339890 loss: 0.0019 lr: 0.02\n",
      "iteration: 339900 loss: 0.0017 lr: 0.02\n",
      "iteration: 339910 loss: 0.0012 lr: 0.02\n",
      "iteration: 339920 loss: 0.0022 lr: 0.02\n",
      "iteration: 339930 loss: 0.0018 lr: 0.02\n",
      "iteration: 339940 loss: 0.0017 lr: 0.02\n",
      "iteration: 339950 loss: 0.0013 lr: 0.02\n",
      "iteration: 339960 loss: 0.0025 lr: 0.02\n",
      "iteration: 339970 loss: 0.0017 lr: 0.02\n",
      "iteration: 339980 loss: 0.0033 lr: 0.02\n",
      "iteration: 339990 loss: 0.0025 lr: 0.02\n",
      "iteration: 340000 loss: 0.0021 lr: 0.02\n",
      "iteration: 340010 loss: 0.0012 lr: 0.02\n",
      "iteration: 340020 loss: 0.0019 lr: 0.02\n",
      "iteration: 340030 loss: 0.0031 lr: 0.02\n",
      "iteration: 340040 loss: 0.0013 lr: 0.02\n",
      "iteration: 340050 loss: 0.0016 lr: 0.02\n",
      "iteration: 340060 loss: 0.0017 lr: 0.02\n",
      "iteration: 340070 loss: 0.0015 lr: 0.02\n",
      "iteration: 340080 loss: 0.0019 lr: 0.02\n",
      "iteration: 340090 loss: 0.0013 lr: 0.02\n",
      "iteration: 340100 loss: 0.0020 lr: 0.02\n",
      "iteration: 340110 loss: 0.0013 lr: 0.02\n",
      "iteration: 340120 loss: 0.0019 lr: 0.02\n",
      "iteration: 340130 loss: 0.0012 lr: 0.02\n",
      "iteration: 340140 loss: 0.0015 lr: 0.02\n",
      "iteration: 340150 loss: 0.0020 lr: 0.02\n",
      "iteration: 340160 loss: 0.0015 lr: 0.02\n",
      "iteration: 340170 loss: 0.0016 lr: 0.02\n",
      "iteration: 340180 loss: 0.0014 lr: 0.02\n",
      "iteration: 340190 loss: 0.0017 lr: 0.02\n",
      "iteration: 340200 loss: 0.0020 lr: 0.02\n",
      "iteration: 340210 loss: 0.0018 lr: 0.02\n",
      "iteration: 340220 loss: 0.0021 lr: 0.02\n",
      "iteration: 340230 loss: 0.0036 lr: 0.02\n",
      "iteration: 340240 loss: 0.0015 lr: 0.02\n",
      "iteration: 340250 loss: 0.0021 lr: 0.02\n",
      "iteration: 340260 loss: 0.0013 lr: 0.02\n",
      "iteration: 340270 loss: 0.0016 lr: 0.02\n",
      "iteration: 340280 loss: 0.0020 lr: 0.02\n",
      "iteration: 340290 loss: 0.0011 lr: 0.02\n",
      "iteration: 340300 loss: 0.0020 lr: 0.02\n",
      "iteration: 340310 loss: 0.0021 lr: 0.02\n",
      "iteration: 340320 loss: 0.0019 lr: 0.02\n",
      "iteration: 340330 loss: 0.0015 lr: 0.02\n",
      "iteration: 340340 loss: 0.0014 lr: 0.02\n",
      "iteration: 340350 loss: 0.0013 lr: 0.02\n",
      "iteration: 340360 loss: 0.0019 lr: 0.02\n",
      "iteration: 340370 loss: 0.0012 lr: 0.02\n",
      "iteration: 340380 loss: 0.0014 lr: 0.02\n",
      "iteration: 340390 loss: 0.0014 lr: 0.02\n",
      "iteration: 340400 loss: 0.0021 lr: 0.02\n",
      "iteration: 340410 loss: 0.0020 lr: 0.02\n",
      "iteration: 340420 loss: 0.0019 lr: 0.02\n",
      "iteration: 340430 loss: 0.0016 lr: 0.02\n",
      "iteration: 340440 loss: 0.0018 lr: 0.02\n",
      "iteration: 340450 loss: 0.0014 lr: 0.02\n",
      "iteration: 340460 loss: 0.0018 lr: 0.02\n",
      "iteration: 340470 loss: 0.0016 lr: 0.02\n",
      "iteration: 340480 loss: 0.0022 lr: 0.02\n",
      "iteration: 340490 loss: 0.0017 lr: 0.02\n",
      "iteration: 340500 loss: 0.0016 lr: 0.02\n",
      "iteration: 340510 loss: 0.0016 lr: 0.02\n",
      "iteration: 340520 loss: 0.0015 lr: 0.02\n",
      "iteration: 340530 loss: 0.0018 lr: 0.02\n",
      "iteration: 340540 loss: 0.0013 lr: 0.02\n",
      "iteration: 340550 loss: 0.0023 lr: 0.02\n",
      "iteration: 340560 loss: 0.0019 lr: 0.02\n",
      "iteration: 340570 loss: 0.0026 lr: 0.02\n",
      "iteration: 340580 loss: 0.0011 lr: 0.02\n",
      "iteration: 340590 loss: 0.0012 lr: 0.02\n",
      "iteration: 340600 loss: 0.0014 lr: 0.02\n",
      "iteration: 340610 loss: 0.0017 lr: 0.02\n",
      "iteration: 340620 loss: 0.0013 lr: 0.02\n",
      "iteration: 340630 loss: 0.0017 lr: 0.02\n",
      "iteration: 340640 loss: 0.0020 lr: 0.02\n",
      "iteration: 340650 loss: 0.0011 lr: 0.02\n",
      "iteration: 340660 loss: 0.0019 lr: 0.02\n",
      "iteration: 340670 loss: 0.0017 lr: 0.02\n",
      "iteration: 340680 loss: 0.0020 lr: 0.02\n",
      "iteration: 340690 loss: 0.0016 lr: 0.02\n",
      "iteration: 340700 loss: 0.0015 lr: 0.02\n",
      "iteration: 340710 loss: 0.0014 lr: 0.02\n",
      "iteration: 340720 loss: 0.0016 lr: 0.02\n",
      "iteration: 340730 loss: 0.0018 lr: 0.02\n",
      "iteration: 340740 loss: 0.0015 lr: 0.02\n",
      "iteration: 340750 loss: 0.0012 lr: 0.02\n",
      "iteration: 340760 loss: 0.0015 lr: 0.02\n",
      "iteration: 340770 loss: 0.0016 lr: 0.02\n",
      "iteration: 340780 loss: 0.0018 lr: 0.02\n",
      "iteration: 340790 loss: 0.0019 lr: 0.02\n",
      "iteration: 340800 loss: 0.0013 lr: 0.02\n",
      "iteration: 340810 loss: 0.0015 lr: 0.02\n",
      "iteration: 340820 loss: 0.0022 lr: 0.02\n",
      "iteration: 340830 loss: 0.0022 lr: 0.02\n",
      "iteration: 340840 loss: 0.0015 lr: 0.02\n",
      "iteration: 340850 loss: 0.0014 lr: 0.02\n",
      "iteration: 340860 loss: 0.0014 lr: 0.02\n",
      "iteration: 340870 loss: 0.0021 lr: 0.02\n",
      "iteration: 340880 loss: 0.0013 lr: 0.02\n",
      "iteration: 340890 loss: 0.0012 lr: 0.02\n",
      "iteration: 340900 loss: 0.0014 lr: 0.02\n",
      "iteration: 340910 loss: 0.0015 lr: 0.02\n",
      "iteration: 340920 loss: 0.0047 lr: 0.02\n",
      "iteration: 340930 loss: 0.0014 lr: 0.02\n",
      "iteration: 340940 loss: 0.0017 lr: 0.02\n",
      "iteration: 340950 loss: 0.0020 lr: 0.02\n",
      "iteration: 340960 loss: 0.0026 lr: 0.02\n",
      "iteration: 340970 loss: 0.0020 lr: 0.02\n",
      "iteration: 340980 loss: 0.0013 lr: 0.02\n",
      "iteration: 340990 loss: 0.0011 lr: 0.02\n",
      "iteration: 341000 loss: 0.0018 lr: 0.02\n",
      "iteration: 341010 loss: 0.0024 lr: 0.02\n",
      "iteration: 341020 loss: 0.0016 lr: 0.02\n",
      "iteration: 341030 loss: 0.0017 lr: 0.02\n",
      "iteration: 341040 loss: 0.0013 lr: 0.02\n",
      "iteration: 341050 loss: 0.0016 lr: 0.02\n",
      "iteration: 341060 loss: 0.0027 lr: 0.02\n",
      "iteration: 341070 loss: 0.0020 lr: 0.02\n",
      "iteration: 341080 loss: 0.0019 lr: 0.02\n",
      "iteration: 341090 loss: 0.0020 lr: 0.02\n",
      "iteration: 341100 loss: 0.0019 lr: 0.02\n",
      "iteration: 341110 loss: 0.0017 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 341120 loss: 0.0014 lr: 0.02\n",
      "iteration: 341130 loss: 0.0021 lr: 0.02\n",
      "iteration: 341140 loss: 0.0022 lr: 0.02\n",
      "iteration: 341150 loss: 0.0023 lr: 0.02\n",
      "iteration: 341160 loss: 0.0016 lr: 0.02\n",
      "iteration: 341170 loss: 0.0014 lr: 0.02\n",
      "iteration: 341180 loss: 0.0022 lr: 0.02\n",
      "iteration: 341190 loss: 0.0011 lr: 0.02\n",
      "iteration: 341200 loss: 0.0015 lr: 0.02\n",
      "iteration: 341210 loss: 0.0014 lr: 0.02\n",
      "iteration: 341220 loss: 0.0019 lr: 0.02\n",
      "iteration: 341230 loss: 0.0020 lr: 0.02\n",
      "iteration: 341240 loss: 0.0020 lr: 0.02\n",
      "iteration: 341250 loss: 0.0017 lr: 0.02\n",
      "iteration: 341260 loss: 0.0017 lr: 0.02\n",
      "iteration: 341270 loss: 0.0015 lr: 0.02\n",
      "iteration: 341280 loss: 0.0017 lr: 0.02\n",
      "iteration: 341290 loss: 0.0014 lr: 0.02\n",
      "iteration: 341300 loss: 0.0014 lr: 0.02\n",
      "iteration: 341310 loss: 0.0016 lr: 0.02\n",
      "iteration: 341320 loss: 0.0019 lr: 0.02\n",
      "iteration: 341330 loss: 0.0020 lr: 0.02\n",
      "iteration: 341340 loss: 0.0023 lr: 0.02\n",
      "iteration: 341350 loss: 0.0011 lr: 0.02\n",
      "iteration: 341360 loss: 0.0015 lr: 0.02\n",
      "iteration: 341370 loss: 0.0012 lr: 0.02\n",
      "iteration: 341380 loss: 0.0012 lr: 0.02\n",
      "iteration: 341390 loss: 0.0022 lr: 0.02\n",
      "iteration: 341400 loss: 0.0019 lr: 0.02\n",
      "iteration: 341410 loss: 0.0016 lr: 0.02\n",
      "iteration: 341420 loss: 0.0016 lr: 0.02\n",
      "iteration: 341430 loss: 0.0017 lr: 0.02\n",
      "iteration: 341440 loss: 0.0015 lr: 0.02\n",
      "iteration: 341450 loss: 0.0015 lr: 0.02\n",
      "iteration: 341460 loss: 0.0017 lr: 0.02\n",
      "iteration: 341470 loss: 0.0014 lr: 0.02\n",
      "iteration: 341480 loss: 0.0016 lr: 0.02\n",
      "iteration: 341490 loss: 0.0018 lr: 0.02\n",
      "iteration: 341500 loss: 0.0017 lr: 0.02\n",
      "iteration: 341510 loss: 0.0017 lr: 0.02\n",
      "iteration: 341520 loss: 0.0015 lr: 0.02\n",
      "iteration: 341530 loss: 0.0010 lr: 0.02\n",
      "iteration: 341540 loss: 0.0011 lr: 0.02\n",
      "iteration: 341550 loss: 0.0009 lr: 0.02\n",
      "iteration: 341560 loss: 0.0021 lr: 0.02\n",
      "iteration: 341570 loss: 0.0023 lr: 0.02\n",
      "iteration: 341580 loss: 0.0021 lr: 0.02\n",
      "iteration: 341590 loss: 0.0024 lr: 0.02\n",
      "iteration: 341600 loss: 0.0016 lr: 0.02\n",
      "iteration: 341610 loss: 0.0012 lr: 0.02\n",
      "iteration: 341620 loss: 0.0024 lr: 0.02\n",
      "iteration: 341630 loss: 0.0016 lr: 0.02\n",
      "iteration: 341640 loss: 0.0027 lr: 0.02\n",
      "iteration: 341650 loss: 0.0013 lr: 0.02\n",
      "iteration: 341660 loss: 0.0016 lr: 0.02\n",
      "iteration: 341670 loss: 0.0012 lr: 0.02\n",
      "iteration: 341680 loss: 0.0015 lr: 0.02\n",
      "iteration: 341690 loss: 0.0016 lr: 0.02\n",
      "iteration: 341700 loss: 0.0023 lr: 0.02\n",
      "iteration: 341710 loss: 0.0013 lr: 0.02\n",
      "iteration: 341720 loss: 0.0013 lr: 0.02\n",
      "iteration: 341730 loss: 0.0020 lr: 0.02\n",
      "iteration: 341740 loss: 0.0013 lr: 0.02\n",
      "iteration: 341750 loss: 0.0018 lr: 0.02\n",
      "iteration: 341760 loss: 0.0019 lr: 0.02\n",
      "iteration: 341770 loss: 0.0014 lr: 0.02\n",
      "iteration: 341780 loss: 0.0011 lr: 0.02\n",
      "iteration: 341790 loss: 0.0017 lr: 0.02\n",
      "iteration: 341800 loss: 0.0015 lr: 0.02\n",
      "iteration: 341810 loss: 0.0017 lr: 0.02\n",
      "iteration: 341820 loss: 0.0013 lr: 0.02\n",
      "iteration: 341830 loss: 0.0014 lr: 0.02\n",
      "iteration: 341840 loss: 0.0016 lr: 0.02\n",
      "iteration: 341850 loss: 0.0019 lr: 0.02\n",
      "iteration: 341860 loss: 0.0019 lr: 0.02\n",
      "iteration: 341870 loss: 0.0015 lr: 0.02\n",
      "iteration: 341880 loss: 0.0013 lr: 0.02\n",
      "iteration: 341890 loss: 0.0020 lr: 0.02\n",
      "iteration: 341900 loss: 0.0015 lr: 0.02\n",
      "iteration: 341910 loss: 0.0018 lr: 0.02\n",
      "iteration: 341920 loss: 0.0021 lr: 0.02\n",
      "iteration: 341930 loss: 0.0019 lr: 0.02\n",
      "iteration: 341940 loss: 0.0016 lr: 0.02\n",
      "iteration: 341950 loss: 0.0018 lr: 0.02\n",
      "iteration: 341960 loss: 0.0017 lr: 0.02\n",
      "iteration: 341970 loss: 0.0014 lr: 0.02\n",
      "iteration: 341980 loss: 0.0013 lr: 0.02\n",
      "iteration: 341990 loss: 0.0021 lr: 0.02\n",
      "iteration: 342000 loss: 0.0018 lr: 0.02\n",
      "iteration: 342010 loss: 0.0018 lr: 0.02\n",
      "iteration: 342020 loss: 0.0021 lr: 0.02\n",
      "iteration: 342030 loss: 0.0017 lr: 0.02\n",
      "iteration: 342040 loss: 0.0017 lr: 0.02\n",
      "iteration: 342050 loss: 0.0013 lr: 0.02\n",
      "iteration: 342060 loss: 0.0014 lr: 0.02\n",
      "iteration: 342070 loss: 0.0022 lr: 0.02\n",
      "iteration: 342080 loss: 0.0015 lr: 0.02\n",
      "iteration: 342090 loss: 0.0016 lr: 0.02\n",
      "iteration: 342100 loss: 0.0021 lr: 0.02\n",
      "iteration: 342110 loss: 0.0022 lr: 0.02\n",
      "iteration: 342120 loss: 0.0017 lr: 0.02\n",
      "iteration: 342130 loss: 0.0015 lr: 0.02\n",
      "iteration: 342140 loss: 0.0014 lr: 0.02\n",
      "iteration: 342150 loss: 0.0016 lr: 0.02\n",
      "iteration: 342160 loss: 0.0017 lr: 0.02\n",
      "iteration: 342170 loss: 0.0016 lr: 0.02\n",
      "iteration: 342180 loss: 0.0020 lr: 0.02\n",
      "iteration: 342190 loss: 0.0016 lr: 0.02\n",
      "iteration: 342200 loss: 0.0016 lr: 0.02\n",
      "iteration: 342210 loss: 0.0017 lr: 0.02\n",
      "iteration: 342220 loss: 0.0015 lr: 0.02\n",
      "iteration: 342230 loss: 0.0021 lr: 0.02\n",
      "iteration: 342240 loss: 0.0017 lr: 0.02\n",
      "iteration: 342250 loss: 0.0014 lr: 0.02\n",
      "iteration: 342260 loss: 0.0013 lr: 0.02\n",
      "iteration: 342270 loss: 0.0013 lr: 0.02\n",
      "iteration: 342280 loss: 0.0011 lr: 0.02\n",
      "iteration: 342290 loss: 0.0016 lr: 0.02\n",
      "iteration: 342300 loss: 0.0013 lr: 0.02\n",
      "iteration: 342310 loss: 0.0017 lr: 0.02\n",
      "iteration: 342320 loss: 0.0021 lr: 0.02\n",
      "iteration: 342330 loss: 0.0016 lr: 0.02\n",
      "iteration: 342340 loss: 0.0022 lr: 0.02\n",
      "iteration: 342350 loss: 0.0020 lr: 0.02\n",
      "iteration: 342360 loss: 0.0014 lr: 0.02\n",
      "iteration: 342370 loss: 0.0027 lr: 0.02\n",
      "iteration: 342380 loss: 0.0017 lr: 0.02\n",
      "iteration: 342390 loss: 0.0015 lr: 0.02\n",
      "iteration: 342400 loss: 0.0026 lr: 0.02\n",
      "iteration: 342410 loss: 0.0021 lr: 0.02\n",
      "iteration: 342420 loss: 0.0016 lr: 0.02\n",
      "iteration: 342430 loss: 0.0010 lr: 0.02\n",
      "iteration: 342440 loss: 0.0013 lr: 0.02\n",
      "iteration: 342450 loss: 0.0010 lr: 0.02\n",
      "iteration: 342460 loss: 0.0017 lr: 0.02\n",
      "iteration: 342470 loss: 0.0014 lr: 0.02\n",
      "iteration: 342480 loss: 0.0015 lr: 0.02\n",
      "iteration: 342490 loss: 0.0035 lr: 0.02\n",
      "iteration: 342500 loss: 0.0014 lr: 0.02\n",
      "iteration: 342510 loss: 0.0023 lr: 0.02\n",
      "iteration: 342520 loss: 0.0015 lr: 0.02\n",
      "iteration: 342530 loss: 0.0019 lr: 0.02\n",
      "iteration: 342540 loss: 0.0014 lr: 0.02\n",
      "iteration: 342550 loss: 0.0019 lr: 0.02\n",
      "iteration: 342560 loss: 0.0015 lr: 0.02\n",
      "iteration: 342570 loss: 0.0016 lr: 0.02\n",
      "iteration: 342580 loss: 0.0012 lr: 0.02\n",
      "iteration: 342590 loss: 0.0015 lr: 0.02\n",
      "iteration: 342600 loss: 0.0017 lr: 0.02\n",
      "iteration: 342610 loss: 0.0017 lr: 0.02\n",
      "iteration: 342620 loss: 0.0015 lr: 0.02\n",
      "iteration: 342630 loss: 0.0017 lr: 0.02\n",
      "iteration: 342640 loss: 0.0021 lr: 0.02\n",
      "iteration: 342650 loss: 0.0017 lr: 0.02\n",
      "iteration: 342660 loss: 0.0018 lr: 0.02\n",
      "iteration: 342670 loss: 0.0014 lr: 0.02\n",
      "iteration: 342680 loss: 0.0018 lr: 0.02\n",
      "iteration: 342690 loss: 0.0017 lr: 0.02\n",
      "iteration: 342700 loss: 0.0015 lr: 0.02\n",
      "iteration: 342710 loss: 0.0017 lr: 0.02\n",
      "iteration: 342720 loss: 0.0020 lr: 0.02\n",
      "iteration: 342730 loss: 0.0018 lr: 0.02\n",
      "iteration: 342740 loss: 0.0016 lr: 0.02\n",
      "iteration: 342750 loss: 0.0025 lr: 0.02\n",
      "iteration: 342760 loss: 0.0020 lr: 0.02\n",
      "iteration: 342770 loss: 0.0016 lr: 0.02\n",
      "iteration: 342780 loss: 0.0018 lr: 0.02\n",
      "iteration: 342790 loss: 0.0016 lr: 0.02\n",
      "iteration: 342800 loss: 0.0017 lr: 0.02\n",
      "iteration: 342810 loss: 0.0012 lr: 0.02\n",
      "iteration: 342820 loss: 0.0018 lr: 0.02\n",
      "iteration: 342830 loss: 0.0015 lr: 0.02\n",
      "iteration: 342840 loss: 0.0014 lr: 0.02\n",
      "iteration: 342850 loss: 0.0014 lr: 0.02\n",
      "iteration: 342860 loss: 0.0021 lr: 0.02\n",
      "iteration: 342870 loss: 0.0014 lr: 0.02\n",
      "iteration: 342880 loss: 0.0019 lr: 0.02\n",
      "iteration: 342890 loss: 0.0016 lr: 0.02\n",
      "iteration: 342900 loss: 0.0018 lr: 0.02\n",
      "iteration: 342910 loss: 0.0020 lr: 0.02\n",
      "iteration: 342920 loss: 0.0013 lr: 0.02\n",
      "iteration: 342930 loss: 0.0014 lr: 0.02\n",
      "iteration: 342940 loss: 0.0012 lr: 0.02\n",
      "iteration: 342950 loss: 0.0027 lr: 0.02\n",
      "iteration: 342960 loss: 0.0013 lr: 0.02\n",
      "iteration: 342970 loss: 0.0015 lr: 0.02\n",
      "iteration: 342980 loss: 0.0015 lr: 0.02\n",
      "iteration: 342990 loss: 0.0022 lr: 0.02\n",
      "iteration: 343000 loss: 0.0013 lr: 0.02\n",
      "iteration: 343010 loss: 0.0015 lr: 0.02\n",
      "iteration: 343020 loss: 0.0020 lr: 0.02\n",
      "iteration: 343030 loss: 0.0010 lr: 0.02\n",
      "iteration: 343040 loss: 0.0013 lr: 0.02\n",
      "iteration: 343050 loss: 0.0016 lr: 0.02\n",
      "iteration: 343060 loss: 0.0016 lr: 0.02\n",
      "iteration: 343070 loss: 0.0015 lr: 0.02\n",
      "iteration: 343080 loss: 0.0021 lr: 0.02\n",
      "iteration: 343090 loss: 0.0022 lr: 0.02\n",
      "iteration: 343100 loss: 0.0016 lr: 0.02\n",
      "iteration: 343110 loss: 0.0014 lr: 0.02\n",
      "iteration: 343120 loss: 0.0012 lr: 0.02\n",
      "iteration: 343130 loss: 0.0016 lr: 0.02\n",
      "iteration: 343140 loss: 0.0016 lr: 0.02\n",
      "iteration: 343150 loss: 0.0024 lr: 0.02\n",
      "iteration: 343160 loss: 0.0015 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 343170 loss: 0.0015 lr: 0.02\n",
      "iteration: 343180 loss: 0.0024 lr: 0.02\n",
      "iteration: 343190 loss: 0.0014 lr: 0.02\n",
      "iteration: 343200 loss: 0.0018 lr: 0.02\n",
      "iteration: 343210 loss: 0.0027 lr: 0.02\n",
      "iteration: 343220 loss: 0.0014 lr: 0.02\n",
      "iteration: 343230 loss: 0.0013 lr: 0.02\n",
      "iteration: 343240 loss: 0.0024 lr: 0.02\n",
      "iteration: 343250 loss: 0.0012 lr: 0.02\n",
      "iteration: 343260 loss: 0.0015 lr: 0.02\n",
      "iteration: 343270 loss: 0.0024 lr: 0.02\n",
      "iteration: 343280 loss: 0.0014 lr: 0.02\n",
      "iteration: 343290 loss: 0.0020 lr: 0.02\n",
      "iteration: 343300 loss: 0.0023 lr: 0.02\n",
      "iteration: 343310 loss: 0.0017 lr: 0.02\n",
      "iteration: 343320 loss: 0.0022 lr: 0.02\n",
      "iteration: 343330 loss: 0.0013 lr: 0.02\n",
      "iteration: 343340 loss: 0.0015 lr: 0.02\n",
      "iteration: 343350 loss: 0.0015 lr: 0.02\n",
      "iteration: 343360 loss: 0.0017 lr: 0.02\n",
      "iteration: 343370 loss: 0.0020 lr: 0.02\n",
      "iteration: 343380 loss: 0.0013 lr: 0.02\n",
      "iteration: 343390 loss: 0.0016 lr: 0.02\n",
      "iteration: 343400 loss: 0.0014 lr: 0.02\n",
      "iteration: 343410 loss: 0.0015 lr: 0.02\n",
      "iteration: 343420 loss: 0.0021 lr: 0.02\n",
      "iteration: 343430 loss: 0.0022 lr: 0.02\n",
      "iteration: 343440 loss: 0.0018 lr: 0.02\n",
      "iteration: 343450 loss: 0.0019 lr: 0.02\n",
      "iteration: 343460 loss: 0.0013 lr: 0.02\n",
      "iteration: 343470 loss: 0.0018 lr: 0.02\n",
      "iteration: 343480 loss: 0.0020 lr: 0.02\n",
      "iteration: 343490 loss: 0.0032 lr: 0.02\n",
      "iteration: 343500 loss: 0.0015 lr: 0.02\n",
      "iteration: 343510 loss: 0.0018 lr: 0.02\n",
      "iteration: 343520 loss: 0.0018 lr: 0.02\n",
      "iteration: 343530 loss: 0.0017 lr: 0.02\n",
      "iteration: 343540 loss: 0.0012 lr: 0.02\n",
      "iteration: 343550 loss: 0.0022 lr: 0.02\n",
      "iteration: 343560 loss: 0.0020 lr: 0.02\n",
      "iteration: 343570 loss: 0.0020 lr: 0.02\n",
      "iteration: 343580 loss: 0.0017 lr: 0.02\n",
      "iteration: 343590 loss: 0.0012 lr: 0.02\n",
      "iteration: 343600 loss: 0.0024 lr: 0.02\n",
      "iteration: 343610 loss: 0.0015 lr: 0.02\n",
      "iteration: 343620 loss: 0.0011 lr: 0.02\n",
      "iteration: 343630 loss: 0.0012 lr: 0.02\n",
      "iteration: 343640 loss: 0.0020 lr: 0.02\n",
      "iteration: 343650 loss: 0.0016 lr: 0.02\n",
      "iteration: 343660 loss: 0.0015 lr: 0.02\n",
      "iteration: 343670 loss: 0.0011 lr: 0.02\n",
      "iteration: 343680 loss: 0.0019 lr: 0.02\n",
      "iteration: 343690 loss: 0.0020 lr: 0.02\n",
      "iteration: 343700 loss: 0.0019 lr: 0.02\n",
      "iteration: 343710 loss: 0.0020 lr: 0.02\n",
      "iteration: 343720 loss: 0.0017 lr: 0.02\n",
      "iteration: 343730 loss: 0.0017 lr: 0.02\n",
      "iteration: 343740 loss: 0.0019 lr: 0.02\n",
      "iteration: 343750 loss: 0.0018 lr: 0.02\n",
      "iteration: 343760 loss: 0.0014 lr: 0.02\n",
      "iteration: 343770 loss: 0.0018 lr: 0.02\n",
      "iteration: 343780 loss: 0.0028 lr: 0.02\n",
      "iteration: 343790 loss: 0.0015 lr: 0.02\n",
      "iteration: 343800 loss: 0.0019 lr: 0.02\n",
      "iteration: 343810 loss: 0.0014 lr: 0.02\n",
      "iteration: 343820 loss: 0.0019 lr: 0.02\n",
      "iteration: 343830 loss: 0.0018 lr: 0.02\n",
      "iteration: 343840 loss: 0.0018 lr: 0.02\n",
      "iteration: 343850 loss: 0.0016 lr: 0.02\n",
      "iteration: 343860 loss: 0.0024 lr: 0.02\n",
      "iteration: 343870 loss: 0.0015 lr: 0.02\n",
      "iteration: 343880 loss: 0.0016 lr: 0.02\n",
      "iteration: 343890 loss: 0.0014 lr: 0.02\n",
      "iteration: 343900 loss: 0.0014 lr: 0.02\n",
      "iteration: 343910 loss: 0.0025 lr: 0.02\n",
      "iteration: 343920 loss: 0.0017 lr: 0.02\n",
      "iteration: 343930 loss: 0.0013 lr: 0.02\n",
      "iteration: 343940 loss: 0.0016 lr: 0.02\n",
      "iteration: 343950 loss: 0.0009 lr: 0.02\n",
      "iteration: 343960 loss: 0.0012 lr: 0.02\n",
      "iteration: 343970 loss: 0.0012 lr: 0.02\n",
      "iteration: 343980 loss: 0.0011 lr: 0.02\n",
      "iteration: 343990 loss: 0.0018 lr: 0.02\n",
      "iteration: 344000 loss: 0.0016 lr: 0.02\n",
      "iteration: 344010 loss: 0.0017 lr: 0.02\n",
      "iteration: 344020 loss: 0.0019 lr: 0.02\n",
      "iteration: 344030 loss: 0.0021 lr: 0.02\n",
      "iteration: 344040 loss: 0.0021 lr: 0.02\n",
      "iteration: 344050 loss: 0.0021 lr: 0.02\n",
      "iteration: 344060 loss: 0.0017 lr: 0.02\n",
      "iteration: 344070 loss: 0.0022 lr: 0.02\n",
      "iteration: 344080 loss: 0.0018 lr: 0.02\n",
      "iteration: 344090 loss: 0.0015 lr: 0.02\n",
      "iteration: 344100 loss: 0.0016 lr: 0.02\n",
      "iteration: 344110 loss: 0.0012 lr: 0.02\n",
      "iteration: 344120 loss: 0.0014 lr: 0.02\n",
      "iteration: 344130 loss: 0.0021 lr: 0.02\n",
      "iteration: 344140 loss: 0.0018 lr: 0.02\n",
      "iteration: 344150 loss: 0.0013 lr: 0.02\n",
      "iteration: 344160 loss: 0.0017 lr: 0.02\n",
      "iteration: 344170 loss: 0.0014 lr: 0.02\n",
      "iteration: 344180 loss: 0.0020 lr: 0.02\n",
      "iteration: 344190 loss: 0.0021 lr: 0.02\n",
      "iteration: 344200 loss: 0.0014 lr: 0.02\n",
      "iteration: 344210 loss: 0.0016 lr: 0.02\n",
      "iteration: 344220 loss: 0.0014 lr: 0.02\n",
      "iteration: 344230 loss: 0.0017 lr: 0.02\n",
      "iteration: 344240 loss: 0.0019 lr: 0.02\n",
      "iteration: 344250 loss: 0.0018 lr: 0.02\n",
      "iteration: 344260 loss: 0.0013 lr: 0.02\n",
      "iteration: 344270 loss: 0.0017 lr: 0.02\n",
      "iteration: 344280 loss: 0.0017 lr: 0.02\n",
      "iteration: 344290 loss: 0.0017 lr: 0.02\n",
      "iteration: 344300 loss: 0.0016 lr: 0.02\n",
      "iteration: 344310 loss: 0.0031 lr: 0.02\n",
      "iteration: 344320 loss: 0.0016 lr: 0.02\n",
      "iteration: 344330 loss: 0.0018 lr: 0.02\n",
      "iteration: 344340 loss: 0.0015 lr: 0.02\n",
      "iteration: 344350 loss: 0.0017 lr: 0.02\n",
      "iteration: 344360 loss: 0.0014 lr: 0.02\n",
      "iteration: 344370 loss: 0.0015 lr: 0.02\n",
      "iteration: 344380 loss: 0.0017 lr: 0.02\n",
      "iteration: 344390 loss: 0.0019 lr: 0.02\n",
      "iteration: 344400 loss: 0.0015 lr: 0.02\n",
      "iteration: 344410 loss: 0.0025 lr: 0.02\n",
      "iteration: 344420 loss: 0.0018 lr: 0.02\n",
      "iteration: 344430 loss: 0.0017 lr: 0.02\n",
      "iteration: 344440 loss: 0.0017 lr: 0.02\n",
      "iteration: 344450 loss: 0.0017 lr: 0.02\n",
      "iteration: 344460 loss: 0.0026 lr: 0.02\n",
      "iteration: 344470 loss: 0.0017 lr: 0.02\n",
      "iteration: 344480 loss: 0.0018 lr: 0.02\n",
      "iteration: 344490 loss: 0.0015 lr: 0.02\n",
      "iteration: 344500 loss: 0.0011 lr: 0.02\n",
      "iteration: 344510 loss: 0.0021 lr: 0.02\n",
      "iteration: 344520 loss: 0.0017 lr: 0.02\n",
      "iteration: 344530 loss: 0.0015 lr: 0.02\n",
      "iteration: 344540 loss: 0.0021 lr: 0.02\n",
      "iteration: 344550 loss: 0.0016 lr: 0.02\n",
      "iteration: 344560 loss: 0.0012 lr: 0.02\n",
      "iteration: 344570 loss: 0.0011 lr: 0.02\n",
      "iteration: 344580 loss: 0.0019 lr: 0.02\n",
      "iteration: 344590 loss: 0.0018 lr: 0.02\n",
      "iteration: 344600 loss: 0.0021 lr: 0.02\n",
      "iteration: 344610 loss: 0.0018 lr: 0.02\n",
      "iteration: 344620 loss: 0.0017 lr: 0.02\n",
      "iteration: 344630 loss: 0.0015 lr: 0.02\n",
      "iteration: 344640 loss: 0.0012 lr: 0.02\n",
      "iteration: 344650 loss: 0.0020 lr: 0.02\n",
      "iteration: 344660 loss: 0.0015 lr: 0.02\n",
      "iteration: 344670 loss: 0.0021 lr: 0.02\n",
      "iteration: 344680 loss: 0.0015 lr: 0.02\n",
      "iteration: 344690 loss: 0.0011 lr: 0.02\n",
      "iteration: 344700 loss: 0.0013 lr: 0.02\n",
      "iteration: 344710 loss: 0.0013 lr: 0.02\n",
      "iteration: 344720 loss: 0.0015 lr: 0.02\n",
      "iteration: 344730 loss: 0.0016 lr: 0.02\n",
      "iteration: 344740 loss: 0.0013 lr: 0.02\n",
      "iteration: 344750 loss: 0.0021 lr: 0.02\n",
      "iteration: 344760 loss: 0.0011 lr: 0.02\n",
      "iteration: 344770 loss: 0.0012 lr: 0.02\n",
      "iteration: 344780 loss: 0.0012 lr: 0.02\n",
      "iteration: 344790 loss: 0.0019 lr: 0.02\n",
      "iteration: 344800 loss: 0.0018 lr: 0.02\n",
      "iteration: 344810 loss: 0.0016 lr: 0.02\n",
      "iteration: 344820 loss: 0.0012 lr: 0.02\n",
      "iteration: 344830 loss: 0.0023 lr: 0.02\n",
      "iteration: 344840 loss: 0.0023 lr: 0.02\n",
      "iteration: 344850 loss: 0.0018 lr: 0.02\n",
      "iteration: 344860 loss: 0.0030 lr: 0.02\n",
      "iteration: 344870 loss: 0.0014 lr: 0.02\n",
      "iteration: 344880 loss: 0.0025 lr: 0.02\n",
      "iteration: 344890 loss: 0.0020 lr: 0.02\n",
      "iteration: 344900 loss: 0.0018 lr: 0.02\n",
      "iteration: 344910 loss: 0.0011 lr: 0.02\n",
      "iteration: 344920 loss: 0.0014 lr: 0.02\n",
      "iteration: 344930 loss: 0.0018 lr: 0.02\n",
      "iteration: 344940 loss: 0.0012 lr: 0.02\n",
      "iteration: 344950 loss: 0.0016 lr: 0.02\n",
      "iteration: 344960 loss: 0.0015 lr: 0.02\n",
      "iteration: 344970 loss: 0.0013 lr: 0.02\n",
      "iteration: 344980 loss: 0.0015 lr: 0.02\n",
      "iteration: 344990 loss: 0.0016 lr: 0.02\n",
      "iteration: 345000 loss: 0.0027 lr: 0.02\n",
      "iteration: 345010 loss: 0.0015 lr: 0.02\n",
      "iteration: 345020 loss: 0.0018 lr: 0.02\n",
      "iteration: 345030 loss: 0.0022 lr: 0.02\n",
      "iteration: 345040 loss: 0.0017 lr: 0.02\n",
      "iteration: 345050 loss: 0.0018 lr: 0.02\n",
      "iteration: 345060 loss: 0.0026 lr: 0.02\n",
      "iteration: 345070 loss: 0.0012 lr: 0.02\n",
      "iteration: 345080 loss: 0.0015 lr: 0.02\n",
      "iteration: 345090 loss: 0.0017 lr: 0.02\n",
      "iteration: 345100 loss: 0.0013 lr: 0.02\n",
      "iteration: 345110 loss: 0.0012 lr: 0.02\n",
      "iteration: 345120 loss: 0.0014 lr: 0.02\n",
      "iteration: 345130 loss: 0.0021 lr: 0.02\n",
      "iteration: 345140 loss: 0.0017 lr: 0.02\n",
      "iteration: 345150 loss: 0.0014 lr: 0.02\n",
      "iteration: 345160 loss: 0.0010 lr: 0.02\n",
      "iteration: 345170 loss: 0.0020 lr: 0.02\n",
      "iteration: 345180 loss: 0.0016 lr: 0.02\n",
      "iteration: 345190 loss: 0.0022 lr: 0.02\n",
      "iteration: 345200 loss: 0.0015 lr: 0.02\n",
      "iteration: 345210 loss: 0.0020 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 345220 loss: 0.0022 lr: 0.02\n",
      "iteration: 345230 loss: 0.0021 lr: 0.02\n",
      "iteration: 345240 loss: 0.0022 lr: 0.02\n",
      "iteration: 345250 loss: 0.0013 lr: 0.02\n",
      "iteration: 345260 loss: 0.0025 lr: 0.02\n",
      "iteration: 345270 loss: 0.0013 lr: 0.02\n",
      "iteration: 345280 loss: 0.0013 lr: 0.02\n",
      "iteration: 345290 loss: 0.0014 lr: 0.02\n",
      "iteration: 345300 loss: 0.0015 lr: 0.02\n",
      "iteration: 345310 loss: 0.0014 lr: 0.02\n",
      "iteration: 345320 loss: 0.0012 lr: 0.02\n",
      "iteration: 345330 loss: 0.0013 lr: 0.02\n",
      "iteration: 345340 loss: 0.0015 lr: 0.02\n",
      "iteration: 345350 loss: 0.0017 lr: 0.02\n",
      "iteration: 345360 loss: 0.0014 lr: 0.02\n",
      "iteration: 345370 loss: 0.0017 lr: 0.02\n",
      "iteration: 345380 loss: 0.0022 lr: 0.02\n",
      "iteration: 345390 loss: 0.0018 lr: 0.02\n",
      "iteration: 345400 loss: 0.0018 lr: 0.02\n",
      "iteration: 345410 loss: 0.0017 lr: 0.02\n",
      "iteration: 345420 loss: 0.0014 lr: 0.02\n",
      "iteration: 345430 loss: 0.0011 lr: 0.02\n",
      "iteration: 345440 loss: 0.0016 lr: 0.02\n",
      "iteration: 345450 loss: 0.0017 lr: 0.02\n",
      "iteration: 345460 loss: 0.0017 lr: 0.02\n",
      "iteration: 345470 loss: 0.0031 lr: 0.02\n",
      "iteration: 345480 loss: 0.0014 lr: 0.02\n",
      "iteration: 345490 loss: 0.0017 lr: 0.02\n",
      "iteration: 345500 loss: 0.0018 lr: 0.02\n",
      "iteration: 345510 loss: 0.0023 lr: 0.02\n",
      "iteration: 345520 loss: 0.0017 lr: 0.02\n",
      "iteration: 345530 loss: 0.0016 lr: 0.02\n",
      "iteration: 345540 loss: 0.0014 lr: 0.02\n",
      "iteration: 345550 loss: 0.0019 lr: 0.02\n",
      "iteration: 345560 loss: 0.0020 lr: 0.02\n",
      "iteration: 345570 loss: 0.0016 lr: 0.02\n",
      "iteration: 345580 loss: 0.0022 lr: 0.02\n",
      "iteration: 345590 loss: 0.0015 lr: 0.02\n",
      "iteration: 345600 loss: 0.0017 lr: 0.02\n",
      "iteration: 345610 loss: 0.0021 lr: 0.02\n",
      "iteration: 345620 loss: 0.0014 lr: 0.02\n",
      "iteration: 345630 loss: 0.0017 lr: 0.02\n",
      "iteration: 345640 loss: 0.0012 lr: 0.02\n",
      "iteration: 345650 loss: 0.0016 lr: 0.02\n",
      "iteration: 345660 loss: 0.0025 lr: 0.02\n",
      "iteration: 345670 loss: 0.0019 lr: 0.02\n",
      "iteration: 345680 loss: 0.0016 lr: 0.02\n",
      "iteration: 345690 loss: 0.0017 lr: 0.02\n",
      "iteration: 345700 loss: 0.0014 lr: 0.02\n",
      "iteration: 345710 loss: 0.0016 lr: 0.02\n",
      "iteration: 345720 loss: 0.0027 lr: 0.02\n",
      "iteration: 345730 loss: 0.0018 lr: 0.02\n",
      "iteration: 345740 loss: 0.0019 lr: 0.02\n",
      "iteration: 345750 loss: 0.0018 lr: 0.02\n",
      "iteration: 345760 loss: 0.0019 lr: 0.02\n",
      "iteration: 345770 loss: 0.0017 lr: 0.02\n",
      "iteration: 345780 loss: 0.0022 lr: 0.02\n",
      "iteration: 345790 loss: 0.0018 lr: 0.02\n",
      "iteration: 345800 loss: 0.0017 lr: 0.02\n",
      "iteration: 345810 loss: 0.0010 lr: 0.02\n",
      "iteration: 345820 loss: 0.0016 lr: 0.02\n",
      "iteration: 345830 loss: 0.0014 lr: 0.02\n",
      "iteration: 345840 loss: 0.0014 lr: 0.02\n",
      "iteration: 345850 loss: 0.0015 lr: 0.02\n",
      "iteration: 345860 loss: 0.0016 lr: 0.02\n",
      "iteration: 345870 loss: 0.0018 lr: 0.02\n",
      "iteration: 345880 loss: 0.0015 lr: 0.02\n",
      "iteration: 345890 loss: 0.0016 lr: 0.02\n",
      "iteration: 345900 loss: 0.0020 lr: 0.02\n",
      "iteration: 345910 loss: 0.0019 lr: 0.02\n",
      "iteration: 345920 loss: 0.0013 lr: 0.02\n",
      "iteration: 345930 loss: 0.0013 lr: 0.02\n",
      "iteration: 345940 loss: 0.0020 lr: 0.02\n",
      "iteration: 345950 loss: 0.0018 lr: 0.02\n",
      "iteration: 345960 loss: 0.0018 lr: 0.02\n",
      "iteration: 345970 loss: 0.0011 lr: 0.02\n",
      "iteration: 345980 loss: 0.0011 lr: 0.02\n",
      "iteration: 345990 loss: 0.0011 lr: 0.02\n",
      "iteration: 346000 loss: 0.0015 lr: 0.02\n",
      "iteration: 346010 loss: 0.0018 lr: 0.02\n",
      "iteration: 346020 loss: 0.0016 lr: 0.02\n",
      "iteration: 346030 loss: 0.0016 lr: 0.02\n",
      "iteration: 346040 loss: 0.0018 lr: 0.02\n",
      "iteration: 346050 loss: 0.0015 lr: 0.02\n",
      "iteration: 346060 loss: 0.0016 lr: 0.02\n",
      "iteration: 346070 loss: 0.0022 lr: 0.02\n",
      "iteration: 346080 loss: 0.0015 lr: 0.02\n",
      "iteration: 346090 loss: 0.0016 lr: 0.02\n",
      "iteration: 346100 loss: 0.0018 lr: 0.02\n",
      "iteration: 346110 loss: 0.0016 lr: 0.02\n",
      "iteration: 346120 loss: 0.0010 lr: 0.02\n",
      "iteration: 346130 loss: 0.0019 lr: 0.02\n",
      "iteration: 346140 loss: 0.0023 lr: 0.02\n",
      "iteration: 346150 loss: 0.0018 lr: 0.02\n",
      "iteration: 346160 loss: 0.0015 lr: 0.02\n",
      "iteration: 346170 loss: 0.0013 lr: 0.02\n",
      "iteration: 346180 loss: 0.0012 lr: 0.02\n",
      "iteration: 346190 loss: 0.0018 lr: 0.02\n",
      "iteration: 346200 loss: 0.0016 lr: 0.02\n",
      "iteration: 346210 loss: 0.0018 lr: 0.02\n",
      "iteration: 346220 loss: 0.0026 lr: 0.02\n",
      "iteration: 346230 loss: 0.0017 lr: 0.02\n",
      "iteration: 346240 loss: 0.0018 lr: 0.02\n",
      "iteration: 346250 loss: 0.0010 lr: 0.02\n",
      "iteration: 346260 loss: 0.0017 lr: 0.02\n",
      "iteration: 346270 loss: 0.0035 lr: 0.02\n",
      "iteration: 346280 loss: 0.0013 lr: 0.02\n",
      "iteration: 346290 loss: 0.0018 lr: 0.02\n",
      "iteration: 346300 loss: 0.0026 lr: 0.02\n",
      "iteration: 346310 loss: 0.0012 lr: 0.02\n",
      "iteration: 346320 loss: 0.0013 lr: 0.02\n",
      "iteration: 346330 loss: 0.0017 lr: 0.02\n",
      "iteration: 346340 loss: 0.0012 lr: 0.02\n",
      "iteration: 346350 loss: 0.0012 lr: 0.02\n",
      "iteration: 346360 loss: 0.0022 lr: 0.02\n",
      "iteration: 346370 loss: 0.0012 lr: 0.02\n",
      "iteration: 346380 loss: 0.0019 lr: 0.02\n",
      "iteration: 346390 loss: 0.0016 lr: 0.02\n",
      "iteration: 346400 loss: 0.0014 lr: 0.02\n",
      "iteration: 346410 loss: 0.0017 lr: 0.02\n",
      "iteration: 346420 loss: 0.0026 lr: 0.02\n",
      "iteration: 346430 loss: 0.0016 lr: 0.02\n",
      "iteration: 346440 loss: 0.0019 lr: 0.02\n",
      "iteration: 346450 loss: 0.0013 lr: 0.02\n",
      "iteration: 346460 loss: 0.0019 lr: 0.02\n",
      "iteration: 346470 loss: 0.0017 lr: 0.02\n",
      "iteration: 346480 loss: 0.0011 lr: 0.02\n",
      "iteration: 346490 loss: 0.0019 lr: 0.02\n",
      "iteration: 346500 loss: 0.0020 lr: 0.02\n",
      "iteration: 346510 loss: 0.0013 lr: 0.02\n",
      "iteration: 346520 loss: 0.0014 lr: 0.02\n",
      "iteration: 346530 loss: 0.0016 lr: 0.02\n",
      "iteration: 346540 loss: 0.0016 lr: 0.02\n",
      "iteration: 346550 loss: 0.0017 lr: 0.02\n",
      "iteration: 346560 loss: 0.0018 lr: 0.02\n",
      "iteration: 346570 loss: 0.0015 lr: 0.02\n",
      "iteration: 346580 loss: 0.0023 lr: 0.02\n",
      "iteration: 346590 loss: 0.0018 lr: 0.02\n",
      "iteration: 346600 loss: 0.0016 lr: 0.02\n",
      "iteration: 346610 loss: 0.0027 lr: 0.02\n",
      "iteration: 346620 loss: 0.0016 lr: 0.02\n",
      "iteration: 346630 loss: 0.0020 lr: 0.02\n",
      "iteration: 346640 loss: 0.0011 lr: 0.02\n",
      "iteration: 346650 loss: 0.0015 lr: 0.02\n",
      "iteration: 346660 loss: 0.0016 lr: 0.02\n",
      "iteration: 346670 loss: 0.0013 lr: 0.02\n",
      "iteration: 346680 loss: 0.0022 lr: 0.02\n",
      "iteration: 346690 loss: 0.0015 lr: 0.02\n",
      "iteration: 346700 loss: 0.0015 lr: 0.02\n",
      "iteration: 346710 loss: 0.0023 lr: 0.02\n",
      "iteration: 346720 loss: 0.0017 lr: 0.02\n",
      "iteration: 346730 loss: 0.0022 lr: 0.02\n",
      "iteration: 346740 loss: 0.0019 lr: 0.02\n",
      "iteration: 346750 loss: 0.0013 lr: 0.02\n",
      "iteration: 346760 loss: 0.0014 lr: 0.02\n",
      "iteration: 346770 loss: 0.0019 lr: 0.02\n",
      "iteration: 346780 loss: 0.0016 lr: 0.02\n",
      "iteration: 346790 loss: 0.0014 lr: 0.02\n",
      "iteration: 346800 loss: 0.0016 lr: 0.02\n",
      "iteration: 346810 loss: 0.0012 lr: 0.02\n",
      "iteration: 346820 loss: 0.0017 lr: 0.02\n",
      "iteration: 346830 loss: 0.0010 lr: 0.02\n",
      "iteration: 346840 loss: 0.0012 lr: 0.02\n",
      "iteration: 346850 loss: 0.0013 lr: 0.02\n",
      "iteration: 346860 loss: 0.0011 lr: 0.02\n",
      "iteration: 346870 loss: 0.0016 lr: 0.02\n",
      "iteration: 346880 loss: 0.0017 lr: 0.02\n",
      "iteration: 346890 loss: 0.0014 lr: 0.02\n",
      "iteration: 346900 loss: 0.0018 lr: 0.02\n",
      "iteration: 346910 loss: 0.0021 lr: 0.02\n",
      "iteration: 346920 loss: 0.0018 lr: 0.02\n",
      "iteration: 346930 loss: 0.0023 lr: 0.02\n",
      "iteration: 346940 loss: 0.0019 lr: 0.02\n",
      "iteration: 346950 loss: 0.0013 lr: 0.02\n",
      "iteration: 346960 loss: 0.0014 lr: 0.02\n",
      "iteration: 346970 loss: 0.0011 lr: 0.02\n",
      "iteration: 346980 loss: 0.0016 lr: 0.02\n",
      "iteration: 346990 loss: 0.0019 lr: 0.02\n",
      "iteration: 347000 loss: 0.0015 lr: 0.02\n",
      "iteration: 347010 loss: 0.0016 lr: 0.02\n",
      "iteration: 347020 loss: 0.0013 lr: 0.02\n",
      "iteration: 347030 loss: 0.0012 lr: 0.02\n",
      "iteration: 347040 loss: 0.0015 lr: 0.02\n",
      "iteration: 347050 loss: 0.0015 lr: 0.02\n",
      "iteration: 347060 loss: 0.0015 lr: 0.02\n",
      "iteration: 347070 loss: 0.0022 lr: 0.02\n",
      "iteration: 347080 loss: 0.0017 lr: 0.02\n",
      "iteration: 347090 loss: 0.0029 lr: 0.02\n",
      "iteration: 347100 loss: 0.0017 lr: 0.02\n",
      "iteration: 347110 loss: 0.0018 lr: 0.02\n",
      "iteration: 347120 loss: 0.0021 lr: 0.02\n",
      "iteration: 347130 loss: 0.0018 lr: 0.02\n",
      "iteration: 347140 loss: 0.0012 lr: 0.02\n",
      "iteration: 347150 loss: 0.0014 lr: 0.02\n",
      "iteration: 347160 loss: 0.0017 lr: 0.02\n",
      "iteration: 347170 loss: 0.0018 lr: 0.02\n",
      "iteration: 347180 loss: 0.0017 lr: 0.02\n",
      "iteration: 347190 loss: 0.0022 lr: 0.02\n",
      "iteration: 347200 loss: 0.0014 lr: 0.02\n",
      "iteration: 347210 loss: 0.0017 lr: 0.02\n",
      "iteration: 347220 loss: 0.0013 lr: 0.02\n",
      "iteration: 347230 loss: 0.0028 lr: 0.02\n",
      "iteration: 347240 loss: 0.0023 lr: 0.02\n",
      "iteration: 347250 loss: 0.0016 lr: 0.02\n",
      "iteration: 347260 loss: 0.0023 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 347270 loss: 0.0027 lr: 0.02\n",
      "iteration: 347280 loss: 0.0018 lr: 0.02\n",
      "iteration: 347290 loss: 0.0019 lr: 0.02\n",
      "iteration: 347300 loss: 0.0015 lr: 0.02\n",
      "iteration: 347310 loss: 0.0019 lr: 0.02\n",
      "iteration: 347320 loss: 0.0015 lr: 0.02\n",
      "iteration: 347330 loss: 0.0015 lr: 0.02\n",
      "iteration: 347340 loss: 0.0015 lr: 0.02\n",
      "iteration: 347350 loss: 0.0017 lr: 0.02\n",
      "iteration: 347360 loss: 0.0014 lr: 0.02\n",
      "iteration: 347370 loss: 0.0019 lr: 0.02\n",
      "iteration: 347380 loss: 0.0017 lr: 0.02\n",
      "iteration: 347390 loss: 0.0015 lr: 0.02\n",
      "iteration: 347400 loss: 0.0016 lr: 0.02\n",
      "iteration: 347410 loss: 0.0016 lr: 0.02\n",
      "iteration: 347420 loss: 0.0015 lr: 0.02\n",
      "iteration: 347430 loss: 0.0017 lr: 0.02\n",
      "iteration: 347440 loss: 0.0014 lr: 0.02\n",
      "iteration: 347450 loss: 0.0014 lr: 0.02\n",
      "iteration: 347460 loss: 0.0017 lr: 0.02\n",
      "iteration: 347470 loss: 0.0017 lr: 0.02\n",
      "iteration: 347480 loss: 0.0021 lr: 0.02\n",
      "iteration: 347490 loss: 0.0016 lr: 0.02\n",
      "iteration: 347500 loss: 0.0017 lr: 0.02\n",
      "iteration: 347510 loss: 0.0018 lr: 0.02\n",
      "iteration: 347520 loss: 0.0017 lr: 0.02\n",
      "iteration: 347530 loss: 0.0017 lr: 0.02\n",
      "iteration: 347540 loss: 0.0025 lr: 0.02\n",
      "iteration: 347550 loss: 0.0019 lr: 0.02\n",
      "iteration: 347560 loss: 0.0017 lr: 0.02\n",
      "iteration: 347570 loss: 0.0022 lr: 0.02\n",
      "iteration: 347580 loss: 0.0016 lr: 0.02\n",
      "iteration: 347590 loss: 0.0016 lr: 0.02\n",
      "iteration: 347600 loss: 0.0020 lr: 0.02\n",
      "iteration: 347610 loss: 0.0015 lr: 0.02\n",
      "iteration: 347620 loss: 0.0017 lr: 0.02\n",
      "iteration: 347630 loss: 0.0016 lr: 0.02\n",
      "iteration: 347640 loss: 0.0021 lr: 0.02\n",
      "iteration: 347650 loss: 0.0012 lr: 0.02\n",
      "iteration: 347660 loss: 0.0012 lr: 0.02\n",
      "iteration: 347670 loss: 0.0015 lr: 0.02\n",
      "iteration: 347680 loss: 0.0019 lr: 0.02\n",
      "iteration: 347690 loss: 0.0018 lr: 0.02\n",
      "iteration: 347700 loss: 0.0023 lr: 0.02\n",
      "iteration: 347710 loss: 0.0020 lr: 0.02\n",
      "iteration: 347720 loss: 0.0020 lr: 0.02\n",
      "iteration: 347730 loss: 0.0016 lr: 0.02\n",
      "iteration: 347740 loss: 0.0020 lr: 0.02\n",
      "iteration: 347750 loss: 0.0016 lr: 0.02\n",
      "iteration: 347760 loss: 0.0018 lr: 0.02\n",
      "iteration: 347770 loss: 0.0016 lr: 0.02\n",
      "iteration: 347780 loss: 0.0017 lr: 0.02\n",
      "iteration: 347790 loss: 0.0014 lr: 0.02\n",
      "iteration: 347800 loss: 0.0017 lr: 0.02\n",
      "iteration: 347810 loss: 0.0015 lr: 0.02\n",
      "iteration: 347820 loss: 0.0017 lr: 0.02\n",
      "iteration: 347830 loss: 0.0018 lr: 0.02\n",
      "iteration: 347840 loss: 0.0016 lr: 0.02\n",
      "iteration: 347850 loss: 0.0013 lr: 0.02\n",
      "iteration: 347860 loss: 0.0016 lr: 0.02\n",
      "iteration: 347870 loss: 0.0016 lr: 0.02\n",
      "iteration: 347880 loss: 0.0015 lr: 0.02\n",
      "iteration: 347890 loss: 0.0018 lr: 0.02\n",
      "iteration: 347900 loss: 0.0023 lr: 0.02\n",
      "iteration: 347910 loss: 0.0029 lr: 0.02\n",
      "iteration: 347920 loss: 0.0018 lr: 0.02\n",
      "iteration: 347930 loss: 0.0014 lr: 0.02\n",
      "iteration: 347940 loss: 0.0010 lr: 0.02\n",
      "iteration: 347950 loss: 0.0019 lr: 0.02\n",
      "iteration: 347960 loss: 0.0012 lr: 0.02\n",
      "iteration: 347970 loss: 0.0015 lr: 0.02\n",
      "iteration: 347980 loss: 0.0021 lr: 0.02\n",
      "iteration: 347990 loss: 0.0013 lr: 0.02\n",
      "iteration: 348000 loss: 0.0013 lr: 0.02\n",
      "iteration: 348010 loss: 0.0018 lr: 0.02\n",
      "iteration: 348020 loss: 0.0017 lr: 0.02\n",
      "iteration: 348030 loss: 0.0017 lr: 0.02\n",
      "iteration: 348040 loss: 0.0018 lr: 0.02\n",
      "iteration: 348050 loss: 0.0013 lr: 0.02\n",
      "iteration: 348060 loss: 0.0015 lr: 0.02\n",
      "iteration: 348070 loss: 0.0013 lr: 0.02\n",
      "iteration: 348080 loss: 0.0011 lr: 0.02\n",
      "iteration: 348090 loss: 0.0013 lr: 0.02\n",
      "iteration: 348100 loss: 0.0017 lr: 0.02\n",
      "iteration: 348110 loss: 0.0019 lr: 0.02\n",
      "iteration: 348120 loss: 0.0017 lr: 0.02\n",
      "iteration: 348130 loss: 0.0016 lr: 0.02\n",
      "iteration: 348140 loss: 0.0020 lr: 0.02\n",
      "iteration: 348150 loss: 0.0013 lr: 0.02\n",
      "iteration: 348160 loss: 0.0034 lr: 0.02\n",
      "iteration: 348170 loss: 0.0015 lr: 0.02\n",
      "iteration: 348180 loss: 0.0016 lr: 0.02\n",
      "iteration: 348190 loss: 0.0022 lr: 0.02\n",
      "iteration: 348200 loss: 0.0021 lr: 0.02\n",
      "iteration: 348210 loss: 0.0014 lr: 0.02\n",
      "iteration: 348220 loss: 0.0014 lr: 0.02\n",
      "iteration: 348230 loss: 0.0016 lr: 0.02\n",
      "iteration: 348240 loss: 0.0022 lr: 0.02\n",
      "iteration: 348250 loss: 0.0018 lr: 0.02\n",
      "iteration: 348260 loss: 0.0019 lr: 0.02\n",
      "iteration: 348270 loss: 0.0015 lr: 0.02\n",
      "iteration: 348280 loss: 0.0015 lr: 0.02\n",
      "iteration: 348290 loss: 0.0015 lr: 0.02\n",
      "iteration: 348300 loss: 0.0019 lr: 0.02\n",
      "iteration: 348310 loss: 0.0017 lr: 0.02\n",
      "iteration: 348320 loss: 0.0012 lr: 0.02\n",
      "iteration: 348330 loss: 0.0017 lr: 0.02\n",
      "iteration: 348340 loss: 0.0011 lr: 0.02\n",
      "iteration: 348350 loss: 0.0015 lr: 0.02\n",
      "iteration: 348360 loss: 0.0015 lr: 0.02\n",
      "iteration: 348370 loss: 0.0015 lr: 0.02\n",
      "iteration: 348380 loss: 0.0033 lr: 0.02\n",
      "iteration: 348390 loss: 0.0020 lr: 0.02\n",
      "iteration: 348400 loss: 0.0013 lr: 0.02\n",
      "iteration: 348410 loss: 0.0017 lr: 0.02\n",
      "iteration: 348420 loss: 0.0015 lr: 0.02\n",
      "iteration: 348430 loss: 0.0018 lr: 0.02\n",
      "iteration: 348440 loss: 0.0015 lr: 0.02\n",
      "iteration: 348450 loss: 0.0016 lr: 0.02\n",
      "iteration: 348460 loss: 0.0025 lr: 0.02\n",
      "iteration: 348470 loss: 0.0017 lr: 0.02\n",
      "iteration: 348480 loss: 0.0014 lr: 0.02\n",
      "iteration: 348490 loss: 0.0015 lr: 0.02\n",
      "iteration: 348500 loss: 0.0015 lr: 0.02\n",
      "iteration: 348510 loss: 0.0018 lr: 0.02\n",
      "iteration: 348520 loss: 0.0016 lr: 0.02\n",
      "iteration: 348530 loss: 0.0020 lr: 0.02\n",
      "iteration: 348540 loss: 0.0017 lr: 0.02\n",
      "iteration: 348550 loss: 0.0011 lr: 0.02\n",
      "iteration: 348560 loss: 0.0017 lr: 0.02\n",
      "iteration: 348570 loss: 0.0023 lr: 0.02\n",
      "iteration: 348580 loss: 0.0016 lr: 0.02\n",
      "iteration: 348590 loss: 0.0014 lr: 0.02\n",
      "iteration: 348600 loss: 0.0013 lr: 0.02\n",
      "iteration: 348610 loss: 0.0013 lr: 0.02\n",
      "iteration: 348620 loss: 0.0018 lr: 0.02\n",
      "iteration: 348630 loss: 0.0030 lr: 0.02\n",
      "iteration: 348640 loss: 0.0015 lr: 0.02\n",
      "iteration: 348650 loss: 0.0020 lr: 0.02\n",
      "iteration: 348660 loss: 0.0016 lr: 0.02\n",
      "iteration: 348670 loss: 0.0015 lr: 0.02\n",
      "iteration: 348680 loss: 0.0012 lr: 0.02\n",
      "iteration: 348690 loss: 0.0018 lr: 0.02\n",
      "iteration: 348700 loss: 0.0020 lr: 0.02\n",
      "iteration: 348710 loss: 0.0015 lr: 0.02\n",
      "iteration: 348720 loss: 0.0020 lr: 0.02\n",
      "iteration: 348730 loss: 0.0013 lr: 0.02\n",
      "iteration: 348740 loss: 0.0018 lr: 0.02\n",
      "iteration: 348750 loss: 0.0020 lr: 0.02\n",
      "iteration: 348760 loss: 0.0013 lr: 0.02\n",
      "iteration: 348770 loss: 0.0028 lr: 0.02\n",
      "iteration: 348780 loss: 0.0017 lr: 0.02\n",
      "iteration: 348790 loss: 0.0016 lr: 0.02\n",
      "iteration: 348800 loss: 0.0021 lr: 0.02\n",
      "iteration: 348810 loss: 0.0019 lr: 0.02\n",
      "iteration: 348820 loss: 0.0015 lr: 0.02\n",
      "iteration: 348830 loss: 0.0013 lr: 0.02\n",
      "iteration: 348840 loss: 0.0015 lr: 0.02\n",
      "iteration: 348850 loss: 0.0021 lr: 0.02\n",
      "iteration: 348860 loss: 0.0020 lr: 0.02\n",
      "iteration: 348870 loss: 0.0019 lr: 0.02\n",
      "iteration: 348880 loss: 0.0016 lr: 0.02\n",
      "iteration: 348890 loss: 0.0021 lr: 0.02\n",
      "iteration: 348900 loss: 0.0016 lr: 0.02\n",
      "iteration: 348910 loss: 0.0012 lr: 0.02\n",
      "iteration: 348920 loss: 0.0015 lr: 0.02\n",
      "iteration: 348930 loss: 0.0035 lr: 0.02\n",
      "iteration: 348940 loss: 0.0028 lr: 0.02\n",
      "iteration: 348950 loss: 0.0016 lr: 0.02\n",
      "iteration: 348960 loss: 0.0018 lr: 0.02\n",
      "iteration: 348970 loss: 0.0016 lr: 0.02\n",
      "iteration: 348980 loss: 0.0017 lr: 0.02\n",
      "iteration: 348990 loss: 0.0018 lr: 0.02\n",
      "iteration: 349000 loss: 0.0029 lr: 0.02\n",
      "iteration: 349010 loss: 0.0014 lr: 0.02\n",
      "iteration: 349020 loss: 0.0018 lr: 0.02\n",
      "iteration: 349030 loss: 0.0014 lr: 0.02\n",
      "iteration: 349040 loss: 0.0014 lr: 0.02\n",
      "iteration: 349050 loss: 0.0015 lr: 0.02\n",
      "iteration: 349060 loss: 0.0013 lr: 0.02\n",
      "iteration: 349070 loss: 0.0018 lr: 0.02\n",
      "iteration: 349080 loss: 0.0015 lr: 0.02\n",
      "iteration: 349090 loss: 0.0019 lr: 0.02\n",
      "iteration: 349100 loss: 0.0023 lr: 0.02\n",
      "iteration: 349110 loss: 0.0016 lr: 0.02\n",
      "iteration: 349120 loss: 0.0020 lr: 0.02\n",
      "iteration: 349130 loss: 0.0018 lr: 0.02\n",
      "iteration: 349140 loss: 0.0015 lr: 0.02\n",
      "iteration: 349150 loss: 0.0014 lr: 0.02\n",
      "iteration: 349160 loss: 0.0019 lr: 0.02\n",
      "iteration: 349170 loss: 0.0012 lr: 0.02\n",
      "iteration: 349180 loss: 0.0024 lr: 0.02\n",
      "iteration: 349190 loss: 0.0012 lr: 0.02\n",
      "iteration: 349200 loss: 0.0012 lr: 0.02\n",
      "iteration: 349210 loss: 0.0020 lr: 0.02\n",
      "iteration: 349220 loss: 0.0020 lr: 0.02\n",
      "iteration: 349230 loss: 0.0018 lr: 0.02\n",
      "iteration: 349240 loss: 0.0016 lr: 0.02\n",
      "iteration: 349250 loss: 0.0015 lr: 0.02\n",
      "iteration: 349260 loss: 0.0023 lr: 0.02\n",
      "iteration: 349270 loss: 0.0021 lr: 0.02\n",
      "iteration: 349280 loss: 0.0014 lr: 0.02\n",
      "iteration: 349290 loss: 0.0021 lr: 0.02\n",
      "iteration: 349300 loss: 0.0017 lr: 0.02\n",
      "iteration: 349310 loss: 0.0017 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 349320 loss: 0.0013 lr: 0.02\n",
      "iteration: 349330 loss: 0.0015 lr: 0.02\n",
      "iteration: 349340 loss: 0.0014 lr: 0.02\n",
      "iteration: 349350 loss: 0.0014 lr: 0.02\n",
      "iteration: 349360 loss: 0.0016 lr: 0.02\n",
      "iteration: 349370 loss: 0.0016 lr: 0.02\n",
      "iteration: 349380 loss: 0.0020 lr: 0.02\n",
      "iteration: 349390 loss: 0.0014 lr: 0.02\n",
      "iteration: 349400 loss: 0.0027 lr: 0.02\n",
      "iteration: 349410 loss: 0.0017 lr: 0.02\n",
      "iteration: 349420 loss: 0.0017 lr: 0.02\n",
      "iteration: 349430 loss: 0.0018 lr: 0.02\n",
      "iteration: 349440 loss: 0.0017 lr: 0.02\n",
      "iteration: 349450 loss: 0.0017 lr: 0.02\n",
      "iteration: 349460 loss: 0.0011 lr: 0.02\n",
      "iteration: 349470 loss: 0.0017 lr: 0.02\n",
      "iteration: 349480 loss: 0.0023 lr: 0.02\n",
      "iteration: 349490 loss: 0.0009 lr: 0.02\n",
      "iteration: 349500 loss: 0.0020 lr: 0.02\n",
      "iteration: 349510 loss: 0.0021 lr: 0.02\n",
      "iteration: 349520 loss: 0.0016 lr: 0.02\n",
      "iteration: 349530 loss: 0.0011 lr: 0.02\n",
      "iteration: 349540 loss: 0.0020 lr: 0.02\n",
      "iteration: 349550 loss: 0.0019 lr: 0.02\n",
      "iteration: 349560 loss: 0.0017 lr: 0.02\n",
      "iteration: 349570 loss: 0.0018 lr: 0.02\n",
      "iteration: 349580 loss: 0.0019 lr: 0.02\n",
      "iteration: 349590 loss: 0.0015 lr: 0.02\n",
      "iteration: 349600 loss: 0.0011 lr: 0.02\n",
      "iteration: 349610 loss: 0.0019 lr: 0.02\n",
      "iteration: 349620 loss: 0.0014 lr: 0.02\n",
      "iteration: 349630 loss: 0.0028 lr: 0.02\n",
      "iteration: 349640 loss: 0.0015 lr: 0.02\n",
      "iteration: 349650 loss: 0.0012 lr: 0.02\n",
      "iteration: 349660 loss: 0.0016 lr: 0.02\n",
      "iteration: 349670 loss: 0.0018 lr: 0.02\n",
      "iteration: 349680 loss: 0.0012 lr: 0.02\n",
      "iteration: 349690 loss: 0.0012 lr: 0.02\n",
      "iteration: 349700 loss: 0.0014 lr: 0.02\n",
      "iteration: 349710 loss: 0.0021 lr: 0.02\n",
      "iteration: 349720 loss: 0.0018 lr: 0.02\n",
      "iteration: 349730 loss: 0.0019 lr: 0.02\n",
      "iteration: 349740 loss: 0.0014 lr: 0.02\n",
      "iteration: 349750 loss: 0.0016 lr: 0.02\n",
      "iteration: 349760 loss: 0.0012 lr: 0.02\n",
      "iteration: 349770 loss: 0.0016 lr: 0.02\n",
      "iteration: 349780 loss: 0.0019 lr: 0.02\n",
      "iteration: 349790 loss: 0.0029 lr: 0.02\n",
      "iteration: 349800 loss: 0.0019 lr: 0.02\n",
      "iteration: 349810 loss: 0.0022 lr: 0.02\n",
      "iteration: 349820 loss: 0.0019 lr: 0.02\n",
      "iteration: 349830 loss: 0.0014 lr: 0.02\n",
      "iteration: 349840 loss: 0.0022 lr: 0.02\n",
      "iteration: 349850 loss: 0.0016 lr: 0.02\n",
      "iteration: 349860 loss: 0.0014 lr: 0.02\n",
      "iteration: 349870 loss: 0.0014 lr: 0.02\n",
      "iteration: 349880 loss: 0.0017 lr: 0.02\n",
      "iteration: 349890 loss: 0.0018 lr: 0.02\n",
      "iteration: 349900 loss: 0.0020 lr: 0.02\n",
      "iteration: 349910 loss: 0.0023 lr: 0.02\n",
      "iteration: 349920 loss: 0.0015 lr: 0.02\n",
      "iteration: 349930 loss: 0.0014 lr: 0.02\n",
      "iteration: 349940 loss: 0.0020 lr: 0.02\n",
      "iteration: 349950 loss: 0.0013 lr: 0.02\n",
      "iteration: 349960 loss: 0.0025 lr: 0.02\n",
      "iteration: 349970 loss: 0.0019 lr: 0.02\n",
      "iteration: 349980 loss: 0.0017 lr: 0.02\n",
      "iteration: 349990 loss: 0.0017 lr: 0.02\n",
      "iteration: 350000 loss: 0.0017 lr: 0.02\n",
      "iteration: 350010 loss: 0.0023 lr: 0.02\n",
      "iteration: 350020 loss: 0.0016 lr: 0.02\n",
      "iteration: 350030 loss: 0.0019 lr: 0.02\n",
      "iteration: 350040 loss: 0.0015 lr: 0.02\n",
      "iteration: 350050 loss: 0.0024 lr: 0.02\n",
      "iteration: 350060 loss: 0.0022 lr: 0.02\n",
      "iteration: 350070 loss: 0.0015 lr: 0.02\n",
      "iteration: 350080 loss: 0.0017 lr: 0.02\n",
      "iteration: 350090 loss: 0.0017 lr: 0.02\n",
      "iteration: 350100 loss: 0.0013 lr: 0.02\n",
      "iteration: 350110 loss: 0.0009 lr: 0.02\n",
      "iteration: 350120 loss: 0.0019 lr: 0.02\n",
      "iteration: 350130 loss: 0.0013 lr: 0.02\n",
      "iteration: 350140 loss: 0.0015 lr: 0.02\n",
      "iteration: 350150 loss: 0.0014 lr: 0.02\n",
      "iteration: 350160 loss: 0.0012 lr: 0.02\n",
      "iteration: 350170 loss: 0.0014 lr: 0.02\n",
      "iteration: 350180 loss: 0.0013 lr: 0.02\n",
      "iteration: 350190 loss: 0.0015 lr: 0.02\n",
      "iteration: 350200 loss: 0.0026 lr: 0.02\n",
      "iteration: 350210 loss: 0.0016 lr: 0.02\n",
      "iteration: 350220 loss: 0.0015 lr: 0.02\n",
      "iteration: 350230 loss: 0.0017 lr: 0.02\n",
      "iteration: 350240 loss: 0.0014 lr: 0.02\n",
      "iteration: 350250 loss: 0.0014 lr: 0.02\n",
      "iteration: 350260 loss: 0.0021 lr: 0.02\n",
      "iteration: 350270 loss: 0.0021 lr: 0.02\n",
      "iteration: 350280 loss: 0.0029 lr: 0.02\n",
      "iteration: 350290 loss: 0.0018 lr: 0.02\n",
      "iteration: 350300 loss: 0.0017 lr: 0.02\n",
      "iteration: 350310 loss: 0.0018 lr: 0.02\n",
      "iteration: 350320 loss: 0.0009 lr: 0.02\n",
      "iteration: 350330 loss: 0.0016 lr: 0.02\n",
      "iteration: 350340 loss: 0.0013 lr: 0.02\n",
      "iteration: 350350 loss: 0.0016 lr: 0.02\n",
      "iteration: 350360 loss: 0.0014 lr: 0.02\n",
      "iteration: 350370 loss: 0.0017 lr: 0.02\n",
      "iteration: 350380 loss: 0.0013 lr: 0.02\n",
      "iteration: 350390 loss: 0.0015 lr: 0.02\n",
      "iteration: 350400 loss: 0.0017 lr: 0.02\n",
      "iteration: 350410 loss: 0.0012 lr: 0.02\n",
      "iteration: 350420 loss: 0.0019 lr: 0.02\n",
      "iteration: 350430 loss: 0.0017 lr: 0.02\n",
      "iteration: 350440 loss: 0.0017 lr: 0.02\n",
      "iteration: 350450 loss: 0.0023 lr: 0.02\n",
      "iteration: 350460 loss: 0.0014 lr: 0.02\n",
      "iteration: 350470 loss: 0.0022 lr: 0.02\n",
      "iteration: 350480 loss: 0.0015 lr: 0.02\n",
      "iteration: 350490 loss: 0.0011 lr: 0.02\n",
      "iteration: 350500 loss: 0.0019 lr: 0.02\n",
      "iteration: 350510 loss: 0.0021 lr: 0.02\n",
      "iteration: 350520 loss: 0.0014 lr: 0.02\n",
      "iteration: 350530 loss: 0.0020 lr: 0.02\n",
      "iteration: 350540 loss: 0.0018 lr: 0.02\n",
      "iteration: 350550 loss: 0.0025 lr: 0.02\n",
      "iteration: 350560 loss: 0.0015 lr: 0.02\n",
      "iteration: 350570 loss: 0.0021 lr: 0.02\n",
      "iteration: 350580 loss: 0.0022 lr: 0.02\n",
      "iteration: 350590 loss: 0.0013 lr: 0.02\n",
      "iteration: 350600 loss: 0.0013 lr: 0.02\n",
      "iteration: 350610 loss: 0.0017 lr: 0.02\n",
      "iteration: 350620 loss: 0.0021 lr: 0.02\n",
      "iteration: 350630 loss: 0.0013 lr: 0.02\n",
      "iteration: 350640 loss: 0.0014 lr: 0.02\n",
      "iteration: 350650 loss: 0.0013 lr: 0.02\n",
      "iteration: 350660 loss: 0.0014 lr: 0.02\n",
      "iteration: 350670 loss: 0.0026 lr: 0.02\n",
      "iteration: 350680 loss: 0.0014 lr: 0.02\n",
      "iteration: 350690 loss: 0.0016 lr: 0.02\n",
      "iteration: 350700 loss: 0.0013 lr: 0.02\n",
      "iteration: 350710 loss: 0.0025 lr: 0.02\n",
      "iteration: 350720 loss: 0.0016 lr: 0.02\n",
      "iteration: 350730 loss: 0.0016 lr: 0.02\n",
      "iteration: 350740 loss: 0.0016 lr: 0.02\n",
      "iteration: 350750 loss: 0.0012 lr: 0.02\n",
      "iteration: 350760 loss: 0.0018 lr: 0.02\n",
      "iteration: 350770 loss: 0.0017 lr: 0.02\n",
      "iteration: 350780 loss: 0.0022 lr: 0.02\n",
      "iteration: 350790 loss: 0.0012 lr: 0.02\n",
      "iteration: 350800 loss: 0.0020 lr: 0.02\n",
      "iteration: 350810 loss: 0.0024 lr: 0.02\n",
      "iteration: 350820 loss: 0.0014 lr: 0.02\n",
      "iteration: 350830 loss: 0.0013 lr: 0.02\n",
      "iteration: 350840 loss: 0.0013 lr: 0.02\n",
      "iteration: 350850 loss: 0.0014 lr: 0.02\n",
      "iteration: 350860 loss: 0.0025 lr: 0.02\n",
      "iteration: 350870 loss: 0.0014 lr: 0.02\n",
      "iteration: 350880 loss: 0.0011 lr: 0.02\n",
      "iteration: 350890 loss: 0.0016 lr: 0.02\n",
      "iteration: 350900 loss: 0.0016 lr: 0.02\n",
      "iteration: 350910 loss: 0.0013 lr: 0.02\n",
      "iteration: 350920 loss: 0.0015 lr: 0.02\n",
      "iteration: 350930 loss: 0.0016 lr: 0.02\n",
      "iteration: 350940 loss: 0.0015 lr: 0.02\n",
      "iteration: 350950 loss: 0.0014 lr: 0.02\n",
      "iteration: 350960 loss: 0.0017 lr: 0.02\n",
      "iteration: 350970 loss: 0.0016 lr: 0.02\n",
      "iteration: 350980 loss: 0.0015 lr: 0.02\n",
      "iteration: 350990 loss: 0.0010 lr: 0.02\n",
      "iteration: 351000 loss: 0.0012 lr: 0.02\n",
      "iteration: 351010 loss: 0.0014 lr: 0.02\n",
      "iteration: 351020 loss: 0.0016 lr: 0.02\n",
      "iteration: 351030 loss: 0.0014 lr: 0.02\n",
      "iteration: 351040 loss: 0.0014 lr: 0.02\n",
      "iteration: 351050 loss: 0.0019 lr: 0.02\n",
      "iteration: 351060 loss: 0.0013 lr: 0.02\n",
      "iteration: 351070 loss: 0.0014 lr: 0.02\n",
      "iteration: 351080 loss: 0.0019 lr: 0.02\n",
      "iteration: 351090 loss: 0.0012 lr: 0.02\n",
      "iteration: 351100 loss: 0.0017 lr: 0.02\n",
      "iteration: 351110 loss: 0.0021 lr: 0.02\n",
      "iteration: 351120 loss: 0.0017 lr: 0.02\n",
      "iteration: 351130 loss: 0.0014 lr: 0.02\n",
      "iteration: 351140 loss: 0.0024 lr: 0.02\n",
      "iteration: 351150 loss: 0.0017 lr: 0.02\n",
      "iteration: 351160 loss: 0.0018 lr: 0.02\n",
      "iteration: 351170 loss: 0.0026 lr: 0.02\n",
      "iteration: 351180 loss: 0.0016 lr: 0.02\n",
      "iteration: 351190 loss: 0.0019 lr: 0.02\n",
      "iteration: 351200 loss: 0.0025 lr: 0.02\n",
      "iteration: 351210 loss: 0.0014 lr: 0.02\n",
      "iteration: 351220 loss: 0.0019 lr: 0.02\n",
      "iteration: 351230 loss: 0.0018 lr: 0.02\n",
      "iteration: 351240 loss: 0.0018 lr: 0.02\n",
      "iteration: 351250 loss: 0.0014 lr: 0.02\n",
      "iteration: 351260 loss: 0.0016 lr: 0.02\n",
      "iteration: 351270 loss: 0.0022 lr: 0.02\n",
      "iteration: 351280 loss: 0.0017 lr: 0.02\n",
      "iteration: 351290 loss: 0.0021 lr: 0.02\n",
      "iteration: 351300 loss: 0.0013 lr: 0.02\n",
      "iteration: 351310 loss: 0.0010 lr: 0.02\n",
      "iteration: 351320 loss: 0.0013 lr: 0.02\n",
      "iteration: 351330 loss: 0.0020 lr: 0.02\n",
      "iteration: 351340 loss: 0.0012 lr: 0.02\n",
      "iteration: 351350 loss: 0.0018 lr: 0.02\n",
      "iteration: 351360 loss: 0.0014 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 351370 loss: 0.0016 lr: 0.02\n",
      "iteration: 351380 loss: 0.0014 lr: 0.02\n",
      "iteration: 351390 loss: 0.0013 lr: 0.02\n",
      "iteration: 351400 loss: 0.0016 lr: 0.02\n",
      "iteration: 351410 loss: 0.0009 lr: 0.02\n",
      "iteration: 351420 loss: 0.0019 lr: 0.02\n",
      "iteration: 351430 loss: 0.0017 lr: 0.02\n",
      "iteration: 351440 loss: 0.0017 lr: 0.02\n",
      "iteration: 351450 loss: 0.0022 lr: 0.02\n",
      "iteration: 351460 loss: 0.0023 lr: 0.02\n",
      "iteration: 351470 loss: 0.0016 lr: 0.02\n",
      "iteration: 351480 loss: 0.0017 lr: 0.02\n",
      "iteration: 351490 loss: 0.0019 lr: 0.02\n",
      "iteration: 351500 loss: 0.0019 lr: 0.02\n",
      "iteration: 351510 loss: 0.0019 lr: 0.02\n",
      "iteration: 351520 loss: 0.0022 lr: 0.02\n",
      "iteration: 351530 loss: 0.0019 lr: 0.02\n",
      "iteration: 351540 loss: 0.0014 lr: 0.02\n",
      "iteration: 351550 loss: 0.0019 lr: 0.02\n",
      "iteration: 351560 loss: 0.0013 lr: 0.02\n",
      "iteration: 351570 loss: 0.0013 lr: 0.02\n",
      "iteration: 351580 loss: 0.0011 lr: 0.02\n",
      "iteration: 351590 loss: 0.0012 lr: 0.02\n",
      "iteration: 351600 loss: 0.0018 lr: 0.02\n",
      "iteration: 351610 loss: 0.0013 lr: 0.02\n",
      "iteration: 351620 loss: 0.0017 lr: 0.02\n",
      "iteration: 351630 loss: 0.0022 lr: 0.02\n",
      "iteration: 351640 loss: 0.0019 lr: 0.02\n",
      "iteration: 351650 loss: 0.0015 lr: 0.02\n",
      "iteration: 351660 loss: 0.0015 lr: 0.02\n",
      "iteration: 351670 loss: 0.0020 lr: 0.02\n",
      "iteration: 351680 loss: 0.0020 lr: 0.02\n",
      "iteration: 351690 loss: 0.0016 lr: 0.02\n",
      "iteration: 351700 loss: 0.0018 lr: 0.02\n",
      "iteration: 351710 loss: 0.0019 lr: 0.02\n",
      "iteration: 351720 loss: 0.0017 lr: 0.02\n",
      "iteration: 351730 loss: 0.0016 lr: 0.02\n",
      "iteration: 351740 loss: 0.0013 lr: 0.02\n",
      "iteration: 351750 loss: 0.0012 lr: 0.02\n",
      "iteration: 351760 loss: 0.0015 lr: 0.02\n",
      "iteration: 351770 loss: 0.0018 lr: 0.02\n",
      "iteration: 351780 loss: 0.0016 lr: 0.02\n",
      "iteration: 351790 loss: 0.0013 lr: 0.02\n",
      "iteration: 351800 loss: 0.0046 lr: 0.02\n",
      "iteration: 351810 loss: 0.0020 lr: 0.02\n",
      "iteration: 351820 loss: 0.0013 lr: 0.02\n",
      "iteration: 351830 loss: 0.0015 lr: 0.02\n",
      "iteration: 351840 loss: 0.0019 lr: 0.02\n",
      "iteration: 351850 loss: 0.0021 lr: 0.02\n",
      "iteration: 351860 loss: 0.0012 lr: 0.02\n",
      "iteration: 351870 loss: 0.0022 lr: 0.02\n",
      "iteration: 351880 loss: 0.0017 lr: 0.02\n",
      "iteration: 351890 loss: 0.0012 lr: 0.02\n",
      "iteration: 351900 loss: 0.0016 lr: 0.02\n",
      "iteration: 351910 loss: 0.0019 lr: 0.02\n",
      "iteration: 351920 loss: 0.0014 lr: 0.02\n",
      "iteration: 351930 loss: 0.0019 lr: 0.02\n",
      "iteration: 351940 loss: 0.0030 lr: 0.02\n",
      "iteration: 351950 loss: 0.0017 lr: 0.02\n",
      "iteration: 351960 loss: 0.0020 lr: 0.02\n",
      "iteration: 351970 loss: 0.0021 lr: 0.02\n",
      "iteration: 351980 loss: 0.0016 lr: 0.02\n",
      "iteration: 351990 loss: 0.0018 lr: 0.02\n",
      "iteration: 352000 loss: 0.0015 lr: 0.02\n",
      "iteration: 352010 loss: 0.0021 lr: 0.02\n",
      "iteration: 352020 loss: 0.0011 lr: 0.02\n",
      "iteration: 352030 loss: 0.0023 lr: 0.02\n",
      "iteration: 352040 loss: 0.0014 lr: 0.02\n",
      "iteration: 352050 loss: 0.0018 lr: 0.02\n",
      "iteration: 352060 loss: 0.0013 lr: 0.02\n",
      "iteration: 352070 loss: 0.0023 lr: 0.02\n",
      "iteration: 352080 loss: 0.0014 lr: 0.02\n",
      "iteration: 352090 loss: 0.0023 lr: 0.02\n",
      "iteration: 352100 loss: 0.0018 lr: 0.02\n",
      "iteration: 352110 loss: 0.0013 lr: 0.02\n",
      "iteration: 352120 loss: 0.0014 lr: 0.02\n",
      "iteration: 352130 loss: 0.0015 lr: 0.02\n",
      "iteration: 352140 loss: 0.0020 lr: 0.02\n",
      "iteration: 352150 loss: 0.0016 lr: 0.02\n",
      "iteration: 352160 loss: 0.0013 lr: 0.02\n",
      "iteration: 352170 loss: 0.0016 lr: 0.02\n",
      "iteration: 352180 loss: 0.0013 lr: 0.02\n",
      "iteration: 352190 loss: 0.0015 lr: 0.02\n",
      "iteration: 352200 loss: 0.0018 lr: 0.02\n",
      "iteration: 352210 loss: 0.0012 lr: 0.02\n",
      "iteration: 352220 loss: 0.0019 lr: 0.02\n",
      "iteration: 352230 loss: 0.0021 lr: 0.02\n",
      "iteration: 352240 loss: 0.0016 lr: 0.02\n",
      "iteration: 352250 loss: 0.0015 lr: 0.02\n",
      "iteration: 352260 loss: 0.0013 lr: 0.02\n",
      "iteration: 352270 loss: 0.0022 lr: 0.02\n",
      "iteration: 352280 loss: 0.0019 lr: 0.02\n",
      "iteration: 352290 loss: 0.0010 lr: 0.02\n",
      "iteration: 352300 loss: 0.0016 lr: 0.02\n",
      "iteration: 352310 loss: 0.0015 lr: 0.02\n",
      "iteration: 352320 loss: 0.0018 lr: 0.02\n",
      "iteration: 352330 loss: 0.0013 lr: 0.02\n",
      "iteration: 352340 loss: 0.0015 lr: 0.02\n",
      "iteration: 352350 loss: 0.0016 lr: 0.02\n",
      "iteration: 352360 loss: 0.0021 lr: 0.02\n",
      "iteration: 352370 loss: 0.0016 lr: 0.02\n",
      "iteration: 352380 loss: 0.0021 lr: 0.02\n",
      "iteration: 352390 loss: 0.0017 lr: 0.02\n",
      "iteration: 352400 loss: 0.0014 lr: 0.02\n",
      "iteration: 352410 loss: 0.0015 lr: 0.02\n",
      "iteration: 352420 loss: 0.0018 lr: 0.02\n",
      "iteration: 352430 loss: 0.0013 lr: 0.02\n",
      "iteration: 352440 loss: 0.0013 lr: 0.02\n",
      "iteration: 352450 loss: 0.0017 lr: 0.02\n",
      "iteration: 352460 loss: 0.0012 lr: 0.02\n",
      "iteration: 352470 loss: 0.0012 lr: 0.02\n",
      "iteration: 352480 loss: 0.0015 lr: 0.02\n",
      "iteration: 352490 loss: 0.0010 lr: 0.02\n",
      "iteration: 352500 loss: 0.0021 lr: 0.02\n",
      "iteration: 352510 loss: 0.0013 lr: 0.02\n",
      "iteration: 352520 loss: 0.0020 lr: 0.02\n",
      "iteration: 352530 loss: 0.0016 lr: 0.02\n",
      "iteration: 352540 loss: 0.0016 lr: 0.02\n",
      "iteration: 352550 loss: 0.0020 lr: 0.02\n",
      "iteration: 352560 loss: 0.0015 lr: 0.02\n",
      "iteration: 352570 loss: 0.0017 lr: 0.02\n",
      "iteration: 352580 loss: 0.0014 lr: 0.02\n",
      "iteration: 352590 loss: 0.0013 lr: 0.02\n",
      "iteration: 352600 loss: 0.0015 lr: 0.02\n",
      "iteration: 352610 loss: 0.0020 lr: 0.02\n",
      "iteration: 352620 loss: 0.0016 lr: 0.02\n",
      "iteration: 352630 loss: 0.0012 lr: 0.02\n",
      "iteration: 352640 loss: 0.0013 lr: 0.02\n",
      "iteration: 352650 loss: 0.0014 lr: 0.02\n",
      "iteration: 352660 loss: 0.0017 lr: 0.02\n",
      "iteration: 352670 loss: 0.0016 lr: 0.02\n",
      "iteration: 352680 loss: 0.0023 lr: 0.02\n",
      "iteration: 352690 loss: 0.0013 lr: 0.02\n",
      "iteration: 352700 loss: 0.0014 lr: 0.02\n",
      "iteration: 352710 loss: 0.0017 lr: 0.02\n",
      "iteration: 352720 loss: 0.0019 lr: 0.02\n",
      "iteration: 352730 loss: 0.0013 lr: 0.02\n",
      "iteration: 352740 loss: 0.0021 lr: 0.02\n",
      "iteration: 352750 loss: 0.0023 lr: 0.02\n",
      "iteration: 352760 loss: 0.0020 lr: 0.02\n",
      "iteration: 352770 loss: 0.0018 lr: 0.02\n",
      "iteration: 352780 loss: 0.0017 lr: 0.02\n",
      "iteration: 352790 loss: 0.0021 lr: 0.02\n",
      "iteration: 352800 loss: 0.0018 lr: 0.02\n",
      "iteration: 352810 loss: 0.0017 lr: 0.02\n",
      "iteration: 352820 loss: 0.0015 lr: 0.02\n",
      "iteration: 352830 loss: 0.0022 lr: 0.02\n",
      "iteration: 352840 loss: 0.0016 lr: 0.02\n",
      "iteration: 352850 loss: 0.0013 lr: 0.02\n",
      "iteration: 352860 loss: 0.0013 lr: 0.02\n",
      "iteration: 352870 loss: 0.0021 lr: 0.02\n",
      "iteration: 352880 loss: 0.0012 lr: 0.02\n",
      "iteration: 352890 loss: 0.0011 lr: 0.02\n",
      "iteration: 352900 loss: 0.0019 lr: 0.02\n",
      "iteration: 352910 loss: 0.0015 lr: 0.02\n",
      "iteration: 352920 loss: 0.0019 lr: 0.02\n",
      "iteration: 352930 loss: 0.0012 lr: 0.02\n",
      "iteration: 352940 loss: 0.0016 lr: 0.02\n",
      "iteration: 352950 loss: 0.0010 lr: 0.02\n",
      "iteration: 352960 loss: 0.0023 lr: 0.02\n",
      "iteration: 352970 loss: 0.0016 lr: 0.02\n",
      "iteration: 352980 loss: 0.0015 lr: 0.02\n",
      "iteration: 352990 loss: 0.0015 lr: 0.02\n",
      "iteration: 353000 loss: 0.0013 lr: 0.02\n",
      "iteration: 353010 loss: 0.0017 lr: 0.02\n",
      "iteration: 353020 loss: 0.0022 lr: 0.02\n",
      "iteration: 353030 loss: 0.0021 lr: 0.02\n",
      "iteration: 353040 loss: 0.0014 lr: 0.02\n",
      "iteration: 353050 loss: 0.0016 lr: 0.02\n",
      "iteration: 353060 loss: 0.0015 lr: 0.02\n",
      "iteration: 353070 loss: 0.0020 lr: 0.02\n",
      "iteration: 353080 loss: 0.0014 lr: 0.02\n",
      "iteration: 353090 loss: 0.0021 lr: 0.02\n",
      "iteration: 353100 loss: 0.0015 lr: 0.02\n",
      "iteration: 353110 loss: 0.0013 lr: 0.02\n",
      "iteration: 353120 loss: 0.0014 lr: 0.02\n",
      "iteration: 353130 loss: 0.0019 lr: 0.02\n",
      "iteration: 353140 loss: 0.0022 lr: 0.02\n",
      "iteration: 353150 loss: 0.0014 lr: 0.02\n",
      "iteration: 353160 loss: 0.0015 lr: 0.02\n",
      "iteration: 353170 loss: 0.0014 lr: 0.02\n",
      "iteration: 353180 loss: 0.0013 lr: 0.02\n",
      "iteration: 353190 loss: 0.0013 lr: 0.02\n",
      "iteration: 353200 loss: 0.0016 lr: 0.02\n",
      "iteration: 353210 loss: 0.0012 lr: 0.02\n",
      "iteration: 353220 loss: 0.0019 lr: 0.02\n",
      "iteration: 353230 loss: 0.0018 lr: 0.02\n",
      "iteration: 353240 loss: 0.0016 lr: 0.02\n",
      "iteration: 353250 loss: 0.0015 lr: 0.02\n",
      "iteration: 353260 loss: 0.0027 lr: 0.02\n",
      "iteration: 353270 loss: 0.0018 lr: 0.02\n",
      "iteration: 353280 loss: 0.0015 lr: 0.02\n",
      "iteration: 353290 loss: 0.0023 lr: 0.02\n",
      "iteration: 353300 loss: 0.0019 lr: 0.02\n",
      "iteration: 353310 loss: 0.0014 lr: 0.02\n",
      "iteration: 353320 loss: 0.0019 lr: 0.02\n",
      "iteration: 353330 loss: 0.0016 lr: 0.02\n",
      "iteration: 353340 loss: 0.0013 lr: 0.02\n",
      "iteration: 353350 loss: 0.0015 lr: 0.02\n",
      "iteration: 353360 loss: 0.0010 lr: 0.02\n",
      "iteration: 353370 loss: 0.0016 lr: 0.02\n",
      "iteration: 353380 loss: 0.0018 lr: 0.02\n",
      "iteration: 353390 loss: 0.0017 lr: 0.02\n",
      "iteration: 353400 loss: 0.0017 lr: 0.02\n",
      "iteration: 353410 loss: 0.0013 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 353420 loss: 0.0014 lr: 0.02\n",
      "iteration: 353430 loss: 0.0012 lr: 0.02\n",
      "iteration: 353440 loss: 0.0033 lr: 0.02\n",
      "iteration: 353450 loss: 0.0012 lr: 0.02\n",
      "iteration: 353460 loss: 0.0015 lr: 0.02\n",
      "iteration: 353470 loss: 0.0013 lr: 0.02\n",
      "iteration: 353480 loss: 0.0014 lr: 0.02\n",
      "iteration: 353490 loss: 0.0017 lr: 0.02\n",
      "iteration: 353500 loss: 0.0017 lr: 0.02\n",
      "iteration: 353510 loss: 0.0017 lr: 0.02\n",
      "iteration: 353520 loss: 0.0013 lr: 0.02\n",
      "iteration: 353530 loss: 0.0013 lr: 0.02\n",
      "iteration: 353540 loss: 0.0014 lr: 0.02\n",
      "iteration: 353550 loss: 0.0010 lr: 0.02\n",
      "iteration: 353560 loss: 0.0016 lr: 0.02\n",
      "iteration: 353570 loss: 0.0022 lr: 0.02\n",
      "iteration: 353580 loss: 0.0011 lr: 0.02\n",
      "iteration: 353590 loss: 0.0013 lr: 0.02\n",
      "iteration: 353600 loss: 0.0026 lr: 0.02\n",
      "iteration: 353610 loss: 0.0016 lr: 0.02\n",
      "iteration: 353620 loss: 0.0015 lr: 0.02\n",
      "iteration: 353630 loss: 0.0015 lr: 0.02\n",
      "iteration: 353640 loss: 0.0019 lr: 0.02\n",
      "iteration: 353650 loss: 0.0012 lr: 0.02\n",
      "iteration: 353660 loss: 0.0015 lr: 0.02\n",
      "iteration: 353670 loss: 0.0013 lr: 0.02\n",
      "iteration: 353680 loss: 0.0014 lr: 0.02\n",
      "iteration: 353690 loss: 0.0013 lr: 0.02\n",
      "iteration: 353700 loss: 0.0016 lr: 0.02\n",
      "iteration: 353710 loss: 0.0015 lr: 0.02\n",
      "iteration: 353720 loss: 0.0018 lr: 0.02\n",
      "iteration: 353730 loss: 0.0020 lr: 0.02\n",
      "iteration: 353740 loss: 0.0014 lr: 0.02\n",
      "iteration: 353750 loss: 0.0018 lr: 0.02\n",
      "iteration: 353760 loss: 0.0016 lr: 0.02\n",
      "iteration: 353770 loss: 0.0028 lr: 0.02\n",
      "iteration: 353780 loss: 0.0023 lr: 0.02\n",
      "iteration: 353790 loss: 0.0013 lr: 0.02\n",
      "iteration: 353800 loss: 0.0020 lr: 0.02\n",
      "iteration: 353810 loss: 0.0021 lr: 0.02\n",
      "iteration: 353820 loss: 0.0013 lr: 0.02\n",
      "iteration: 353830 loss: 0.0017 lr: 0.02\n",
      "iteration: 353840 loss: 0.0015 lr: 0.02\n",
      "iteration: 353850 loss: 0.0014 lr: 0.02\n",
      "iteration: 353860 loss: 0.0013 lr: 0.02\n",
      "iteration: 353870 loss: 0.0014 lr: 0.02\n",
      "iteration: 353880 loss: 0.0016 lr: 0.02\n",
      "iteration: 353890 loss: 0.0019 lr: 0.02\n",
      "iteration: 353900 loss: 0.0013 lr: 0.02\n",
      "iteration: 353910 loss: 0.0016 lr: 0.02\n",
      "iteration: 353920 loss: 0.0013 lr: 0.02\n",
      "iteration: 353930 loss: 0.0026 lr: 0.02\n",
      "iteration: 353940 loss: 0.0014 lr: 0.02\n",
      "iteration: 353950 loss: 0.0024 lr: 0.02\n",
      "iteration: 353960 loss: 0.0011 lr: 0.02\n",
      "iteration: 353970 loss: 0.0015 lr: 0.02\n",
      "iteration: 353980 loss: 0.0023 lr: 0.02\n",
      "iteration: 353990 loss: 0.0018 lr: 0.02\n",
      "iteration: 354000 loss: 0.0020 lr: 0.02\n",
      "iteration: 354010 loss: 0.0019 lr: 0.02\n",
      "iteration: 354020 loss: 0.0016 lr: 0.02\n",
      "iteration: 354030 loss: 0.0019 lr: 0.02\n",
      "iteration: 354040 loss: 0.0017 lr: 0.02\n",
      "iteration: 354050 loss: 0.0010 lr: 0.02\n",
      "iteration: 354060 loss: 0.0013 lr: 0.02\n",
      "iteration: 354070 loss: 0.0015 lr: 0.02\n",
      "iteration: 354080 loss: 0.0013 lr: 0.02\n",
      "iteration: 354090 loss: 0.0021 lr: 0.02\n",
      "iteration: 354100 loss: 0.0017 lr: 0.02\n",
      "iteration: 354110 loss: 0.0021 lr: 0.02\n",
      "iteration: 354120 loss: 0.0020 lr: 0.02\n",
      "iteration: 354130 loss: 0.0021 lr: 0.02\n",
      "iteration: 354140 loss: 0.0019 lr: 0.02\n",
      "iteration: 354150 loss: 0.0012 lr: 0.02\n",
      "iteration: 354160 loss: 0.0012 lr: 0.02\n",
      "iteration: 354170 loss: 0.0019 lr: 0.02\n",
      "iteration: 354180 loss: 0.0013 lr: 0.02\n",
      "iteration: 354190 loss: 0.0017 lr: 0.02\n",
      "iteration: 354200 loss: 0.0029 lr: 0.02\n",
      "iteration: 354210 loss: 0.0012 lr: 0.02\n",
      "iteration: 354220 loss: 0.0017 lr: 0.02\n",
      "iteration: 354230 loss: 0.0016 lr: 0.02\n",
      "iteration: 354240 loss: 0.0011 lr: 0.02\n",
      "iteration: 354250 loss: 0.0009 lr: 0.02\n",
      "iteration: 354260 loss: 0.0010 lr: 0.02\n",
      "iteration: 354270 loss: 0.0014 lr: 0.02\n",
      "iteration: 354280 loss: 0.0016 lr: 0.02\n",
      "iteration: 354290 loss: 0.0014 lr: 0.02\n",
      "iteration: 354300 loss: 0.0017 lr: 0.02\n",
      "iteration: 354310 loss: 0.0012 lr: 0.02\n",
      "iteration: 354320 loss: 0.0018 lr: 0.02\n",
      "iteration: 354330 loss: 0.0021 lr: 0.02\n",
      "iteration: 354340 loss: 0.0017 lr: 0.02\n",
      "iteration: 354350 loss: 0.0018 lr: 0.02\n",
      "iteration: 354360 loss: 0.0023 lr: 0.02\n",
      "iteration: 354370 loss: 0.0022 lr: 0.02\n",
      "iteration: 354380 loss: 0.0016 lr: 0.02\n",
      "iteration: 354390 loss: 0.0020 lr: 0.02\n",
      "iteration: 354400 loss: 0.0019 lr: 0.02\n",
      "iteration: 354410 loss: 0.0016 lr: 0.02\n",
      "iteration: 354420 loss: 0.0012 lr: 0.02\n",
      "iteration: 354430 loss: 0.0014 lr: 0.02\n",
      "iteration: 354440 loss: 0.0020 lr: 0.02\n",
      "iteration: 354450 loss: 0.0021 lr: 0.02\n",
      "iteration: 354460 loss: 0.0019 lr: 0.02\n",
      "iteration: 354470 loss: 0.0022 lr: 0.02\n",
      "iteration: 354480 loss: 0.0018 lr: 0.02\n",
      "iteration: 354490 loss: 0.0016 lr: 0.02\n",
      "iteration: 354500 loss: 0.0019 lr: 0.02\n",
      "iteration: 354510 loss: 0.0015 lr: 0.02\n",
      "iteration: 354520 loss: 0.0025 lr: 0.02\n",
      "iteration: 354530 loss: 0.0018 lr: 0.02\n",
      "iteration: 354540 loss: 0.0017 lr: 0.02\n",
      "iteration: 354550 loss: 0.0014 lr: 0.02\n",
      "iteration: 354560 loss: 0.0018 lr: 0.02\n",
      "iteration: 354570 loss: 0.0020 lr: 0.02\n",
      "iteration: 354580 loss: 0.0021 lr: 0.02\n",
      "iteration: 354590 loss: 0.0021 lr: 0.02\n",
      "iteration: 354600 loss: 0.0020 lr: 0.02\n",
      "iteration: 354610 loss: 0.0017 lr: 0.02\n",
      "iteration: 354620 loss: 0.0018 lr: 0.02\n",
      "iteration: 354630 loss: 0.0019 lr: 0.02\n",
      "iteration: 354640 loss: 0.0017 lr: 0.02\n",
      "iteration: 354650 loss: 0.0013 lr: 0.02\n",
      "iteration: 354660 loss: 0.0017 lr: 0.02\n",
      "iteration: 354670 loss: 0.0017 lr: 0.02\n",
      "iteration: 354680 loss: 0.0010 lr: 0.02\n",
      "iteration: 354690 loss: 0.0017 lr: 0.02\n",
      "iteration: 354700 loss: 0.0014 lr: 0.02\n",
      "iteration: 354710 loss: 0.0024 lr: 0.02\n",
      "iteration: 354720 loss: 0.0016 lr: 0.02\n",
      "iteration: 354730 loss: 0.0018 lr: 0.02\n",
      "iteration: 354740 loss: 0.0018 lr: 0.02\n",
      "iteration: 354750 loss: 0.0013 lr: 0.02\n",
      "iteration: 354760 loss: 0.0020 lr: 0.02\n",
      "iteration: 354770 loss: 0.0022 lr: 0.02\n",
      "iteration: 354780 loss: 0.0018 lr: 0.02\n",
      "iteration: 354790 loss: 0.0019 lr: 0.02\n",
      "iteration: 354800 loss: 0.0018 lr: 0.02\n",
      "iteration: 354810 loss: 0.0017 lr: 0.02\n",
      "iteration: 354820 loss: 0.0013 lr: 0.02\n",
      "iteration: 354830 loss: 0.0020 lr: 0.02\n",
      "iteration: 354840 loss: 0.0019 lr: 0.02\n",
      "iteration: 354850 loss: 0.0014 lr: 0.02\n",
      "iteration: 354860 loss: 0.0025 lr: 0.02\n",
      "iteration: 354870 loss: 0.0033 lr: 0.02\n",
      "iteration: 354880 loss: 0.0019 lr: 0.02\n",
      "iteration: 354890 loss: 0.0022 lr: 0.02\n",
      "iteration: 354900 loss: 0.0013 lr: 0.02\n",
      "iteration: 354910 loss: 0.0018 lr: 0.02\n",
      "iteration: 354920 loss: 0.0016 lr: 0.02\n",
      "iteration: 354930 loss: 0.0015 lr: 0.02\n",
      "iteration: 354940 loss: 0.0015 lr: 0.02\n",
      "iteration: 354950 loss: 0.0019 lr: 0.02\n",
      "iteration: 354960 loss: 0.0015 lr: 0.02\n",
      "iteration: 354970 loss: 0.0016 lr: 0.02\n",
      "iteration: 354980 loss: 0.0021 lr: 0.02\n",
      "iteration: 354990 loss: 0.0022 lr: 0.02\n",
      "iteration: 355000 loss: 0.0021 lr: 0.02\n",
      "iteration: 355010 loss: 0.0022 lr: 0.02\n",
      "iteration: 355020 loss: 0.0017 lr: 0.02\n",
      "iteration: 355030 loss: 0.0014 lr: 0.02\n",
      "iteration: 355040 loss: 0.0023 lr: 0.02\n",
      "iteration: 355050 loss: 0.0016 lr: 0.02\n",
      "iteration: 355060 loss: 0.0021 lr: 0.02\n",
      "iteration: 355070 loss: 0.0017 lr: 0.02\n",
      "iteration: 355080 loss: 0.0016 lr: 0.02\n",
      "iteration: 355090 loss: 0.0017 lr: 0.02\n",
      "iteration: 355100 loss: 0.0014 lr: 0.02\n",
      "iteration: 355110 loss: 0.0013 lr: 0.02\n",
      "iteration: 355120 loss: 0.0023 lr: 0.02\n",
      "iteration: 355130 loss: 0.0012 lr: 0.02\n",
      "iteration: 355140 loss: 0.0016 lr: 0.02\n",
      "iteration: 355150 loss: 0.0015 lr: 0.02\n",
      "iteration: 355160 loss: 0.0016 lr: 0.02\n",
      "iteration: 355170 loss: 0.0021 lr: 0.02\n",
      "iteration: 355180 loss: 0.0015 lr: 0.02\n",
      "iteration: 355190 loss: 0.0012 lr: 0.02\n",
      "iteration: 355200 loss: 0.0016 lr: 0.02\n",
      "iteration: 355210 loss: 0.0027 lr: 0.02\n",
      "iteration: 355220 loss: 0.0020 lr: 0.02\n",
      "iteration: 355230 loss: 0.0018 lr: 0.02\n",
      "iteration: 355240 loss: 0.0017 lr: 0.02\n",
      "iteration: 355250 loss: 0.0019 lr: 0.02\n",
      "iteration: 355260 loss: 0.0016 lr: 0.02\n",
      "iteration: 355270 loss: 0.0018 lr: 0.02\n",
      "iteration: 355280 loss: 0.0015 lr: 0.02\n",
      "iteration: 355290 loss: 0.0015 lr: 0.02\n",
      "iteration: 355300 loss: 0.0018 lr: 0.02\n",
      "iteration: 355310 loss: 0.0023 lr: 0.02\n",
      "iteration: 355320 loss: 0.0015 lr: 0.02\n",
      "iteration: 355330 loss: 0.0014 lr: 0.02\n",
      "iteration: 355340 loss: 0.0017 lr: 0.02\n",
      "iteration: 355350 loss: 0.0015 lr: 0.02\n",
      "iteration: 355360 loss: 0.0012 lr: 0.02\n",
      "iteration: 355370 loss: 0.0013 lr: 0.02\n",
      "iteration: 355380 loss: 0.0015 lr: 0.02\n",
      "iteration: 355390 loss: 0.0017 lr: 0.02\n",
      "iteration: 355400 loss: 0.0012 lr: 0.02\n",
      "iteration: 355410 loss: 0.0012 lr: 0.02\n",
      "iteration: 355420 loss: 0.0021 lr: 0.02\n",
      "iteration: 355430 loss: 0.0012 lr: 0.02\n",
      "iteration: 355440 loss: 0.0012 lr: 0.02\n",
      "iteration: 355450 loss: 0.0012 lr: 0.02\n",
      "iteration: 355460 loss: 0.0013 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 355470 loss: 0.0014 lr: 0.02\n",
      "iteration: 355480 loss: 0.0010 lr: 0.02\n",
      "iteration: 355490 loss: 0.0019 lr: 0.02\n",
      "iteration: 355500 loss: 0.0018 lr: 0.02\n",
      "iteration: 355510 loss: 0.0016 lr: 0.02\n",
      "iteration: 355520 loss: 0.0013 lr: 0.02\n",
      "iteration: 355530 loss: 0.0017 lr: 0.02\n",
      "iteration: 355540 loss: 0.0013 lr: 0.02\n",
      "iteration: 355550 loss: 0.0018 lr: 0.02\n",
      "iteration: 355560 loss: 0.0020 lr: 0.02\n",
      "iteration: 355570 loss: 0.0021 lr: 0.02\n",
      "iteration: 355580 loss: 0.0016 lr: 0.02\n",
      "iteration: 355590 loss: 0.0014 lr: 0.02\n",
      "iteration: 355600 loss: 0.0021 lr: 0.02\n",
      "iteration: 355610 loss: 0.0017 lr: 0.02\n",
      "iteration: 355620 loss: 0.0020 lr: 0.02\n",
      "iteration: 355630 loss: 0.0013 lr: 0.02\n",
      "iteration: 355640 loss: 0.0016 lr: 0.02\n",
      "iteration: 355650 loss: 0.0017 lr: 0.02\n",
      "iteration: 355660 loss: 0.0019 lr: 0.02\n",
      "iteration: 355670 loss: 0.0013 lr: 0.02\n",
      "iteration: 355680 loss: 0.0014 lr: 0.02\n",
      "iteration: 355690 loss: 0.0021 lr: 0.02\n",
      "iteration: 355700 loss: 0.0018 lr: 0.02\n",
      "iteration: 355710 loss: 0.0017 lr: 0.02\n",
      "iteration: 355720 loss: 0.0021 lr: 0.02\n",
      "iteration: 355730 loss: 0.0017 lr: 0.02\n",
      "iteration: 355740 loss: 0.0020 lr: 0.02\n",
      "iteration: 355750 loss: 0.0025 lr: 0.02\n",
      "iteration: 355760 loss: 0.0012 lr: 0.02\n",
      "iteration: 355770 loss: 0.0014 lr: 0.02\n",
      "iteration: 355780 loss: 0.0017 lr: 0.02\n",
      "iteration: 355790 loss: 0.0021 lr: 0.02\n",
      "iteration: 355800 loss: 0.0018 lr: 0.02\n",
      "iteration: 355810 loss: 0.0018 lr: 0.02\n",
      "iteration: 355820 loss: 0.0017 lr: 0.02\n",
      "iteration: 355830 loss: 0.0017 lr: 0.02\n",
      "iteration: 355840 loss: 0.0012 lr: 0.02\n",
      "iteration: 355850 loss: 0.0014 lr: 0.02\n",
      "iteration: 355860 loss: 0.0017 lr: 0.02\n",
      "iteration: 355870 loss: 0.0016 lr: 0.02\n",
      "iteration: 355880 loss: 0.0015 lr: 0.02\n",
      "iteration: 355890 loss: 0.0013 lr: 0.02\n",
      "iteration: 355900 loss: 0.0028 lr: 0.02\n",
      "iteration: 355910 loss: 0.0026 lr: 0.02\n",
      "iteration: 355920 loss: 0.0018 lr: 0.02\n",
      "iteration: 355930 loss: 0.0018 lr: 0.02\n",
      "iteration: 355940 loss: 0.0013 lr: 0.02\n",
      "iteration: 355950 loss: 0.0019 lr: 0.02\n",
      "iteration: 355960 loss: 0.0017 lr: 0.02\n",
      "iteration: 355970 loss: 0.0020 lr: 0.02\n",
      "iteration: 355980 loss: 0.0016 lr: 0.02\n",
      "iteration: 355990 loss: 0.0015 lr: 0.02\n",
      "iteration: 356000 loss: 0.0018 lr: 0.02\n",
      "iteration: 356010 loss: 0.0017 lr: 0.02\n",
      "iteration: 356020 loss: 0.0018 lr: 0.02\n",
      "iteration: 356030 loss: 0.0030 lr: 0.02\n",
      "iteration: 356040 loss: 0.0015 lr: 0.02\n",
      "iteration: 356050 loss: 0.0015 lr: 0.02\n",
      "iteration: 356060 loss: 0.0014 lr: 0.02\n",
      "iteration: 356070 loss: 0.0014 lr: 0.02\n",
      "iteration: 356080 loss: 0.0012 lr: 0.02\n",
      "iteration: 356090 loss: 0.0016 lr: 0.02\n",
      "iteration: 356100 loss: 0.0015 lr: 0.02\n",
      "iteration: 356110 loss: 0.0019 lr: 0.02\n",
      "iteration: 356120 loss: 0.0022 lr: 0.02\n",
      "iteration: 356130 loss: 0.0013 lr: 0.02\n",
      "iteration: 356140 loss: 0.0018 lr: 0.02\n",
      "iteration: 356150 loss: 0.0017 lr: 0.02\n",
      "iteration: 356160 loss: 0.0014 lr: 0.02\n",
      "iteration: 356170 loss: 0.0012 lr: 0.02\n",
      "iteration: 356180 loss: 0.0014 lr: 0.02\n",
      "iteration: 356190 loss: 0.0020 lr: 0.02\n",
      "iteration: 356200 loss: 0.0015 lr: 0.02\n",
      "iteration: 356210 loss: 0.0025 lr: 0.02\n",
      "iteration: 356220 loss: 0.0020 lr: 0.02\n",
      "iteration: 356230 loss: 0.0017 lr: 0.02\n",
      "iteration: 356240 loss: 0.0015 lr: 0.02\n",
      "iteration: 356250 loss: 0.0019 lr: 0.02\n",
      "iteration: 356260 loss: 0.0014 lr: 0.02\n",
      "iteration: 356270 loss: 0.0024 lr: 0.02\n",
      "iteration: 356280 loss: 0.0017 lr: 0.02\n",
      "iteration: 356290 loss: 0.0013 lr: 0.02\n",
      "iteration: 356300 loss: 0.0017 lr: 0.02\n",
      "iteration: 356310 loss: 0.0015 lr: 0.02\n",
      "iteration: 356320 loss: 0.0017 lr: 0.02\n",
      "iteration: 356330 loss: 0.0011 lr: 0.02\n",
      "iteration: 356340 loss: 0.0018 lr: 0.02\n",
      "iteration: 356350 loss: 0.0013 lr: 0.02\n",
      "iteration: 356360 loss: 0.0015 lr: 0.02\n",
      "iteration: 356370 loss: 0.0014 lr: 0.02\n",
      "iteration: 356380 loss: 0.0016 lr: 0.02\n",
      "iteration: 356390 loss: 0.0011 lr: 0.02\n",
      "iteration: 356400 loss: 0.0021 lr: 0.02\n",
      "iteration: 356410 loss: 0.0013 lr: 0.02\n",
      "iteration: 356420 loss: 0.0012 lr: 0.02\n",
      "iteration: 356430 loss: 0.0011 lr: 0.02\n",
      "iteration: 356440 loss: 0.0013 lr: 0.02\n",
      "iteration: 356450 loss: 0.0016 lr: 0.02\n",
      "iteration: 356460 loss: 0.0022 lr: 0.02\n",
      "iteration: 356470 loss: 0.0014 lr: 0.02\n",
      "iteration: 356480 loss: 0.0014 lr: 0.02\n",
      "iteration: 356490 loss: 0.0015 lr: 0.02\n",
      "iteration: 356500 loss: 0.0017 lr: 0.02\n",
      "iteration: 356510 loss: 0.0017 lr: 0.02\n",
      "iteration: 356520 loss: 0.0021 lr: 0.02\n",
      "iteration: 356530 loss: 0.0011 lr: 0.02\n",
      "iteration: 356540 loss: 0.0017 lr: 0.02\n",
      "iteration: 356550 loss: 0.0011 lr: 0.02\n",
      "iteration: 356560 loss: 0.0014 lr: 0.02\n",
      "iteration: 356570 loss: 0.0017 lr: 0.02\n",
      "iteration: 356580 loss: 0.0014 lr: 0.02\n",
      "iteration: 356590 loss: 0.0017 lr: 0.02\n",
      "iteration: 356600 loss: 0.0012 lr: 0.02\n",
      "iteration: 356610 loss: 0.0021 lr: 0.02\n",
      "iteration: 356620 loss: 0.0016 lr: 0.02\n",
      "iteration: 356630 loss: 0.0025 lr: 0.02\n",
      "iteration: 356640 loss: 0.0016 lr: 0.02\n",
      "iteration: 356650 loss: 0.0015 lr: 0.02\n",
      "iteration: 356660 loss: 0.0012 lr: 0.02\n",
      "iteration: 356670 loss: 0.0011 lr: 0.02\n",
      "iteration: 356680 loss: 0.0028 lr: 0.02\n",
      "iteration: 356690 loss: 0.0016 lr: 0.02\n",
      "iteration: 356700 loss: 0.0016 lr: 0.02\n",
      "iteration: 356710 loss: 0.0020 lr: 0.02\n",
      "iteration: 356720 loss: 0.0018 lr: 0.02\n",
      "iteration: 356730 loss: 0.0016 lr: 0.02\n",
      "iteration: 356740 loss: 0.0014 lr: 0.02\n",
      "iteration: 356750 loss: 0.0020 lr: 0.02\n",
      "iteration: 356760 loss: 0.0025 lr: 0.02\n",
      "iteration: 356770 loss: 0.0022 lr: 0.02\n",
      "iteration: 356780 loss: 0.0025 lr: 0.02\n",
      "iteration: 356790 loss: 0.0020 lr: 0.02\n",
      "iteration: 356800 loss: 0.0015 lr: 0.02\n",
      "iteration: 356810 loss: 0.0033 lr: 0.02\n",
      "iteration: 356820 loss: 0.0019 lr: 0.02\n",
      "iteration: 356830 loss: 0.0018 lr: 0.02\n",
      "iteration: 356840 loss: 0.0016 lr: 0.02\n",
      "iteration: 356850 loss: 0.0012 lr: 0.02\n",
      "iteration: 356860 loss: 0.0019 lr: 0.02\n",
      "iteration: 356870 loss: 0.0026 lr: 0.02\n",
      "iteration: 356880 loss: 0.0015 lr: 0.02\n",
      "iteration: 356890 loss: 0.0019 lr: 0.02\n",
      "iteration: 356900 loss: 0.0032 lr: 0.02\n",
      "iteration: 356910 loss: 0.0016 lr: 0.02\n",
      "iteration: 356920 loss: 0.0013 lr: 0.02\n",
      "iteration: 356930 loss: 0.0027 lr: 0.02\n",
      "iteration: 356940 loss: 0.0014 lr: 0.02\n",
      "iteration: 356950 loss: 0.0014 lr: 0.02\n",
      "iteration: 356960 loss: 0.0023 lr: 0.02\n",
      "iteration: 356970 loss: 0.0023 lr: 0.02\n",
      "iteration: 356980 loss: 0.0012 lr: 0.02\n",
      "iteration: 356990 loss: 0.0015 lr: 0.02\n",
      "iteration: 357000 loss: 0.0018 lr: 0.02\n",
      "iteration: 357010 loss: 0.0019 lr: 0.02\n",
      "iteration: 357020 loss: 0.0020 lr: 0.02\n",
      "iteration: 357030 loss: 0.0011 lr: 0.02\n",
      "iteration: 357040 loss: 0.0018 lr: 0.02\n",
      "iteration: 357050 loss: 0.0019 lr: 0.02\n",
      "iteration: 357060 loss: 0.0016 lr: 0.02\n",
      "iteration: 357070 loss: 0.0015 lr: 0.02\n",
      "iteration: 357080 loss: 0.0019 lr: 0.02\n",
      "iteration: 357090 loss: 0.0022 lr: 0.02\n",
      "iteration: 357100 loss: 0.0014 lr: 0.02\n",
      "iteration: 357110 loss: 0.0022 lr: 0.02\n",
      "iteration: 357120 loss: 0.0013 lr: 0.02\n",
      "iteration: 357130 loss: 0.0012 lr: 0.02\n",
      "iteration: 357140 loss: 0.0022 lr: 0.02\n",
      "iteration: 357150 loss: 0.0019 lr: 0.02\n",
      "iteration: 357160 loss: 0.0015 lr: 0.02\n",
      "iteration: 357170 loss: 0.0014 lr: 0.02\n",
      "iteration: 357180 loss: 0.0014 lr: 0.02\n",
      "iteration: 357190 loss: 0.0014 lr: 0.02\n",
      "iteration: 357200 loss: 0.0016 lr: 0.02\n",
      "iteration: 357210 loss: 0.0018 lr: 0.02\n",
      "iteration: 357220 loss: 0.0015 lr: 0.02\n",
      "iteration: 357230 loss: 0.0014 lr: 0.02\n",
      "iteration: 357240 loss: 0.0011 lr: 0.02\n",
      "iteration: 357250 loss: 0.0023 lr: 0.02\n",
      "iteration: 357260 loss: 0.0013 lr: 0.02\n",
      "iteration: 357270 loss: 0.0014 lr: 0.02\n",
      "iteration: 357280 loss: 0.0012 lr: 0.02\n",
      "iteration: 357290 loss: 0.0014 lr: 0.02\n",
      "iteration: 357300 loss: 0.0018 lr: 0.02\n",
      "iteration: 357310 loss: 0.0022 lr: 0.02\n",
      "iteration: 357320 loss: 0.0014 lr: 0.02\n",
      "iteration: 357330 loss: 0.0017 lr: 0.02\n",
      "iteration: 357340 loss: 0.0015 lr: 0.02\n",
      "iteration: 357350 loss: 0.0015 lr: 0.02\n",
      "iteration: 357360 loss: 0.0012 lr: 0.02\n",
      "iteration: 357370 loss: 0.0014 lr: 0.02\n",
      "iteration: 357380 loss: 0.0013 lr: 0.02\n",
      "iteration: 357390 loss: 0.0019 lr: 0.02\n",
      "iteration: 357400 loss: 0.0020 lr: 0.02\n",
      "iteration: 357410 loss: 0.0013 lr: 0.02\n",
      "iteration: 357420 loss: 0.0014 lr: 0.02\n",
      "iteration: 357430 loss: 0.0015 lr: 0.02\n",
      "iteration: 357440 loss: 0.0013 lr: 0.02\n",
      "iteration: 357450 loss: 0.0018 lr: 0.02\n",
      "iteration: 357460 loss: 0.0010 lr: 0.02\n",
      "iteration: 357470 loss: 0.0013 lr: 0.02\n",
      "iteration: 357480 loss: 0.0015 lr: 0.02\n",
      "iteration: 357490 loss: 0.0015 lr: 0.02\n",
      "iteration: 357500 loss: 0.0020 lr: 0.02\n",
      "iteration: 357510 loss: 0.0016 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 357520 loss: 0.0028 lr: 0.02\n",
      "iteration: 357530 loss: 0.0017 lr: 0.02\n",
      "iteration: 357540 loss: 0.0018 lr: 0.02\n",
      "iteration: 357550 loss: 0.0023 lr: 0.02\n",
      "iteration: 357560 loss: 0.0017 lr: 0.02\n",
      "iteration: 357570 loss: 0.0019 lr: 0.02\n",
      "iteration: 357580 loss: 0.0016 lr: 0.02\n",
      "iteration: 357590 loss: 0.0015 lr: 0.02\n",
      "iteration: 357600 loss: 0.0012 lr: 0.02\n",
      "iteration: 357610 loss: 0.0016 lr: 0.02\n",
      "iteration: 357620 loss: 0.0019 lr: 0.02\n",
      "iteration: 357630 loss: 0.0014 lr: 0.02\n",
      "iteration: 357640 loss: 0.0016 lr: 0.02\n",
      "iteration: 357650 loss: 0.0027 lr: 0.02\n",
      "iteration: 357660 loss: 0.0018 lr: 0.02\n",
      "iteration: 357670 loss: 0.0021 lr: 0.02\n",
      "iteration: 357680 loss: 0.0016 lr: 0.02\n",
      "iteration: 357690 loss: 0.0016 lr: 0.02\n",
      "iteration: 357700 loss: 0.0022 lr: 0.02\n",
      "iteration: 357710 loss: 0.0018 lr: 0.02\n",
      "iteration: 357720 loss: 0.0023 lr: 0.02\n",
      "iteration: 357730 loss: 0.0015 lr: 0.02\n",
      "iteration: 357740 loss: 0.0021 lr: 0.02\n",
      "iteration: 357750 loss: 0.0016 lr: 0.02\n",
      "iteration: 357760 loss: 0.0017 lr: 0.02\n",
      "iteration: 357770 loss: 0.0017 lr: 0.02\n",
      "iteration: 357780 loss: 0.0016 lr: 0.02\n",
      "iteration: 357790 loss: 0.0014 lr: 0.02\n",
      "iteration: 357800 loss: 0.0018 lr: 0.02\n",
      "iteration: 357810 loss: 0.0014 lr: 0.02\n",
      "iteration: 357820 loss: 0.0019 lr: 0.02\n",
      "iteration: 357830 loss: 0.0019 lr: 0.02\n",
      "iteration: 357840 loss: 0.0017 lr: 0.02\n",
      "iteration: 357850 loss: 0.0018 lr: 0.02\n",
      "iteration: 357860 loss: 0.0016 lr: 0.02\n",
      "iteration: 357870 loss: 0.0014 lr: 0.02\n",
      "iteration: 357880 loss: 0.0014 lr: 0.02\n",
      "iteration: 357890 loss: 0.0012 lr: 0.02\n",
      "iteration: 357900 loss: 0.0010 lr: 0.02\n",
      "iteration: 357910 loss: 0.0014 lr: 0.02\n",
      "iteration: 357920 loss: 0.0016 lr: 0.02\n",
      "iteration: 357930 loss: 0.0026 lr: 0.02\n",
      "iteration: 357940 loss: 0.0016 lr: 0.02\n",
      "iteration: 357950 loss: 0.0016 lr: 0.02\n",
      "iteration: 357960 loss: 0.0033 lr: 0.02\n",
      "iteration: 357970 loss: 0.0017 lr: 0.02\n",
      "iteration: 357980 loss: 0.0014 lr: 0.02\n",
      "iteration: 357990 loss: 0.0020 lr: 0.02\n",
      "iteration: 358000 loss: 0.0016 lr: 0.02\n",
      "iteration: 358010 loss: 0.0016 lr: 0.02\n",
      "iteration: 358020 loss: 0.0016 lr: 0.02\n",
      "iteration: 358030 loss: 0.0032 lr: 0.02\n",
      "iteration: 358040 loss: 0.0021 lr: 0.02\n",
      "iteration: 358050 loss: 0.0014 lr: 0.02\n",
      "iteration: 358060 loss: 0.0024 lr: 0.02\n",
      "iteration: 358070 loss: 0.0013 lr: 0.02\n",
      "iteration: 358080 loss: 0.0019 lr: 0.02\n",
      "iteration: 358090 loss: 0.0021 lr: 0.02\n",
      "iteration: 358100 loss: 0.0018 lr: 0.02\n",
      "iteration: 358110 loss: 0.0019 lr: 0.02\n",
      "iteration: 358120 loss: 0.0017 lr: 0.02\n",
      "iteration: 358130 loss: 0.0013 lr: 0.02\n",
      "iteration: 358140 loss: 0.0012 lr: 0.02\n",
      "iteration: 358150 loss: 0.0020 lr: 0.02\n",
      "iteration: 358160 loss: 0.0022 lr: 0.02\n",
      "iteration: 358170 loss: 0.0017 lr: 0.02\n",
      "iteration: 358180 loss: 0.0018 lr: 0.02\n",
      "iteration: 358190 loss: 0.0029 lr: 0.02\n",
      "iteration: 358200 loss: 0.0013 lr: 0.02\n",
      "iteration: 358210 loss: 0.0015 lr: 0.02\n",
      "iteration: 358220 loss: 0.0017 lr: 0.02\n",
      "iteration: 358230 loss: 0.0016 lr: 0.02\n",
      "iteration: 358240 loss: 0.0020 lr: 0.02\n",
      "iteration: 358250 loss: 0.0029 lr: 0.02\n",
      "iteration: 358260 loss: 0.0019 lr: 0.02\n",
      "iteration: 358270 loss: 0.0015 lr: 0.02\n",
      "iteration: 358280 loss: 0.0015 lr: 0.02\n",
      "iteration: 358290 loss: 0.0021 lr: 0.02\n",
      "iteration: 358300 loss: 0.0013 lr: 0.02\n",
      "iteration: 358310 loss: 0.0013 lr: 0.02\n",
      "iteration: 358320 loss: 0.0014 lr: 0.02\n",
      "iteration: 358330 loss: 0.0014 lr: 0.02\n",
      "iteration: 358340 loss: 0.0013 lr: 0.02\n",
      "iteration: 358350 loss: 0.0016 lr: 0.02\n",
      "iteration: 358360 loss: 0.0026 lr: 0.02\n",
      "iteration: 358370 loss: 0.0021 lr: 0.02\n",
      "iteration: 358380 loss: 0.0017 lr: 0.02\n",
      "iteration: 358390 loss: 0.0031 lr: 0.02\n",
      "iteration: 358400 loss: 0.0031 lr: 0.02\n",
      "iteration: 358410 loss: 0.0029 lr: 0.02\n",
      "iteration: 358420 loss: 0.0017 lr: 0.02\n",
      "iteration: 358430 loss: 0.0018 lr: 0.02\n",
      "iteration: 358440 loss: 0.0017 lr: 0.02\n",
      "iteration: 358450 loss: 0.0022 lr: 0.02\n",
      "iteration: 358460 loss: 0.0012 lr: 0.02\n",
      "iteration: 358470 loss: 0.0014 lr: 0.02\n",
      "iteration: 358480 loss: 0.0013 lr: 0.02\n",
      "iteration: 358490 loss: 0.0017 lr: 0.02\n",
      "iteration: 358500 loss: 0.0019 lr: 0.02\n",
      "iteration: 358510 loss: 0.0018 lr: 0.02\n",
      "iteration: 358520 loss: 0.0014 lr: 0.02\n",
      "iteration: 358530 loss: 0.0038 lr: 0.02\n",
      "iteration: 358540 loss: 0.0020 lr: 0.02\n",
      "iteration: 358550 loss: 0.0018 lr: 0.02\n",
      "iteration: 358560 loss: 0.0015 lr: 0.02\n",
      "iteration: 358570 loss: 0.0011 lr: 0.02\n",
      "iteration: 358580 loss: 0.0015 lr: 0.02\n",
      "iteration: 358590 loss: 0.0015 lr: 0.02\n",
      "iteration: 358600 loss: 0.0020 lr: 0.02\n",
      "iteration: 358610 loss: 0.0012 lr: 0.02\n",
      "iteration: 358620 loss: 0.0023 lr: 0.02\n",
      "iteration: 358630 loss: 0.0019 lr: 0.02\n",
      "iteration: 358640 loss: 0.0015 lr: 0.02\n",
      "iteration: 358650 loss: 0.0017 lr: 0.02\n",
      "iteration: 358660 loss: 0.0012 lr: 0.02\n",
      "iteration: 358670 loss: 0.0012 lr: 0.02\n",
      "iteration: 358680 loss: 0.0016 lr: 0.02\n",
      "iteration: 358690 loss: 0.0016 lr: 0.02\n",
      "iteration: 358700 loss: 0.0026 lr: 0.02\n",
      "iteration: 358710 loss: 0.0018 lr: 0.02\n",
      "iteration: 358720 loss: 0.0014 lr: 0.02\n",
      "iteration: 358730 loss: 0.0020 lr: 0.02\n",
      "iteration: 358740 loss: 0.0015 lr: 0.02\n",
      "iteration: 358750 loss: 0.0018 lr: 0.02\n",
      "iteration: 358760 loss: 0.0014 lr: 0.02\n",
      "iteration: 358770 loss: 0.0014 lr: 0.02\n",
      "iteration: 358780 loss: 0.0016 lr: 0.02\n",
      "iteration: 358790 loss: 0.0016 lr: 0.02\n",
      "iteration: 358800 loss: 0.0019 lr: 0.02\n",
      "iteration: 358810 loss: 0.0018 lr: 0.02\n",
      "iteration: 358820 loss: 0.0019 lr: 0.02\n",
      "iteration: 358830 loss: 0.0012 lr: 0.02\n",
      "iteration: 358840 loss: 0.0014 lr: 0.02\n",
      "iteration: 358850 loss: 0.0013 lr: 0.02\n",
      "iteration: 358860 loss: 0.0014 lr: 0.02\n",
      "iteration: 358870 loss: 0.0011 lr: 0.02\n",
      "iteration: 358880 loss: 0.0021 lr: 0.02\n",
      "iteration: 358890 loss: 0.0014 lr: 0.02\n",
      "iteration: 358900 loss: 0.0014 lr: 0.02\n",
      "iteration: 358910 loss: 0.0013 lr: 0.02\n",
      "iteration: 358920 loss: 0.0019 lr: 0.02\n",
      "iteration: 358930 loss: 0.0026 lr: 0.02\n",
      "iteration: 358940 loss: 0.0014 lr: 0.02\n",
      "iteration: 358950 loss: 0.0014 lr: 0.02\n",
      "iteration: 358960 loss: 0.0013 lr: 0.02\n",
      "iteration: 358970 loss: 0.0019 lr: 0.02\n",
      "iteration: 358980 loss: 0.0020 lr: 0.02\n",
      "iteration: 358990 loss: 0.0013 lr: 0.02\n",
      "iteration: 359000 loss: 0.0016 lr: 0.02\n",
      "iteration: 359010 loss: 0.0016 lr: 0.02\n",
      "iteration: 359020 loss: 0.0028 lr: 0.02\n",
      "iteration: 359030 loss: 0.0021 lr: 0.02\n",
      "iteration: 359040 loss: 0.0022 lr: 0.02\n",
      "iteration: 359050 loss: 0.0018 lr: 0.02\n",
      "iteration: 359060 loss: 0.0015 lr: 0.02\n",
      "iteration: 359070 loss: 0.0017 lr: 0.02\n",
      "iteration: 359080 loss: 0.0015 lr: 0.02\n",
      "iteration: 359090 loss: 0.0020 lr: 0.02\n",
      "iteration: 359100 loss: 0.0016 lr: 0.02\n",
      "iteration: 359110 loss: 0.0013 lr: 0.02\n",
      "iteration: 359120 loss: 0.0015 lr: 0.02\n",
      "iteration: 359130 loss: 0.0023 lr: 0.02\n",
      "iteration: 359140 loss: 0.0013 lr: 0.02\n",
      "iteration: 359150 loss: 0.0017 lr: 0.02\n",
      "iteration: 359160 loss: 0.0025 lr: 0.02\n",
      "iteration: 359170 loss: 0.0024 lr: 0.02\n",
      "iteration: 359180 loss: 0.0014 lr: 0.02\n",
      "iteration: 359190 loss: 0.0018 lr: 0.02\n",
      "iteration: 359200 loss: 0.0016 lr: 0.02\n",
      "iteration: 359210 loss: 0.0015 lr: 0.02\n",
      "iteration: 359220 loss: 0.0015 lr: 0.02\n",
      "iteration: 359230 loss: 0.0016 lr: 0.02\n",
      "iteration: 359240 loss: 0.0013 lr: 0.02\n",
      "iteration: 359250 loss: 0.0013 lr: 0.02\n",
      "iteration: 359260 loss: 0.0013 lr: 0.02\n",
      "iteration: 359270 loss: 0.0022 lr: 0.02\n",
      "iteration: 359280 loss: 0.0019 lr: 0.02\n",
      "iteration: 359290 loss: 0.0013 lr: 0.02\n",
      "iteration: 359300 loss: 0.0011 lr: 0.02\n",
      "iteration: 359310 loss: 0.0011 lr: 0.02\n",
      "iteration: 359320 loss: 0.0014 lr: 0.02\n",
      "iteration: 359330 loss: 0.0015 lr: 0.02\n",
      "iteration: 359340 loss: 0.0015 lr: 0.02\n",
      "iteration: 359350 loss: 0.0011 lr: 0.02\n",
      "iteration: 359360 loss: 0.0017 lr: 0.02\n",
      "iteration: 359370 loss: 0.0015 lr: 0.02\n",
      "iteration: 359380 loss: 0.0011 lr: 0.02\n",
      "iteration: 359390 loss: 0.0010 lr: 0.02\n",
      "iteration: 359400 loss: 0.0025 lr: 0.02\n",
      "iteration: 359410 loss: 0.0020 lr: 0.02\n",
      "iteration: 359420 loss: 0.0012 lr: 0.02\n",
      "iteration: 359430 loss: 0.0018 lr: 0.02\n",
      "iteration: 359440 loss: 0.0017 lr: 0.02\n",
      "iteration: 359450 loss: 0.0012 lr: 0.02\n",
      "iteration: 359460 loss: 0.0019 lr: 0.02\n",
      "iteration: 359470 loss: 0.0011 lr: 0.02\n",
      "iteration: 359480 loss: 0.0016 lr: 0.02\n",
      "iteration: 359490 loss: 0.0013 lr: 0.02\n",
      "iteration: 359500 loss: 0.0015 lr: 0.02\n",
      "iteration: 359510 loss: 0.0016 lr: 0.02\n",
      "iteration: 359520 loss: 0.0021 lr: 0.02\n",
      "iteration: 359530 loss: 0.0016 lr: 0.02\n",
      "iteration: 359540 loss: 0.0020 lr: 0.02\n",
      "iteration: 359550 loss: 0.0015 lr: 0.02\n",
      "iteration: 359560 loss: 0.0014 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 359570 loss: 0.0012 lr: 0.02\n",
      "iteration: 359580 loss: 0.0015 lr: 0.02\n",
      "iteration: 359590 loss: 0.0012 lr: 0.02\n",
      "iteration: 359600 loss: 0.0012 lr: 0.02\n",
      "iteration: 359610 loss: 0.0015 lr: 0.02\n",
      "iteration: 359620 loss: 0.0013 lr: 0.02\n",
      "iteration: 359630 loss: 0.0024 lr: 0.02\n",
      "iteration: 359640 loss: 0.0014 lr: 0.02\n",
      "iteration: 359650 loss: 0.0028 lr: 0.02\n",
      "iteration: 359660 loss: 0.0017 lr: 0.02\n",
      "iteration: 359670 loss: 0.0015 lr: 0.02\n",
      "iteration: 359680 loss: 0.0014 lr: 0.02\n",
      "iteration: 359690 loss: 0.0021 lr: 0.02\n",
      "iteration: 359700 loss: 0.0016 lr: 0.02\n",
      "iteration: 359710 loss: 0.0016 lr: 0.02\n",
      "iteration: 359720 loss: 0.0017 lr: 0.02\n",
      "iteration: 359730 loss: 0.0014 lr: 0.02\n",
      "iteration: 359740 loss: 0.0015 lr: 0.02\n",
      "iteration: 359750 loss: 0.0014 lr: 0.02\n",
      "iteration: 359760 loss: 0.0017 lr: 0.02\n",
      "iteration: 359770 loss: 0.0011 lr: 0.02\n",
      "iteration: 359780 loss: 0.0017 lr: 0.02\n",
      "iteration: 359790 loss: 0.0018 lr: 0.02\n",
      "iteration: 359800 loss: 0.0019 lr: 0.02\n",
      "iteration: 359810 loss: 0.0016 lr: 0.02\n",
      "iteration: 359820 loss: 0.0018 lr: 0.02\n",
      "iteration: 359830 loss: 0.0019 lr: 0.02\n",
      "iteration: 359840 loss: 0.0023 lr: 0.02\n",
      "iteration: 359850 loss: 0.0018 lr: 0.02\n",
      "iteration: 359860 loss: 0.0018 lr: 0.02\n",
      "iteration: 359870 loss: 0.0017 lr: 0.02\n",
      "iteration: 359880 loss: 0.0019 lr: 0.02\n",
      "iteration: 359890 loss: 0.0015 lr: 0.02\n",
      "iteration: 359900 loss: 0.0019 lr: 0.02\n",
      "iteration: 359910 loss: 0.0026 lr: 0.02\n",
      "iteration: 359920 loss: 0.0014 lr: 0.02\n",
      "iteration: 359930 loss: 0.0015 lr: 0.02\n",
      "iteration: 359940 loss: 0.0016 lr: 0.02\n",
      "iteration: 359950 loss: 0.0020 lr: 0.02\n",
      "iteration: 359960 loss: 0.0020 lr: 0.02\n",
      "iteration: 359970 loss: 0.0013 lr: 0.02\n",
      "iteration: 359980 loss: 0.0011 lr: 0.02\n",
      "iteration: 359990 loss: 0.0013 lr: 0.02\n",
      "iteration: 360000 loss: 0.0013 lr: 0.02\n",
      "iteration: 360010 loss: 0.0016 lr: 0.02\n",
      "iteration: 360020 loss: 0.0019 lr: 0.02\n",
      "iteration: 360030 loss: 0.0016 lr: 0.02\n",
      "iteration: 360040 loss: 0.0016 lr: 0.02\n",
      "iteration: 360050 loss: 0.0014 lr: 0.02\n",
      "iteration: 360060 loss: 0.0016 lr: 0.02\n",
      "iteration: 360070 loss: 0.0015 lr: 0.02\n",
      "iteration: 360080 loss: 0.0018 lr: 0.02\n",
      "iteration: 360090 loss: 0.0016 lr: 0.02\n",
      "iteration: 360100 loss: 0.0017 lr: 0.02\n",
      "iteration: 360110 loss: 0.0016 lr: 0.02\n",
      "iteration: 360120 loss: 0.0018 lr: 0.02\n",
      "iteration: 360130 loss: 0.0018 lr: 0.02\n",
      "iteration: 360140 loss: 0.0017 lr: 0.02\n",
      "iteration: 360150 loss: 0.0012 lr: 0.02\n",
      "iteration: 360160 loss: 0.0015 lr: 0.02\n",
      "iteration: 360170 loss: 0.0023 lr: 0.02\n",
      "iteration: 360180 loss: 0.0015 lr: 0.02\n",
      "iteration: 360190 loss: 0.0012 lr: 0.02\n",
      "iteration: 360200 loss: 0.0014 lr: 0.02\n",
      "iteration: 360210 loss: 0.0013 lr: 0.02\n",
      "iteration: 360220 loss: 0.0021 lr: 0.02\n",
      "iteration: 360230 loss: 0.0021 lr: 0.02\n",
      "iteration: 360240 loss: 0.0013 lr: 0.02\n",
      "iteration: 360250 loss: 0.0019 lr: 0.02\n",
      "iteration: 360260 loss: 0.0012 lr: 0.02\n",
      "iteration: 360270 loss: 0.0017 lr: 0.02\n",
      "iteration: 360280 loss: 0.0015 lr: 0.02\n",
      "iteration: 360290 loss: 0.0015 lr: 0.02\n",
      "iteration: 360300 loss: 0.0013 lr: 0.02\n",
      "iteration: 360310 loss: 0.0012 lr: 0.02\n",
      "iteration: 360320 loss: 0.0016 lr: 0.02\n",
      "iteration: 360330 loss: 0.0014 lr: 0.02\n",
      "iteration: 360340 loss: 0.0014 lr: 0.02\n",
      "iteration: 360350 loss: 0.0013 lr: 0.02\n",
      "iteration: 360360 loss: 0.0017 lr: 0.02\n",
      "iteration: 360370 loss: 0.0018 lr: 0.02\n",
      "iteration: 360380 loss: 0.0012 lr: 0.02\n",
      "iteration: 360390 loss: 0.0014 lr: 0.02\n",
      "iteration: 360400 loss: 0.0011 lr: 0.02\n",
      "iteration: 360410 loss: 0.0015 lr: 0.02\n",
      "iteration: 360420 loss: 0.0014 lr: 0.02\n",
      "iteration: 360430 loss: 0.0018 lr: 0.02\n",
      "iteration: 360440 loss: 0.0019 lr: 0.02\n",
      "iteration: 360450 loss: 0.0018 lr: 0.02\n",
      "iteration: 360460 loss: 0.0014 lr: 0.02\n",
      "iteration: 360470 loss: 0.0013 lr: 0.02\n",
      "iteration: 360480 loss: 0.0025 lr: 0.02\n",
      "iteration: 360490 loss: 0.0018 lr: 0.02\n",
      "iteration: 360500 loss: 0.0017 lr: 0.02\n",
      "iteration: 360510 loss: 0.0030 lr: 0.02\n",
      "iteration: 360520 loss: 0.0019 lr: 0.02\n",
      "iteration: 360530 loss: 0.0021 lr: 0.02\n",
      "iteration: 360540 loss: 0.0016 lr: 0.02\n",
      "iteration: 360550 loss: 0.0022 lr: 0.02\n",
      "iteration: 360560 loss: 0.0019 lr: 0.02\n",
      "iteration: 360570 loss: 0.0011 lr: 0.02\n",
      "iteration: 360580 loss: 0.0011 lr: 0.02\n",
      "iteration: 360590 loss: 0.0018 lr: 0.02\n",
      "iteration: 360600 loss: 0.0020 lr: 0.02\n",
      "iteration: 360610 loss: 0.0016 lr: 0.02\n",
      "iteration: 360620 loss: 0.0018 lr: 0.02\n",
      "iteration: 360630 loss: 0.0014 lr: 0.02\n",
      "iteration: 360640 loss: 0.0016 lr: 0.02\n",
      "iteration: 360650 loss: 0.0014 lr: 0.02\n",
      "iteration: 360660 loss: 0.0016 lr: 0.02\n",
      "iteration: 360670 loss: 0.0019 lr: 0.02\n",
      "iteration: 360680 loss: 0.0023 lr: 0.02\n",
      "iteration: 360690 loss: 0.0018 lr: 0.02\n",
      "iteration: 360700 loss: 0.0022 lr: 0.02\n",
      "iteration: 360710 loss: 0.0018 lr: 0.02\n",
      "iteration: 360720 loss: 0.0016 lr: 0.02\n",
      "iteration: 360730 loss: 0.0018 lr: 0.02\n",
      "iteration: 360740 loss: 0.0019 lr: 0.02\n",
      "iteration: 360750 loss: 0.0012 lr: 0.02\n",
      "iteration: 360760 loss: 0.0022 lr: 0.02\n",
      "iteration: 360770 loss: 0.0015 lr: 0.02\n",
      "iteration: 360780 loss: 0.0014 lr: 0.02\n",
      "iteration: 360790 loss: 0.0016 lr: 0.02\n",
      "iteration: 360800 loss: 0.0014 lr: 0.02\n",
      "iteration: 360810 loss: 0.0016 lr: 0.02\n",
      "iteration: 360820 loss: 0.0020 lr: 0.02\n",
      "iteration: 360830 loss: 0.0019 lr: 0.02\n",
      "iteration: 360840 loss: 0.0016 lr: 0.02\n",
      "iteration: 360850 loss: 0.0012 lr: 0.02\n",
      "iteration: 360860 loss: 0.0019 lr: 0.02\n",
      "iteration: 360870 loss: 0.0021 lr: 0.02\n",
      "iteration: 360880 loss: 0.0013 lr: 0.02\n",
      "iteration: 360890 loss: 0.0016 lr: 0.02\n",
      "iteration: 360900 loss: 0.0017 lr: 0.02\n",
      "iteration: 360910 loss: 0.0015 lr: 0.02\n",
      "iteration: 360920 loss: 0.0012 lr: 0.02\n",
      "iteration: 360930 loss: 0.0012 lr: 0.02\n",
      "iteration: 360940 loss: 0.0013 lr: 0.02\n",
      "iteration: 360950 loss: 0.0012 lr: 0.02\n",
      "iteration: 360960 loss: 0.0024 lr: 0.02\n",
      "iteration: 360970 loss: 0.0015 lr: 0.02\n",
      "iteration: 360980 loss: 0.0017 lr: 0.02\n",
      "iteration: 360990 loss: 0.0011 lr: 0.02\n",
      "iteration: 361000 loss: 0.0015 lr: 0.02\n",
      "iteration: 361010 loss: 0.0024 lr: 0.02\n",
      "iteration: 361020 loss: 0.0021 lr: 0.02\n",
      "iteration: 361030 loss: 0.0015 lr: 0.02\n",
      "iteration: 361040 loss: 0.0012 lr: 0.02\n",
      "iteration: 361050 loss: 0.0022 lr: 0.02\n",
      "iteration: 361060 loss: 0.0037 lr: 0.02\n",
      "iteration: 361070 loss: 0.0018 lr: 0.02\n",
      "iteration: 361080 loss: 0.0020 lr: 0.02\n",
      "iteration: 361090 loss: 0.0024 lr: 0.02\n",
      "iteration: 361100 loss: 0.0014 lr: 0.02\n",
      "iteration: 361110 loss: 0.0014 lr: 0.02\n",
      "iteration: 361120 loss: 0.0017 lr: 0.02\n",
      "iteration: 361130 loss: 0.0016 lr: 0.02\n",
      "iteration: 361140 loss: 0.0015 lr: 0.02\n",
      "iteration: 361150 loss: 0.0012 lr: 0.02\n",
      "iteration: 361160 loss: 0.0014 lr: 0.02\n",
      "iteration: 361170 loss: 0.0017 lr: 0.02\n",
      "iteration: 361180 loss: 0.0015 lr: 0.02\n",
      "iteration: 361190 loss: 0.0023 lr: 0.02\n",
      "iteration: 361200 loss: 0.0016 lr: 0.02\n",
      "iteration: 361210 loss: 0.0014 lr: 0.02\n",
      "iteration: 361220 loss: 0.0015 lr: 0.02\n",
      "iteration: 361230 loss: 0.0018 lr: 0.02\n",
      "iteration: 361240 loss: 0.0014 lr: 0.02\n",
      "iteration: 361250 loss: 0.0014 lr: 0.02\n",
      "iteration: 361260 loss: 0.0012 lr: 0.02\n",
      "iteration: 361270 loss: 0.0013 lr: 0.02\n",
      "iteration: 361280 loss: 0.0013 lr: 0.02\n",
      "iteration: 361290 loss: 0.0029 lr: 0.02\n",
      "iteration: 361300 loss: 0.0017 lr: 0.02\n",
      "iteration: 361310 loss: 0.0015 lr: 0.02\n",
      "iteration: 361320 loss: 0.0012 lr: 0.02\n",
      "iteration: 361330 loss: 0.0021 lr: 0.02\n",
      "iteration: 361340 loss: 0.0013 lr: 0.02\n",
      "iteration: 361350 loss: 0.0017 lr: 0.02\n",
      "iteration: 361360 loss: 0.0015 lr: 0.02\n",
      "iteration: 361370 loss: 0.0023 lr: 0.02\n",
      "iteration: 361380 loss: 0.0016 lr: 0.02\n",
      "iteration: 361390 loss: 0.0016 lr: 0.02\n",
      "iteration: 361400 loss: 0.0012 lr: 0.02\n",
      "iteration: 361410 loss: 0.0012 lr: 0.02\n",
      "iteration: 361420 loss: 0.0017 lr: 0.02\n",
      "iteration: 361430 loss: 0.0014 lr: 0.02\n",
      "iteration: 361440 loss: 0.0013 lr: 0.02\n",
      "iteration: 361450 loss: 0.0021 lr: 0.02\n",
      "iteration: 361460 loss: 0.0018 lr: 0.02\n",
      "iteration: 361470 loss: 0.0015 lr: 0.02\n",
      "iteration: 361480 loss: 0.0018 lr: 0.02\n",
      "iteration: 361490 loss: 0.0012 lr: 0.02\n",
      "iteration: 361500 loss: 0.0020 lr: 0.02\n",
      "iteration: 361510 loss: 0.0030 lr: 0.02\n",
      "iteration: 361520 loss: 0.0019 lr: 0.02\n",
      "iteration: 361530 loss: 0.0020 lr: 0.02\n",
      "iteration: 361540 loss: 0.0016 lr: 0.02\n",
      "iteration: 361550 loss: 0.0015 lr: 0.02\n",
      "iteration: 361560 loss: 0.0018 lr: 0.02\n",
      "iteration: 361570 loss: 0.0013 lr: 0.02\n",
      "iteration: 361580 loss: 0.0017 lr: 0.02\n",
      "iteration: 361590 loss: 0.0012 lr: 0.02\n",
      "iteration: 361600 loss: 0.0011 lr: 0.02\n",
      "iteration: 361610 loss: 0.0017 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 361620 loss: 0.0021 lr: 0.02\n",
      "iteration: 361630 loss: 0.0013 lr: 0.02\n",
      "iteration: 361640 loss: 0.0015 lr: 0.02\n",
      "iteration: 361650 loss: 0.0016 lr: 0.02\n",
      "iteration: 361660 loss: 0.0016 lr: 0.02\n",
      "iteration: 361670 loss: 0.0022 lr: 0.02\n",
      "iteration: 361680 loss: 0.0020 lr: 0.02\n",
      "iteration: 361690 loss: 0.0012 lr: 0.02\n",
      "iteration: 361700 loss: 0.0024 lr: 0.02\n",
      "iteration: 361710 loss: 0.0024 lr: 0.02\n",
      "iteration: 361720 loss: 0.0017 lr: 0.02\n",
      "iteration: 361730 loss: 0.0018 lr: 0.02\n",
      "iteration: 361740 loss: 0.0016 lr: 0.02\n",
      "iteration: 361750 loss: 0.0018 lr: 0.02\n",
      "iteration: 361760 loss: 0.0013 lr: 0.02\n",
      "iteration: 361770 loss: 0.0012 lr: 0.02\n",
      "iteration: 361780 loss: 0.0013 lr: 0.02\n",
      "iteration: 361790 loss: 0.0030 lr: 0.02\n",
      "iteration: 361800 loss: 0.0012 lr: 0.02\n",
      "iteration: 361810 loss: 0.0015 lr: 0.02\n",
      "iteration: 361820 loss: 0.0017 lr: 0.02\n",
      "iteration: 361830 loss: 0.0016 lr: 0.02\n",
      "iteration: 361840 loss: 0.0013 lr: 0.02\n",
      "iteration: 361850 loss: 0.0013 lr: 0.02\n",
      "iteration: 361860 loss: 0.0014 lr: 0.02\n",
      "iteration: 361870 loss: 0.0022 lr: 0.02\n",
      "iteration: 361880 loss: 0.0019 lr: 0.02\n",
      "iteration: 361890 loss: 0.0018 lr: 0.02\n",
      "iteration: 361900 loss: 0.0015 lr: 0.02\n",
      "iteration: 361910 loss: 0.0018 lr: 0.02\n",
      "iteration: 361920 loss: 0.0017 lr: 0.02\n",
      "iteration: 361930 loss: 0.0018 lr: 0.02\n",
      "iteration: 361940 loss: 0.0013 lr: 0.02\n",
      "iteration: 361950 loss: 0.0015 lr: 0.02\n",
      "iteration: 361960 loss: 0.0015 lr: 0.02\n",
      "iteration: 361970 loss: 0.0018 lr: 0.02\n",
      "iteration: 361980 loss: 0.0016 lr: 0.02\n",
      "iteration: 361990 loss: 0.0014 lr: 0.02\n",
      "iteration: 362000 loss: 0.0013 lr: 0.02\n",
      "iteration: 362010 loss: 0.0010 lr: 0.02\n",
      "iteration: 362020 loss: 0.0015 lr: 0.02\n",
      "iteration: 362030 loss: 0.0019 lr: 0.02\n",
      "iteration: 362040 loss: 0.0014 lr: 0.02\n",
      "iteration: 362050 loss: 0.0015 lr: 0.02\n",
      "iteration: 362060 loss: 0.0020 lr: 0.02\n",
      "iteration: 362070 loss: 0.0021 lr: 0.02\n",
      "iteration: 362080 loss: 0.0014 lr: 0.02\n",
      "iteration: 362090 loss: 0.0014 lr: 0.02\n",
      "iteration: 362100 loss: 0.0010 lr: 0.02\n",
      "iteration: 362110 loss: 0.0017 lr: 0.02\n",
      "iteration: 362120 loss: 0.0019 lr: 0.02\n",
      "iteration: 362130 loss: 0.0011 lr: 0.02\n",
      "iteration: 362140 loss: 0.0015 lr: 0.02\n",
      "iteration: 362150 loss: 0.0012 lr: 0.02\n",
      "iteration: 362160 loss: 0.0014 lr: 0.02\n",
      "iteration: 362170 loss: 0.0017 lr: 0.02\n",
      "iteration: 362180 loss: 0.0018 lr: 0.02\n",
      "iteration: 362190 loss: 0.0024 lr: 0.02\n",
      "iteration: 362200 loss: 0.0016 lr: 0.02\n",
      "iteration: 362210 loss: 0.0015 lr: 0.02\n",
      "iteration: 362220 loss: 0.0014 lr: 0.02\n",
      "iteration: 362230 loss: 0.0019 lr: 0.02\n",
      "iteration: 362240 loss: 0.0020 lr: 0.02\n",
      "iteration: 362250 loss: 0.0015 lr: 0.02\n",
      "iteration: 362260 loss: 0.0016 lr: 0.02\n",
      "iteration: 362270 loss: 0.0012 lr: 0.02\n",
      "iteration: 362280 loss: 0.0018 lr: 0.02\n",
      "iteration: 362290 loss: 0.0019 lr: 0.02\n",
      "iteration: 362300 loss: 0.0015 lr: 0.02\n",
      "iteration: 362310 loss: 0.0020 lr: 0.02\n",
      "iteration: 362320 loss: 0.0015 lr: 0.02\n",
      "iteration: 362330 loss: 0.0014 lr: 0.02\n",
      "iteration: 362340 loss: 0.0011 lr: 0.02\n",
      "iteration: 362350 loss: 0.0017 lr: 0.02\n",
      "iteration: 362360 loss: 0.0021 lr: 0.02\n",
      "iteration: 362370 loss: 0.0015 lr: 0.02\n",
      "iteration: 362380 loss: 0.0019 lr: 0.02\n",
      "iteration: 362390 loss: 0.0021 lr: 0.02\n",
      "iteration: 362400 loss: 0.0016 lr: 0.02\n",
      "iteration: 362410 loss: 0.0017 lr: 0.02\n",
      "iteration: 362420 loss: 0.0017 lr: 0.02\n",
      "iteration: 362430 loss: 0.0014 lr: 0.02\n",
      "iteration: 362440 loss: 0.0022 lr: 0.02\n",
      "iteration: 362450 loss: 0.0020 lr: 0.02\n",
      "iteration: 362460 loss: 0.0015 lr: 0.02\n",
      "iteration: 362470 loss: 0.0013 lr: 0.02\n",
      "iteration: 362480 loss: 0.0015 lr: 0.02\n",
      "iteration: 362490 loss: 0.0024 lr: 0.02\n",
      "iteration: 362500 loss: 0.0013 lr: 0.02\n",
      "iteration: 362510 loss: 0.0016 lr: 0.02\n",
      "iteration: 362520 loss: 0.0022 lr: 0.02\n",
      "iteration: 362530 loss: 0.0024 lr: 0.02\n",
      "iteration: 362540 loss: 0.0014 lr: 0.02\n",
      "iteration: 362550 loss: 0.0017 lr: 0.02\n",
      "iteration: 362560 loss: 0.0027 lr: 0.02\n",
      "iteration: 362570 loss: 0.0019 lr: 0.02\n",
      "iteration: 362580 loss: 0.0017 lr: 0.02\n",
      "iteration: 362590 loss: 0.0018 lr: 0.02\n",
      "iteration: 362600 loss: 0.0022 lr: 0.02\n",
      "iteration: 362610 loss: 0.0017 lr: 0.02\n",
      "iteration: 362620 loss: 0.0018 lr: 0.02\n",
      "iteration: 362630 loss: 0.0015 lr: 0.02\n",
      "iteration: 362640 loss: 0.0021 lr: 0.02\n",
      "iteration: 362650 loss: 0.0015 lr: 0.02\n",
      "iteration: 362660 loss: 0.0030 lr: 0.02\n",
      "iteration: 362670 loss: 0.0012 lr: 0.02\n",
      "iteration: 362680 loss: 0.0021 lr: 0.02\n",
      "iteration: 362690 loss: 0.0018 lr: 0.02\n",
      "iteration: 362700 loss: 0.0020 lr: 0.02\n",
      "iteration: 362710 loss: 0.0019 lr: 0.02\n",
      "iteration: 362720 loss: 0.0021 lr: 0.02\n",
      "iteration: 362730 loss: 0.0012 lr: 0.02\n",
      "iteration: 362740 loss: 0.0010 lr: 0.02\n",
      "iteration: 362750 loss: 0.0019 lr: 0.02\n",
      "iteration: 362760 loss: 0.0022 lr: 0.02\n",
      "iteration: 362770 loss: 0.0019 lr: 0.02\n",
      "iteration: 362780 loss: 0.0019 lr: 0.02\n",
      "iteration: 362790 loss: 0.0016 lr: 0.02\n",
      "iteration: 362800 loss: 0.0019 lr: 0.02\n",
      "iteration: 362810 loss: 0.0022 lr: 0.02\n",
      "iteration: 362820 loss: 0.0015 lr: 0.02\n",
      "iteration: 362830 loss: 0.0016 lr: 0.02\n",
      "iteration: 362840 loss: 0.0012 lr: 0.02\n",
      "iteration: 362850 loss: 0.0012 lr: 0.02\n",
      "iteration: 362860 loss: 0.0016 lr: 0.02\n",
      "iteration: 362870 loss: 0.0012 lr: 0.02\n",
      "iteration: 362880 loss: 0.0010 lr: 0.02\n",
      "iteration: 362890 loss: 0.0012 lr: 0.02\n",
      "iteration: 362900 loss: 0.0011 lr: 0.02\n",
      "iteration: 362910 loss: 0.0014 lr: 0.02\n",
      "iteration: 362920 loss: 0.0016 lr: 0.02\n",
      "iteration: 362930 loss: 0.0012 lr: 0.02\n",
      "iteration: 362940 loss: 0.0012 lr: 0.02\n",
      "iteration: 362950 loss: 0.0013 lr: 0.02\n",
      "iteration: 362960 loss: 0.0017 lr: 0.02\n",
      "iteration: 362970 loss: 0.0013 lr: 0.02\n",
      "iteration: 362980 loss: 0.0020 lr: 0.02\n",
      "iteration: 362990 loss: 0.0018 lr: 0.02\n",
      "iteration: 363000 loss: 0.0016 lr: 0.02\n",
      "iteration: 363010 loss: 0.0014 lr: 0.02\n",
      "iteration: 363020 loss: 0.0012 lr: 0.02\n",
      "iteration: 363030 loss: 0.0016 lr: 0.02\n",
      "iteration: 363040 loss: 0.0013 lr: 0.02\n",
      "iteration: 363050 loss: 0.0012 lr: 0.02\n",
      "iteration: 363060 loss: 0.0014 lr: 0.02\n",
      "iteration: 363070 loss: 0.0013 lr: 0.02\n",
      "iteration: 363080 loss: 0.0015 lr: 0.02\n",
      "iteration: 363090 loss: 0.0024 lr: 0.02\n",
      "iteration: 363100 loss: 0.0023 lr: 0.02\n",
      "iteration: 363110 loss: 0.0027 lr: 0.02\n",
      "iteration: 363120 loss: 0.0016 lr: 0.02\n",
      "iteration: 363130 loss: 0.0016 lr: 0.02\n",
      "iteration: 363140 loss: 0.0017 lr: 0.02\n",
      "iteration: 363150 loss: 0.0010 lr: 0.02\n",
      "iteration: 363160 loss: 0.0020 lr: 0.02\n",
      "iteration: 363170 loss: 0.0017 lr: 0.02\n",
      "iteration: 363180 loss: 0.0018 lr: 0.02\n",
      "iteration: 363190 loss: 0.0015 lr: 0.02\n",
      "iteration: 363200 loss: 0.0015 lr: 0.02\n",
      "iteration: 363210 loss: 0.0011 lr: 0.02\n",
      "iteration: 363220 loss: 0.0015 lr: 0.02\n",
      "iteration: 363230 loss: 0.0021 lr: 0.02\n",
      "iteration: 363240 loss: 0.0015 lr: 0.02\n",
      "iteration: 363250 loss: 0.0014 lr: 0.02\n",
      "iteration: 363260 loss: 0.0020 lr: 0.02\n",
      "iteration: 363270 loss: 0.0016 lr: 0.02\n",
      "iteration: 363280 loss: 0.0012 lr: 0.02\n",
      "iteration: 363290 loss: 0.0018 lr: 0.02\n",
      "iteration: 363300 loss: 0.0022 lr: 0.02\n",
      "iteration: 363310 loss: 0.0017 lr: 0.02\n",
      "iteration: 363320 loss: 0.0019 lr: 0.02\n",
      "iteration: 363330 loss: 0.0017 lr: 0.02\n",
      "iteration: 363340 loss: 0.0015 lr: 0.02\n",
      "iteration: 363350 loss: 0.0010 lr: 0.02\n",
      "iteration: 363360 loss: 0.0015 lr: 0.02\n",
      "iteration: 363370 loss: 0.0013 lr: 0.02\n",
      "iteration: 363380 loss: 0.0020 lr: 0.02\n",
      "iteration: 363390 loss: 0.0016 lr: 0.02\n",
      "iteration: 363400 loss: 0.0018 lr: 0.02\n",
      "iteration: 363410 loss: 0.0012 lr: 0.02\n",
      "iteration: 363420 loss: 0.0014 lr: 0.02\n",
      "iteration: 363430 loss: 0.0014 lr: 0.02\n",
      "iteration: 363440 loss: 0.0014 lr: 0.02\n",
      "iteration: 363450 loss: 0.0014 lr: 0.02\n",
      "iteration: 363460 loss: 0.0015 lr: 0.02\n",
      "iteration: 363470 loss: 0.0015 lr: 0.02\n",
      "iteration: 363480 loss: 0.0011 lr: 0.02\n",
      "iteration: 363490 loss: 0.0022 lr: 0.02\n",
      "iteration: 363500 loss: 0.0015 lr: 0.02\n",
      "iteration: 363510 loss: 0.0013 lr: 0.02\n",
      "iteration: 363520 loss: 0.0014 lr: 0.02\n",
      "iteration: 363530 loss: 0.0015 lr: 0.02\n",
      "iteration: 363540 loss: 0.0014 lr: 0.02\n",
      "iteration: 363550 loss: 0.0018 lr: 0.02\n",
      "iteration: 363560 loss: 0.0017 lr: 0.02\n",
      "iteration: 363570 loss: 0.0014 lr: 0.02\n",
      "iteration: 363580 loss: 0.0012 lr: 0.02\n",
      "iteration: 363590 loss: 0.0018 lr: 0.02\n",
      "iteration: 363600 loss: 0.0014 lr: 0.02\n",
      "iteration: 363610 loss: 0.0016 lr: 0.02\n",
      "iteration: 363620 loss: 0.0015 lr: 0.02\n",
      "iteration: 363630 loss: 0.0012 lr: 0.02\n",
      "iteration: 363640 loss: 0.0019 lr: 0.02\n",
      "iteration: 363650 loss: 0.0022 lr: 0.02\n",
      "iteration: 363660 loss: 0.0024 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 363670 loss: 0.0011 lr: 0.02\n",
      "iteration: 363680 loss: 0.0012 lr: 0.02\n",
      "iteration: 363690 loss: 0.0018 lr: 0.02\n",
      "iteration: 363700 loss: 0.0025 lr: 0.02\n",
      "iteration: 363710 loss: 0.0015 lr: 0.02\n",
      "iteration: 363720 loss: 0.0017 lr: 0.02\n",
      "iteration: 363730 loss: 0.0016 lr: 0.02\n",
      "iteration: 363740 loss: 0.0016 lr: 0.02\n",
      "iteration: 363750 loss: 0.0019 lr: 0.02\n",
      "iteration: 363760 loss: 0.0030 lr: 0.02\n",
      "iteration: 363770 loss: 0.0019 lr: 0.02\n",
      "iteration: 363780 loss: 0.0015 lr: 0.02\n",
      "iteration: 363790 loss: 0.0014 lr: 0.02\n",
      "iteration: 363800 loss: 0.0019 lr: 0.02\n",
      "iteration: 363810 loss: 0.0014 lr: 0.02\n",
      "iteration: 363820 loss: 0.0012 lr: 0.02\n",
      "iteration: 363830 loss: 0.0016 lr: 0.02\n",
      "iteration: 363840 loss: 0.0016 lr: 0.02\n",
      "iteration: 363850 loss: 0.0016 lr: 0.02\n",
      "iteration: 363860 loss: 0.0014 lr: 0.02\n",
      "iteration: 363870 loss: 0.0014 lr: 0.02\n",
      "iteration: 363880 loss: 0.0012 lr: 0.02\n",
      "iteration: 363890 loss: 0.0014 lr: 0.02\n",
      "iteration: 363900 loss: 0.0011 lr: 0.02\n",
      "iteration: 363910 loss: 0.0013 lr: 0.02\n",
      "iteration: 363920 loss: 0.0017 lr: 0.02\n",
      "iteration: 363930 loss: 0.0014 lr: 0.02\n",
      "iteration: 363940 loss: 0.0017 lr: 0.02\n",
      "iteration: 363950 loss: 0.0016 lr: 0.02\n",
      "iteration: 363960 loss: 0.0013 lr: 0.02\n",
      "iteration: 363970 loss: 0.0015 lr: 0.02\n",
      "iteration: 363980 loss: 0.0020 lr: 0.02\n",
      "iteration: 363990 loss: 0.0011 lr: 0.02\n",
      "iteration: 364000 loss: 0.0017 lr: 0.02\n",
      "iteration: 364010 loss: 0.0015 lr: 0.02\n",
      "iteration: 364020 loss: 0.0013 lr: 0.02\n",
      "iteration: 364030 loss: 0.0018 lr: 0.02\n",
      "iteration: 364040 loss: 0.0022 lr: 0.02\n",
      "iteration: 364050 loss: 0.0014 lr: 0.02\n",
      "iteration: 364060 loss: 0.0013 lr: 0.02\n",
      "iteration: 364070 loss: 0.0022 lr: 0.02\n",
      "iteration: 364080 loss: 0.0013 lr: 0.02\n",
      "iteration: 364090 loss: 0.0024 lr: 0.02\n",
      "iteration: 364100 loss: 0.0015 lr: 0.02\n",
      "iteration: 364110 loss: 0.0025 lr: 0.02\n",
      "iteration: 364120 loss: 0.0015 lr: 0.02\n",
      "iteration: 364130 loss: 0.0017 lr: 0.02\n",
      "iteration: 364140 loss: 0.0016 lr: 0.02\n",
      "iteration: 364150 loss: 0.0015 lr: 0.02\n",
      "iteration: 364160 loss: 0.0016 lr: 0.02\n",
      "iteration: 364170 loss: 0.0013 lr: 0.02\n",
      "iteration: 364180 loss: 0.0021 lr: 0.02\n",
      "iteration: 364190 loss: 0.0014 lr: 0.02\n",
      "iteration: 364200 loss: 0.0016 lr: 0.02\n",
      "iteration: 364210 loss: 0.0025 lr: 0.02\n",
      "iteration: 364220 loss: 0.0021 lr: 0.02\n",
      "iteration: 364230 loss: 0.0017 lr: 0.02\n",
      "iteration: 364240 loss: 0.0019 lr: 0.02\n",
      "iteration: 364250 loss: 0.0015 lr: 0.02\n",
      "iteration: 364260 loss: 0.0017 lr: 0.02\n",
      "iteration: 364270 loss: 0.0017 lr: 0.02\n",
      "iteration: 364280 loss: 0.0012 lr: 0.02\n",
      "iteration: 364290 loss: 0.0015 lr: 0.02\n",
      "iteration: 364300 loss: 0.0014 lr: 0.02\n",
      "iteration: 364310 loss: 0.0014 lr: 0.02\n",
      "iteration: 364320 loss: 0.0017 lr: 0.02\n",
      "iteration: 364330 loss: 0.0013 lr: 0.02\n",
      "iteration: 364340 loss: 0.0016 lr: 0.02\n",
      "iteration: 364350 loss: 0.0019 lr: 0.02\n",
      "iteration: 364360 loss: 0.0014 lr: 0.02\n",
      "iteration: 364370 loss: 0.0017 lr: 0.02\n",
      "iteration: 364380 loss: 0.0012 lr: 0.02\n",
      "iteration: 364390 loss: 0.0015 lr: 0.02\n",
      "iteration: 364400 loss: 0.0014 lr: 0.02\n",
      "iteration: 364410 loss: 0.0017 lr: 0.02\n",
      "iteration: 364420 loss: 0.0016 lr: 0.02\n",
      "iteration: 364430 loss: 0.0015 lr: 0.02\n",
      "iteration: 364440 loss: 0.0017 lr: 0.02\n",
      "iteration: 364450 loss: 0.0020 lr: 0.02\n",
      "iteration: 364460 loss: 0.0015 lr: 0.02\n",
      "iteration: 364470 loss: 0.0016 lr: 0.02\n",
      "iteration: 364480 loss: 0.0018 lr: 0.02\n",
      "iteration: 364490 loss: 0.0015 lr: 0.02\n",
      "iteration: 364500 loss: 0.0017 lr: 0.02\n",
      "iteration: 364510 loss: 0.0018 lr: 0.02\n",
      "iteration: 364520 loss: 0.0017 lr: 0.02\n",
      "iteration: 364530 loss: 0.0013 lr: 0.02\n",
      "iteration: 364540 loss: 0.0018 lr: 0.02\n",
      "iteration: 364550 loss: 0.0013 lr: 0.02\n",
      "iteration: 364560 loss: 0.0017 lr: 0.02\n",
      "iteration: 364570 loss: 0.0014 lr: 0.02\n",
      "iteration: 364580 loss: 0.0015 lr: 0.02\n",
      "iteration: 364590 loss: 0.0016 lr: 0.02\n",
      "iteration: 364600 loss: 0.0016 lr: 0.02\n",
      "iteration: 364610 loss: 0.0016 lr: 0.02\n",
      "iteration: 364620 loss: 0.0012 lr: 0.02\n",
      "iteration: 364630 loss: 0.0018 lr: 0.02\n",
      "iteration: 364640 loss: 0.0015 lr: 0.02\n",
      "iteration: 364650 loss: 0.0013 lr: 0.02\n",
      "iteration: 364660 loss: 0.0014 lr: 0.02\n",
      "iteration: 364670 loss: 0.0013 lr: 0.02\n",
      "iteration: 364680 loss: 0.0017 lr: 0.02\n",
      "iteration: 364690 loss: 0.0015 lr: 0.02\n",
      "iteration: 364700 loss: 0.0014 lr: 0.02\n",
      "iteration: 364710 loss: 0.0016 lr: 0.02\n",
      "iteration: 364720 loss: 0.0021 lr: 0.02\n",
      "iteration: 364730 loss: 0.0021 lr: 0.02\n",
      "iteration: 364740 loss: 0.0013 lr: 0.02\n",
      "iteration: 364750 loss: 0.0020 lr: 0.02\n",
      "iteration: 364760 loss: 0.0016 lr: 0.02\n",
      "iteration: 364770 loss: 0.0014 lr: 0.02\n",
      "iteration: 364780 loss: 0.0014 lr: 0.02\n",
      "iteration: 364790 loss: 0.0012 lr: 0.02\n",
      "iteration: 364800 loss: 0.0018 lr: 0.02\n",
      "iteration: 364810 loss: 0.0019 lr: 0.02\n",
      "iteration: 364820 loss: 0.0013 lr: 0.02\n",
      "iteration: 364830 loss: 0.0014 lr: 0.02\n",
      "iteration: 364840 loss: 0.0017 lr: 0.02\n",
      "iteration: 364850 loss: 0.0016 lr: 0.02\n",
      "iteration: 364860 loss: 0.0012 lr: 0.02\n",
      "iteration: 364870 loss: 0.0015 lr: 0.02\n",
      "iteration: 364880 loss: 0.0018 lr: 0.02\n",
      "iteration: 364890 loss: 0.0015 lr: 0.02\n",
      "iteration: 364900 loss: 0.0012 lr: 0.02\n",
      "iteration: 364910 loss: 0.0017 lr: 0.02\n",
      "iteration: 364920 loss: 0.0018 lr: 0.02\n",
      "iteration: 364930 loss: 0.0019 lr: 0.02\n",
      "iteration: 364940 loss: 0.0015 lr: 0.02\n",
      "iteration: 364950 loss: 0.0011 lr: 0.02\n",
      "iteration: 364960 loss: 0.0016 lr: 0.02\n",
      "iteration: 364970 loss: 0.0015 lr: 0.02\n",
      "iteration: 364980 loss: 0.0018 lr: 0.02\n",
      "iteration: 364990 loss: 0.0021 lr: 0.02\n",
      "iteration: 365000 loss: 0.0014 lr: 0.02\n",
      "iteration: 365010 loss: 0.0013 lr: 0.02\n",
      "iteration: 365020 loss: 0.0013 lr: 0.02\n",
      "iteration: 365030 loss: 0.0017 lr: 0.02\n",
      "iteration: 365040 loss: 0.0017 lr: 0.02\n",
      "iteration: 365050 loss: 0.0013 lr: 0.02\n",
      "iteration: 365060 loss: 0.0013 lr: 0.02\n",
      "iteration: 365070 loss: 0.0016 lr: 0.02\n",
      "iteration: 365080 loss: 0.0022 lr: 0.02\n",
      "iteration: 365090 loss: 0.0014 lr: 0.02\n",
      "iteration: 365100 loss: 0.0016 lr: 0.02\n",
      "iteration: 365110 loss: 0.0016 lr: 0.02\n",
      "iteration: 365120 loss: 0.0016 lr: 0.02\n",
      "iteration: 365130 loss: 0.0010 lr: 0.02\n",
      "iteration: 365140 loss: 0.0016 lr: 0.02\n",
      "iteration: 365150 loss: 0.0015 lr: 0.02\n",
      "iteration: 365160 loss: 0.0014 lr: 0.02\n",
      "iteration: 365170 loss: 0.0014 lr: 0.02\n",
      "iteration: 365180 loss: 0.0013 lr: 0.02\n",
      "iteration: 365190 loss: 0.0014 lr: 0.02\n",
      "iteration: 365200 loss: 0.0015 lr: 0.02\n",
      "iteration: 365210 loss: 0.0011 lr: 0.02\n",
      "iteration: 365220 loss: 0.0018 lr: 0.02\n",
      "iteration: 365230 loss: 0.0017 lr: 0.02\n",
      "iteration: 365240 loss: 0.0023 lr: 0.02\n",
      "iteration: 365250 loss: 0.0014 lr: 0.02\n",
      "iteration: 365260 loss: 0.0010 lr: 0.02\n",
      "iteration: 365270 loss: 0.0018 lr: 0.02\n",
      "iteration: 365280 loss: 0.0021 lr: 0.02\n",
      "iteration: 365290 loss: 0.0016 lr: 0.02\n",
      "iteration: 365300 loss: 0.0026 lr: 0.02\n",
      "iteration: 365310 loss: 0.0022 lr: 0.02\n",
      "iteration: 365320 loss: 0.0017 lr: 0.02\n",
      "iteration: 365330 loss: 0.0016 lr: 0.02\n",
      "iteration: 365340 loss: 0.0020 lr: 0.02\n",
      "iteration: 365350 loss: 0.0029 lr: 0.02\n",
      "iteration: 365360 loss: 0.0016 lr: 0.02\n",
      "iteration: 365370 loss: 0.0011 lr: 0.02\n",
      "iteration: 365380 loss: 0.0015 lr: 0.02\n",
      "iteration: 365390 loss: 0.0022 lr: 0.02\n",
      "iteration: 365400 loss: 0.0026 lr: 0.02\n",
      "iteration: 365410 loss: 0.0019 lr: 0.02\n",
      "iteration: 365420 loss: 0.0014 lr: 0.02\n",
      "iteration: 365430 loss: 0.0015 lr: 0.02\n",
      "iteration: 365440 loss: 0.0014 lr: 0.02\n",
      "iteration: 365450 loss: 0.0010 lr: 0.02\n",
      "iteration: 365460 loss: 0.0017 lr: 0.02\n",
      "iteration: 365470 loss: 0.0011 lr: 0.02\n",
      "iteration: 365480 loss: 0.0016 lr: 0.02\n",
      "iteration: 365490 loss: 0.0015 lr: 0.02\n",
      "iteration: 365500 loss: 0.0022 lr: 0.02\n",
      "iteration: 365510 loss: 0.0019 lr: 0.02\n",
      "iteration: 365520 loss: 0.0017 lr: 0.02\n",
      "iteration: 365530 loss: 0.0022 lr: 0.02\n",
      "iteration: 365540 loss: 0.0013 lr: 0.02\n",
      "iteration: 365550 loss: 0.0016 lr: 0.02\n",
      "iteration: 365560 loss: 0.0013 lr: 0.02\n",
      "iteration: 365570 loss: 0.0017 lr: 0.02\n",
      "iteration: 365580 loss: 0.0013 lr: 0.02\n",
      "iteration: 365590 loss: 0.0020 lr: 0.02\n",
      "iteration: 365600 loss: 0.0025 lr: 0.02\n",
      "iteration: 365610 loss: 0.0017 lr: 0.02\n",
      "iteration: 365620 loss: 0.0014 lr: 0.02\n",
      "iteration: 365630 loss: 0.0012 lr: 0.02\n",
      "iteration: 365640 loss: 0.0017 lr: 0.02\n",
      "iteration: 365650 loss: 0.0017 lr: 0.02\n",
      "iteration: 365660 loss: 0.0016 lr: 0.02\n",
      "iteration: 365670 loss: 0.0017 lr: 0.02\n",
      "iteration: 365680 loss: 0.0018 lr: 0.02\n",
      "iteration: 365690 loss: 0.0030 lr: 0.02\n",
      "iteration: 365700 loss: 0.0012 lr: 0.02\n",
      "iteration: 365710 loss: 0.0016 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 365720 loss: 0.0020 lr: 0.02\n",
      "iteration: 365730 loss: 0.0022 lr: 0.02\n",
      "iteration: 365740 loss: 0.0012 lr: 0.02\n",
      "iteration: 365750 loss: 0.0019 lr: 0.02\n",
      "iteration: 365760 loss: 0.0014 lr: 0.02\n",
      "iteration: 365770 loss: 0.0019 lr: 0.02\n",
      "iteration: 365780 loss: 0.0017 lr: 0.02\n",
      "iteration: 365790 loss: 0.0017 lr: 0.02\n",
      "iteration: 365800 loss: 0.0013 lr: 0.02\n",
      "iteration: 365810 loss: 0.0013 lr: 0.02\n",
      "iteration: 365820 loss: 0.0010 lr: 0.02\n",
      "iteration: 365830 loss: 0.0015 lr: 0.02\n",
      "iteration: 365840 loss: 0.0018 lr: 0.02\n",
      "iteration: 365850 loss: 0.0016 lr: 0.02\n",
      "iteration: 365860 loss: 0.0014 lr: 0.02\n",
      "iteration: 365870 loss: 0.0017 lr: 0.02\n",
      "iteration: 365880 loss: 0.0017 lr: 0.02\n",
      "iteration: 365890 loss: 0.0011 lr: 0.02\n",
      "iteration: 365900 loss: 0.0021 lr: 0.02\n",
      "iteration: 365910 loss: 0.0020 lr: 0.02\n",
      "iteration: 365920 loss: 0.0013 lr: 0.02\n",
      "iteration: 365930 loss: 0.0019 lr: 0.02\n",
      "iteration: 365940 loss: 0.0014 lr: 0.02\n",
      "iteration: 365950 loss: 0.0017 lr: 0.02\n",
      "iteration: 365960 loss: 0.0016 lr: 0.02\n",
      "iteration: 365970 loss: 0.0014 lr: 0.02\n",
      "iteration: 365980 loss: 0.0017 lr: 0.02\n",
      "iteration: 365990 loss: 0.0017 lr: 0.02\n",
      "iteration: 366000 loss: 0.0018 lr: 0.02\n",
      "iteration: 366010 loss: 0.0012 lr: 0.02\n",
      "iteration: 366020 loss: 0.0016 lr: 0.02\n",
      "iteration: 366030 loss: 0.0015 lr: 0.02\n",
      "iteration: 366040 loss: 0.0017 lr: 0.02\n",
      "iteration: 366050 loss: 0.0016 lr: 0.02\n",
      "iteration: 366060 loss: 0.0011 lr: 0.02\n",
      "iteration: 366070 loss: 0.0011 lr: 0.02\n",
      "iteration: 366080 loss: 0.0019 lr: 0.02\n",
      "iteration: 366090 loss: 0.0012 lr: 0.02\n",
      "iteration: 366100 loss: 0.0014 lr: 0.02\n",
      "iteration: 366110 loss: 0.0012 lr: 0.02\n",
      "iteration: 366120 loss: 0.0022 lr: 0.02\n",
      "iteration: 366130 loss: 0.0019 lr: 0.02\n",
      "iteration: 366140 loss: 0.0015 lr: 0.02\n",
      "iteration: 366150 loss: 0.0013 lr: 0.02\n",
      "iteration: 366160 loss: 0.0015 lr: 0.02\n",
      "iteration: 366170 loss: 0.0015 lr: 0.02\n",
      "iteration: 366180 loss: 0.0015 lr: 0.02\n",
      "iteration: 366190 loss: 0.0017 lr: 0.02\n",
      "iteration: 366200 loss: 0.0019 lr: 0.02\n",
      "iteration: 366210 loss: 0.0020 lr: 0.02\n",
      "iteration: 366220 loss: 0.0021 lr: 0.02\n",
      "iteration: 366230 loss: 0.0019 lr: 0.02\n",
      "iteration: 366240 loss: 0.0020 lr: 0.02\n",
      "iteration: 366250 loss: 0.0019 lr: 0.02\n",
      "iteration: 366260 loss: 0.0020 lr: 0.02\n",
      "iteration: 366270 loss: 0.0016 lr: 0.02\n",
      "iteration: 366280 loss: 0.0017 lr: 0.02\n",
      "iteration: 366290 loss: 0.0019 lr: 0.02\n",
      "iteration: 366300 loss: 0.0009 lr: 0.02\n",
      "iteration: 366310 loss: 0.0018 lr: 0.02\n",
      "iteration: 366320 loss: 0.0014 lr: 0.02\n",
      "iteration: 366330 loss: 0.0014 lr: 0.02\n",
      "iteration: 366340 loss: 0.0015 lr: 0.02\n",
      "iteration: 366350 loss: 0.0012 lr: 0.02\n",
      "iteration: 366360 loss: 0.0012 lr: 0.02\n",
      "iteration: 366370 loss: 0.0014 lr: 0.02\n",
      "iteration: 366380 loss: 0.0014 lr: 0.02\n",
      "iteration: 366390 loss: 0.0014 lr: 0.02\n",
      "iteration: 366400 loss: 0.0016 lr: 0.02\n",
      "iteration: 366410 loss: 0.0022 lr: 0.02\n",
      "iteration: 366420 loss: 0.0020 lr: 0.02\n",
      "iteration: 366430 loss: 0.0012 lr: 0.02\n",
      "iteration: 366440 loss: 0.0020 lr: 0.02\n",
      "iteration: 366450 loss: 0.0019 lr: 0.02\n",
      "iteration: 366460 loss: 0.0025 lr: 0.02\n",
      "iteration: 366470 loss: 0.0012 lr: 0.02\n",
      "iteration: 366480 loss: 0.0016 lr: 0.02\n",
      "iteration: 366490 loss: 0.0018 lr: 0.02\n",
      "iteration: 366500 loss: 0.0013 lr: 0.02\n",
      "iteration: 366510 loss: 0.0015 lr: 0.02\n",
      "iteration: 366520 loss: 0.0014 lr: 0.02\n",
      "iteration: 366530 loss: 0.0014 lr: 0.02\n",
      "iteration: 366540 loss: 0.0013 lr: 0.02\n",
      "iteration: 366550 loss: 0.0027 lr: 0.02\n",
      "iteration: 366560 loss: 0.0013 lr: 0.02\n",
      "iteration: 366570 loss: 0.0014 lr: 0.02\n",
      "iteration: 366580 loss: 0.0033 lr: 0.02\n",
      "iteration: 366590 loss: 0.0014 lr: 0.02\n",
      "iteration: 366600 loss: 0.0017 lr: 0.02\n",
      "iteration: 366610 loss: 0.0025 lr: 0.02\n",
      "iteration: 366620 loss: 0.0014 lr: 0.02\n",
      "iteration: 366630 loss: 0.0016 lr: 0.02\n",
      "iteration: 366640 loss: 0.0011 lr: 0.02\n",
      "iteration: 366650 loss: 0.0011 lr: 0.02\n",
      "iteration: 366660 loss: 0.0016 lr: 0.02\n",
      "iteration: 366670 loss: 0.0010 lr: 0.02\n",
      "iteration: 366680 loss: 0.0018 lr: 0.02\n",
      "iteration: 366690 loss: 0.0020 lr: 0.02\n",
      "iteration: 366700 loss: 0.0015 lr: 0.02\n",
      "iteration: 366710 loss: 0.0025 lr: 0.02\n",
      "iteration: 366720 loss: 0.0028 lr: 0.02\n",
      "iteration: 366730 loss: 0.0017 lr: 0.02\n",
      "iteration: 366740 loss: 0.0016 lr: 0.02\n",
      "iteration: 366750 loss: 0.0012 lr: 0.02\n",
      "iteration: 366760 loss: 0.0019 lr: 0.02\n",
      "iteration: 366770 loss: 0.0020 lr: 0.02\n",
      "iteration: 366780 loss: 0.0037 lr: 0.02\n",
      "iteration: 366790 loss: 0.0013 lr: 0.02\n",
      "iteration: 366800 loss: 0.0017 lr: 0.02\n",
      "iteration: 366810 loss: 0.0022 lr: 0.02\n",
      "iteration: 366820 loss: 0.0012 lr: 0.02\n",
      "iteration: 366830 loss: 0.0012 lr: 0.02\n",
      "iteration: 366840 loss: 0.0017 lr: 0.02\n",
      "iteration: 366850 loss: 0.0019 lr: 0.02\n",
      "iteration: 366860 loss: 0.0015 lr: 0.02\n",
      "iteration: 366870 loss: 0.0021 lr: 0.02\n",
      "iteration: 366880 loss: 0.0016 lr: 0.02\n",
      "iteration: 366890 loss: 0.0017 lr: 0.02\n",
      "iteration: 366900 loss: 0.0012 lr: 0.02\n",
      "iteration: 366910 loss: 0.0010 lr: 0.02\n",
      "iteration: 366920 loss: 0.0013 lr: 0.02\n",
      "iteration: 366930 loss: 0.0013 lr: 0.02\n",
      "iteration: 366940 loss: 0.0016 lr: 0.02\n",
      "iteration: 366950 loss: 0.0017 lr: 0.02\n",
      "iteration: 366960 loss: 0.0014 lr: 0.02\n",
      "iteration: 366970 loss: 0.0014 lr: 0.02\n",
      "iteration: 366980 loss: 0.0018 lr: 0.02\n",
      "iteration: 366990 loss: 0.0013 lr: 0.02\n",
      "iteration: 367000 loss: 0.0022 lr: 0.02\n",
      "iteration: 367010 loss: 0.0018 lr: 0.02\n",
      "iteration: 367020 loss: 0.0013 lr: 0.02\n",
      "iteration: 367030 loss: 0.0015 lr: 0.02\n",
      "iteration: 367040 loss: 0.0014 lr: 0.02\n",
      "iteration: 367050 loss: 0.0017 lr: 0.02\n",
      "iteration: 367060 loss: 0.0018 lr: 0.02\n",
      "iteration: 367070 loss: 0.0022 lr: 0.02\n",
      "iteration: 367080 loss: 0.0030 lr: 0.02\n",
      "iteration: 367090 loss: 0.0022 lr: 0.02\n",
      "iteration: 367100 loss: 0.0013 lr: 0.02\n",
      "iteration: 367110 loss: 0.0018 lr: 0.02\n",
      "iteration: 367120 loss: 0.0023 lr: 0.02\n",
      "iteration: 367130 loss: 0.0012 lr: 0.02\n",
      "iteration: 367140 loss: 0.0014 lr: 0.02\n",
      "iteration: 367150 loss: 0.0012 lr: 0.02\n",
      "iteration: 367160 loss: 0.0022 lr: 0.02\n",
      "iteration: 367170 loss: 0.0015 lr: 0.02\n",
      "iteration: 367180 loss: 0.0014 lr: 0.02\n",
      "iteration: 367190 loss: 0.0014 lr: 0.02\n",
      "iteration: 367200 loss: 0.0016 lr: 0.02\n",
      "iteration: 367210 loss: 0.0017 lr: 0.02\n",
      "iteration: 367220 loss: 0.0016 lr: 0.02\n",
      "iteration: 367230 loss: 0.0020 lr: 0.02\n",
      "iteration: 367240 loss: 0.0013 lr: 0.02\n",
      "iteration: 367250 loss: 0.0030 lr: 0.02\n",
      "iteration: 367260 loss: 0.0013 lr: 0.02\n",
      "iteration: 367270 loss: 0.0016 lr: 0.02\n",
      "iteration: 367280 loss: 0.0016 lr: 0.02\n",
      "iteration: 367290 loss: 0.0011 lr: 0.02\n",
      "iteration: 367300 loss: 0.0018 lr: 0.02\n",
      "iteration: 367310 loss: 0.0011 lr: 0.02\n",
      "iteration: 367320 loss: 0.0015 lr: 0.02\n",
      "iteration: 367330 loss: 0.0017 lr: 0.02\n",
      "iteration: 367340 loss: 0.0012 lr: 0.02\n",
      "iteration: 367350 loss: 0.0018 lr: 0.02\n",
      "iteration: 367360 loss: 0.0014 lr: 0.02\n",
      "iteration: 367370 loss: 0.0020 lr: 0.02\n",
      "iteration: 367380 loss: 0.0022 lr: 0.02\n",
      "iteration: 367390 loss: 0.0024 lr: 0.02\n",
      "iteration: 367400 loss: 0.0017 lr: 0.02\n",
      "iteration: 367410 loss: 0.0018 lr: 0.02\n",
      "iteration: 367420 loss: 0.0015 lr: 0.02\n",
      "iteration: 367430 loss: 0.0016 lr: 0.02\n",
      "iteration: 367440 loss: 0.0015 lr: 0.02\n",
      "iteration: 367450 loss: 0.0020 lr: 0.02\n",
      "iteration: 367460 loss: 0.0014 lr: 0.02\n",
      "iteration: 367470 loss: 0.0021 lr: 0.02\n",
      "iteration: 367480 loss: 0.0017 lr: 0.02\n",
      "iteration: 367490 loss: 0.0014 lr: 0.02\n",
      "iteration: 367500 loss: 0.0017 lr: 0.02\n",
      "iteration: 367510 loss: 0.0011 lr: 0.02\n",
      "iteration: 367520 loss: 0.0016 lr: 0.02\n",
      "iteration: 367530 loss: 0.0017 lr: 0.02\n",
      "iteration: 367540 loss: 0.0018 lr: 0.02\n",
      "iteration: 367550 loss: 0.0013 lr: 0.02\n",
      "iteration: 367560 loss: 0.0023 lr: 0.02\n",
      "iteration: 367570 loss: 0.0021 lr: 0.02\n",
      "iteration: 367580 loss: 0.0022 lr: 0.02\n",
      "iteration: 367590 loss: 0.0018 lr: 0.02\n",
      "iteration: 367600 loss: 0.0021 lr: 0.02\n",
      "iteration: 367610 loss: 0.0012 lr: 0.02\n",
      "iteration: 367620 loss: 0.0009 lr: 0.02\n",
      "iteration: 367630 loss: 0.0017 lr: 0.02\n",
      "iteration: 367640 loss: 0.0022 lr: 0.02\n",
      "iteration: 367650 loss: 0.0019 lr: 0.02\n",
      "iteration: 367660 loss: 0.0015 lr: 0.02\n",
      "iteration: 367670 loss: 0.0014 lr: 0.02\n",
      "iteration: 367680 loss: 0.0019 lr: 0.02\n",
      "iteration: 367690 loss: 0.0013 lr: 0.02\n",
      "iteration: 367700 loss: 0.0014 lr: 0.02\n",
      "iteration: 367710 loss: 0.0013 lr: 0.02\n",
      "iteration: 367720 loss: 0.0018 lr: 0.02\n",
      "iteration: 367730 loss: 0.0015 lr: 0.02\n",
      "iteration: 367740 loss: 0.0015 lr: 0.02\n",
      "iteration: 367750 loss: 0.0017 lr: 0.02\n",
      "iteration: 367760 loss: 0.0013 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 367770 loss: 0.0014 lr: 0.02\n",
      "iteration: 367780 loss: 0.0018 lr: 0.02\n",
      "iteration: 367790 loss: 0.0023 lr: 0.02\n",
      "iteration: 367800 loss: 0.0014 lr: 0.02\n",
      "iteration: 367810 loss: 0.0016 lr: 0.02\n",
      "iteration: 367820 loss: 0.0018 lr: 0.02\n",
      "iteration: 367830 loss: 0.0018 lr: 0.02\n",
      "iteration: 367840 loss: 0.0013 lr: 0.02\n",
      "iteration: 367850 loss: 0.0022 lr: 0.02\n",
      "iteration: 367860 loss: 0.0018 lr: 0.02\n",
      "iteration: 367870 loss: 0.0015 lr: 0.02\n",
      "iteration: 367880 loss: 0.0013 lr: 0.02\n",
      "iteration: 367890 loss: 0.0017 lr: 0.02\n",
      "iteration: 367900 loss: 0.0019 lr: 0.02\n",
      "iteration: 367910 loss: 0.0016 lr: 0.02\n",
      "iteration: 367920 loss: 0.0018 lr: 0.02\n",
      "iteration: 367930 loss: 0.0023 lr: 0.02\n",
      "iteration: 367940 loss: 0.0026 lr: 0.02\n",
      "iteration: 367950 loss: 0.0013 lr: 0.02\n",
      "iteration: 367960 loss: 0.0019 lr: 0.02\n",
      "iteration: 367970 loss: 0.0017 lr: 0.02\n",
      "iteration: 367980 loss: 0.0019 lr: 0.02\n",
      "iteration: 367990 loss: 0.0014 lr: 0.02\n",
      "iteration: 368000 loss: 0.0019 lr: 0.02\n",
      "iteration: 368010 loss: 0.0013 lr: 0.02\n",
      "iteration: 368020 loss: 0.0011 lr: 0.02\n",
      "iteration: 368030 loss: 0.0016 lr: 0.02\n",
      "iteration: 368040 loss: 0.0022 lr: 0.02\n",
      "iteration: 368050 loss: 0.0023 lr: 0.02\n",
      "iteration: 368060 loss: 0.0016 lr: 0.02\n",
      "iteration: 368070 loss: 0.0015 lr: 0.02\n",
      "iteration: 368080 loss: 0.0017 lr: 0.02\n",
      "iteration: 368090 loss: 0.0014 lr: 0.02\n",
      "iteration: 368100 loss: 0.0017 lr: 0.02\n",
      "iteration: 368110 loss: 0.0017 lr: 0.02\n",
      "iteration: 368120 loss: 0.0016 lr: 0.02\n",
      "iteration: 368130 loss: 0.0015 lr: 0.02\n",
      "iteration: 368140 loss: 0.0013 lr: 0.02\n",
      "iteration: 368150 loss: 0.0020 lr: 0.02\n",
      "iteration: 368160 loss: 0.0012 lr: 0.02\n",
      "iteration: 368170 loss: 0.0011 lr: 0.02\n",
      "iteration: 368180 loss: 0.0012 lr: 0.02\n",
      "iteration: 368190 loss: 0.0015 lr: 0.02\n",
      "iteration: 368200 loss: 0.0014 lr: 0.02\n",
      "iteration: 368210 loss: 0.0016 lr: 0.02\n",
      "iteration: 368220 loss: 0.0013 lr: 0.02\n",
      "iteration: 368230 loss: 0.0020 lr: 0.02\n",
      "iteration: 368240 loss: 0.0016 lr: 0.02\n",
      "iteration: 368250 loss: 0.0012 lr: 0.02\n",
      "iteration: 368260 loss: 0.0012 lr: 0.02\n",
      "iteration: 368270 loss: 0.0017 lr: 0.02\n",
      "iteration: 368280 loss: 0.0017 lr: 0.02\n",
      "iteration: 368290 loss: 0.0022 lr: 0.02\n",
      "iteration: 368300 loss: 0.0013 lr: 0.02\n",
      "iteration: 368310 loss: 0.0017 lr: 0.02\n",
      "iteration: 368320 loss: 0.0020 lr: 0.02\n",
      "iteration: 368330 loss: 0.0020 lr: 0.02\n",
      "iteration: 368340 loss: 0.0016 lr: 0.02\n",
      "iteration: 368350 loss: 0.0018 lr: 0.02\n",
      "iteration: 368360 loss: 0.0012 lr: 0.02\n",
      "iteration: 368370 loss: 0.0015 lr: 0.02\n",
      "iteration: 368380 loss: 0.0012 lr: 0.02\n",
      "iteration: 368390 loss: 0.0018 lr: 0.02\n",
      "iteration: 368400 loss: 0.0042 lr: 0.02\n",
      "iteration: 368410 loss: 0.0013 lr: 0.02\n",
      "iteration: 368420 loss: 0.0018 lr: 0.02\n",
      "iteration: 368430 loss: 0.0017 lr: 0.02\n",
      "iteration: 368440 loss: 0.0018 lr: 0.02\n",
      "iteration: 368450 loss: 0.0024 lr: 0.02\n",
      "iteration: 368460 loss: 0.0023 lr: 0.02\n",
      "iteration: 368470 loss: 0.0021 lr: 0.02\n",
      "iteration: 368480 loss: 0.0021 lr: 0.02\n",
      "iteration: 368490 loss: 0.0030 lr: 0.02\n",
      "iteration: 368500 loss: 0.0014 lr: 0.02\n",
      "iteration: 368510 loss: 0.0013 lr: 0.02\n",
      "iteration: 368520 loss: 0.0012 lr: 0.02\n",
      "iteration: 368530 loss: 0.0014 lr: 0.02\n",
      "iteration: 368540 loss: 0.0014 lr: 0.02\n",
      "iteration: 368550 loss: 0.0012 lr: 0.02\n",
      "iteration: 368560 loss: 0.0018 lr: 0.02\n",
      "iteration: 368570 loss: 0.0023 lr: 0.02\n",
      "iteration: 368580 loss: 0.0018 lr: 0.02\n",
      "iteration: 368590 loss: 0.0020 lr: 0.02\n",
      "iteration: 368600 loss: 0.0012 lr: 0.02\n",
      "iteration: 368610 loss: 0.0023 lr: 0.02\n",
      "iteration: 368620 loss: 0.0019 lr: 0.02\n",
      "iteration: 368630 loss: 0.0015 lr: 0.02\n",
      "iteration: 368640 loss: 0.0017 lr: 0.02\n",
      "iteration: 368650 loss: 0.0017 lr: 0.02\n",
      "iteration: 368660 loss: 0.0016 lr: 0.02\n",
      "iteration: 368670 loss: 0.0024 lr: 0.02\n",
      "iteration: 368680 loss: 0.0011 lr: 0.02\n",
      "iteration: 368690 loss: 0.0022 lr: 0.02\n",
      "iteration: 368700 loss: 0.0017 lr: 0.02\n",
      "iteration: 368710 loss: 0.0017 lr: 0.02\n",
      "iteration: 368720 loss: 0.0016 lr: 0.02\n",
      "iteration: 368730 loss: 0.0012 lr: 0.02\n",
      "iteration: 368740 loss: 0.0020 lr: 0.02\n",
      "iteration: 368750 loss: 0.0017 lr: 0.02\n",
      "iteration: 368760 loss: 0.0021 lr: 0.02\n",
      "iteration: 368770 loss: 0.0018 lr: 0.02\n",
      "iteration: 368780 loss: 0.0018 lr: 0.02\n",
      "iteration: 368790 loss: 0.0014 lr: 0.02\n",
      "iteration: 368800 loss: 0.0025 lr: 0.02\n",
      "iteration: 368810 loss: 0.0019 lr: 0.02\n",
      "iteration: 368820 loss: 0.0013 lr: 0.02\n",
      "iteration: 368830 loss: 0.0015 lr: 0.02\n",
      "iteration: 368840 loss: 0.0023 lr: 0.02\n",
      "iteration: 368850 loss: 0.0022 lr: 0.02\n",
      "iteration: 368860 loss: 0.0027 lr: 0.02\n",
      "iteration: 368870 loss: 0.0015 lr: 0.02\n",
      "iteration: 368880 loss: 0.0019 lr: 0.02\n",
      "iteration: 368890 loss: 0.0021 lr: 0.02\n",
      "iteration: 368900 loss: 0.0014 lr: 0.02\n",
      "iteration: 368910 loss: 0.0017 lr: 0.02\n",
      "iteration: 368920 loss: 0.0025 lr: 0.02\n",
      "iteration: 368930 loss: 0.0009 lr: 0.02\n",
      "iteration: 368940 loss: 0.0013 lr: 0.02\n",
      "iteration: 368950 loss: 0.0013 lr: 0.02\n",
      "iteration: 368960 loss: 0.0018 lr: 0.02\n",
      "iteration: 368970 loss: 0.0016 lr: 0.02\n",
      "iteration: 368980 loss: 0.0013 lr: 0.02\n",
      "iteration: 368990 loss: 0.0012 lr: 0.02\n",
      "iteration: 369000 loss: 0.0014 lr: 0.02\n",
      "iteration: 369010 loss: 0.0016 lr: 0.02\n",
      "iteration: 369020 loss: 0.0021 lr: 0.02\n",
      "iteration: 369030 loss: 0.0019 lr: 0.02\n",
      "iteration: 369040 loss: 0.0015 lr: 0.02\n",
      "iteration: 369050 loss: 0.0023 lr: 0.02\n",
      "iteration: 369060 loss: 0.0022 lr: 0.02\n",
      "iteration: 369070 loss: 0.0018 lr: 0.02\n",
      "iteration: 369080 loss: 0.0019 lr: 0.02\n",
      "iteration: 369090 loss: 0.0013 lr: 0.02\n",
      "iteration: 369100 loss: 0.0014 lr: 0.02\n",
      "iteration: 369110 loss: 0.0018 lr: 0.02\n",
      "iteration: 369120 loss: 0.0016 lr: 0.02\n",
      "iteration: 369130 loss: 0.0013 lr: 0.02\n",
      "iteration: 369140 loss: 0.0030 lr: 0.02\n",
      "iteration: 369150 loss: 0.0016 lr: 0.02\n",
      "iteration: 369160 loss: 0.0016 lr: 0.02\n",
      "iteration: 369170 loss: 0.0020 lr: 0.02\n",
      "iteration: 369180 loss: 0.0016 lr: 0.02\n",
      "iteration: 369190 loss: 0.0018 lr: 0.02\n",
      "iteration: 369200 loss: 0.0011 lr: 0.02\n",
      "iteration: 369210 loss: 0.0018 lr: 0.02\n",
      "iteration: 369220 loss: 0.0010 lr: 0.02\n",
      "iteration: 369230 loss: 0.0010 lr: 0.02\n",
      "iteration: 369240 loss: 0.0017 lr: 0.02\n",
      "iteration: 369250 loss: 0.0031 lr: 0.02\n",
      "iteration: 369260 loss: 0.0023 lr: 0.02\n",
      "iteration: 369270 loss: 0.0016 lr: 0.02\n",
      "iteration: 369280 loss: 0.0021 lr: 0.02\n",
      "iteration: 369290 loss: 0.0012 lr: 0.02\n",
      "iteration: 369300 loss: 0.0013 lr: 0.02\n",
      "iteration: 369310 loss: 0.0015 lr: 0.02\n",
      "iteration: 369320 loss: 0.0016 lr: 0.02\n",
      "iteration: 369330 loss: 0.0020 lr: 0.02\n",
      "iteration: 369340 loss: 0.0017 lr: 0.02\n",
      "iteration: 369350 loss: 0.0015 lr: 0.02\n",
      "iteration: 369360 loss: 0.0026 lr: 0.02\n",
      "iteration: 369370 loss: 0.0015 lr: 0.02\n",
      "iteration: 369380 loss: 0.0013 lr: 0.02\n",
      "iteration: 369390 loss: 0.0021 lr: 0.02\n",
      "iteration: 369400 loss: 0.0012 lr: 0.02\n",
      "iteration: 369410 loss: 0.0012 lr: 0.02\n",
      "iteration: 369420 loss: 0.0011 lr: 0.02\n",
      "iteration: 369430 loss: 0.0015 lr: 0.02\n",
      "iteration: 369440 loss: 0.0016 lr: 0.02\n",
      "iteration: 369450 loss: 0.0028 lr: 0.02\n",
      "iteration: 369460 loss: 0.0016 lr: 0.02\n",
      "iteration: 369470 loss: 0.0016 lr: 0.02\n",
      "iteration: 369480 loss: 0.0012 lr: 0.02\n",
      "iteration: 369490 loss: 0.0014 lr: 0.02\n",
      "iteration: 369500 loss: 0.0013 lr: 0.02\n",
      "iteration: 369510 loss: 0.0019 lr: 0.02\n",
      "iteration: 369520 loss: 0.0021 lr: 0.02\n",
      "iteration: 369530 loss: 0.0010 lr: 0.02\n",
      "iteration: 369540 loss: 0.0022 lr: 0.02\n",
      "iteration: 369550 loss: 0.0013 lr: 0.02\n",
      "iteration: 369560 loss: 0.0015 lr: 0.02\n",
      "iteration: 369570 loss: 0.0012 lr: 0.02\n",
      "iteration: 369580 loss: 0.0015 lr: 0.02\n",
      "iteration: 369590 loss: 0.0011 lr: 0.02\n",
      "iteration: 369600 loss: 0.0011 lr: 0.02\n",
      "iteration: 369610 loss: 0.0024 lr: 0.02\n",
      "iteration: 369620 loss: 0.0019 lr: 0.02\n",
      "iteration: 369630 loss: 0.0017 lr: 0.02\n",
      "iteration: 369640 loss: 0.0017 lr: 0.02\n",
      "iteration: 369650 loss: 0.0015 lr: 0.02\n",
      "iteration: 369660 loss: 0.0018 lr: 0.02\n",
      "iteration: 369670 loss: 0.0021 lr: 0.02\n",
      "iteration: 369680 loss: 0.0014 lr: 0.02\n",
      "iteration: 369690 loss: 0.0018 lr: 0.02\n",
      "iteration: 369700 loss: 0.0014 lr: 0.02\n",
      "iteration: 369710 loss: 0.0017 lr: 0.02\n",
      "iteration: 369720 loss: 0.0029 lr: 0.02\n",
      "iteration: 369730 loss: 0.0015 lr: 0.02\n",
      "iteration: 369740 loss: 0.0035 lr: 0.02\n",
      "iteration: 369750 loss: 0.0012 lr: 0.02\n",
      "iteration: 369760 loss: 0.0015 lr: 0.02\n",
      "iteration: 369770 loss: 0.0012 lr: 0.02\n",
      "iteration: 369780 loss: 0.0015 lr: 0.02\n",
      "iteration: 369790 loss: 0.0028 lr: 0.02\n",
      "iteration: 369800 loss: 0.0011 lr: 0.02\n",
      "iteration: 369810 loss: 0.0018 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 369820 loss: 0.0014 lr: 0.02\n",
      "iteration: 369830 loss: 0.0013 lr: 0.02\n",
      "iteration: 369840 loss: 0.0015 lr: 0.02\n",
      "iteration: 369850 loss: 0.0016 lr: 0.02\n",
      "iteration: 369860 loss: 0.0020 lr: 0.02\n",
      "iteration: 369870 loss: 0.0022 lr: 0.02\n",
      "iteration: 369880 loss: 0.0011 lr: 0.02\n",
      "iteration: 369890 loss: 0.0017 lr: 0.02\n",
      "iteration: 369900 loss: 0.0015 lr: 0.02\n",
      "iteration: 369910 loss: 0.0017 lr: 0.02\n",
      "iteration: 369920 loss: 0.0015 lr: 0.02\n",
      "iteration: 369930 loss: 0.0015 lr: 0.02\n",
      "iteration: 369940 loss: 0.0014 lr: 0.02\n",
      "iteration: 369950 loss: 0.0016 lr: 0.02\n",
      "iteration: 369960 loss: 0.0025 lr: 0.02\n",
      "iteration: 369970 loss: 0.0017 lr: 0.02\n",
      "iteration: 369980 loss: 0.0013 lr: 0.02\n",
      "iteration: 369990 loss: 0.0022 lr: 0.02\n",
      "iteration: 370000 loss: 0.0019 lr: 0.02\n",
      "iteration: 370010 loss: 0.0017 lr: 0.02\n",
      "iteration: 370020 loss: 0.0024 lr: 0.02\n",
      "iteration: 370030 loss: 0.0017 lr: 0.02\n",
      "iteration: 370040 loss: 0.0025 lr: 0.02\n",
      "iteration: 370050 loss: 0.0020 lr: 0.02\n",
      "iteration: 370060 loss: 0.0017 lr: 0.02\n",
      "iteration: 370070 loss: 0.0018 lr: 0.02\n",
      "iteration: 370080 loss: 0.0015 lr: 0.02\n",
      "iteration: 370090 loss: 0.0018 lr: 0.02\n",
      "iteration: 370100 loss: 0.0019 lr: 0.02\n",
      "iteration: 370110 loss: 0.0021 lr: 0.02\n",
      "iteration: 370120 loss: 0.0020 lr: 0.02\n",
      "iteration: 370130 loss: 0.0016 lr: 0.02\n",
      "iteration: 370140 loss: 0.0025 lr: 0.02\n",
      "iteration: 370150 loss: 0.0029 lr: 0.02\n",
      "iteration: 370160 loss: 0.0016 lr: 0.02\n",
      "iteration: 370170 loss: 0.0016 lr: 0.02\n",
      "iteration: 370180 loss: 0.0020 lr: 0.02\n",
      "iteration: 370190 loss: 0.0021 lr: 0.02\n",
      "iteration: 370200 loss: 0.0032 lr: 0.02\n",
      "iteration: 370210 loss: 0.0020 lr: 0.02\n",
      "iteration: 370220 loss: 0.0012 lr: 0.02\n",
      "iteration: 370230 loss: 0.0014 lr: 0.02\n",
      "iteration: 370240 loss: 0.0025 lr: 0.02\n",
      "iteration: 370250 loss: 0.0016 lr: 0.02\n",
      "iteration: 370260 loss: 0.0013 lr: 0.02\n",
      "iteration: 370270 loss: 0.0017 lr: 0.02\n",
      "iteration: 370280 loss: 0.0015 lr: 0.02\n",
      "iteration: 370290 loss: 0.0018 lr: 0.02\n",
      "iteration: 370300 loss: 0.0014 lr: 0.02\n",
      "iteration: 370310 loss: 0.0016 lr: 0.02\n",
      "iteration: 370320 loss: 0.0025 lr: 0.02\n",
      "iteration: 370330 loss: 0.0014 lr: 0.02\n",
      "iteration: 370340 loss: 0.0021 lr: 0.02\n",
      "iteration: 370350 loss: 0.0018 lr: 0.02\n",
      "iteration: 370360 loss: 0.0014 lr: 0.02\n",
      "iteration: 370370 loss: 0.0019 lr: 0.02\n",
      "iteration: 370380 loss: 0.0017 lr: 0.02\n",
      "iteration: 370390 loss: 0.0011 lr: 0.02\n",
      "iteration: 370400 loss: 0.0019 lr: 0.02\n",
      "iteration: 370410 loss: 0.0016 lr: 0.02\n",
      "iteration: 370420 loss: 0.0018 lr: 0.02\n",
      "iteration: 370430 loss: 0.0012 lr: 0.02\n",
      "iteration: 370440 loss: 0.0018 lr: 0.02\n",
      "iteration: 370450 loss: 0.0016 lr: 0.02\n",
      "iteration: 370460 loss: 0.0016 lr: 0.02\n",
      "iteration: 370470 loss: 0.0018 lr: 0.02\n",
      "iteration: 370480 loss: 0.0016 lr: 0.02\n",
      "iteration: 370490 loss: 0.0015 lr: 0.02\n",
      "iteration: 370500 loss: 0.0014 lr: 0.02\n",
      "iteration: 370510 loss: 0.0017 lr: 0.02\n",
      "iteration: 370520 loss: 0.0018 lr: 0.02\n",
      "iteration: 370530 loss: 0.0015 lr: 0.02\n",
      "iteration: 370540 loss: 0.0012 lr: 0.02\n",
      "iteration: 370550 loss: 0.0016 lr: 0.02\n",
      "iteration: 370560 loss: 0.0019 lr: 0.02\n",
      "iteration: 370570 loss: 0.0016 lr: 0.02\n",
      "iteration: 370580 loss: 0.0013 lr: 0.02\n",
      "iteration: 370590 loss: 0.0023 lr: 0.02\n",
      "iteration: 370600 loss: 0.0016 lr: 0.02\n",
      "iteration: 370610 loss: 0.0022 lr: 0.02\n",
      "iteration: 370620 loss: 0.0010 lr: 0.02\n",
      "iteration: 370630 loss: 0.0016 lr: 0.02\n",
      "iteration: 370640 loss: 0.0020 lr: 0.02\n",
      "iteration: 370650 loss: 0.0016 lr: 0.02\n",
      "iteration: 370660 loss: 0.0016 lr: 0.02\n",
      "iteration: 370670 loss: 0.0018 lr: 0.02\n",
      "iteration: 370680 loss: 0.0019 lr: 0.02\n",
      "iteration: 370690 loss: 0.0013 lr: 0.02\n",
      "iteration: 370700 loss: 0.0014 lr: 0.02\n",
      "iteration: 370710 loss: 0.0013 lr: 0.02\n",
      "iteration: 370720 loss: 0.0015 lr: 0.02\n",
      "iteration: 370730 loss: 0.0020 lr: 0.02\n",
      "iteration: 370740 loss: 0.0018 lr: 0.02\n",
      "iteration: 370750 loss: 0.0010 lr: 0.02\n",
      "iteration: 370760 loss: 0.0016 lr: 0.02\n",
      "iteration: 370770 loss: 0.0028 lr: 0.02\n",
      "iteration: 370780 loss: 0.0015 lr: 0.02\n",
      "iteration: 370790 loss: 0.0016 lr: 0.02\n",
      "iteration: 370800 loss: 0.0015 lr: 0.02\n",
      "iteration: 370810 loss: 0.0016 lr: 0.02\n",
      "iteration: 370820 loss: 0.0015 lr: 0.02\n",
      "iteration: 370830 loss: 0.0022 lr: 0.02\n",
      "iteration: 370840 loss: 0.0012 lr: 0.02\n",
      "iteration: 370850 loss: 0.0016 lr: 0.02\n",
      "iteration: 370860 loss: 0.0022 lr: 0.02\n",
      "iteration: 370870 loss: 0.0023 lr: 0.02\n",
      "iteration: 370880 loss: 0.0023 lr: 0.02\n",
      "iteration: 370890 loss: 0.0030 lr: 0.02\n",
      "iteration: 370900 loss: 0.0018 lr: 0.02\n",
      "iteration: 370910 loss: 0.0013 lr: 0.02\n",
      "iteration: 370920 loss: 0.0015 lr: 0.02\n",
      "iteration: 370930 loss: 0.0021 lr: 0.02\n",
      "iteration: 370940 loss: 0.0017 lr: 0.02\n",
      "iteration: 370950 loss: 0.0019 lr: 0.02\n",
      "iteration: 370960 loss: 0.0015 lr: 0.02\n",
      "iteration: 370970 loss: 0.0016 lr: 0.02\n",
      "iteration: 370980 loss: 0.0014 lr: 0.02\n",
      "iteration: 370990 loss: 0.0017 lr: 0.02\n",
      "iteration: 371000 loss: 0.0015 lr: 0.02\n",
      "iteration: 371010 loss: 0.0020 lr: 0.02\n",
      "iteration: 371020 loss: 0.0014 lr: 0.02\n",
      "iteration: 371030 loss: 0.0017 lr: 0.02\n",
      "iteration: 371040 loss: 0.0012 lr: 0.02\n",
      "iteration: 371050 loss: 0.0012 lr: 0.02\n",
      "iteration: 371060 loss: 0.0009 lr: 0.02\n",
      "iteration: 371070 loss: 0.0018 lr: 0.02\n",
      "iteration: 371080 loss: 0.0013 lr: 0.02\n",
      "iteration: 371090 loss: 0.0015 lr: 0.02\n",
      "iteration: 371100 loss: 0.0018 lr: 0.02\n",
      "iteration: 371110 loss: 0.0019 lr: 0.02\n",
      "iteration: 371120 loss: 0.0038 lr: 0.02\n",
      "iteration: 371130 loss: 0.0020 lr: 0.02\n",
      "iteration: 371140 loss: 0.0014 lr: 0.02\n",
      "iteration: 371150 loss: 0.0013 lr: 0.02\n",
      "iteration: 371160 loss: 0.0021 lr: 0.02\n",
      "iteration: 371170 loss: 0.0019 lr: 0.02\n",
      "iteration: 371180 loss: 0.0021 lr: 0.02\n",
      "iteration: 371190 loss: 0.0022 lr: 0.02\n",
      "iteration: 371200 loss: 0.0021 lr: 0.02\n",
      "iteration: 371210 loss: 0.0013 lr: 0.02\n",
      "iteration: 371220 loss: 0.0028 lr: 0.02\n",
      "iteration: 371230 loss: 0.0018 lr: 0.02\n",
      "iteration: 371240 loss: 0.0013 lr: 0.02\n",
      "iteration: 371250 loss: 0.0014 lr: 0.02\n",
      "iteration: 371260 loss: 0.0016 lr: 0.02\n",
      "iteration: 371270 loss: 0.0009 lr: 0.02\n",
      "iteration: 371280 loss: 0.0015 lr: 0.02\n",
      "iteration: 371290 loss: 0.0018 lr: 0.02\n",
      "iteration: 371300 loss: 0.0018 lr: 0.02\n",
      "iteration: 371310 loss: 0.0014 lr: 0.02\n",
      "iteration: 371320 loss: 0.0021 lr: 0.02\n",
      "iteration: 371330 loss: 0.0013 lr: 0.02\n",
      "iteration: 371340 loss: 0.0018 lr: 0.02\n",
      "iteration: 371350 loss: 0.0012 lr: 0.02\n",
      "iteration: 371360 loss: 0.0012 lr: 0.02\n",
      "iteration: 371370 loss: 0.0013 lr: 0.02\n",
      "iteration: 371380 loss: 0.0015 lr: 0.02\n",
      "iteration: 371390 loss: 0.0015 lr: 0.02\n",
      "iteration: 371400 loss: 0.0013 lr: 0.02\n",
      "iteration: 371410 loss: 0.0014 lr: 0.02\n",
      "iteration: 371420 loss: 0.0016 lr: 0.02\n",
      "iteration: 371430 loss: 0.0016 lr: 0.02\n",
      "iteration: 371440 loss: 0.0018 lr: 0.02\n",
      "iteration: 371450 loss: 0.0014 lr: 0.02\n",
      "iteration: 371460 loss: 0.0012 lr: 0.02\n",
      "iteration: 371470 loss: 0.0020 lr: 0.02\n",
      "iteration: 371480 loss: 0.0018 lr: 0.02\n",
      "iteration: 371490 loss: 0.0020 lr: 0.02\n",
      "iteration: 371500 loss: 0.0018 lr: 0.02\n",
      "iteration: 371510 loss: 0.0018 lr: 0.02\n",
      "iteration: 371520 loss: 0.0014 lr: 0.02\n",
      "iteration: 371530 loss: 0.0017 lr: 0.02\n",
      "iteration: 371540 loss: 0.0016 lr: 0.02\n",
      "iteration: 371550 loss: 0.0016 lr: 0.02\n",
      "iteration: 371560 loss: 0.0015 lr: 0.02\n",
      "iteration: 371570 loss: 0.0015 lr: 0.02\n",
      "iteration: 371580 loss: 0.0016 lr: 0.02\n",
      "iteration: 371590 loss: 0.0015 lr: 0.02\n",
      "iteration: 371600 loss: 0.0020 lr: 0.02\n",
      "iteration: 371610 loss: 0.0019 lr: 0.02\n",
      "iteration: 371620 loss: 0.0016 lr: 0.02\n",
      "iteration: 371630 loss: 0.0017 lr: 0.02\n",
      "iteration: 371640 loss: 0.0031 lr: 0.02\n",
      "iteration: 371650 loss: 0.0016 lr: 0.02\n",
      "iteration: 371660 loss: 0.0013 lr: 0.02\n",
      "iteration: 371670 loss: 0.0017 lr: 0.02\n",
      "iteration: 371680 loss: 0.0014 lr: 0.02\n",
      "iteration: 371690 loss: 0.0016 lr: 0.02\n",
      "iteration: 371700 loss: 0.0028 lr: 0.02\n",
      "iteration: 371710 loss: 0.0011 lr: 0.02\n",
      "iteration: 371720 loss: 0.0022 lr: 0.02\n",
      "iteration: 371730 loss: 0.0014 lr: 0.02\n",
      "iteration: 371740 loss: 0.0013 lr: 0.02\n",
      "iteration: 371750 loss: 0.0014 lr: 0.02\n",
      "iteration: 371760 loss: 0.0013 lr: 0.02\n",
      "iteration: 371770 loss: 0.0016 lr: 0.02\n",
      "iteration: 371780 loss: 0.0017 lr: 0.02\n",
      "iteration: 371790 loss: 0.0020 lr: 0.02\n",
      "iteration: 371800 loss: 0.0015 lr: 0.02\n",
      "iteration: 371810 loss: 0.0017 lr: 0.02\n",
      "iteration: 371820 loss: 0.0009 lr: 0.02\n",
      "iteration: 371830 loss: 0.0015 lr: 0.02\n",
      "iteration: 371840 loss: 0.0018 lr: 0.02\n",
      "iteration: 371850 loss: 0.0014 lr: 0.02\n",
      "iteration: 371860 loss: 0.0012 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 371870 loss: 0.0024 lr: 0.02\n",
      "iteration: 371880 loss: 0.0014 lr: 0.02\n",
      "iteration: 371890 loss: 0.0013 lr: 0.02\n",
      "iteration: 371900 loss: 0.0019 lr: 0.02\n",
      "iteration: 371910 loss: 0.0013 lr: 0.02\n",
      "iteration: 371920 loss: 0.0014 lr: 0.02\n",
      "iteration: 371930 loss: 0.0013 lr: 0.02\n",
      "iteration: 371940 loss: 0.0015 lr: 0.02\n",
      "iteration: 371950 loss: 0.0013 lr: 0.02\n",
      "iteration: 371960 loss: 0.0019 lr: 0.02\n",
      "iteration: 371970 loss: 0.0016 lr: 0.02\n",
      "iteration: 371980 loss: 0.0016 lr: 0.02\n",
      "iteration: 371990 loss: 0.0015 lr: 0.02\n",
      "iteration: 372000 loss: 0.0014 lr: 0.02\n",
      "iteration: 372010 loss: 0.0017 lr: 0.02\n",
      "iteration: 372020 loss: 0.0012 lr: 0.02\n",
      "iteration: 372030 loss: 0.0017 lr: 0.02\n",
      "iteration: 372040 loss: 0.0021 lr: 0.02\n",
      "iteration: 372050 loss: 0.0023 lr: 0.02\n",
      "iteration: 372060 loss: 0.0025 lr: 0.02\n",
      "iteration: 372070 loss: 0.0017 lr: 0.02\n",
      "iteration: 372080 loss: 0.0017 lr: 0.02\n",
      "iteration: 372090 loss: 0.0015 lr: 0.02\n",
      "iteration: 372100 loss: 0.0011 lr: 0.02\n",
      "iteration: 372110 loss: 0.0012 lr: 0.02\n",
      "iteration: 372120 loss: 0.0016 lr: 0.02\n",
      "iteration: 372130 loss: 0.0018 lr: 0.02\n",
      "iteration: 372140 loss: 0.0012 lr: 0.02\n",
      "iteration: 372150 loss: 0.0010 lr: 0.02\n",
      "iteration: 372160 loss: 0.0010 lr: 0.02\n",
      "iteration: 372170 loss: 0.0015 lr: 0.02\n",
      "iteration: 372180 loss: 0.0019 lr: 0.02\n",
      "iteration: 372190 loss: 0.0018 lr: 0.02\n",
      "iteration: 372200 loss: 0.0019 lr: 0.02\n",
      "iteration: 372210 loss: 0.0015 lr: 0.02\n",
      "iteration: 372220 loss: 0.0013 lr: 0.02\n",
      "iteration: 372230 loss: 0.0016 lr: 0.02\n",
      "iteration: 372240 loss: 0.0013 lr: 0.02\n",
      "iteration: 372250 loss: 0.0014 lr: 0.02\n",
      "iteration: 372260 loss: 0.0014 lr: 0.02\n",
      "iteration: 372270 loss: 0.0013 lr: 0.02\n",
      "iteration: 372280 loss: 0.0013 lr: 0.02\n",
      "iteration: 372290 loss: 0.0011 lr: 0.02\n",
      "iteration: 372300 loss: 0.0014 lr: 0.02\n",
      "iteration: 372310 loss: 0.0012 lr: 0.02\n",
      "iteration: 372320 loss: 0.0011 lr: 0.02\n",
      "iteration: 372330 loss: 0.0016 lr: 0.02\n",
      "iteration: 372340 loss: 0.0012 lr: 0.02\n",
      "iteration: 372350 loss: 0.0016 lr: 0.02\n",
      "iteration: 372360 loss: 0.0012 lr: 0.02\n",
      "iteration: 372370 loss: 0.0014 lr: 0.02\n",
      "iteration: 372380 loss: 0.0009 lr: 0.02\n",
      "iteration: 372390 loss: 0.0010 lr: 0.02\n",
      "iteration: 372400 loss: 0.0012 lr: 0.02\n",
      "iteration: 372410 loss: 0.0018 lr: 0.02\n",
      "iteration: 372420 loss: 0.0021 lr: 0.02\n",
      "iteration: 372430 loss: 0.0020 lr: 0.02\n",
      "iteration: 372440 loss: 0.0019 lr: 0.02\n",
      "iteration: 372450 loss: 0.0015 lr: 0.02\n",
      "iteration: 372460 loss: 0.0017 lr: 0.02\n",
      "iteration: 372470 loss: 0.0014 lr: 0.02\n",
      "iteration: 372480 loss: 0.0020 lr: 0.02\n",
      "iteration: 372490 loss: 0.0018 lr: 0.02\n",
      "iteration: 372500 loss: 0.0018 lr: 0.02\n",
      "iteration: 372510 loss: 0.0016 lr: 0.02\n",
      "iteration: 372520 loss: 0.0017 lr: 0.02\n",
      "iteration: 372530 loss: 0.0012 lr: 0.02\n",
      "iteration: 372540 loss: 0.0016 lr: 0.02\n",
      "iteration: 372550 loss: 0.0021 lr: 0.02\n",
      "iteration: 372560 loss: 0.0022 lr: 0.02\n",
      "iteration: 372570 loss: 0.0014 lr: 0.02\n",
      "iteration: 372580 loss: 0.0019 lr: 0.02\n",
      "iteration: 372590 loss: 0.0013 lr: 0.02\n",
      "iteration: 372600 loss: 0.0014 lr: 0.02\n",
      "iteration: 372610 loss: 0.0015 lr: 0.02\n",
      "iteration: 372620 loss: 0.0015 lr: 0.02\n",
      "iteration: 372630 loss: 0.0010 lr: 0.02\n",
      "iteration: 372640 loss: 0.0015 lr: 0.02\n",
      "iteration: 372650 loss: 0.0013 lr: 0.02\n",
      "iteration: 372660 loss: 0.0015 lr: 0.02\n",
      "iteration: 372670 loss: 0.0021 lr: 0.02\n",
      "iteration: 372680 loss: 0.0016 lr: 0.02\n",
      "iteration: 372690 loss: 0.0016 lr: 0.02\n",
      "iteration: 372700 loss: 0.0014 lr: 0.02\n",
      "iteration: 372710 loss: 0.0016 lr: 0.02\n",
      "iteration: 372720 loss: 0.0012 lr: 0.02\n",
      "iteration: 372730 loss: 0.0012 lr: 0.02\n",
      "iteration: 372740 loss: 0.0010 lr: 0.02\n",
      "iteration: 372750 loss: 0.0021 lr: 0.02\n",
      "iteration: 372760 loss: 0.0020 lr: 0.02\n",
      "iteration: 372770 loss: 0.0017 lr: 0.02\n",
      "iteration: 372780 loss: 0.0014 lr: 0.02\n",
      "iteration: 372790 loss: 0.0013 lr: 0.02\n",
      "iteration: 372800 loss: 0.0023 lr: 0.02\n",
      "iteration: 372810 loss: 0.0014 lr: 0.02\n",
      "iteration: 372820 loss: 0.0017 lr: 0.02\n",
      "iteration: 372830 loss: 0.0021 lr: 0.02\n",
      "iteration: 372840 loss: 0.0013 lr: 0.02\n",
      "iteration: 372850 loss: 0.0020 lr: 0.02\n",
      "iteration: 372860 loss: 0.0015 lr: 0.02\n",
      "iteration: 372870 loss: 0.0012 lr: 0.02\n",
      "iteration: 372880 loss: 0.0014 lr: 0.02\n",
      "iteration: 372890 loss: 0.0016 lr: 0.02\n",
      "iteration: 372900 loss: 0.0024 lr: 0.02\n",
      "iteration: 372910 loss: 0.0017 lr: 0.02\n",
      "iteration: 372920 loss: 0.0011 lr: 0.02\n",
      "iteration: 372930 loss: 0.0011 lr: 0.02\n",
      "iteration: 372940 loss: 0.0021 lr: 0.02\n",
      "iteration: 372950 loss: 0.0014 lr: 0.02\n",
      "iteration: 372960 loss: 0.0010 lr: 0.02\n",
      "iteration: 372970 loss: 0.0016 lr: 0.02\n",
      "iteration: 372980 loss: 0.0015 lr: 0.02\n",
      "iteration: 372990 loss: 0.0011 lr: 0.02\n",
      "iteration: 373000 loss: 0.0015 lr: 0.02\n",
      "iteration: 373010 loss: 0.0018 lr: 0.02\n",
      "iteration: 373020 loss: 0.0014 lr: 0.02\n",
      "iteration: 373030 loss: 0.0011 lr: 0.02\n",
      "iteration: 373040 loss: 0.0013 lr: 0.02\n",
      "iteration: 373050 loss: 0.0014 lr: 0.02\n",
      "iteration: 373060 loss: 0.0021 lr: 0.02\n",
      "iteration: 373070 loss: 0.0012 lr: 0.02\n",
      "iteration: 373080 loss: 0.0013 lr: 0.02\n",
      "iteration: 373090 loss: 0.0017 lr: 0.02\n",
      "iteration: 373100 loss: 0.0010 lr: 0.02\n",
      "iteration: 373110 loss: 0.0020 lr: 0.02\n",
      "iteration: 373120 loss: 0.0023 lr: 0.02\n",
      "iteration: 373130 loss: 0.0022 lr: 0.02\n",
      "iteration: 373140 loss: 0.0013 lr: 0.02\n",
      "iteration: 373150 loss: 0.0022 lr: 0.02\n",
      "iteration: 373160 loss: 0.0012 lr: 0.02\n",
      "iteration: 373170 loss: 0.0014 lr: 0.02\n",
      "iteration: 373180 loss: 0.0019 lr: 0.02\n",
      "iteration: 373190 loss: 0.0022 lr: 0.02\n",
      "iteration: 373200 loss: 0.0017 lr: 0.02\n",
      "iteration: 373210 loss: 0.0010 lr: 0.02\n",
      "iteration: 373220 loss: 0.0018 lr: 0.02\n",
      "iteration: 373230 loss: 0.0012 lr: 0.02\n",
      "iteration: 373240 loss: 0.0020 lr: 0.02\n",
      "iteration: 373250 loss: 0.0010 lr: 0.02\n",
      "iteration: 373260 loss: 0.0017 lr: 0.02\n",
      "iteration: 373270 loss: 0.0016 lr: 0.02\n",
      "iteration: 373280 loss: 0.0019 lr: 0.02\n",
      "iteration: 373290 loss: 0.0020 lr: 0.02\n",
      "iteration: 373300 loss: 0.0021 lr: 0.02\n",
      "iteration: 373310 loss: 0.0017 lr: 0.02\n",
      "iteration: 373320 loss: 0.0018 lr: 0.02\n",
      "iteration: 373330 loss: 0.0016 lr: 0.02\n",
      "iteration: 373340 loss: 0.0022 lr: 0.02\n",
      "iteration: 373350 loss: 0.0042 lr: 0.02\n",
      "iteration: 373360 loss: 0.0014 lr: 0.02\n",
      "iteration: 373370 loss: 0.0017 lr: 0.02\n",
      "iteration: 373380 loss: 0.0019 lr: 0.02\n",
      "iteration: 373390 loss: 0.0016 lr: 0.02\n",
      "iteration: 373400 loss: 0.0047 lr: 0.02\n",
      "iteration: 373410 loss: 0.0017 lr: 0.02\n",
      "iteration: 373420 loss: 0.0017 lr: 0.02\n",
      "iteration: 373430 loss: 0.0014 lr: 0.02\n",
      "iteration: 373440 loss: 0.0016 lr: 0.02\n",
      "iteration: 373450 loss: 0.0017 lr: 0.02\n",
      "iteration: 373460 loss: 0.0015 lr: 0.02\n",
      "iteration: 373470 loss: 0.0013 lr: 0.02\n",
      "iteration: 373480 loss: 0.0016 lr: 0.02\n",
      "iteration: 373490 loss: 0.0044 lr: 0.02\n",
      "iteration: 373500 loss: 0.0017 lr: 0.02\n",
      "iteration: 373510 loss: 0.0014 lr: 0.02\n",
      "iteration: 373520 loss: 0.0011 lr: 0.02\n",
      "iteration: 373530 loss: 0.0024 lr: 0.02\n",
      "iteration: 373540 loss: 0.0013 lr: 0.02\n",
      "iteration: 373550 loss: 0.0021 lr: 0.02\n",
      "iteration: 373560 loss: 0.0018 lr: 0.02\n",
      "iteration: 373570 loss: 0.0014 lr: 0.02\n",
      "iteration: 373580 loss: 0.0012 lr: 0.02\n",
      "iteration: 373590 loss: 0.0017 lr: 0.02\n",
      "iteration: 373600 loss: 0.0016 lr: 0.02\n",
      "iteration: 373610 loss: 0.0019 lr: 0.02\n",
      "iteration: 373620 loss: 0.0017 lr: 0.02\n",
      "iteration: 373630 loss: 0.0022 lr: 0.02\n",
      "iteration: 373640 loss: 0.0014 lr: 0.02\n",
      "iteration: 373650 loss: 0.0018 lr: 0.02\n",
      "iteration: 373660 loss: 0.0015 lr: 0.02\n",
      "iteration: 373670 loss: 0.0015 lr: 0.02\n",
      "iteration: 373680 loss: 0.0018 lr: 0.02\n",
      "iteration: 373690 loss: 0.0025 lr: 0.02\n",
      "iteration: 373700 loss: 0.0014 lr: 0.02\n",
      "iteration: 373710 loss: 0.0017 lr: 0.02\n",
      "iteration: 373720 loss: 0.0020 lr: 0.02\n",
      "iteration: 373730 loss: 0.0015 lr: 0.02\n",
      "iteration: 373740 loss: 0.0019 lr: 0.02\n",
      "iteration: 373750 loss: 0.0018 lr: 0.02\n",
      "iteration: 373760 loss: 0.0012 lr: 0.02\n",
      "iteration: 373770 loss: 0.0011 lr: 0.02\n",
      "iteration: 373780 loss: 0.0012 lr: 0.02\n",
      "iteration: 373790 loss: 0.0016 lr: 0.02\n",
      "iteration: 373800 loss: 0.0014 lr: 0.02\n",
      "iteration: 373810 loss: 0.0013 lr: 0.02\n",
      "iteration: 373820 loss: 0.0016 lr: 0.02\n",
      "iteration: 373830 loss: 0.0012 lr: 0.02\n",
      "iteration: 373840 loss: 0.0024 lr: 0.02\n",
      "iteration: 373850 loss: 0.0013 lr: 0.02\n",
      "iteration: 373860 loss: 0.0027 lr: 0.02\n",
      "iteration: 373870 loss: 0.0018 lr: 0.02\n",
      "iteration: 373880 loss: 0.0016 lr: 0.02\n",
      "iteration: 373890 loss: 0.0015 lr: 0.02\n",
      "iteration: 373900 loss: 0.0015 lr: 0.02\n",
      "iteration: 373910 loss: 0.0012 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 373920 loss: 0.0017 lr: 0.02\n",
      "iteration: 373930 loss: 0.0013 lr: 0.02\n",
      "iteration: 373940 loss: 0.0028 lr: 0.02\n",
      "iteration: 373950 loss: 0.0017 lr: 0.02\n",
      "iteration: 373960 loss: 0.0018 lr: 0.02\n",
      "iteration: 373970 loss: 0.0024 lr: 0.02\n",
      "iteration: 373980 loss: 0.0016 lr: 0.02\n",
      "iteration: 373990 loss: 0.0024 lr: 0.02\n",
      "iteration: 374000 loss: 0.0018 lr: 0.02\n",
      "iteration: 374010 loss: 0.0013 lr: 0.02\n",
      "iteration: 374020 loss: 0.0016 lr: 0.02\n",
      "iteration: 374030 loss: 0.0012 lr: 0.02\n",
      "iteration: 374040 loss: 0.0010 lr: 0.02\n",
      "iteration: 374050 loss: 0.0015 lr: 0.02\n",
      "iteration: 374060 loss: 0.0015 lr: 0.02\n",
      "iteration: 374070 loss: 0.0031 lr: 0.02\n",
      "iteration: 374080 loss: 0.0016 lr: 0.02\n",
      "iteration: 374090 loss: 0.0016 lr: 0.02\n",
      "iteration: 374100 loss: 0.0014 lr: 0.02\n",
      "iteration: 374110 loss: 0.0015 lr: 0.02\n",
      "iteration: 374120 loss: 0.0016 lr: 0.02\n",
      "iteration: 374130 loss: 0.0017 lr: 0.02\n",
      "iteration: 374140 loss: 0.0022 lr: 0.02\n",
      "iteration: 374150 loss: 0.0014 lr: 0.02\n",
      "iteration: 374160 loss: 0.0013 lr: 0.02\n",
      "iteration: 374170 loss: 0.0016 lr: 0.02\n",
      "iteration: 374180 loss: 0.0019 lr: 0.02\n",
      "iteration: 374190 loss: 0.0011 lr: 0.02\n",
      "iteration: 374200 loss: 0.0024 lr: 0.02\n",
      "iteration: 374210 loss: 0.0022 lr: 0.02\n",
      "iteration: 374220 loss: 0.0013 lr: 0.02\n",
      "iteration: 374230 loss: 0.0022 lr: 0.02\n",
      "iteration: 374240 loss: 0.0018 lr: 0.02\n",
      "iteration: 374250 loss: 0.0015 lr: 0.02\n",
      "iteration: 374260 loss: 0.0024 lr: 0.02\n",
      "iteration: 374270 loss: 0.0021 lr: 0.02\n",
      "iteration: 374280 loss: 0.0021 lr: 0.02\n",
      "iteration: 374290 loss: 0.0014 lr: 0.02\n",
      "iteration: 374300 loss: 0.0013 lr: 0.02\n",
      "iteration: 374310 loss: 0.0020 lr: 0.02\n",
      "iteration: 374320 loss: 0.0013 lr: 0.02\n",
      "iteration: 374330 loss: 0.0017 lr: 0.02\n",
      "iteration: 374340 loss: 0.0018 lr: 0.02\n",
      "iteration: 374350 loss: 0.0035 lr: 0.02\n",
      "iteration: 374360 loss: 0.0025 lr: 0.02\n",
      "iteration: 374370 loss: 0.0013 lr: 0.02\n",
      "iteration: 374380 loss: 0.0011 lr: 0.02\n",
      "iteration: 374390 loss: 0.0017 lr: 0.02\n",
      "iteration: 374400 loss: 0.0011 lr: 0.02\n",
      "iteration: 374410 loss: 0.0011 lr: 0.02\n",
      "iteration: 374420 loss: 0.0016 lr: 0.02\n",
      "iteration: 374430 loss: 0.0013 lr: 0.02\n",
      "iteration: 374440 loss: 0.0014 lr: 0.02\n",
      "iteration: 374450 loss: 0.0014 lr: 0.02\n",
      "iteration: 374460 loss: 0.0019 lr: 0.02\n",
      "iteration: 374470 loss: 0.0012 lr: 0.02\n",
      "iteration: 374480 loss: 0.0012 lr: 0.02\n",
      "iteration: 374490 loss: 0.0018 lr: 0.02\n",
      "iteration: 374500 loss: 0.0023 lr: 0.02\n",
      "iteration: 374510 loss: 0.0017 lr: 0.02\n",
      "iteration: 374520 loss: 0.0016 lr: 0.02\n",
      "iteration: 374530 loss: 0.0011 lr: 0.02\n",
      "iteration: 374540 loss: 0.0019 lr: 0.02\n",
      "iteration: 374550 loss: 0.0015 lr: 0.02\n",
      "iteration: 374560 loss: 0.0015 lr: 0.02\n",
      "iteration: 374570 loss: 0.0014 lr: 0.02\n",
      "iteration: 374580 loss: 0.0018 lr: 0.02\n",
      "iteration: 374590 loss: 0.0011 lr: 0.02\n",
      "iteration: 374600 loss: 0.0014 lr: 0.02\n",
      "iteration: 374610 loss: 0.0012 lr: 0.02\n",
      "iteration: 374620 loss: 0.0014 lr: 0.02\n",
      "iteration: 374630 loss: 0.0029 lr: 0.02\n",
      "iteration: 374640 loss: 0.0021 lr: 0.02\n",
      "iteration: 374650 loss: 0.0017 lr: 0.02\n",
      "iteration: 374660 loss: 0.0018 lr: 0.02\n",
      "iteration: 374670 loss: 0.0013 lr: 0.02\n",
      "iteration: 374680 loss: 0.0017 lr: 0.02\n",
      "iteration: 374690 loss: 0.0021 lr: 0.02\n",
      "iteration: 374700 loss: 0.0015 lr: 0.02\n",
      "iteration: 374710 loss: 0.0032 lr: 0.02\n",
      "iteration: 374720 loss: 0.0017 lr: 0.02\n",
      "iteration: 374730 loss: 0.0014 lr: 0.02\n",
      "iteration: 374740 loss: 0.0016 lr: 0.02\n",
      "iteration: 374750 loss: 0.0013 lr: 0.02\n",
      "iteration: 374760 loss: 0.0016 lr: 0.02\n",
      "iteration: 374770 loss: 0.0012 lr: 0.02\n",
      "iteration: 374780 loss: 0.0022 lr: 0.02\n",
      "iteration: 374790 loss: 0.0026 lr: 0.02\n",
      "iteration: 374800 loss: 0.0018 lr: 0.02\n",
      "iteration: 374810 loss: 0.0020 lr: 0.02\n",
      "iteration: 374820 loss: 0.0018 lr: 0.02\n",
      "iteration: 374830 loss: 0.0013 lr: 0.02\n",
      "iteration: 374840 loss: 0.0017 lr: 0.02\n",
      "iteration: 374850 loss: 0.0019 lr: 0.02\n",
      "iteration: 374860 loss: 0.0017 lr: 0.02\n",
      "iteration: 374870 loss: 0.0015 lr: 0.02\n",
      "iteration: 374880 loss: 0.0018 lr: 0.02\n",
      "iteration: 374890 loss: 0.0016 lr: 0.02\n",
      "iteration: 374900 loss: 0.0015 lr: 0.02\n",
      "iteration: 374910 loss: 0.0021 lr: 0.02\n",
      "iteration: 374920 loss: 0.0015 lr: 0.02\n",
      "iteration: 374930 loss: 0.0016 lr: 0.02\n",
      "iteration: 374940 loss: 0.0010 lr: 0.02\n",
      "iteration: 374950 loss: 0.0014 lr: 0.02\n",
      "iteration: 374960 loss: 0.0013 lr: 0.02\n",
      "iteration: 374970 loss: 0.0012 lr: 0.02\n",
      "iteration: 374980 loss: 0.0018 lr: 0.02\n",
      "iteration: 374990 loss: 0.0020 lr: 0.02\n",
      "iteration: 375000 loss: 0.0012 lr: 0.02\n",
      "iteration: 375010 loss: 0.0016 lr: 0.02\n",
      "iteration: 375020 loss: 0.0025 lr: 0.02\n",
      "iteration: 375030 loss: 0.0013 lr: 0.02\n",
      "iteration: 375040 loss: 0.0029 lr: 0.02\n",
      "iteration: 375050 loss: 0.0019 lr: 0.02\n",
      "iteration: 375060 loss: 0.0017 lr: 0.02\n",
      "iteration: 375070 loss: 0.0013 lr: 0.02\n",
      "iteration: 375080 loss: 0.0010 lr: 0.02\n",
      "iteration: 375090 loss: 0.0013 lr: 0.02\n",
      "iteration: 375100 loss: 0.0020 lr: 0.02\n",
      "iteration: 375110 loss: 0.0023 lr: 0.02\n",
      "iteration: 375120 loss: 0.0012 lr: 0.02\n",
      "iteration: 375130 loss: 0.0028 lr: 0.02\n",
      "iteration: 375140 loss: 0.0020 lr: 0.02\n",
      "iteration: 375150 loss: 0.0016 lr: 0.02\n",
      "iteration: 375160 loss: 0.0014 lr: 0.02\n",
      "iteration: 375170 loss: 0.0023 lr: 0.02\n",
      "iteration: 375180 loss: 0.0010 lr: 0.02\n",
      "iteration: 375190 loss: 0.0019 lr: 0.02\n",
      "iteration: 375200 loss: 0.0025 lr: 0.02\n",
      "iteration: 375210 loss: 0.0027 lr: 0.02\n",
      "iteration: 375220 loss: 0.0032 lr: 0.02\n",
      "iteration: 375230 loss: 0.0027 lr: 0.02\n",
      "iteration: 375240 loss: 0.0023 lr: 0.02\n",
      "iteration: 375250 loss: 0.0027 lr: 0.02\n",
      "iteration: 375260 loss: 0.0020 lr: 0.02\n",
      "iteration: 375270 loss: 0.0015 lr: 0.02\n",
      "iteration: 375280 loss: 0.0018 lr: 0.02\n",
      "iteration: 375290 loss: 0.0021 lr: 0.02\n",
      "iteration: 375300 loss: 0.0013 lr: 0.02\n",
      "iteration: 375310 loss: 0.0018 lr: 0.02\n",
      "iteration: 375320 loss: 0.0016 lr: 0.02\n",
      "iteration: 375330 loss: 0.0014 lr: 0.02\n",
      "iteration: 375340 loss: 0.0016 lr: 0.02\n",
      "iteration: 375350 loss: 0.0020 lr: 0.02\n",
      "iteration: 375360 loss: 0.0017 lr: 0.02\n",
      "iteration: 375370 loss: 0.0012 lr: 0.02\n",
      "iteration: 375380 loss: 0.0016 lr: 0.02\n",
      "iteration: 375390 loss: 0.0015 lr: 0.02\n",
      "iteration: 375400 loss: 0.0014 lr: 0.02\n",
      "iteration: 375410 loss: 0.0011 lr: 0.02\n",
      "iteration: 375420 loss: 0.0015 lr: 0.02\n",
      "iteration: 375430 loss: 0.0020 lr: 0.02\n",
      "iteration: 375440 loss: 0.0013 lr: 0.02\n",
      "iteration: 375450 loss: 0.0014 lr: 0.02\n",
      "iteration: 375460 loss: 0.0017 lr: 0.02\n",
      "iteration: 375470 loss: 0.0020 lr: 0.02\n",
      "iteration: 375480 loss: 0.0018 lr: 0.02\n",
      "iteration: 375490 loss: 0.0011 lr: 0.02\n",
      "iteration: 375500 loss: 0.0016 lr: 0.02\n",
      "iteration: 375510 loss: 0.0014 lr: 0.02\n",
      "iteration: 375520 loss: 0.0014 lr: 0.02\n",
      "iteration: 375530 loss: 0.0018 lr: 0.02\n",
      "iteration: 375540 loss: 0.0021 lr: 0.02\n",
      "iteration: 375550 loss: 0.0016 lr: 0.02\n",
      "iteration: 375560 loss: 0.0019 lr: 0.02\n",
      "iteration: 375570 loss: 0.0022 lr: 0.02\n",
      "iteration: 375580 loss: 0.0012 lr: 0.02\n",
      "iteration: 375590 loss: 0.0016 lr: 0.02\n",
      "iteration: 375600 loss: 0.0016 lr: 0.02\n",
      "iteration: 375610 loss: 0.0012 lr: 0.02\n",
      "iteration: 375620 loss: 0.0013 lr: 0.02\n",
      "iteration: 375630 loss: 0.0016 lr: 0.02\n",
      "iteration: 375640 loss: 0.0013 lr: 0.02\n",
      "iteration: 375650 loss: 0.0015 lr: 0.02\n",
      "iteration: 375660 loss: 0.0016 lr: 0.02\n",
      "iteration: 375670 loss: 0.0014 lr: 0.02\n",
      "iteration: 375680 loss: 0.0018 lr: 0.02\n",
      "iteration: 375690 loss: 0.0013 lr: 0.02\n",
      "iteration: 375700 loss: 0.0012 lr: 0.02\n",
      "iteration: 375710 loss: 0.0022 lr: 0.02\n",
      "iteration: 375720 loss: 0.0019 lr: 0.02\n",
      "iteration: 375730 loss: 0.0022 lr: 0.02\n",
      "iteration: 375740 loss: 0.0019 lr: 0.02\n",
      "iteration: 375750 loss: 0.0016 lr: 0.02\n",
      "iteration: 375760 loss: 0.0018 lr: 0.02\n",
      "iteration: 375770 loss: 0.0014 lr: 0.02\n",
      "iteration: 375780 loss: 0.0018 lr: 0.02\n",
      "iteration: 375790 loss: 0.0016 lr: 0.02\n",
      "iteration: 375800 loss: 0.0022 lr: 0.02\n",
      "iteration: 375810 loss: 0.0016 lr: 0.02\n",
      "iteration: 375820 loss: 0.0016 lr: 0.02\n",
      "iteration: 375830 loss: 0.0017 lr: 0.02\n",
      "iteration: 375840 loss: 0.0016 lr: 0.02\n",
      "iteration: 375850 loss: 0.0016 lr: 0.02\n",
      "iteration: 375860 loss: 0.0015 lr: 0.02\n",
      "iteration: 375870 loss: 0.0029 lr: 0.02\n",
      "iteration: 375880 loss: 0.0013 lr: 0.02\n",
      "iteration: 375890 loss: 0.0010 lr: 0.02\n",
      "iteration: 375900 loss: 0.0027 lr: 0.02\n",
      "iteration: 375910 loss: 0.0015 lr: 0.02\n",
      "iteration: 375920 loss: 0.0013 lr: 0.02\n",
      "iteration: 375930 loss: 0.0017 lr: 0.02\n",
      "iteration: 375940 loss: 0.0011 lr: 0.02\n",
      "iteration: 375950 loss: 0.0017 lr: 0.02\n",
      "iteration: 375960 loss: 0.0012 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 375970 loss: 0.0020 lr: 0.02\n",
      "iteration: 375980 loss: 0.0030 lr: 0.02\n",
      "iteration: 375990 loss: 0.0015 lr: 0.02\n",
      "iteration: 376000 loss: 0.0013 lr: 0.02\n",
      "iteration: 376010 loss: 0.0021 lr: 0.02\n",
      "iteration: 376020 loss: 0.0018 lr: 0.02\n",
      "iteration: 376030 loss: 0.0027 lr: 0.02\n",
      "iteration: 376040 loss: 0.0031 lr: 0.02\n",
      "iteration: 376050 loss: 0.0024 lr: 0.02\n",
      "iteration: 376060 loss: 0.0016 lr: 0.02\n",
      "iteration: 376070 loss: 0.0014 lr: 0.02\n",
      "iteration: 376080 loss: 0.0020 lr: 0.02\n",
      "iteration: 376090 loss: 0.0018 lr: 0.02\n",
      "iteration: 376100 loss: 0.0016 lr: 0.02\n",
      "iteration: 376110 loss: 0.0020 lr: 0.02\n",
      "iteration: 376120 loss: 0.0012 lr: 0.02\n",
      "iteration: 376130 loss: 0.0009 lr: 0.02\n",
      "iteration: 376140 loss: 0.0019 lr: 0.02\n",
      "iteration: 376150 loss: 0.0012 lr: 0.02\n",
      "iteration: 376160 loss: 0.0021 lr: 0.02\n",
      "iteration: 376170 loss: 0.0014 lr: 0.02\n",
      "iteration: 376180 loss: 0.0017 lr: 0.02\n",
      "iteration: 376190 loss: 0.0020 lr: 0.02\n",
      "iteration: 376200 loss: 0.0017 lr: 0.02\n",
      "iteration: 376210 loss: 0.0013 lr: 0.02\n",
      "iteration: 376220 loss: 0.0013 lr: 0.02\n",
      "iteration: 376230 loss: 0.0014 lr: 0.02\n",
      "iteration: 376240 loss: 0.0015 lr: 0.02\n",
      "iteration: 376250 loss: 0.0014 lr: 0.02\n",
      "iteration: 376260 loss: 0.0019 lr: 0.02\n",
      "iteration: 376270 loss: 0.0022 lr: 0.02\n",
      "iteration: 376280 loss: 0.0016 lr: 0.02\n",
      "iteration: 376290 loss: 0.0033 lr: 0.02\n",
      "iteration: 376300 loss: 0.0021 lr: 0.02\n",
      "iteration: 376310 loss: 0.0017 lr: 0.02\n",
      "iteration: 376320 loss: 0.0018 lr: 0.02\n",
      "iteration: 376330 loss: 0.0012 lr: 0.02\n",
      "iteration: 376340 loss: 0.0017 lr: 0.02\n",
      "iteration: 376350 loss: 0.0014 lr: 0.02\n",
      "iteration: 376360 loss: 0.0014 lr: 0.02\n",
      "iteration: 376370 loss: 0.0013 lr: 0.02\n",
      "iteration: 376380 loss: 0.0017 lr: 0.02\n",
      "iteration: 376390 loss: 0.0014 lr: 0.02\n",
      "iteration: 376400 loss: 0.0016 lr: 0.02\n",
      "iteration: 376410 loss: 0.0015 lr: 0.02\n",
      "iteration: 376420 loss: 0.0021 lr: 0.02\n",
      "iteration: 376430 loss: 0.0012 lr: 0.02\n",
      "iteration: 376440 loss: 0.0021 lr: 0.02\n",
      "iteration: 376450 loss: 0.0014 lr: 0.02\n",
      "iteration: 376460 loss: 0.0019 lr: 0.02\n",
      "iteration: 376470 loss: 0.0016 lr: 0.02\n",
      "iteration: 376480 loss: 0.0012 lr: 0.02\n",
      "iteration: 376490 loss: 0.0015 lr: 0.02\n",
      "iteration: 376500 loss: 0.0014 lr: 0.02\n",
      "iteration: 376510 loss: 0.0016 lr: 0.02\n",
      "iteration: 376520 loss: 0.0021 lr: 0.02\n",
      "iteration: 376530 loss: 0.0014 lr: 0.02\n",
      "iteration: 376540 loss: 0.0011 lr: 0.02\n",
      "iteration: 376550 loss: 0.0013 lr: 0.02\n",
      "iteration: 376560 loss: 0.0040 lr: 0.02\n",
      "iteration: 376570 loss: 0.0010 lr: 0.02\n",
      "iteration: 376580 loss: 0.0018 lr: 0.02\n",
      "iteration: 376590 loss: 0.0015 lr: 0.02\n",
      "iteration: 376600 loss: 0.0012 lr: 0.02\n",
      "iteration: 376610 loss: 0.0017 lr: 0.02\n",
      "iteration: 376620 loss: 0.0019 lr: 0.02\n",
      "iteration: 376630 loss: 0.0011 lr: 0.02\n",
      "iteration: 376640 loss: 0.0012 lr: 0.02\n",
      "iteration: 376650 loss: 0.0015 lr: 0.02\n",
      "iteration: 376660 loss: 0.0012 lr: 0.02\n",
      "iteration: 376670 loss: 0.0022 lr: 0.02\n",
      "iteration: 376680 loss: 0.0016 lr: 0.02\n",
      "iteration: 376690 loss: 0.0014 lr: 0.02\n",
      "iteration: 376700 loss: 0.0015 lr: 0.02\n",
      "iteration: 376710 loss: 0.0020 lr: 0.02\n",
      "iteration: 376720 loss: 0.0012 lr: 0.02\n",
      "iteration: 376730 loss: 0.0021 lr: 0.02\n",
      "iteration: 376740 loss: 0.0011 lr: 0.02\n",
      "iteration: 376750 loss: 0.0014 lr: 0.02\n",
      "iteration: 376760 loss: 0.0013 lr: 0.02\n",
      "iteration: 376770 loss: 0.0014 lr: 0.02\n",
      "iteration: 376780 loss: 0.0017 lr: 0.02\n",
      "iteration: 376790 loss: 0.0013 lr: 0.02\n",
      "iteration: 376800 loss: 0.0011 lr: 0.02\n",
      "iteration: 376810 loss: 0.0013 lr: 0.02\n",
      "iteration: 376820 loss: 0.0021 lr: 0.02\n",
      "iteration: 376830 loss: 0.0015 lr: 0.02\n",
      "iteration: 376840 loss: 0.0014 lr: 0.02\n",
      "iteration: 376850 loss: 0.0013 lr: 0.02\n",
      "iteration: 376860 loss: 0.0016 lr: 0.02\n",
      "iteration: 376870 loss: 0.0025 lr: 0.02\n",
      "iteration: 376880 loss: 0.0018 lr: 0.02\n",
      "iteration: 376890 loss: 0.0017 lr: 0.02\n",
      "iteration: 376900 loss: 0.0019 lr: 0.02\n",
      "iteration: 376910 loss: 0.0017 lr: 0.02\n",
      "iteration: 376920 loss: 0.0019 lr: 0.02\n",
      "iteration: 376930 loss: 0.0015 lr: 0.02\n",
      "iteration: 376940 loss: 0.0017 lr: 0.02\n",
      "iteration: 376950 loss: 0.0014 lr: 0.02\n",
      "iteration: 376960 loss: 0.0019 lr: 0.02\n",
      "iteration: 376970 loss: 0.0032 lr: 0.02\n",
      "iteration: 376980 loss: 0.0016 lr: 0.02\n",
      "iteration: 376990 loss: 0.0015 lr: 0.02\n",
      "iteration: 377000 loss: 0.0014 lr: 0.02\n",
      "iteration: 377010 loss: 0.0017 lr: 0.02\n",
      "iteration: 377020 loss: 0.0017 lr: 0.02\n",
      "iteration: 377030 loss: 0.0019 lr: 0.02\n",
      "iteration: 377040 loss: 0.0013 lr: 0.02\n",
      "iteration: 377050 loss: 0.0019 lr: 0.02\n",
      "iteration: 377060 loss: 0.0016 lr: 0.02\n",
      "iteration: 377070 loss: 0.0012 lr: 0.02\n",
      "iteration: 377080 loss: 0.0015 lr: 0.02\n",
      "iteration: 377090 loss: 0.0020 lr: 0.02\n",
      "iteration: 377100 loss: 0.0020 lr: 0.02\n",
      "iteration: 377110 loss: 0.0016 lr: 0.02\n",
      "iteration: 377120 loss: 0.0013 lr: 0.02\n",
      "iteration: 377130 loss: 0.0015 lr: 0.02\n",
      "iteration: 377140 loss: 0.0019 lr: 0.02\n",
      "iteration: 377150 loss: 0.0016 lr: 0.02\n",
      "iteration: 377160 loss: 0.0016 lr: 0.02\n",
      "iteration: 377170 loss: 0.0012 lr: 0.02\n",
      "iteration: 377180 loss: 0.0019 lr: 0.02\n",
      "iteration: 377190 loss: 0.0020 lr: 0.02\n",
      "iteration: 377200 loss: 0.0016 lr: 0.02\n",
      "iteration: 377210 loss: 0.0015 lr: 0.02\n",
      "iteration: 377220 loss: 0.0009 lr: 0.02\n",
      "iteration: 377230 loss: 0.0018 lr: 0.02\n",
      "iteration: 377240 loss: 0.0021 lr: 0.02\n",
      "iteration: 377250 loss: 0.0021 lr: 0.02\n",
      "iteration: 377260 loss: 0.0011 lr: 0.02\n",
      "iteration: 377270 loss: 0.0009 lr: 0.02\n",
      "iteration: 377280 loss: 0.0015 lr: 0.02\n",
      "iteration: 377290 loss: 0.0012 lr: 0.02\n",
      "iteration: 377300 loss: 0.0019 lr: 0.02\n",
      "iteration: 377310 loss: 0.0013 lr: 0.02\n",
      "iteration: 377320 loss: 0.0018 lr: 0.02\n",
      "iteration: 377330 loss: 0.0019 lr: 0.02\n",
      "iteration: 377340 loss: 0.0022 lr: 0.02\n",
      "iteration: 377350 loss: 0.0015 lr: 0.02\n",
      "iteration: 377360 loss: 0.0014 lr: 0.02\n",
      "iteration: 377370 loss: 0.0013 lr: 0.02\n",
      "iteration: 377380 loss: 0.0020 lr: 0.02\n",
      "iteration: 377390 loss: 0.0015 lr: 0.02\n",
      "iteration: 377400 loss: 0.0019 lr: 0.02\n",
      "iteration: 377410 loss: 0.0025 lr: 0.02\n",
      "iteration: 377420 loss: 0.0014 lr: 0.02\n",
      "iteration: 377430 loss: 0.0018 lr: 0.02\n",
      "iteration: 377440 loss: 0.0015 lr: 0.02\n",
      "iteration: 377450 loss: 0.0028 lr: 0.02\n",
      "iteration: 377460 loss: 0.0016 lr: 0.02\n",
      "iteration: 377470 loss: 0.0020 lr: 0.02\n",
      "iteration: 377480 loss: 0.0015 lr: 0.02\n",
      "iteration: 377490 loss: 0.0020 lr: 0.02\n",
      "iteration: 377500 loss: 0.0013 lr: 0.02\n",
      "iteration: 377510 loss: 0.0015 lr: 0.02\n",
      "iteration: 377520 loss: 0.0014 lr: 0.02\n",
      "iteration: 377530 loss: 0.0014 lr: 0.02\n",
      "iteration: 377540 loss: 0.0016 lr: 0.02\n",
      "iteration: 377550 loss: 0.0016 lr: 0.02\n",
      "iteration: 377560 loss: 0.0018 lr: 0.02\n",
      "iteration: 377570 loss: 0.0018 lr: 0.02\n",
      "iteration: 377580 loss: 0.0019 lr: 0.02\n",
      "iteration: 377590 loss: 0.0015 lr: 0.02\n",
      "iteration: 377600 loss: 0.0021 lr: 0.02\n",
      "iteration: 377610 loss: 0.0013 lr: 0.02\n",
      "iteration: 377620 loss: 0.0014 lr: 0.02\n",
      "iteration: 377630 loss: 0.0015 lr: 0.02\n",
      "iteration: 377640 loss: 0.0015 lr: 0.02\n",
      "iteration: 377650 loss: 0.0013 lr: 0.02\n",
      "iteration: 377660 loss: 0.0023 lr: 0.02\n",
      "iteration: 377670 loss: 0.0022 lr: 0.02\n",
      "iteration: 377680 loss: 0.0013 lr: 0.02\n",
      "iteration: 377690 loss: 0.0013 lr: 0.02\n",
      "iteration: 377700 loss: 0.0019 lr: 0.02\n",
      "iteration: 377710 loss: 0.0017 lr: 0.02\n",
      "iteration: 377720 loss: 0.0021 lr: 0.02\n",
      "iteration: 377730 loss: 0.0015 lr: 0.02\n",
      "iteration: 377740 loss: 0.0018 lr: 0.02\n",
      "iteration: 377750 loss: 0.0014 lr: 0.02\n",
      "iteration: 377760 loss: 0.0016 lr: 0.02\n",
      "iteration: 377770 loss: 0.0014 lr: 0.02\n",
      "iteration: 377780 loss: 0.0013 lr: 0.02\n",
      "iteration: 377790 loss: 0.0019 lr: 0.02\n",
      "iteration: 377800 loss: 0.0018 lr: 0.02\n",
      "iteration: 377810 loss: 0.0013 lr: 0.02\n",
      "iteration: 377820 loss: 0.0015 lr: 0.02\n",
      "iteration: 377830 loss: 0.0016 lr: 0.02\n",
      "iteration: 377840 loss: 0.0012 lr: 0.02\n",
      "iteration: 377850 loss: 0.0020 lr: 0.02\n",
      "iteration: 377860 loss: 0.0019 lr: 0.02\n",
      "iteration: 377870 loss: 0.0024 lr: 0.02\n",
      "iteration: 377880 loss: 0.0014 lr: 0.02\n",
      "iteration: 377890 loss: 0.0020 lr: 0.02\n",
      "iteration: 377900 loss: 0.0015 lr: 0.02\n",
      "iteration: 377910 loss: 0.0014 lr: 0.02\n",
      "iteration: 377920 loss: 0.0013 lr: 0.02\n",
      "iteration: 377930 loss: 0.0013 lr: 0.02\n",
      "iteration: 377940 loss: 0.0013 lr: 0.02\n",
      "iteration: 377950 loss: 0.0013 lr: 0.02\n",
      "iteration: 377960 loss: 0.0018 lr: 0.02\n",
      "iteration: 377970 loss: 0.0010 lr: 0.02\n",
      "iteration: 377980 loss: 0.0013 lr: 0.02\n",
      "iteration: 377990 loss: 0.0020 lr: 0.02\n",
      "iteration: 378000 loss: 0.0014 lr: 0.02\n",
      "iteration: 378010 loss: 0.0012 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 378020 loss: 0.0017 lr: 0.02\n",
      "iteration: 378030 loss: 0.0022 lr: 0.02\n",
      "iteration: 378040 loss: 0.0020 lr: 0.02\n",
      "iteration: 378050 loss: 0.0023 lr: 0.02\n",
      "iteration: 378060 loss: 0.0017 lr: 0.02\n",
      "iteration: 378070 loss: 0.0017 lr: 0.02\n",
      "iteration: 378080 loss: 0.0013 lr: 0.02\n",
      "iteration: 378090 loss: 0.0016 lr: 0.02\n",
      "iteration: 378100 loss: 0.0013 lr: 0.02\n",
      "iteration: 378110 loss: 0.0012 lr: 0.02\n",
      "iteration: 378120 loss: 0.0012 lr: 0.02\n",
      "iteration: 378130 loss: 0.0020 lr: 0.02\n",
      "iteration: 378140 loss: 0.0022 lr: 0.02\n",
      "iteration: 378150 loss: 0.0014 lr: 0.02\n",
      "iteration: 378160 loss: 0.0011 lr: 0.02\n",
      "iteration: 378170 loss: 0.0015 lr: 0.02\n",
      "iteration: 378180 loss: 0.0020 lr: 0.02\n",
      "iteration: 378190 loss: 0.0019 lr: 0.02\n",
      "iteration: 378200 loss: 0.0015 lr: 0.02\n",
      "iteration: 378210 loss: 0.0021 lr: 0.02\n",
      "iteration: 378220 loss: 0.0018 lr: 0.02\n",
      "iteration: 378230 loss: 0.0011 lr: 0.02\n",
      "iteration: 378240 loss: 0.0015 lr: 0.02\n",
      "iteration: 378250 loss: 0.0018 lr: 0.02\n",
      "iteration: 378260 loss: 0.0014 lr: 0.02\n",
      "iteration: 378270 loss: 0.0013 lr: 0.02\n",
      "iteration: 378280 loss: 0.0021 lr: 0.02\n",
      "iteration: 378290 loss: 0.0020 lr: 0.02\n",
      "iteration: 378300 loss: 0.0014 lr: 0.02\n",
      "iteration: 378310 loss: 0.0013 lr: 0.02\n",
      "iteration: 378320 loss: 0.0018 lr: 0.02\n",
      "iteration: 378330 loss: 0.0012 lr: 0.02\n",
      "iteration: 378340 loss: 0.0024 lr: 0.02\n",
      "iteration: 378350 loss: 0.0010 lr: 0.02\n",
      "iteration: 378360 loss: 0.0019 lr: 0.02\n",
      "iteration: 378370 loss: 0.0013 lr: 0.02\n",
      "iteration: 378380 loss: 0.0011 lr: 0.02\n",
      "iteration: 378390 loss: 0.0020 lr: 0.02\n",
      "iteration: 378400 loss: 0.0013 lr: 0.02\n",
      "iteration: 378410 loss: 0.0024 lr: 0.02\n",
      "iteration: 378420 loss: 0.0014 lr: 0.02\n",
      "iteration: 378430 loss: 0.0022 lr: 0.02\n",
      "iteration: 378440 loss: 0.0013 lr: 0.02\n",
      "iteration: 378450 loss: 0.0019 lr: 0.02\n",
      "iteration: 378460 loss: 0.0012 lr: 0.02\n",
      "iteration: 378470 loss: 0.0014 lr: 0.02\n",
      "iteration: 378480 loss: 0.0014 lr: 0.02\n",
      "iteration: 378490 loss: 0.0015 lr: 0.02\n",
      "iteration: 378500 loss: 0.0020 lr: 0.02\n",
      "iteration: 378510 loss: 0.0017 lr: 0.02\n",
      "iteration: 378520 loss: 0.0021 lr: 0.02\n",
      "iteration: 378530 loss: 0.0018 lr: 0.02\n",
      "iteration: 378540 loss: 0.0017 lr: 0.02\n",
      "iteration: 378550 loss: 0.0011 lr: 0.02\n",
      "iteration: 378560 loss: 0.0022 lr: 0.02\n",
      "iteration: 378570 loss: 0.0025 lr: 0.02\n",
      "iteration: 378580 loss: 0.0021 lr: 0.02\n",
      "iteration: 378590 loss: 0.0015 lr: 0.02\n",
      "iteration: 378600 loss: 0.0015 lr: 0.02\n",
      "iteration: 378610 loss: 0.0019 lr: 0.02\n",
      "iteration: 378620 loss: 0.0026 lr: 0.02\n",
      "iteration: 378630 loss: 0.0021 lr: 0.02\n",
      "iteration: 378640 loss: 0.0016 lr: 0.02\n",
      "iteration: 378650 loss: 0.0012 lr: 0.02\n",
      "iteration: 378660 loss: 0.0010 lr: 0.02\n",
      "iteration: 378670 loss: 0.0018 lr: 0.02\n",
      "iteration: 378680 loss: 0.0019 lr: 0.02\n",
      "iteration: 378690 loss: 0.0016 lr: 0.02\n",
      "iteration: 378700 loss: 0.0013 lr: 0.02\n",
      "iteration: 378710 loss: 0.0018 lr: 0.02\n",
      "iteration: 378720 loss: 0.0019 lr: 0.02\n",
      "iteration: 378730 loss: 0.0012 lr: 0.02\n",
      "iteration: 378740 loss: 0.0011 lr: 0.02\n",
      "iteration: 378750 loss: 0.0014 lr: 0.02\n",
      "iteration: 378760 loss: 0.0017 lr: 0.02\n",
      "iteration: 378770 loss: 0.0015 lr: 0.02\n",
      "iteration: 378780 loss: 0.0012 lr: 0.02\n",
      "iteration: 378790 loss: 0.0011 lr: 0.02\n",
      "iteration: 378800 loss: 0.0017 lr: 0.02\n",
      "iteration: 378810 loss: 0.0020 lr: 0.02\n",
      "iteration: 378820 loss: 0.0019 lr: 0.02\n",
      "iteration: 378830 loss: 0.0011 lr: 0.02\n",
      "iteration: 378840 loss: 0.0019 lr: 0.02\n",
      "iteration: 378850 loss: 0.0020 lr: 0.02\n",
      "iteration: 378860 loss: 0.0013 lr: 0.02\n",
      "iteration: 378870 loss: 0.0014 lr: 0.02\n",
      "iteration: 378880 loss: 0.0015 lr: 0.02\n",
      "iteration: 378890 loss: 0.0013 lr: 0.02\n",
      "iteration: 378900 loss: 0.0015 lr: 0.02\n",
      "iteration: 378910 loss: 0.0019 lr: 0.02\n",
      "iteration: 378920 loss: 0.0017 lr: 0.02\n",
      "iteration: 378930 loss: 0.0019 lr: 0.02\n",
      "iteration: 378940 loss: 0.0012 lr: 0.02\n",
      "iteration: 378950 loss: 0.0017 lr: 0.02\n",
      "iteration: 378960 loss: 0.0014 lr: 0.02\n",
      "iteration: 378970 loss: 0.0019 lr: 0.02\n",
      "iteration: 378980 loss: 0.0013 lr: 0.02\n",
      "iteration: 378990 loss: 0.0017 lr: 0.02\n",
      "iteration: 379000 loss: 0.0015 lr: 0.02\n",
      "iteration: 379010 loss: 0.0017 lr: 0.02\n",
      "iteration: 379020 loss: 0.0016 lr: 0.02\n",
      "iteration: 379030 loss: 0.0017 lr: 0.02\n",
      "iteration: 379040 loss: 0.0015 lr: 0.02\n",
      "iteration: 379050 loss: 0.0019 lr: 0.02\n",
      "iteration: 379060 loss: 0.0019 lr: 0.02\n",
      "iteration: 379070 loss: 0.0014 lr: 0.02\n",
      "iteration: 379080 loss: 0.0010 lr: 0.02\n",
      "iteration: 379090 loss: 0.0017 lr: 0.02\n",
      "iteration: 379100 loss: 0.0014 lr: 0.02\n",
      "iteration: 379110 loss: 0.0013 lr: 0.02\n",
      "iteration: 379120 loss: 0.0017 lr: 0.02\n",
      "iteration: 379130 loss: 0.0014 lr: 0.02\n",
      "iteration: 379140 loss: 0.0019 lr: 0.02\n",
      "iteration: 379150 loss: 0.0015 lr: 0.02\n",
      "iteration: 379160 loss: 0.0010 lr: 0.02\n",
      "iteration: 379170 loss: 0.0017 lr: 0.02\n",
      "iteration: 379180 loss: 0.0016 lr: 0.02\n",
      "iteration: 379190 loss: 0.0016 lr: 0.02\n",
      "iteration: 379200 loss: 0.0012 lr: 0.02\n",
      "iteration: 379210 loss: 0.0015 lr: 0.02\n",
      "iteration: 379220 loss: 0.0017 lr: 0.02\n",
      "iteration: 379230 loss: 0.0013 lr: 0.02\n",
      "iteration: 379240 loss: 0.0016 lr: 0.02\n",
      "iteration: 379250 loss: 0.0011 lr: 0.02\n",
      "iteration: 379260 loss: 0.0017 lr: 0.02\n",
      "iteration: 379270 loss: 0.0016 lr: 0.02\n",
      "iteration: 379280 loss: 0.0018 lr: 0.02\n",
      "iteration: 379290 loss: 0.0018 lr: 0.02\n",
      "iteration: 379300 loss: 0.0017 lr: 0.02\n",
      "iteration: 379310 loss: 0.0015 lr: 0.02\n",
      "iteration: 379320 loss: 0.0016 lr: 0.02\n",
      "iteration: 379330 loss: 0.0011 lr: 0.02\n",
      "iteration: 379340 loss: 0.0014 lr: 0.02\n",
      "iteration: 379350 loss: 0.0025 lr: 0.02\n",
      "iteration: 379360 loss: 0.0022 lr: 0.02\n",
      "iteration: 379370 loss: 0.0019 lr: 0.02\n",
      "iteration: 379380 loss: 0.0014 lr: 0.02\n",
      "iteration: 379390 loss: 0.0012 lr: 0.02\n",
      "iteration: 379400 loss: 0.0018 lr: 0.02\n",
      "iteration: 379410 loss: 0.0018 lr: 0.02\n",
      "iteration: 379420 loss: 0.0014 lr: 0.02\n",
      "iteration: 379430 loss: 0.0017 lr: 0.02\n",
      "iteration: 379440 loss: 0.0032 lr: 0.02\n",
      "iteration: 379450 loss: 0.0010 lr: 0.02\n",
      "iteration: 379460 loss: 0.0019 lr: 0.02\n",
      "iteration: 379470 loss: 0.0014 lr: 0.02\n",
      "iteration: 379480 loss: 0.0016 lr: 0.02\n",
      "iteration: 379490 loss: 0.0016 lr: 0.02\n",
      "iteration: 379500 loss: 0.0008 lr: 0.02\n",
      "iteration: 379510 loss: 0.0009 lr: 0.02\n",
      "iteration: 379520 loss: 0.0013 lr: 0.02\n",
      "iteration: 379530 loss: 0.0017 lr: 0.02\n",
      "iteration: 379540 loss: 0.0019 lr: 0.02\n",
      "iteration: 379550 loss: 0.0017 lr: 0.02\n",
      "iteration: 379560 loss: 0.0013 lr: 0.02\n",
      "iteration: 379570 loss: 0.0017 lr: 0.02\n",
      "iteration: 379580 loss: 0.0013 lr: 0.02\n",
      "iteration: 379590 loss: 0.0015 lr: 0.02\n",
      "iteration: 379600 loss: 0.0012 lr: 0.02\n",
      "iteration: 379610 loss: 0.0014 lr: 0.02\n",
      "iteration: 379620 loss: 0.0016 lr: 0.02\n",
      "iteration: 379630 loss: 0.0012 lr: 0.02\n",
      "iteration: 379640 loss: 0.0012 lr: 0.02\n",
      "iteration: 379650 loss: 0.0021 lr: 0.02\n",
      "iteration: 379660 loss: 0.0011 lr: 0.02\n",
      "iteration: 379670 loss: 0.0014 lr: 0.02\n",
      "iteration: 379680 loss: 0.0013 lr: 0.02\n",
      "iteration: 379690 loss: 0.0013 lr: 0.02\n",
      "iteration: 379700 loss: 0.0018 lr: 0.02\n",
      "iteration: 379710 loss: 0.0017 lr: 0.02\n",
      "iteration: 379720 loss: 0.0017 lr: 0.02\n",
      "iteration: 379730 loss: 0.0019 lr: 0.02\n",
      "iteration: 379740 loss: 0.0024 lr: 0.02\n",
      "iteration: 379750 loss: 0.0018 lr: 0.02\n",
      "iteration: 379760 loss: 0.0013 lr: 0.02\n",
      "iteration: 379770 loss: 0.0014 lr: 0.02\n",
      "iteration: 379780 loss: 0.0010 lr: 0.02\n",
      "iteration: 379790 loss: 0.0012 lr: 0.02\n",
      "iteration: 379800 loss: 0.0015 lr: 0.02\n",
      "iteration: 379810 loss: 0.0019 lr: 0.02\n",
      "iteration: 379820 loss: 0.0013 lr: 0.02\n",
      "iteration: 379830 loss: 0.0018 lr: 0.02\n",
      "iteration: 379840 loss: 0.0019 lr: 0.02\n",
      "iteration: 379850 loss: 0.0017 lr: 0.02\n",
      "iteration: 379860 loss: 0.0019 lr: 0.02\n",
      "iteration: 379870 loss: 0.0014 lr: 0.02\n",
      "iteration: 379880 loss: 0.0019 lr: 0.02\n",
      "iteration: 379890 loss: 0.0017 lr: 0.02\n",
      "iteration: 379900 loss: 0.0019 lr: 0.02\n",
      "iteration: 379910 loss: 0.0016 lr: 0.02\n",
      "iteration: 379920 loss: 0.0019 lr: 0.02\n",
      "iteration: 379930 loss: 0.0015 lr: 0.02\n",
      "iteration: 379940 loss: 0.0018 lr: 0.02\n",
      "iteration: 379950 loss: 0.0027 lr: 0.02\n",
      "iteration: 379960 loss: 0.0014 lr: 0.02\n",
      "iteration: 379970 loss: 0.0013 lr: 0.02\n",
      "iteration: 379980 loss: 0.0019 lr: 0.02\n",
      "iteration: 379990 loss: 0.0016 lr: 0.02\n",
      "iteration: 380000 loss: 0.0016 lr: 0.02\n",
      "iteration: 380010 loss: 0.0026 lr: 0.02\n",
      "iteration: 380020 loss: 0.0011 lr: 0.02\n",
      "iteration: 380030 loss: 0.0025 lr: 0.02\n",
      "iteration: 380040 loss: 0.0016 lr: 0.02\n",
      "iteration: 380050 loss: 0.0015 lr: 0.02\n",
      "iteration: 380060 loss: 0.0020 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 380070 loss: 0.0016 lr: 0.02\n",
      "iteration: 380080 loss: 0.0016 lr: 0.02\n",
      "iteration: 380090 loss: 0.0014 lr: 0.02\n",
      "iteration: 380100 loss: 0.0017 lr: 0.02\n",
      "iteration: 380110 loss: 0.0018 lr: 0.02\n",
      "iteration: 380120 loss: 0.0017 lr: 0.02\n",
      "iteration: 380130 loss: 0.0017 lr: 0.02\n",
      "iteration: 380140 loss: 0.0013 lr: 0.02\n",
      "iteration: 380150 loss: 0.0019 lr: 0.02\n",
      "iteration: 380160 loss: 0.0023 lr: 0.02\n",
      "iteration: 380170 loss: 0.0027 lr: 0.02\n",
      "iteration: 380180 loss: 0.0016 lr: 0.02\n",
      "iteration: 380190 loss: 0.0015 lr: 0.02\n",
      "iteration: 380200 loss: 0.0017 lr: 0.02\n",
      "iteration: 380210 loss: 0.0011 lr: 0.02\n",
      "iteration: 380220 loss: 0.0015 lr: 0.02\n",
      "iteration: 380230 loss: 0.0010 lr: 0.02\n",
      "iteration: 380240 loss: 0.0038 lr: 0.02\n",
      "iteration: 380250 loss: 0.0013 lr: 0.02\n",
      "iteration: 380260 loss: 0.0013 lr: 0.02\n",
      "iteration: 380270 loss: 0.0021 lr: 0.02\n",
      "iteration: 380280 loss: 0.0015 lr: 0.02\n",
      "iteration: 380290 loss: 0.0021 lr: 0.02\n",
      "iteration: 380300 loss: 0.0013 lr: 0.02\n",
      "iteration: 380310 loss: 0.0036 lr: 0.02\n",
      "iteration: 380320 loss: 0.0023 lr: 0.02\n",
      "iteration: 380330 loss: 0.0016 lr: 0.02\n",
      "iteration: 380340 loss: 0.0018 lr: 0.02\n",
      "iteration: 380350 loss: 0.0015 lr: 0.02\n",
      "iteration: 380360 loss: 0.0020 lr: 0.02\n",
      "iteration: 380370 loss: 0.0021 lr: 0.02\n",
      "iteration: 380380 loss: 0.0018 lr: 0.02\n",
      "iteration: 380390 loss: 0.0011 lr: 0.02\n",
      "iteration: 380400 loss: 0.0016 lr: 0.02\n",
      "iteration: 380410 loss: 0.0019 lr: 0.02\n",
      "iteration: 380420 loss: 0.0011 lr: 0.02\n",
      "iteration: 380430 loss: 0.0015 lr: 0.02\n",
      "iteration: 380440 loss: 0.0027 lr: 0.02\n",
      "iteration: 380450 loss: 0.0012 lr: 0.02\n",
      "iteration: 380460 loss: 0.0027 lr: 0.02\n",
      "iteration: 380470 loss: 0.0016 lr: 0.02\n",
      "iteration: 380480 loss: 0.0014 lr: 0.02\n",
      "iteration: 380490 loss: 0.0017 lr: 0.02\n",
      "iteration: 380500 loss: 0.0023 lr: 0.02\n",
      "iteration: 380510 loss: 0.0026 lr: 0.02\n",
      "iteration: 380520 loss: 0.0013 lr: 0.02\n",
      "iteration: 380530 loss: 0.0016 lr: 0.02\n",
      "iteration: 380540 loss: 0.0013 lr: 0.02\n",
      "iteration: 380550 loss: 0.0011 lr: 0.02\n",
      "iteration: 380560 loss: 0.0014 lr: 0.02\n",
      "iteration: 380570 loss: 0.0012 lr: 0.02\n",
      "iteration: 380580 loss: 0.0015 lr: 0.02\n",
      "iteration: 380590 loss: 0.0015 lr: 0.02\n",
      "iteration: 380600 loss: 0.0012 lr: 0.02\n",
      "iteration: 380610 loss: 0.0015 lr: 0.02\n",
      "iteration: 380620 loss: 0.0019 lr: 0.02\n",
      "iteration: 380630 loss: 0.0022 lr: 0.02\n",
      "iteration: 380640 loss: 0.0023 lr: 0.02\n",
      "iteration: 380650 loss: 0.0015 lr: 0.02\n",
      "iteration: 380660 loss: 0.0022 lr: 0.02\n",
      "iteration: 380670 loss: 0.0014 lr: 0.02\n",
      "iteration: 380680 loss: 0.0014 lr: 0.02\n",
      "iteration: 380690 loss: 0.0008 lr: 0.02\n",
      "iteration: 380700 loss: 0.0013 lr: 0.02\n",
      "iteration: 380710 loss: 0.0018 lr: 0.02\n",
      "iteration: 380720 loss: 0.0012 lr: 0.02\n",
      "iteration: 380730 loss: 0.0016 lr: 0.02\n",
      "iteration: 380740 loss: 0.0012 lr: 0.02\n",
      "iteration: 380750 loss: 0.0019 lr: 0.02\n",
      "iteration: 380760 loss: 0.0010 lr: 0.02\n",
      "iteration: 380770 loss: 0.0016 lr: 0.02\n",
      "iteration: 380780 loss: 0.0018 lr: 0.02\n",
      "iteration: 380790 loss: 0.0018 lr: 0.02\n",
      "iteration: 380800 loss: 0.0015 lr: 0.02\n",
      "iteration: 380810 loss: 0.0023 lr: 0.02\n",
      "iteration: 380820 loss: 0.0016 lr: 0.02\n",
      "iteration: 380830 loss: 0.0011 lr: 0.02\n",
      "iteration: 380840 loss: 0.0021 lr: 0.02\n",
      "iteration: 380850 loss: 0.0010 lr: 0.02\n",
      "iteration: 380860 loss: 0.0014 lr: 0.02\n",
      "iteration: 380870 loss: 0.0013 lr: 0.02\n",
      "iteration: 380880 loss: 0.0013 lr: 0.02\n",
      "iteration: 380890 loss: 0.0014 lr: 0.02\n",
      "iteration: 380900 loss: 0.0013 lr: 0.02\n",
      "iteration: 380910 loss: 0.0017 lr: 0.02\n",
      "iteration: 380920 loss: 0.0014 lr: 0.02\n",
      "iteration: 380930 loss: 0.0009 lr: 0.02\n",
      "iteration: 380940 loss: 0.0017 lr: 0.02\n",
      "iteration: 380950 loss: 0.0021 lr: 0.02\n",
      "iteration: 380960 loss: 0.0020 lr: 0.02\n",
      "iteration: 380970 loss: 0.0015 lr: 0.02\n",
      "iteration: 380980 loss: 0.0020 lr: 0.02\n",
      "iteration: 380990 loss: 0.0015 lr: 0.02\n",
      "iteration: 381000 loss: 0.0016 lr: 0.02\n",
      "iteration: 381010 loss: 0.0014 lr: 0.02\n",
      "iteration: 381020 loss: 0.0012 lr: 0.02\n",
      "iteration: 381030 loss: 0.0014 lr: 0.02\n",
      "iteration: 381040 loss: 0.0015 lr: 0.02\n",
      "iteration: 381050 loss: 0.0016 lr: 0.02\n",
      "iteration: 381060 loss: 0.0013 lr: 0.02\n",
      "iteration: 381070 loss: 0.0014 lr: 0.02\n",
      "iteration: 381080 loss: 0.0011 lr: 0.02\n",
      "iteration: 381090 loss: 0.0014 lr: 0.02\n",
      "iteration: 381100 loss: 0.0014 lr: 0.02\n",
      "iteration: 381110 loss: 0.0015 lr: 0.02\n",
      "iteration: 381120 loss: 0.0016 lr: 0.02\n",
      "iteration: 381130 loss: 0.0014 lr: 0.02\n",
      "iteration: 381140 loss: 0.0021 lr: 0.02\n",
      "iteration: 381150 loss: 0.0028 lr: 0.02\n",
      "iteration: 381160 loss: 0.0014 lr: 0.02\n",
      "iteration: 381170 loss: 0.0014 lr: 0.02\n",
      "iteration: 381180 loss: 0.0017 lr: 0.02\n",
      "iteration: 381190 loss: 0.0018 lr: 0.02\n",
      "iteration: 381200 loss: 0.0018 lr: 0.02\n",
      "iteration: 381210 loss: 0.0020 lr: 0.02\n",
      "iteration: 381220 loss: 0.0014 lr: 0.02\n",
      "iteration: 381230 loss: 0.0018 lr: 0.02\n",
      "iteration: 381240 loss: 0.0016 lr: 0.02\n",
      "iteration: 381250 loss: 0.0018 lr: 0.02\n",
      "iteration: 381260 loss: 0.0012 lr: 0.02\n",
      "iteration: 381270 loss: 0.0017 lr: 0.02\n",
      "iteration: 381280 loss: 0.0012 lr: 0.02\n",
      "iteration: 381290 loss: 0.0014 lr: 0.02\n",
      "iteration: 381300 loss: 0.0014 lr: 0.02\n",
      "iteration: 381310 loss: 0.0019 lr: 0.02\n",
      "iteration: 381320 loss: 0.0022 lr: 0.02\n",
      "iteration: 381330 loss: 0.0017 lr: 0.02\n",
      "iteration: 381340 loss: 0.0019 lr: 0.02\n",
      "iteration: 381350 loss: 0.0014 lr: 0.02\n",
      "iteration: 381360 loss: 0.0020 lr: 0.02\n",
      "iteration: 381370 loss: 0.0012 lr: 0.02\n",
      "iteration: 381380 loss: 0.0014 lr: 0.02\n",
      "iteration: 381390 loss: 0.0012 lr: 0.02\n",
      "iteration: 381400 loss: 0.0014 lr: 0.02\n",
      "iteration: 381410 loss: 0.0013 lr: 0.02\n",
      "iteration: 381420 loss: 0.0017 lr: 0.02\n",
      "iteration: 381430 loss: 0.0013 lr: 0.02\n",
      "iteration: 381440 loss: 0.0014 lr: 0.02\n",
      "iteration: 381450 loss: 0.0013 lr: 0.02\n",
      "iteration: 381460 loss: 0.0014 lr: 0.02\n",
      "iteration: 381470 loss: 0.0013 lr: 0.02\n",
      "iteration: 381480 loss: 0.0022 lr: 0.02\n",
      "iteration: 381490 loss: 0.0018 lr: 0.02\n",
      "iteration: 381500 loss: 0.0013 lr: 0.02\n",
      "iteration: 381510 loss: 0.0012 lr: 0.02\n",
      "iteration: 381520 loss: 0.0011 lr: 0.02\n",
      "iteration: 381530 loss: 0.0015 lr: 0.02\n",
      "iteration: 381540 loss: 0.0014 lr: 0.02\n",
      "iteration: 381550 loss: 0.0024 lr: 0.02\n",
      "iteration: 381560 loss: 0.0013 lr: 0.02\n",
      "iteration: 381570 loss: 0.0015 lr: 0.02\n",
      "iteration: 381580 loss: 0.0014 lr: 0.02\n",
      "iteration: 381590 loss: 0.0021 lr: 0.02\n",
      "iteration: 381600 loss: 0.0017 lr: 0.02\n",
      "iteration: 381610 loss: 0.0016 lr: 0.02\n",
      "iteration: 381620 loss: 0.0014 lr: 0.02\n",
      "iteration: 381630 loss: 0.0015 lr: 0.02\n",
      "iteration: 381640 loss: 0.0018 lr: 0.02\n",
      "iteration: 381650 loss: 0.0028 lr: 0.02\n",
      "iteration: 381660 loss: 0.0013 lr: 0.02\n",
      "iteration: 381670 loss: 0.0016 lr: 0.02\n",
      "iteration: 381680 loss: 0.0012 lr: 0.02\n",
      "iteration: 381690 loss: 0.0012 lr: 0.02\n",
      "iteration: 381700 loss: 0.0020 lr: 0.02\n",
      "iteration: 381710 loss: 0.0020 lr: 0.02\n",
      "iteration: 381720 loss: 0.0017 lr: 0.02\n",
      "iteration: 381730 loss: 0.0019 lr: 0.02\n",
      "iteration: 381740 loss: 0.0013 lr: 0.02\n",
      "iteration: 381750 loss: 0.0013 lr: 0.02\n",
      "iteration: 381760 loss: 0.0024 lr: 0.02\n",
      "iteration: 381770 loss: 0.0018 lr: 0.02\n",
      "iteration: 381780 loss: 0.0014 lr: 0.02\n",
      "iteration: 381790 loss: 0.0011 lr: 0.02\n",
      "iteration: 381800 loss: 0.0038 lr: 0.02\n",
      "iteration: 381810 loss: 0.0015 lr: 0.02\n",
      "iteration: 381820 loss: 0.0015 lr: 0.02\n",
      "iteration: 381830 loss: 0.0013 lr: 0.02\n",
      "iteration: 381840 loss: 0.0015 lr: 0.02\n",
      "iteration: 381850 loss: 0.0018 lr: 0.02\n",
      "iteration: 381860 loss: 0.0019 lr: 0.02\n",
      "iteration: 381870 loss: 0.0017 lr: 0.02\n",
      "iteration: 381880 loss: 0.0012 lr: 0.02\n",
      "iteration: 381890 loss: 0.0015 lr: 0.02\n",
      "iteration: 381900 loss: 0.0013 lr: 0.02\n",
      "iteration: 381910 loss: 0.0013 lr: 0.02\n",
      "iteration: 381920 loss: 0.0021 lr: 0.02\n",
      "iteration: 381930 loss: 0.0013 lr: 0.02\n",
      "iteration: 381940 loss: 0.0011 lr: 0.02\n",
      "iteration: 381950 loss: 0.0013 lr: 0.02\n",
      "iteration: 381960 loss: 0.0015 lr: 0.02\n",
      "iteration: 381970 loss: 0.0020 lr: 0.02\n",
      "iteration: 381980 loss: 0.0013 lr: 0.02\n",
      "iteration: 381990 loss: 0.0015 lr: 0.02\n",
      "iteration: 382000 loss: 0.0019 lr: 0.02\n",
      "iteration: 382010 loss: 0.0012 lr: 0.02\n",
      "iteration: 382020 loss: 0.0019 lr: 0.02\n",
      "iteration: 382030 loss: 0.0022 lr: 0.02\n",
      "iteration: 382040 loss: 0.0014 lr: 0.02\n",
      "iteration: 382050 loss: 0.0028 lr: 0.02\n",
      "iteration: 382060 loss: 0.0017 lr: 0.02\n",
      "iteration: 382070 loss: 0.0016 lr: 0.02\n",
      "iteration: 382080 loss: 0.0015 lr: 0.02\n",
      "iteration: 382090 loss: 0.0014 lr: 0.02\n",
      "iteration: 382100 loss: 0.0011 lr: 0.02\n",
      "iteration: 382110 loss: 0.0014 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 382120 loss: 0.0021 lr: 0.02\n",
      "iteration: 382130 loss: 0.0021 lr: 0.02\n",
      "iteration: 382140 loss: 0.0025 lr: 0.02\n",
      "iteration: 382150 loss: 0.0022 lr: 0.02\n",
      "iteration: 382160 loss: 0.0020 lr: 0.02\n",
      "iteration: 382170 loss: 0.0011 lr: 0.02\n",
      "iteration: 382180 loss: 0.0012 lr: 0.02\n",
      "iteration: 382190 loss: 0.0021 lr: 0.02\n",
      "iteration: 382200 loss: 0.0018 lr: 0.02\n",
      "iteration: 382210 loss: 0.0016 lr: 0.02\n",
      "iteration: 382220 loss: 0.0021 lr: 0.02\n",
      "iteration: 382230 loss: 0.0027 lr: 0.02\n",
      "iteration: 382240 loss: 0.0014 lr: 0.02\n",
      "iteration: 382250 loss: 0.0017 lr: 0.02\n",
      "iteration: 382260 loss: 0.0019 lr: 0.02\n",
      "iteration: 382270 loss: 0.0014 lr: 0.02\n",
      "iteration: 382280 loss: 0.0015 lr: 0.02\n",
      "iteration: 382290 loss: 0.0015 lr: 0.02\n",
      "iteration: 382300 loss: 0.0012 lr: 0.02\n",
      "iteration: 382310 loss: 0.0017 lr: 0.02\n",
      "iteration: 382320 loss: 0.0012 lr: 0.02\n",
      "iteration: 382330 loss: 0.0014 lr: 0.02\n",
      "iteration: 382340 loss: 0.0016 lr: 0.02\n",
      "iteration: 382350 loss: 0.0018 lr: 0.02\n",
      "iteration: 382360 loss: 0.0015 lr: 0.02\n",
      "iteration: 382370 loss: 0.0021 lr: 0.02\n",
      "iteration: 382380 loss: 0.0011 lr: 0.02\n",
      "iteration: 382390 loss: 0.0013 lr: 0.02\n",
      "iteration: 382400 loss: 0.0024 lr: 0.02\n",
      "iteration: 382410 loss: 0.0015 lr: 0.02\n",
      "iteration: 382420 loss: 0.0011 lr: 0.02\n",
      "iteration: 382430 loss: 0.0016 lr: 0.02\n",
      "iteration: 382440 loss: 0.0015 lr: 0.02\n",
      "iteration: 382450 loss: 0.0013 lr: 0.02\n",
      "iteration: 382460 loss: 0.0015 lr: 0.02\n",
      "iteration: 382470 loss: 0.0019 lr: 0.02\n",
      "iteration: 382480 loss: 0.0019 lr: 0.02\n",
      "iteration: 382490 loss: 0.0014 lr: 0.02\n",
      "iteration: 382500 loss: 0.0018 lr: 0.02\n",
      "iteration: 382510 loss: 0.0014 lr: 0.02\n",
      "iteration: 382520 loss: 0.0015 lr: 0.02\n",
      "iteration: 382530 loss: 0.0022 lr: 0.02\n",
      "iteration: 382540 loss: 0.0014 lr: 0.02\n",
      "iteration: 382550 loss: 0.0021 lr: 0.02\n",
      "iteration: 382560 loss: 0.0014 lr: 0.02\n",
      "iteration: 382570 loss: 0.0016 lr: 0.02\n",
      "iteration: 382580 loss: 0.0023 lr: 0.02\n",
      "iteration: 382590 loss: 0.0019 lr: 0.02\n",
      "iteration: 382600 loss: 0.0021 lr: 0.02\n",
      "iteration: 382610 loss: 0.0011 lr: 0.02\n",
      "iteration: 382620 loss: 0.0016 lr: 0.02\n",
      "iteration: 382630 loss: 0.0020 lr: 0.02\n",
      "iteration: 382640 loss: 0.0014 lr: 0.02\n",
      "iteration: 382650 loss: 0.0013 lr: 0.02\n",
      "iteration: 382660 loss: 0.0024 lr: 0.02\n",
      "iteration: 382670 loss: 0.0013 lr: 0.02\n",
      "iteration: 382680 loss: 0.0016 lr: 0.02\n",
      "iteration: 382690 loss: 0.0013 lr: 0.02\n",
      "iteration: 382700 loss: 0.0015 lr: 0.02\n",
      "iteration: 382710 loss: 0.0010 lr: 0.02\n",
      "iteration: 382720 loss: 0.0018 lr: 0.02\n",
      "iteration: 382730 loss: 0.0025 lr: 0.02\n",
      "iteration: 382740 loss: 0.0014 lr: 0.02\n",
      "iteration: 382750 loss: 0.0013 lr: 0.02\n",
      "iteration: 382760 loss: 0.0013 lr: 0.02\n",
      "iteration: 382770 loss: 0.0018 lr: 0.02\n",
      "iteration: 382780 loss: 0.0016 lr: 0.02\n",
      "iteration: 382790 loss: 0.0016 lr: 0.02\n",
      "iteration: 382800 loss: 0.0013 lr: 0.02\n",
      "iteration: 382810 loss: 0.0013 lr: 0.02\n",
      "iteration: 382820 loss: 0.0017 lr: 0.02\n",
      "iteration: 382830 loss: 0.0012 lr: 0.02\n",
      "iteration: 382840 loss: 0.0021 lr: 0.02\n",
      "iteration: 382850 loss: 0.0021 lr: 0.02\n",
      "iteration: 382860 loss: 0.0012 lr: 0.02\n",
      "iteration: 382870 loss: 0.0015 lr: 0.02\n",
      "iteration: 382880 loss: 0.0019 lr: 0.02\n",
      "iteration: 382890 loss: 0.0018 lr: 0.02\n",
      "iteration: 382900 loss: 0.0019 lr: 0.02\n",
      "iteration: 382910 loss: 0.0015 lr: 0.02\n",
      "iteration: 382920 loss: 0.0017 lr: 0.02\n",
      "iteration: 382930 loss: 0.0016 lr: 0.02\n",
      "iteration: 382940 loss: 0.0017 lr: 0.02\n",
      "iteration: 382950 loss: 0.0020 lr: 0.02\n",
      "iteration: 382960 loss: 0.0011 lr: 0.02\n",
      "iteration: 382970 loss: 0.0010 lr: 0.02\n",
      "iteration: 382980 loss: 0.0017 lr: 0.02\n",
      "iteration: 382990 loss: 0.0014 lr: 0.02\n",
      "iteration: 383000 loss: 0.0022 lr: 0.02\n",
      "iteration: 383010 loss: 0.0016 lr: 0.02\n",
      "iteration: 383020 loss: 0.0014 lr: 0.02\n",
      "iteration: 383030 loss: 0.0017 lr: 0.02\n",
      "iteration: 383040 loss: 0.0013 lr: 0.02\n",
      "iteration: 383050 loss: 0.0018 lr: 0.02\n",
      "iteration: 383060 loss: 0.0023 lr: 0.02\n",
      "iteration: 383070 loss: 0.0013 lr: 0.02\n",
      "iteration: 383080 loss: 0.0016 lr: 0.02\n",
      "iteration: 383090 loss: 0.0014 lr: 0.02\n",
      "iteration: 383100 loss: 0.0016 lr: 0.02\n",
      "iteration: 383110 loss: 0.0017 lr: 0.02\n",
      "iteration: 383120 loss: 0.0020 lr: 0.02\n",
      "iteration: 383130 loss: 0.0011 lr: 0.02\n",
      "iteration: 383140 loss: 0.0013 lr: 0.02\n",
      "iteration: 383150 loss: 0.0013 lr: 0.02\n",
      "iteration: 383160 loss: 0.0017 lr: 0.02\n",
      "iteration: 383170 loss: 0.0020 lr: 0.02\n",
      "iteration: 383180 loss: 0.0013 lr: 0.02\n",
      "iteration: 383190 loss: 0.0021 lr: 0.02\n",
      "iteration: 383200 loss: 0.0014 lr: 0.02\n",
      "iteration: 383210 loss: 0.0017 lr: 0.02\n",
      "iteration: 383220 loss: 0.0018 lr: 0.02\n",
      "iteration: 383230 loss: 0.0013 lr: 0.02\n",
      "iteration: 383240 loss: 0.0013 lr: 0.02\n",
      "iteration: 383250 loss: 0.0013 lr: 0.02\n",
      "iteration: 383260 loss: 0.0016 lr: 0.02\n",
      "iteration: 383270 loss: 0.0010 lr: 0.02\n",
      "iteration: 383280 loss: 0.0012 lr: 0.02\n",
      "iteration: 383290 loss: 0.0019 lr: 0.02\n",
      "iteration: 383300 loss: 0.0017 lr: 0.02\n",
      "iteration: 383310 loss: 0.0020 lr: 0.02\n",
      "iteration: 383320 loss: 0.0015 lr: 0.02\n",
      "iteration: 383330 loss: 0.0013 lr: 0.02\n",
      "iteration: 383340 loss: 0.0022 lr: 0.02\n",
      "iteration: 383350 loss: 0.0012 lr: 0.02\n",
      "iteration: 383360 loss: 0.0023 lr: 0.02\n",
      "iteration: 383370 loss: 0.0017 lr: 0.02\n",
      "iteration: 383380 loss: 0.0011 lr: 0.02\n",
      "iteration: 383390 loss: 0.0013 lr: 0.02\n",
      "iteration: 383400 loss: 0.0016 lr: 0.02\n",
      "iteration: 383410 loss: 0.0015 lr: 0.02\n",
      "iteration: 383420 loss: 0.0012 lr: 0.02\n",
      "iteration: 383430 loss: 0.0014 lr: 0.02\n",
      "iteration: 383440 loss: 0.0013 lr: 0.02\n",
      "iteration: 383450 loss: 0.0011 lr: 0.02\n",
      "iteration: 383460 loss: 0.0012 lr: 0.02\n",
      "iteration: 383470 loss: 0.0027 lr: 0.02\n",
      "iteration: 383480 loss: 0.0016 lr: 0.02\n",
      "iteration: 383490 loss: 0.0018 lr: 0.02\n",
      "iteration: 383500 loss: 0.0015 lr: 0.02\n",
      "iteration: 383510 loss: 0.0018 lr: 0.02\n",
      "iteration: 383520 loss: 0.0010 lr: 0.02\n",
      "iteration: 383530 loss: 0.0011 lr: 0.02\n",
      "iteration: 383540 loss: 0.0011 lr: 0.02\n",
      "iteration: 383550 loss: 0.0020 lr: 0.02\n",
      "iteration: 383560 loss: 0.0012 lr: 0.02\n",
      "iteration: 383570 loss: 0.0010 lr: 0.02\n",
      "iteration: 383580 loss: 0.0013 lr: 0.02\n",
      "iteration: 383590 loss: 0.0013 lr: 0.02\n",
      "iteration: 383600 loss: 0.0019 lr: 0.02\n",
      "iteration: 383610 loss: 0.0014 lr: 0.02\n",
      "iteration: 383620 loss: 0.0016 lr: 0.02\n",
      "iteration: 383630 loss: 0.0010 lr: 0.02\n",
      "iteration: 383640 loss: 0.0012 lr: 0.02\n",
      "iteration: 383650 loss: 0.0020 lr: 0.02\n",
      "iteration: 383660 loss: 0.0015 lr: 0.02\n",
      "iteration: 383670 loss: 0.0015 lr: 0.02\n",
      "iteration: 383680 loss: 0.0020 lr: 0.02\n",
      "iteration: 383690 loss: 0.0019 lr: 0.02\n",
      "iteration: 383700 loss: 0.0009 lr: 0.02\n",
      "iteration: 383710 loss: 0.0012 lr: 0.02\n",
      "iteration: 383720 loss: 0.0011 lr: 0.02\n",
      "iteration: 383730 loss: 0.0012 lr: 0.02\n",
      "iteration: 383740 loss: 0.0017 lr: 0.02\n",
      "iteration: 383750 loss: 0.0015 lr: 0.02\n",
      "iteration: 383760 loss: 0.0015 lr: 0.02\n",
      "iteration: 383770 loss: 0.0014 lr: 0.02\n",
      "iteration: 383780 loss: 0.0012 lr: 0.02\n",
      "iteration: 383790 loss: 0.0017 lr: 0.02\n",
      "iteration: 383800 loss: 0.0016 lr: 0.02\n",
      "iteration: 383810 loss: 0.0017 lr: 0.02\n",
      "iteration: 383820 loss: 0.0014 lr: 0.02\n",
      "iteration: 383830 loss: 0.0017 lr: 0.02\n",
      "iteration: 383840 loss: 0.0011 lr: 0.02\n",
      "iteration: 383850 loss: 0.0012 lr: 0.02\n",
      "iteration: 383860 loss: 0.0016 lr: 0.02\n",
      "iteration: 383870 loss: 0.0017 lr: 0.02\n",
      "iteration: 383880 loss: 0.0016 lr: 0.02\n",
      "iteration: 383890 loss: 0.0019 lr: 0.02\n",
      "iteration: 383900 loss: 0.0018 lr: 0.02\n",
      "iteration: 383910 loss: 0.0014 lr: 0.02\n",
      "iteration: 383920 loss: 0.0019 lr: 0.02\n",
      "iteration: 383930 loss: 0.0020 lr: 0.02\n",
      "iteration: 383940 loss: 0.0013 lr: 0.02\n",
      "iteration: 383950 loss: 0.0013 lr: 0.02\n",
      "iteration: 383960 loss: 0.0014 lr: 0.02\n",
      "iteration: 383970 loss: 0.0015 lr: 0.02\n",
      "iteration: 383980 loss: 0.0017 lr: 0.02\n",
      "iteration: 383990 loss: 0.0017 lr: 0.02\n",
      "iteration: 384000 loss: 0.0025 lr: 0.02\n",
      "iteration: 384010 loss: 0.0014 lr: 0.02\n",
      "iteration: 384020 loss: 0.0019 lr: 0.02\n",
      "iteration: 384030 loss: 0.0016 lr: 0.02\n",
      "iteration: 384040 loss: 0.0019 lr: 0.02\n",
      "iteration: 384050 loss: 0.0018 lr: 0.02\n",
      "iteration: 384060 loss: 0.0012 lr: 0.02\n",
      "iteration: 384070 loss: 0.0011 lr: 0.02\n",
      "iteration: 384080 loss: 0.0020 lr: 0.02\n",
      "iteration: 384090 loss: 0.0016 lr: 0.02\n",
      "iteration: 384100 loss: 0.0022 lr: 0.02\n",
      "iteration: 384110 loss: 0.0014 lr: 0.02\n",
      "iteration: 384120 loss: 0.0017 lr: 0.02\n",
      "iteration: 384130 loss: 0.0015 lr: 0.02\n",
      "iteration: 384140 loss: 0.0017 lr: 0.02\n",
      "iteration: 384150 loss: 0.0015 lr: 0.02\n",
      "iteration: 384160 loss: 0.0012 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 384170 loss: 0.0011 lr: 0.02\n",
      "iteration: 384180 loss: 0.0013 lr: 0.02\n",
      "iteration: 384190 loss: 0.0013 lr: 0.02\n",
      "iteration: 384200 loss: 0.0024 lr: 0.02\n",
      "iteration: 384210 loss: 0.0019 lr: 0.02\n",
      "iteration: 384220 loss: 0.0011 lr: 0.02\n",
      "iteration: 384230 loss: 0.0024 lr: 0.02\n",
      "iteration: 384240 loss: 0.0012 lr: 0.02\n",
      "iteration: 384250 loss: 0.0020 lr: 0.02\n",
      "iteration: 384260 loss: 0.0017 lr: 0.02\n",
      "iteration: 384270 loss: 0.0015 lr: 0.02\n",
      "iteration: 384280 loss: 0.0014 lr: 0.02\n",
      "iteration: 384290 loss: 0.0022 lr: 0.02\n",
      "iteration: 384300 loss: 0.0017 lr: 0.02\n",
      "iteration: 384310 loss: 0.0019 lr: 0.02\n",
      "iteration: 384320 loss: 0.0008 lr: 0.02\n",
      "iteration: 384330 loss: 0.0015 lr: 0.02\n",
      "iteration: 384340 loss: 0.0019 lr: 0.02\n",
      "iteration: 384350 loss: 0.0016 lr: 0.02\n",
      "iteration: 384360 loss: 0.0013 lr: 0.02\n",
      "iteration: 384370 loss: 0.0015 lr: 0.02\n",
      "iteration: 384380 loss: 0.0018 lr: 0.02\n",
      "iteration: 384390 loss: 0.0019 lr: 0.02\n",
      "iteration: 384400 loss: 0.0020 lr: 0.02\n",
      "iteration: 384410 loss: 0.0017 lr: 0.02\n",
      "iteration: 384420 loss: 0.0018 lr: 0.02\n",
      "iteration: 384430 loss: 0.0017 lr: 0.02\n",
      "iteration: 384440 loss: 0.0023 lr: 0.02\n",
      "iteration: 384450 loss: 0.0015 lr: 0.02\n",
      "iteration: 384460 loss: 0.0018 lr: 0.02\n",
      "iteration: 384470 loss: 0.0016 lr: 0.02\n",
      "iteration: 384480 loss: 0.0016 lr: 0.02\n",
      "iteration: 384490 loss: 0.0018 lr: 0.02\n",
      "iteration: 384500 loss: 0.0013 lr: 0.02\n",
      "iteration: 384510 loss: 0.0013 lr: 0.02\n",
      "iteration: 384520 loss: 0.0014 lr: 0.02\n",
      "iteration: 384530 loss: 0.0019 lr: 0.02\n",
      "iteration: 384540 loss: 0.0015 lr: 0.02\n",
      "iteration: 384550 loss: 0.0018 lr: 0.02\n",
      "iteration: 384560 loss: 0.0026 lr: 0.02\n",
      "iteration: 384570 loss: 0.0015 lr: 0.02\n",
      "iteration: 384580 loss: 0.0014 lr: 0.02\n",
      "iteration: 384590 loss: 0.0012 lr: 0.02\n",
      "iteration: 384600 loss: 0.0015 lr: 0.02\n",
      "iteration: 384610 loss: 0.0016 lr: 0.02\n",
      "iteration: 384620 loss: 0.0017 lr: 0.02\n",
      "iteration: 384630 loss: 0.0011 lr: 0.02\n",
      "iteration: 384640 loss: 0.0016 lr: 0.02\n",
      "iteration: 384650 loss: 0.0014 lr: 0.02\n",
      "iteration: 384660 loss: 0.0011 lr: 0.02\n",
      "iteration: 384670 loss: 0.0019 lr: 0.02\n",
      "iteration: 384680 loss: 0.0022 lr: 0.02\n",
      "iteration: 384690 loss: 0.0015 lr: 0.02\n",
      "iteration: 384700 loss: 0.0019 lr: 0.02\n",
      "iteration: 384710 loss: 0.0013 lr: 0.02\n",
      "iteration: 384720 loss: 0.0018 lr: 0.02\n",
      "iteration: 384730 loss: 0.0029 lr: 0.02\n",
      "iteration: 384740 loss: 0.0015 lr: 0.02\n",
      "iteration: 384750 loss: 0.0015 lr: 0.02\n",
      "iteration: 384760 loss: 0.0016 lr: 0.02\n",
      "iteration: 384770 loss: 0.0020 lr: 0.02\n",
      "iteration: 384780 loss: 0.0020 lr: 0.02\n",
      "iteration: 384790 loss: 0.0014 lr: 0.02\n",
      "iteration: 384800 loss: 0.0019 lr: 0.02\n",
      "iteration: 384810 loss: 0.0013 lr: 0.02\n",
      "iteration: 384820 loss: 0.0017 lr: 0.02\n",
      "iteration: 384830 loss: 0.0026 lr: 0.02\n",
      "iteration: 384840 loss: 0.0021 lr: 0.02\n",
      "iteration: 384850 loss: 0.0017 lr: 0.02\n",
      "iteration: 384860 loss: 0.0013 lr: 0.02\n",
      "iteration: 384870 loss: 0.0016 lr: 0.02\n",
      "iteration: 384880 loss: 0.0031 lr: 0.02\n",
      "iteration: 384890 loss: 0.0016 lr: 0.02\n",
      "iteration: 384900 loss: 0.0013 lr: 0.02\n",
      "iteration: 384910 loss: 0.0026 lr: 0.02\n",
      "iteration: 384920 loss: 0.0015 lr: 0.02\n",
      "iteration: 384930 loss: 0.0017 lr: 0.02\n",
      "iteration: 384940 loss: 0.0013 lr: 0.02\n",
      "iteration: 384950 loss: 0.0014 lr: 0.02\n",
      "iteration: 384960 loss: 0.0016 lr: 0.02\n",
      "iteration: 384970 loss: 0.0012 lr: 0.02\n",
      "iteration: 384980 loss: 0.0015 lr: 0.02\n",
      "iteration: 384990 loss: 0.0015 lr: 0.02\n",
      "iteration: 385000 loss: 0.0017 lr: 0.02\n",
      "iteration: 385010 loss: 0.0019 lr: 0.02\n",
      "iteration: 385020 loss: 0.0015 lr: 0.02\n",
      "iteration: 385030 loss: 0.0020 lr: 0.02\n",
      "iteration: 385040 loss: 0.0014 lr: 0.02\n",
      "iteration: 385050 loss: 0.0012 lr: 0.02\n",
      "iteration: 385060 loss: 0.0019 lr: 0.02\n",
      "iteration: 385070 loss: 0.0013 lr: 0.02\n",
      "iteration: 385080 loss: 0.0025 lr: 0.02\n",
      "iteration: 385090 loss: 0.0012 lr: 0.02\n",
      "iteration: 385100 loss: 0.0012 lr: 0.02\n",
      "iteration: 385110 loss: 0.0018 lr: 0.02\n",
      "iteration: 385120 loss: 0.0015 lr: 0.02\n",
      "iteration: 385130 loss: 0.0018 lr: 0.02\n",
      "iteration: 385140 loss: 0.0012 lr: 0.02\n",
      "iteration: 385150 loss: 0.0020 lr: 0.02\n",
      "iteration: 385160 loss: 0.0013 lr: 0.02\n",
      "iteration: 385170 loss: 0.0014 lr: 0.02\n",
      "iteration: 385180 loss: 0.0019 lr: 0.02\n",
      "iteration: 385190 loss: 0.0014 lr: 0.02\n",
      "iteration: 385200 loss: 0.0013 lr: 0.02\n",
      "iteration: 385210 loss: 0.0012 lr: 0.02\n",
      "iteration: 385220 loss: 0.0014 lr: 0.02\n",
      "iteration: 385230 loss: 0.0013 lr: 0.02\n",
      "iteration: 385240 loss: 0.0018 lr: 0.02\n",
      "iteration: 385250 loss: 0.0015 lr: 0.02\n",
      "iteration: 385260 loss: 0.0013 lr: 0.02\n",
      "iteration: 385270 loss: 0.0015 lr: 0.02\n",
      "iteration: 385280 loss: 0.0020 lr: 0.02\n",
      "iteration: 385290 loss: 0.0021 lr: 0.02\n",
      "iteration: 385300 loss: 0.0013 lr: 0.02\n",
      "iteration: 385310 loss: 0.0012 lr: 0.02\n",
      "iteration: 385320 loss: 0.0014 lr: 0.02\n",
      "iteration: 385330 loss: 0.0010 lr: 0.02\n",
      "iteration: 385340 loss: 0.0021 lr: 0.02\n",
      "iteration: 385350 loss: 0.0017 lr: 0.02\n",
      "iteration: 385360 loss: 0.0013 lr: 0.02\n",
      "iteration: 385370 loss: 0.0011 lr: 0.02\n",
      "iteration: 385380 loss: 0.0016 lr: 0.02\n",
      "iteration: 385390 loss: 0.0015 lr: 0.02\n",
      "iteration: 385400 loss: 0.0011 lr: 0.02\n",
      "iteration: 385410 loss: 0.0012 lr: 0.02\n",
      "iteration: 385420 loss: 0.0011 lr: 0.02\n",
      "iteration: 385430 loss: 0.0014 lr: 0.02\n",
      "iteration: 385440 loss: 0.0012 lr: 0.02\n",
      "iteration: 385450 loss: 0.0015 lr: 0.02\n",
      "iteration: 385460 loss: 0.0013 lr: 0.02\n",
      "iteration: 385470 loss: 0.0018 lr: 0.02\n",
      "iteration: 385480 loss: 0.0014 lr: 0.02\n",
      "iteration: 385490 loss: 0.0011 lr: 0.02\n",
      "iteration: 385500 loss: 0.0016 lr: 0.02\n",
      "iteration: 385510 loss: 0.0011 lr: 0.02\n",
      "iteration: 385520 loss: 0.0013 lr: 0.02\n",
      "iteration: 385530 loss: 0.0010 lr: 0.02\n",
      "iteration: 385540 loss: 0.0018 lr: 0.02\n",
      "iteration: 385550 loss: 0.0014 lr: 0.02\n",
      "iteration: 385560 loss: 0.0009 lr: 0.02\n",
      "iteration: 385570 loss: 0.0016 lr: 0.02\n",
      "iteration: 385580 loss: 0.0016 lr: 0.02\n",
      "iteration: 385590 loss: 0.0015 lr: 0.02\n",
      "iteration: 385600 loss: 0.0013 lr: 0.02\n",
      "iteration: 385610 loss: 0.0015 lr: 0.02\n",
      "iteration: 385620 loss: 0.0020 lr: 0.02\n",
      "iteration: 385630 loss: 0.0014 lr: 0.02\n",
      "iteration: 385640 loss: 0.0019 lr: 0.02\n",
      "iteration: 385650 loss: 0.0014 lr: 0.02\n",
      "iteration: 385660 loss: 0.0012 lr: 0.02\n",
      "iteration: 385670 loss: 0.0016 lr: 0.02\n",
      "iteration: 385680 loss: 0.0012 lr: 0.02\n",
      "iteration: 385690 loss: 0.0019 lr: 0.02\n",
      "iteration: 385700 loss: 0.0020 lr: 0.02\n",
      "iteration: 385710 loss: 0.0013 lr: 0.02\n",
      "iteration: 385720 loss: 0.0014 lr: 0.02\n",
      "iteration: 385730 loss: 0.0015 lr: 0.02\n",
      "iteration: 385740 loss: 0.0012 lr: 0.02\n",
      "iteration: 385750 loss: 0.0023 lr: 0.02\n",
      "iteration: 385760 loss: 0.0017 lr: 0.02\n",
      "iteration: 385770 loss: 0.0023 lr: 0.02\n",
      "iteration: 385780 loss: 0.0014 lr: 0.02\n",
      "iteration: 385790 loss: 0.0014 lr: 0.02\n",
      "iteration: 385800 loss: 0.0014 lr: 0.02\n",
      "iteration: 385810 loss: 0.0016 lr: 0.02\n",
      "iteration: 385820 loss: 0.0017 lr: 0.02\n",
      "iteration: 385830 loss: 0.0019 lr: 0.02\n",
      "iteration: 385840 loss: 0.0017 lr: 0.02\n",
      "iteration: 385850 loss: 0.0013 lr: 0.02\n",
      "iteration: 385860 loss: 0.0020 lr: 0.02\n",
      "iteration: 385870 loss: 0.0012 lr: 0.02\n",
      "iteration: 385880 loss: 0.0018 lr: 0.02\n",
      "iteration: 385890 loss: 0.0017 lr: 0.02\n",
      "iteration: 385900 loss: 0.0017 lr: 0.02\n",
      "iteration: 385910 loss: 0.0016 lr: 0.02\n",
      "iteration: 385920 loss: 0.0012 lr: 0.02\n",
      "iteration: 385930 loss: 0.0010 lr: 0.02\n",
      "iteration: 385940 loss: 0.0011 lr: 0.02\n",
      "iteration: 385950 loss: 0.0015 lr: 0.02\n",
      "iteration: 385960 loss: 0.0014 lr: 0.02\n",
      "iteration: 385970 loss: 0.0014 lr: 0.02\n",
      "iteration: 385980 loss: 0.0016 lr: 0.02\n",
      "iteration: 385990 loss: 0.0019 lr: 0.02\n",
      "iteration: 386000 loss: 0.0021 lr: 0.02\n",
      "iteration: 386010 loss: 0.0013 lr: 0.02\n",
      "iteration: 386020 loss: 0.0018 lr: 0.02\n",
      "iteration: 386030 loss: 0.0015 lr: 0.02\n",
      "iteration: 386040 loss: 0.0018 lr: 0.02\n",
      "iteration: 386050 loss: 0.0029 lr: 0.02\n",
      "iteration: 386060 loss: 0.0012 lr: 0.02\n",
      "iteration: 386070 loss: 0.0011 lr: 0.02\n",
      "iteration: 386080 loss: 0.0013 lr: 0.02\n",
      "iteration: 386090 loss: 0.0020 lr: 0.02\n",
      "iteration: 386100 loss: 0.0014 lr: 0.02\n",
      "iteration: 386110 loss: 0.0012 lr: 0.02\n",
      "iteration: 386120 loss: 0.0021 lr: 0.02\n",
      "iteration: 386130 loss: 0.0013 lr: 0.02\n",
      "iteration: 386140 loss: 0.0013 lr: 0.02\n",
      "iteration: 386150 loss: 0.0018 lr: 0.02\n",
      "iteration: 386160 loss: 0.0024 lr: 0.02\n",
      "iteration: 386170 loss: 0.0014 lr: 0.02\n",
      "iteration: 386180 loss: 0.0015 lr: 0.02\n",
      "iteration: 386190 loss: 0.0012 lr: 0.02\n",
      "iteration: 386200 loss: 0.0024 lr: 0.02\n",
      "iteration: 386210 loss: 0.0019 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 386220 loss: 0.0016 lr: 0.02\n",
      "iteration: 386230 loss: 0.0016 lr: 0.02\n",
      "iteration: 386240 loss: 0.0011 lr: 0.02\n",
      "iteration: 386250 loss: 0.0016 lr: 0.02\n",
      "iteration: 386260 loss: 0.0016 lr: 0.02\n",
      "iteration: 386270 loss: 0.0012 lr: 0.02\n",
      "iteration: 386280 loss: 0.0010 lr: 0.02\n",
      "iteration: 386290 loss: 0.0014 lr: 0.02\n",
      "iteration: 386300 loss: 0.0016 lr: 0.02\n",
      "iteration: 386310 loss: 0.0023 lr: 0.02\n",
      "iteration: 386320 loss: 0.0015 lr: 0.02\n",
      "iteration: 386330 loss: 0.0012 lr: 0.02\n",
      "iteration: 386340 loss: 0.0019 lr: 0.02\n",
      "iteration: 386350 loss: 0.0019 lr: 0.02\n",
      "iteration: 386360 loss: 0.0017 lr: 0.02\n",
      "iteration: 386370 loss: 0.0017 lr: 0.02\n",
      "iteration: 386380 loss: 0.0017 lr: 0.02\n",
      "iteration: 386390 loss: 0.0017 lr: 0.02\n",
      "iteration: 386400 loss: 0.0011 lr: 0.02\n",
      "iteration: 386410 loss: 0.0016 lr: 0.02\n",
      "iteration: 386420 loss: 0.0015 lr: 0.02\n",
      "iteration: 386430 loss: 0.0012 lr: 0.02\n",
      "iteration: 386440 loss: 0.0012 lr: 0.02\n",
      "iteration: 386450 loss: 0.0015 lr: 0.02\n",
      "iteration: 386460 loss: 0.0016 lr: 0.02\n",
      "iteration: 386470 loss: 0.0014 lr: 0.02\n",
      "iteration: 386480 loss: 0.0021 lr: 0.02\n",
      "iteration: 386490 loss: 0.0016 lr: 0.02\n",
      "iteration: 386500 loss: 0.0015 lr: 0.02\n",
      "iteration: 386510 loss: 0.0015 lr: 0.02\n",
      "iteration: 386520 loss: 0.0011 lr: 0.02\n",
      "iteration: 386530 loss: 0.0017 lr: 0.02\n",
      "iteration: 386540 loss: 0.0026 lr: 0.02\n",
      "iteration: 386550 loss: 0.0032 lr: 0.02\n",
      "iteration: 386560 loss: 0.0024 lr: 0.02\n",
      "iteration: 386570 loss: 0.0011 lr: 0.02\n",
      "iteration: 386580 loss: 0.0024 lr: 0.02\n",
      "iteration: 386590 loss: 0.0014 lr: 0.02\n",
      "iteration: 386600 loss: 0.0011 lr: 0.02\n",
      "iteration: 386610 loss: 0.0015 lr: 0.02\n",
      "iteration: 386620 loss: 0.0022 lr: 0.02\n",
      "iteration: 386630 loss: 0.0016 lr: 0.02\n",
      "iteration: 386640 loss: 0.0020 lr: 0.02\n",
      "iteration: 386650 loss: 0.0018 lr: 0.02\n",
      "iteration: 386660 loss: 0.0011 lr: 0.02\n",
      "iteration: 386670 loss: 0.0022 lr: 0.02\n",
      "iteration: 386680 loss: 0.0014 lr: 0.02\n",
      "iteration: 386690 loss: 0.0014 lr: 0.02\n",
      "iteration: 386700 loss: 0.0019 lr: 0.02\n",
      "iteration: 386710 loss: 0.0022 lr: 0.02\n",
      "iteration: 386720 loss: 0.0016 lr: 0.02\n",
      "iteration: 386730 loss: 0.0017 lr: 0.02\n",
      "iteration: 386740 loss: 0.0015 lr: 0.02\n",
      "iteration: 386750 loss: 0.0013 lr: 0.02\n",
      "iteration: 386760 loss: 0.0015 lr: 0.02\n",
      "iteration: 386770 loss: 0.0019 lr: 0.02\n",
      "iteration: 386780 loss: 0.0019 lr: 0.02\n",
      "iteration: 386790 loss: 0.0015 lr: 0.02\n",
      "iteration: 386800 loss: 0.0015 lr: 0.02\n",
      "iteration: 386810 loss: 0.0016 lr: 0.02\n",
      "iteration: 386820 loss: 0.0013 lr: 0.02\n",
      "iteration: 386830 loss: 0.0013 lr: 0.02\n",
      "iteration: 386840 loss: 0.0019 lr: 0.02\n",
      "iteration: 386850 loss: 0.0021 lr: 0.02\n",
      "iteration: 386860 loss: 0.0013 lr: 0.02\n",
      "iteration: 386870 loss: 0.0019 lr: 0.02\n",
      "iteration: 386880 loss: 0.0018 lr: 0.02\n",
      "iteration: 386890 loss: 0.0015 lr: 0.02\n",
      "iteration: 386900 loss: 0.0011 lr: 0.02\n",
      "iteration: 386910 loss: 0.0018 lr: 0.02\n",
      "iteration: 386920 loss: 0.0016 lr: 0.02\n",
      "iteration: 386930 loss: 0.0018 lr: 0.02\n",
      "iteration: 386940 loss: 0.0021 lr: 0.02\n",
      "iteration: 386950 loss: 0.0014 lr: 0.02\n",
      "iteration: 386960 loss: 0.0011 lr: 0.02\n",
      "iteration: 386970 loss: 0.0015 lr: 0.02\n",
      "iteration: 386980 loss: 0.0011 lr: 0.02\n",
      "iteration: 386990 loss: 0.0011 lr: 0.02\n",
      "iteration: 387000 loss: 0.0021 lr: 0.02\n",
      "iteration: 387010 loss: 0.0017 lr: 0.02\n",
      "iteration: 387020 loss: 0.0012 lr: 0.02\n",
      "iteration: 387030 loss: 0.0022 lr: 0.02\n",
      "iteration: 387040 loss: 0.0019 lr: 0.02\n",
      "iteration: 387050 loss: 0.0014 lr: 0.02\n",
      "iteration: 387060 loss: 0.0013 lr: 0.02\n",
      "iteration: 387070 loss: 0.0015 lr: 0.02\n",
      "iteration: 387080 loss: 0.0016 lr: 0.02\n",
      "iteration: 387090 loss: 0.0013 lr: 0.02\n",
      "iteration: 387100 loss: 0.0016 lr: 0.02\n",
      "iteration: 387110 loss: 0.0016 lr: 0.02\n",
      "iteration: 387120 loss: 0.0017 lr: 0.02\n",
      "iteration: 387130 loss: 0.0024 lr: 0.02\n",
      "iteration: 387140 loss: 0.0017 lr: 0.02\n",
      "iteration: 387150 loss: 0.0016 lr: 0.02\n",
      "iteration: 387160 loss: 0.0014 lr: 0.02\n",
      "iteration: 387170 loss: 0.0030 lr: 0.02\n",
      "iteration: 387180 loss: 0.0019 lr: 0.02\n",
      "iteration: 387190 loss: 0.0019 lr: 0.02\n",
      "iteration: 387200 loss: 0.0016 lr: 0.02\n",
      "iteration: 387210 loss: 0.0018 lr: 0.02\n",
      "iteration: 387220 loss: 0.0014 lr: 0.02\n",
      "iteration: 387230 loss: 0.0013 lr: 0.02\n",
      "iteration: 387240 loss: 0.0013 lr: 0.02\n",
      "iteration: 387250 loss: 0.0010 lr: 0.02\n",
      "iteration: 387260 loss: 0.0018 lr: 0.02\n",
      "iteration: 387270 loss: 0.0012 lr: 0.02\n",
      "iteration: 387280 loss: 0.0013 lr: 0.02\n",
      "iteration: 387290 loss: 0.0016 lr: 0.02\n",
      "iteration: 387300 loss: 0.0013 lr: 0.02\n",
      "iteration: 387310 loss: 0.0011 lr: 0.02\n",
      "iteration: 387320 loss: 0.0019 lr: 0.02\n",
      "iteration: 387330 loss: 0.0012 lr: 0.02\n",
      "iteration: 387340 loss: 0.0017 lr: 0.02\n",
      "iteration: 387350 loss: 0.0016 lr: 0.02\n",
      "iteration: 387360 loss: 0.0019 lr: 0.02\n",
      "iteration: 387370 loss: 0.0016 lr: 0.02\n",
      "iteration: 387380 loss: 0.0018 lr: 0.02\n",
      "iteration: 387390 loss: 0.0012 lr: 0.02\n",
      "iteration: 387400 loss: 0.0016 lr: 0.02\n",
      "iteration: 387410 loss: 0.0022 lr: 0.02\n",
      "iteration: 387420 loss: 0.0021 lr: 0.02\n",
      "iteration: 387430 loss: 0.0015 lr: 0.02\n",
      "iteration: 387440 loss: 0.0017 lr: 0.02\n",
      "iteration: 387450 loss: 0.0016 lr: 0.02\n",
      "iteration: 387460 loss: 0.0011 lr: 0.02\n",
      "iteration: 387470 loss: 0.0012 lr: 0.02\n",
      "iteration: 387480 loss: 0.0016 lr: 0.02\n",
      "iteration: 387490 loss: 0.0012 lr: 0.02\n",
      "iteration: 387500 loss: 0.0016 lr: 0.02\n",
      "iteration: 387510 loss: 0.0015 lr: 0.02\n",
      "iteration: 387520 loss: 0.0014 lr: 0.02\n",
      "iteration: 387530 loss: 0.0017 lr: 0.02\n",
      "iteration: 387540 loss: 0.0018 lr: 0.02\n",
      "iteration: 387550 loss: 0.0014 lr: 0.02\n",
      "iteration: 387560 loss: 0.0016 lr: 0.02\n",
      "iteration: 387570 loss: 0.0013 lr: 0.02\n",
      "iteration: 387580 loss: 0.0015 lr: 0.02\n",
      "iteration: 387590 loss: 0.0020 lr: 0.02\n",
      "iteration: 387600 loss: 0.0015 lr: 0.02\n",
      "iteration: 387610 loss: 0.0019 lr: 0.02\n",
      "iteration: 387620 loss: 0.0012 lr: 0.02\n",
      "iteration: 387630 loss: 0.0015 lr: 0.02\n",
      "iteration: 387640 loss: 0.0015 lr: 0.02\n",
      "iteration: 387650 loss: 0.0014 lr: 0.02\n",
      "iteration: 387660 loss: 0.0017 lr: 0.02\n",
      "iteration: 387670 loss: 0.0013 lr: 0.02\n",
      "iteration: 387680 loss: 0.0012 lr: 0.02\n",
      "iteration: 387690 loss: 0.0025 lr: 0.02\n",
      "iteration: 387700 loss: 0.0012 lr: 0.02\n",
      "iteration: 387710 loss: 0.0016 lr: 0.02\n",
      "iteration: 387720 loss: 0.0015 lr: 0.02\n",
      "iteration: 387730 loss: 0.0019 lr: 0.02\n",
      "iteration: 387740 loss: 0.0014 lr: 0.02\n",
      "iteration: 387750 loss: 0.0014 lr: 0.02\n",
      "iteration: 387760 loss: 0.0012 lr: 0.02\n",
      "iteration: 387770 loss: 0.0017 lr: 0.02\n",
      "iteration: 387780 loss: 0.0020 lr: 0.02\n",
      "iteration: 387790 loss: 0.0023 lr: 0.02\n",
      "iteration: 387800 loss: 0.0013 lr: 0.02\n",
      "iteration: 387810 loss: 0.0017 lr: 0.02\n",
      "iteration: 387820 loss: 0.0012 lr: 0.02\n",
      "iteration: 387830 loss: 0.0016 lr: 0.02\n",
      "iteration: 387840 loss: 0.0020 lr: 0.02\n",
      "iteration: 387850 loss: 0.0012 lr: 0.02\n",
      "iteration: 387860 loss: 0.0015 lr: 0.02\n",
      "iteration: 387870 loss: 0.0018 lr: 0.02\n",
      "iteration: 387880 loss: 0.0015 lr: 0.02\n",
      "iteration: 387890 loss: 0.0015 lr: 0.02\n",
      "iteration: 387900 loss: 0.0012 lr: 0.02\n",
      "iteration: 387910 loss: 0.0014 lr: 0.02\n",
      "iteration: 387920 loss: 0.0019 lr: 0.02\n",
      "iteration: 387930 loss: 0.0014 lr: 0.02\n",
      "iteration: 387940 loss: 0.0015 lr: 0.02\n",
      "iteration: 387950 loss: 0.0022 lr: 0.02\n",
      "iteration: 387960 loss: 0.0012 lr: 0.02\n",
      "iteration: 387970 loss: 0.0016 lr: 0.02\n",
      "iteration: 387980 loss: 0.0019 lr: 0.02\n",
      "iteration: 387990 loss: 0.0017 lr: 0.02\n",
      "iteration: 388000 loss: 0.0017 lr: 0.02\n",
      "iteration: 388010 loss: 0.0015 lr: 0.02\n",
      "iteration: 388020 loss: 0.0015 lr: 0.02\n",
      "iteration: 388030 loss: 0.0018 lr: 0.02\n",
      "iteration: 388040 loss: 0.0013 lr: 0.02\n",
      "iteration: 388050 loss: 0.0014 lr: 0.02\n",
      "iteration: 388060 loss: 0.0020 lr: 0.02\n",
      "iteration: 388070 loss: 0.0016 lr: 0.02\n",
      "iteration: 388080 loss: 0.0014 lr: 0.02\n",
      "iteration: 388090 loss: 0.0012 lr: 0.02\n",
      "iteration: 388100 loss: 0.0012 lr: 0.02\n",
      "iteration: 388110 loss: 0.0014 lr: 0.02\n",
      "iteration: 388120 loss: 0.0015 lr: 0.02\n",
      "iteration: 388130 loss: 0.0014 lr: 0.02\n",
      "iteration: 388140 loss: 0.0021 lr: 0.02\n",
      "iteration: 388150 loss: 0.0015 lr: 0.02\n",
      "iteration: 388160 loss: 0.0016 lr: 0.02\n",
      "iteration: 388170 loss: 0.0036 lr: 0.02\n",
      "iteration: 388180 loss: 0.0037 lr: 0.02\n",
      "iteration: 388190 loss: 0.0022 lr: 0.02\n",
      "iteration: 388200 loss: 0.0023 lr: 0.02\n",
      "iteration: 388210 loss: 0.0020 lr: 0.02\n",
      "iteration: 388220 loss: 0.0014 lr: 0.02\n",
      "iteration: 388230 loss: 0.0013 lr: 0.02\n",
      "iteration: 388240 loss: 0.0015 lr: 0.02\n",
      "iteration: 388250 loss: 0.0016 lr: 0.02\n",
      "iteration: 388260 loss: 0.0022 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 388270 loss: 0.0016 lr: 0.02\n",
      "iteration: 388280 loss: 0.0018 lr: 0.02\n",
      "iteration: 388290 loss: 0.0013 lr: 0.02\n",
      "iteration: 388300 loss: 0.0014 lr: 0.02\n",
      "iteration: 388310 loss: 0.0015 lr: 0.02\n",
      "iteration: 388320 loss: 0.0016 lr: 0.02\n",
      "iteration: 388330 loss: 0.0014 lr: 0.02\n",
      "iteration: 388340 loss: 0.0018 lr: 0.02\n",
      "iteration: 388350 loss: 0.0020 lr: 0.02\n",
      "iteration: 388360 loss: 0.0010 lr: 0.02\n",
      "iteration: 388370 loss: 0.0018 lr: 0.02\n",
      "iteration: 388380 loss: 0.0015 lr: 0.02\n",
      "iteration: 388390 loss: 0.0020 lr: 0.02\n",
      "iteration: 388400 loss: 0.0016 lr: 0.02\n",
      "iteration: 388410 loss: 0.0013 lr: 0.02\n",
      "iteration: 388420 loss: 0.0017 lr: 0.02\n",
      "iteration: 388430 loss: 0.0020 lr: 0.02\n",
      "iteration: 388440 loss: 0.0018 lr: 0.02\n",
      "iteration: 388450 loss: 0.0020 lr: 0.02\n",
      "iteration: 388460 loss: 0.0016 lr: 0.02\n",
      "iteration: 388470 loss: 0.0022 lr: 0.02\n",
      "iteration: 388480 loss: 0.0017 lr: 0.02\n",
      "iteration: 388490 loss: 0.0007 lr: 0.02\n",
      "iteration: 388500 loss: 0.0017 lr: 0.02\n",
      "iteration: 388510 loss: 0.0012 lr: 0.02\n",
      "iteration: 388520 loss: 0.0016 lr: 0.02\n",
      "iteration: 388530 loss: 0.0013 lr: 0.02\n",
      "iteration: 388540 loss: 0.0018 lr: 0.02\n",
      "iteration: 388550 loss: 0.0015 lr: 0.02\n",
      "iteration: 388560 loss: 0.0017 lr: 0.02\n",
      "iteration: 388570 loss: 0.0017 lr: 0.02\n",
      "iteration: 388580 loss: 0.0025 lr: 0.02\n",
      "iteration: 388590 loss: 0.0013 lr: 0.02\n",
      "iteration: 388600 loss: 0.0014 lr: 0.02\n",
      "iteration: 388610 loss: 0.0016 lr: 0.02\n",
      "iteration: 388620 loss: 0.0014 lr: 0.02\n",
      "iteration: 388630 loss: 0.0014 lr: 0.02\n",
      "iteration: 388640 loss: 0.0018 lr: 0.02\n",
      "iteration: 388650 loss: 0.0017 lr: 0.02\n",
      "iteration: 388660 loss: 0.0016 lr: 0.02\n",
      "iteration: 388670 loss: 0.0025 lr: 0.02\n",
      "iteration: 388680 loss: 0.0015 lr: 0.02\n",
      "iteration: 388690 loss: 0.0016 lr: 0.02\n",
      "iteration: 388700 loss: 0.0020 lr: 0.02\n",
      "iteration: 388710 loss: 0.0021 lr: 0.02\n",
      "iteration: 388720 loss: 0.0013 lr: 0.02\n",
      "iteration: 388730 loss: 0.0017 lr: 0.02\n",
      "iteration: 388740 loss: 0.0016 lr: 0.02\n",
      "iteration: 388750 loss: 0.0011 lr: 0.02\n",
      "iteration: 388760 loss: 0.0015 lr: 0.02\n",
      "iteration: 388770 loss: 0.0012 lr: 0.02\n",
      "iteration: 388780 loss: 0.0016 lr: 0.02\n",
      "iteration: 388790 loss: 0.0015 lr: 0.02\n",
      "iteration: 388800 loss: 0.0015 lr: 0.02\n",
      "iteration: 388810 loss: 0.0012 lr: 0.02\n",
      "iteration: 388820 loss: 0.0013 lr: 0.02\n",
      "iteration: 388830 loss: 0.0018 lr: 0.02\n",
      "iteration: 388840 loss: 0.0017 lr: 0.02\n",
      "iteration: 388850 loss: 0.0014 lr: 0.02\n",
      "iteration: 388860 loss: 0.0015 lr: 0.02\n",
      "iteration: 388870 loss: 0.0016 lr: 0.02\n",
      "iteration: 388880 loss: 0.0015 lr: 0.02\n",
      "iteration: 388890 loss: 0.0019 lr: 0.02\n",
      "iteration: 388900 loss: 0.0020 lr: 0.02\n",
      "iteration: 388910 loss: 0.0013 lr: 0.02\n",
      "iteration: 388920 loss: 0.0016 lr: 0.02\n",
      "iteration: 388930 loss: 0.0016 lr: 0.02\n",
      "iteration: 388940 loss: 0.0015 lr: 0.02\n",
      "iteration: 388950 loss: 0.0015 lr: 0.02\n",
      "iteration: 388960 loss: 0.0013 lr: 0.02\n",
      "iteration: 388970 loss: 0.0011 lr: 0.02\n",
      "iteration: 388980 loss: 0.0026 lr: 0.02\n",
      "iteration: 388990 loss: 0.0023 lr: 0.02\n",
      "iteration: 389000 loss: 0.0015 lr: 0.02\n",
      "iteration: 389010 loss: 0.0015 lr: 0.02\n",
      "iteration: 389020 loss: 0.0012 lr: 0.02\n",
      "iteration: 389030 loss: 0.0016 lr: 0.02\n",
      "iteration: 389040 loss: 0.0011 lr: 0.02\n",
      "iteration: 389050 loss: 0.0016 lr: 0.02\n",
      "iteration: 389060 loss: 0.0018 lr: 0.02\n",
      "iteration: 389070 loss: 0.0016 lr: 0.02\n",
      "iteration: 389080 loss: 0.0014 lr: 0.02\n",
      "iteration: 389090 loss: 0.0022 lr: 0.02\n",
      "iteration: 389100 loss: 0.0023 lr: 0.02\n",
      "iteration: 389110 loss: 0.0011 lr: 0.02\n",
      "iteration: 389120 loss: 0.0025 lr: 0.02\n",
      "iteration: 389130 loss: 0.0018 lr: 0.02\n",
      "iteration: 389140 loss: 0.0011 lr: 0.02\n",
      "iteration: 389150 loss: 0.0010 lr: 0.02\n",
      "iteration: 389160 loss: 0.0015 lr: 0.02\n",
      "iteration: 389170 loss: 0.0014 lr: 0.02\n",
      "iteration: 389180 loss: 0.0014 lr: 0.02\n",
      "iteration: 389190 loss: 0.0014 lr: 0.02\n",
      "iteration: 389200 loss: 0.0016 lr: 0.02\n",
      "iteration: 389210 loss: 0.0016 lr: 0.02\n",
      "iteration: 389220 loss: 0.0014 lr: 0.02\n",
      "iteration: 389230 loss: 0.0017 lr: 0.02\n",
      "iteration: 389240 loss: 0.0017 lr: 0.02\n",
      "iteration: 389250 loss: 0.0013 lr: 0.02\n",
      "iteration: 389260 loss: 0.0024 lr: 0.02\n",
      "iteration: 389270 loss: 0.0035 lr: 0.02\n",
      "iteration: 389280 loss: 0.0017 lr: 0.02\n",
      "iteration: 389290 loss: 0.0020 lr: 0.02\n",
      "iteration: 389300 loss: 0.0018 lr: 0.02\n",
      "iteration: 389310 loss: 0.0013 lr: 0.02\n",
      "iteration: 389320 loss: 0.0014 lr: 0.02\n",
      "iteration: 389330 loss: 0.0015 lr: 0.02\n",
      "iteration: 389340 loss: 0.0017 lr: 0.02\n",
      "iteration: 389350 loss: 0.0016 lr: 0.02\n",
      "iteration: 389360 loss: 0.0013 lr: 0.02\n",
      "iteration: 389370 loss: 0.0016 lr: 0.02\n",
      "iteration: 389380 loss: 0.0013 lr: 0.02\n",
      "iteration: 389390 loss: 0.0012 lr: 0.02\n",
      "iteration: 389400 loss: 0.0013 lr: 0.02\n",
      "iteration: 389410 loss: 0.0014 lr: 0.02\n",
      "iteration: 389420 loss: 0.0014 lr: 0.02\n",
      "iteration: 389430 loss: 0.0014 lr: 0.02\n",
      "iteration: 389440 loss: 0.0022 lr: 0.02\n",
      "iteration: 389450 loss: 0.0020 lr: 0.02\n",
      "iteration: 389460 loss: 0.0018 lr: 0.02\n",
      "iteration: 389470 loss: 0.0027 lr: 0.02\n",
      "iteration: 389480 loss: 0.0017 lr: 0.02\n",
      "iteration: 389490 loss: 0.0012 lr: 0.02\n",
      "iteration: 389500 loss: 0.0015 lr: 0.02\n",
      "iteration: 389510 loss: 0.0017 lr: 0.02\n",
      "iteration: 389520 loss: 0.0013 lr: 0.02\n",
      "iteration: 389530 loss: 0.0016 lr: 0.02\n",
      "iteration: 389540 loss: 0.0017 lr: 0.02\n",
      "iteration: 389550 loss: 0.0021 lr: 0.02\n",
      "iteration: 389560 loss: 0.0023 lr: 0.02\n",
      "iteration: 389570 loss: 0.0016 lr: 0.02\n",
      "iteration: 389580 loss: 0.0011 lr: 0.02\n",
      "iteration: 389590 loss: 0.0028 lr: 0.02\n",
      "iteration: 389600 loss: 0.0019 lr: 0.02\n",
      "iteration: 389610 loss: 0.0016 lr: 0.02\n",
      "iteration: 389620 loss: 0.0009 lr: 0.02\n",
      "iteration: 389630 loss: 0.0016 lr: 0.02\n",
      "iteration: 389640 loss: 0.0030 lr: 0.02\n",
      "iteration: 389650 loss: 0.0016 lr: 0.02\n",
      "iteration: 389660 loss: 0.0013 lr: 0.02\n",
      "iteration: 389670 loss: 0.0032 lr: 0.02\n",
      "iteration: 389680 loss: 0.0017 lr: 0.02\n",
      "iteration: 389690 loss: 0.0021 lr: 0.02\n",
      "iteration: 389700 loss: 0.0012 lr: 0.02\n",
      "iteration: 389710 loss: 0.0011 lr: 0.02\n",
      "iteration: 389720 loss: 0.0011 lr: 0.02\n",
      "iteration: 389730 loss: 0.0013 lr: 0.02\n",
      "iteration: 389740 loss: 0.0014 lr: 0.02\n",
      "iteration: 389750 loss: 0.0015 lr: 0.02\n",
      "iteration: 389760 loss: 0.0015 lr: 0.02\n",
      "iteration: 389770 loss: 0.0014 lr: 0.02\n",
      "iteration: 389780 loss: 0.0015 lr: 0.02\n",
      "iteration: 389790 loss: 0.0011 lr: 0.02\n",
      "iteration: 389800 loss: 0.0012 lr: 0.02\n",
      "iteration: 389810 loss: 0.0009 lr: 0.02\n",
      "iteration: 389820 loss: 0.0011 lr: 0.02\n",
      "iteration: 389830 loss: 0.0019 lr: 0.02\n",
      "iteration: 389840 loss: 0.0014 lr: 0.02\n",
      "iteration: 389850 loss: 0.0016 lr: 0.02\n",
      "iteration: 389860 loss: 0.0016 lr: 0.02\n",
      "iteration: 389870 loss: 0.0014 lr: 0.02\n",
      "iteration: 389880 loss: 0.0013 lr: 0.02\n",
      "iteration: 389890 loss: 0.0017 lr: 0.02\n",
      "iteration: 389900 loss: 0.0013 lr: 0.02\n",
      "iteration: 389910 loss: 0.0014 lr: 0.02\n",
      "iteration: 389920 loss: 0.0012 lr: 0.02\n",
      "iteration: 389930 loss: 0.0014 lr: 0.02\n",
      "iteration: 389940 loss: 0.0018 lr: 0.02\n",
      "iteration: 389950 loss: 0.0013 lr: 0.02\n",
      "iteration: 389960 loss: 0.0022 lr: 0.02\n",
      "iteration: 389970 loss: 0.0022 lr: 0.02\n",
      "iteration: 389980 loss: 0.0021 lr: 0.02\n",
      "iteration: 389990 loss: 0.0024 lr: 0.02\n",
      "iteration: 390000 loss: 0.0025 lr: 0.02\n",
      "iteration: 390010 loss: 0.0025 lr: 0.02\n",
      "iteration: 390020 loss: 0.0023 lr: 0.02\n",
      "iteration: 390030 loss: 0.0018 lr: 0.02\n",
      "iteration: 390040 loss: 0.0017 lr: 0.02\n",
      "iteration: 390050 loss: 0.0021 lr: 0.02\n",
      "iteration: 390060 loss: 0.0015 lr: 0.02\n",
      "iteration: 390070 loss: 0.0022 lr: 0.02\n",
      "iteration: 390080 loss: 0.0020 lr: 0.02\n",
      "iteration: 390090 loss: 0.0016 lr: 0.02\n",
      "iteration: 390100 loss: 0.0022 lr: 0.02\n",
      "iteration: 390110 loss: 0.0013 lr: 0.02\n",
      "iteration: 390120 loss: 0.0014 lr: 0.02\n",
      "iteration: 390130 loss: 0.0015 lr: 0.02\n",
      "iteration: 390140 loss: 0.0017 lr: 0.02\n",
      "iteration: 390150 loss: 0.0015 lr: 0.02\n",
      "iteration: 390160 loss: 0.0014 lr: 0.02\n",
      "iteration: 390170 loss: 0.0015 lr: 0.02\n",
      "iteration: 390180 loss: 0.0017 lr: 0.02\n",
      "iteration: 390190 loss: 0.0019 lr: 0.02\n",
      "iteration: 390200 loss: 0.0019 lr: 0.02\n",
      "iteration: 390210 loss: 0.0018 lr: 0.02\n",
      "iteration: 390220 loss: 0.0010 lr: 0.02\n",
      "iteration: 390230 loss: 0.0019 lr: 0.02\n",
      "iteration: 390240 loss: 0.0015 lr: 0.02\n",
      "iteration: 390250 loss: 0.0020 lr: 0.02\n",
      "iteration: 390260 loss: 0.0011 lr: 0.02\n",
      "iteration: 390270 loss: 0.0012 lr: 0.02\n",
      "iteration: 390280 loss: 0.0013 lr: 0.02\n",
      "iteration: 390290 loss: 0.0014 lr: 0.02\n",
      "iteration: 390300 loss: 0.0024 lr: 0.02\n",
      "iteration: 390310 loss: 0.0018 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 390320 loss: 0.0015 lr: 0.02\n",
      "iteration: 390330 loss: 0.0021 lr: 0.02\n",
      "iteration: 390340 loss: 0.0023 lr: 0.02\n",
      "iteration: 390350 loss: 0.0022 lr: 0.02\n",
      "iteration: 390360 loss: 0.0012 lr: 0.02\n",
      "iteration: 390370 loss: 0.0015 lr: 0.02\n",
      "iteration: 390380 loss: 0.0020 lr: 0.02\n",
      "iteration: 390390 loss: 0.0018 lr: 0.02\n",
      "iteration: 390400 loss: 0.0019 lr: 0.02\n",
      "iteration: 390410 loss: 0.0011 lr: 0.02\n",
      "iteration: 390420 loss: 0.0012 lr: 0.02\n",
      "iteration: 390430 loss: 0.0013 lr: 0.02\n",
      "iteration: 390440 loss: 0.0025 lr: 0.02\n",
      "iteration: 390450 loss: 0.0022 lr: 0.02\n",
      "iteration: 390460 loss: 0.0013 lr: 0.02\n",
      "iteration: 390470 loss: 0.0017 lr: 0.02\n",
      "iteration: 390480 loss: 0.0010 lr: 0.02\n",
      "iteration: 390490 loss: 0.0013 lr: 0.02\n",
      "iteration: 390500 loss: 0.0014 lr: 0.02\n",
      "iteration: 390510 loss: 0.0015 lr: 0.02\n",
      "iteration: 390520 loss: 0.0015 lr: 0.02\n",
      "iteration: 390530 loss: 0.0023 lr: 0.02\n",
      "iteration: 390540 loss: 0.0013 lr: 0.02\n",
      "iteration: 390550 loss: 0.0025 lr: 0.02\n",
      "iteration: 390560 loss: 0.0017 lr: 0.02\n",
      "iteration: 390570 loss: 0.0016 lr: 0.02\n",
      "iteration: 390580 loss: 0.0018 lr: 0.02\n",
      "iteration: 390590 loss: 0.0019 lr: 0.02\n",
      "iteration: 390600 loss: 0.0018 lr: 0.02\n",
      "iteration: 390610 loss: 0.0017 lr: 0.02\n",
      "iteration: 390620 loss: 0.0018 lr: 0.02\n",
      "iteration: 390630 loss: 0.0015 lr: 0.02\n",
      "iteration: 390640 loss: 0.0012 lr: 0.02\n",
      "iteration: 390650 loss: 0.0014 lr: 0.02\n",
      "iteration: 390660 loss: 0.0016 lr: 0.02\n",
      "iteration: 390670 loss: 0.0010 lr: 0.02\n",
      "iteration: 390680 loss: 0.0012 lr: 0.02\n",
      "iteration: 390690 loss: 0.0016 lr: 0.02\n",
      "iteration: 390700 loss: 0.0020 lr: 0.02\n",
      "iteration: 390710 loss: 0.0016 lr: 0.02\n",
      "iteration: 390720 loss: 0.0019 lr: 0.02\n",
      "iteration: 390730 loss: 0.0011 lr: 0.02\n",
      "iteration: 390740 loss: 0.0022 lr: 0.02\n",
      "iteration: 390750 loss: 0.0020 lr: 0.02\n",
      "iteration: 390760 loss: 0.0013 lr: 0.02\n",
      "iteration: 390770 loss: 0.0014 lr: 0.02\n",
      "iteration: 390780 loss: 0.0016 lr: 0.02\n",
      "iteration: 390790 loss: 0.0013 lr: 0.02\n",
      "iteration: 390800 loss: 0.0016 lr: 0.02\n",
      "iteration: 390810 loss: 0.0015 lr: 0.02\n",
      "iteration: 390820 loss: 0.0018 lr: 0.02\n",
      "iteration: 390830 loss: 0.0015 lr: 0.02\n",
      "iteration: 390840 loss: 0.0015 lr: 0.02\n",
      "iteration: 390850 loss: 0.0016 lr: 0.02\n",
      "iteration: 390860 loss: 0.0017 lr: 0.02\n",
      "iteration: 390870 loss: 0.0015 lr: 0.02\n",
      "iteration: 390880 loss: 0.0016 lr: 0.02\n",
      "iteration: 390890 loss: 0.0011 lr: 0.02\n",
      "iteration: 390900 loss: 0.0016 lr: 0.02\n",
      "iteration: 390910 loss: 0.0013 lr: 0.02\n",
      "iteration: 390920 loss: 0.0023 lr: 0.02\n",
      "iteration: 390930 loss: 0.0019 lr: 0.02\n",
      "iteration: 390940 loss: 0.0013 lr: 0.02\n",
      "iteration: 390950 loss: 0.0015 lr: 0.02\n",
      "iteration: 390960 loss: 0.0014 lr: 0.02\n",
      "iteration: 390970 loss: 0.0019 lr: 0.02\n",
      "iteration: 390980 loss: 0.0025 lr: 0.02\n",
      "iteration: 390990 loss: 0.0018 lr: 0.02\n",
      "iteration: 391000 loss: 0.0018 lr: 0.02\n",
      "iteration: 391010 loss: 0.0014 lr: 0.02\n",
      "iteration: 391020 loss: 0.0017 lr: 0.02\n",
      "iteration: 391030 loss: 0.0011 lr: 0.02\n",
      "iteration: 391040 loss: 0.0017 lr: 0.02\n",
      "iteration: 391050 loss: 0.0015 lr: 0.02\n",
      "iteration: 391060 loss: 0.0016 lr: 0.02\n",
      "iteration: 391070 loss: 0.0015 lr: 0.02\n",
      "iteration: 391080 loss: 0.0018 lr: 0.02\n",
      "iteration: 391090 loss: 0.0013 lr: 0.02\n",
      "iteration: 391100 loss: 0.0013 lr: 0.02\n",
      "iteration: 391110 loss: 0.0013 lr: 0.02\n",
      "iteration: 391120 loss: 0.0021 lr: 0.02\n",
      "iteration: 391130 loss: 0.0015 lr: 0.02\n",
      "iteration: 391140 loss: 0.0016 lr: 0.02\n",
      "iteration: 391150 loss: 0.0012 lr: 0.02\n",
      "iteration: 391160 loss: 0.0015 lr: 0.02\n",
      "iteration: 391170 loss: 0.0017 lr: 0.02\n",
      "iteration: 391180 loss: 0.0019 lr: 0.02\n",
      "iteration: 391190 loss: 0.0022 lr: 0.02\n",
      "iteration: 391200 loss: 0.0020 lr: 0.02\n",
      "iteration: 391210 loss: 0.0038 lr: 0.02\n",
      "iteration: 391220 loss: 0.0013 lr: 0.02\n",
      "iteration: 391230 loss: 0.0016 lr: 0.02\n",
      "iteration: 391240 loss: 0.0016 lr: 0.02\n",
      "iteration: 391250 loss: 0.0020 lr: 0.02\n",
      "iteration: 391260 loss: 0.0014 lr: 0.02\n",
      "iteration: 391270 loss: 0.0016 lr: 0.02\n",
      "iteration: 391280 loss: 0.0020 lr: 0.02\n",
      "iteration: 391290 loss: 0.0016 lr: 0.02\n",
      "iteration: 391300 loss: 0.0015 lr: 0.02\n",
      "iteration: 391310 loss: 0.0023 lr: 0.02\n",
      "iteration: 391320 loss: 0.0016 lr: 0.02\n",
      "iteration: 391330 loss: 0.0017 lr: 0.02\n",
      "iteration: 391340 loss: 0.0016 lr: 0.02\n",
      "iteration: 391350 loss: 0.0012 lr: 0.02\n",
      "iteration: 391360 loss: 0.0017 lr: 0.02\n",
      "iteration: 391370 loss: 0.0010 lr: 0.02\n",
      "iteration: 391380 loss: 0.0013 lr: 0.02\n",
      "iteration: 391390 loss: 0.0020 lr: 0.02\n",
      "iteration: 391400 loss: 0.0013 lr: 0.02\n",
      "iteration: 391410 loss: 0.0014 lr: 0.02\n",
      "iteration: 391420 loss: 0.0017 lr: 0.02\n",
      "iteration: 391430 loss: 0.0014 lr: 0.02\n",
      "iteration: 391440 loss: 0.0015 lr: 0.02\n",
      "iteration: 391450 loss: 0.0013 lr: 0.02\n",
      "iteration: 391460 loss: 0.0017 lr: 0.02\n",
      "iteration: 391470 loss: 0.0013 lr: 0.02\n",
      "iteration: 391480 loss: 0.0017 lr: 0.02\n",
      "iteration: 391490 loss: 0.0015 lr: 0.02\n",
      "iteration: 391500 loss: 0.0015 lr: 0.02\n",
      "iteration: 391510 loss: 0.0014 lr: 0.02\n",
      "iteration: 391520 loss: 0.0016 lr: 0.02\n",
      "iteration: 391530 loss: 0.0018 lr: 0.02\n",
      "iteration: 391540 loss: 0.0018 lr: 0.02\n",
      "iteration: 391550 loss: 0.0012 lr: 0.02\n",
      "iteration: 391560 loss: 0.0018 lr: 0.02\n",
      "iteration: 391570 loss: 0.0019 lr: 0.02\n",
      "iteration: 391580 loss: 0.0024 lr: 0.02\n",
      "iteration: 391590 loss: 0.0018 lr: 0.02\n",
      "iteration: 391600 loss: 0.0020 lr: 0.02\n",
      "iteration: 391610 loss: 0.0013 lr: 0.02\n",
      "iteration: 391620 loss: 0.0017 lr: 0.02\n",
      "iteration: 391630 loss: 0.0018 lr: 0.02\n",
      "iteration: 391640 loss: 0.0019 lr: 0.02\n",
      "iteration: 391650 loss: 0.0022 lr: 0.02\n",
      "iteration: 391660 loss: 0.0025 lr: 0.02\n",
      "iteration: 391670 loss: 0.0022 lr: 0.02\n",
      "iteration: 391680 loss: 0.0018 lr: 0.02\n",
      "iteration: 391690 loss: 0.0017 lr: 0.02\n",
      "iteration: 391700 loss: 0.0011 lr: 0.02\n",
      "iteration: 391710 loss: 0.0011 lr: 0.02\n",
      "iteration: 391720 loss: 0.0019 lr: 0.02\n",
      "iteration: 391730 loss: 0.0014 lr: 0.02\n",
      "iteration: 391740 loss: 0.0016 lr: 0.02\n",
      "iteration: 391750 loss: 0.0014 lr: 0.02\n",
      "iteration: 391760 loss: 0.0015 lr: 0.02\n",
      "iteration: 391770 loss: 0.0012 lr: 0.02\n",
      "iteration: 391780 loss: 0.0018 lr: 0.02\n",
      "iteration: 391790 loss: 0.0014 lr: 0.02\n",
      "iteration: 391800 loss: 0.0017 lr: 0.02\n",
      "iteration: 391810 loss: 0.0015 lr: 0.02\n",
      "iteration: 391820 loss: 0.0018 lr: 0.02\n",
      "iteration: 391830 loss: 0.0019 lr: 0.02\n",
      "iteration: 391840 loss: 0.0018 lr: 0.02\n",
      "iteration: 391850 loss: 0.0017 lr: 0.02\n",
      "iteration: 391860 loss: 0.0010 lr: 0.02\n",
      "iteration: 391870 loss: 0.0013 lr: 0.02\n",
      "iteration: 391880 loss: 0.0030 lr: 0.02\n",
      "iteration: 391890 loss: 0.0015 lr: 0.02\n",
      "iteration: 391900 loss: 0.0013 lr: 0.02\n",
      "iteration: 391910 loss: 0.0015 lr: 0.02\n",
      "iteration: 391920 loss: 0.0011 lr: 0.02\n",
      "iteration: 391930 loss: 0.0015 lr: 0.02\n",
      "iteration: 391940 loss: 0.0024 lr: 0.02\n",
      "iteration: 391950 loss: 0.0012 lr: 0.02\n",
      "iteration: 391960 loss: 0.0017 lr: 0.02\n",
      "iteration: 391970 loss: 0.0011 lr: 0.02\n",
      "iteration: 391980 loss: 0.0024 lr: 0.02\n",
      "iteration: 391990 loss: 0.0019 lr: 0.02\n",
      "iteration: 392000 loss: 0.0020 lr: 0.02\n",
      "iteration: 392010 loss: 0.0010 lr: 0.02\n",
      "iteration: 392020 loss: 0.0012 lr: 0.02\n",
      "iteration: 392030 loss: 0.0014 lr: 0.02\n",
      "iteration: 392040 loss: 0.0014 lr: 0.02\n",
      "iteration: 392050 loss: 0.0011 lr: 0.02\n",
      "iteration: 392060 loss: 0.0016 lr: 0.02\n",
      "iteration: 392070 loss: 0.0015 lr: 0.02\n",
      "iteration: 392080 loss: 0.0016 lr: 0.02\n",
      "iteration: 392090 loss: 0.0017 lr: 0.02\n",
      "iteration: 392100 loss: 0.0015 lr: 0.02\n",
      "iteration: 392110 loss: 0.0016 lr: 0.02\n",
      "iteration: 392120 loss: 0.0015 lr: 0.02\n",
      "iteration: 392130 loss: 0.0022 lr: 0.02\n",
      "iteration: 392140 loss: 0.0012 lr: 0.02\n",
      "iteration: 392150 loss: 0.0025 lr: 0.02\n",
      "iteration: 392160 loss: 0.0024 lr: 0.02\n",
      "iteration: 392170 loss: 0.0014 lr: 0.02\n",
      "iteration: 392180 loss: 0.0017 lr: 0.02\n",
      "iteration: 392190 loss: 0.0015 lr: 0.02\n",
      "iteration: 392200 loss: 0.0019 lr: 0.02\n",
      "iteration: 392210 loss: 0.0012 lr: 0.02\n",
      "iteration: 392220 loss: 0.0016 lr: 0.02\n",
      "iteration: 392230 loss: 0.0019 lr: 0.02\n",
      "iteration: 392240 loss: 0.0019 lr: 0.02\n",
      "iteration: 392250 loss: 0.0018 lr: 0.02\n",
      "iteration: 392260 loss: 0.0020 lr: 0.02\n",
      "iteration: 392270 loss: 0.0018 lr: 0.02\n",
      "iteration: 392280 loss: 0.0016 lr: 0.02\n",
      "iteration: 392290 loss: 0.0012 lr: 0.02\n",
      "iteration: 392300 loss: 0.0020 lr: 0.02\n",
      "iteration: 392310 loss: 0.0014 lr: 0.02\n",
      "iteration: 392320 loss: 0.0014 lr: 0.02\n",
      "iteration: 392330 loss: 0.0013 lr: 0.02\n",
      "iteration: 392340 loss: 0.0027 lr: 0.02\n",
      "iteration: 392350 loss: 0.0014 lr: 0.02\n",
      "iteration: 392360 loss: 0.0017 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 392370 loss: 0.0023 lr: 0.02\n",
      "iteration: 392380 loss: 0.0014 lr: 0.02\n",
      "iteration: 392390 loss: 0.0015 lr: 0.02\n",
      "iteration: 392400 loss: 0.0018 lr: 0.02\n",
      "iteration: 392410 loss: 0.0019 lr: 0.02\n",
      "iteration: 392420 loss: 0.0018 lr: 0.02\n",
      "iteration: 392430 loss: 0.0022 lr: 0.02\n",
      "iteration: 392440 loss: 0.0019 lr: 0.02\n",
      "iteration: 392450 loss: 0.0016 lr: 0.02\n",
      "iteration: 392460 loss: 0.0017 lr: 0.02\n",
      "iteration: 392470 loss: 0.0015 lr: 0.02\n",
      "iteration: 392480 loss: 0.0028 lr: 0.02\n",
      "iteration: 392490 loss: 0.0019 lr: 0.02\n",
      "iteration: 392500 loss: 0.0017 lr: 0.02\n",
      "iteration: 392510 loss: 0.0015 lr: 0.02\n",
      "iteration: 392520 loss: 0.0016 lr: 0.02\n",
      "iteration: 392530 loss: 0.0020 lr: 0.02\n",
      "iteration: 392540 loss: 0.0014 lr: 0.02\n",
      "iteration: 392550 loss: 0.0015 lr: 0.02\n",
      "iteration: 392560 loss: 0.0018 lr: 0.02\n",
      "iteration: 392570 loss: 0.0017 lr: 0.02\n",
      "iteration: 392580 loss: 0.0014 lr: 0.02\n",
      "iteration: 392590 loss: 0.0017 lr: 0.02\n",
      "iteration: 392600 loss: 0.0023 lr: 0.02\n",
      "iteration: 392610 loss: 0.0012 lr: 0.02\n",
      "iteration: 392620 loss: 0.0026 lr: 0.02\n",
      "iteration: 392630 loss: 0.0015 lr: 0.02\n",
      "iteration: 392640 loss: 0.0019 lr: 0.02\n",
      "iteration: 392650 loss: 0.0018 lr: 0.02\n",
      "iteration: 392660 loss: 0.0014 lr: 0.02\n",
      "iteration: 392670 loss: 0.0016 lr: 0.02\n",
      "iteration: 392680 loss: 0.0024 lr: 0.02\n",
      "iteration: 392690 loss: 0.0014 lr: 0.02\n",
      "iteration: 392700 loss: 0.0014 lr: 0.02\n",
      "iteration: 392710 loss: 0.0009 lr: 0.02\n",
      "iteration: 392720 loss: 0.0021 lr: 0.02\n",
      "iteration: 392730 loss: 0.0023 lr: 0.02\n",
      "iteration: 392740 loss: 0.0015 lr: 0.02\n",
      "iteration: 392750 loss: 0.0015 lr: 0.02\n",
      "iteration: 392760 loss: 0.0024 lr: 0.02\n",
      "iteration: 392770 loss: 0.0015 lr: 0.02\n",
      "iteration: 392780 loss: 0.0015 lr: 0.02\n",
      "iteration: 392790 loss: 0.0020 lr: 0.02\n",
      "iteration: 392800 loss: 0.0027 lr: 0.02\n",
      "iteration: 392810 loss: 0.0018 lr: 0.02\n",
      "iteration: 392820 loss: 0.0018 lr: 0.02\n",
      "iteration: 392830 loss: 0.0014 lr: 0.02\n",
      "iteration: 392840 loss: 0.0017 lr: 0.02\n",
      "iteration: 392850 loss: 0.0019 lr: 0.02\n",
      "iteration: 392860 loss: 0.0015 lr: 0.02\n",
      "iteration: 392870 loss: 0.0014 lr: 0.02\n",
      "iteration: 392880 loss: 0.0010 lr: 0.02\n",
      "iteration: 392890 loss: 0.0013 lr: 0.02\n",
      "iteration: 392900 loss: 0.0021 lr: 0.02\n",
      "iteration: 392910 loss: 0.0022 lr: 0.02\n",
      "iteration: 392920 loss: 0.0016 lr: 0.02\n",
      "iteration: 392930 loss: 0.0014 lr: 0.02\n",
      "iteration: 392940 loss: 0.0026 lr: 0.02\n",
      "iteration: 392950 loss: 0.0018 lr: 0.02\n",
      "iteration: 392960 loss: 0.0021 lr: 0.02\n",
      "iteration: 392970 loss: 0.0014 lr: 0.02\n",
      "iteration: 392980 loss: 0.0017 lr: 0.02\n",
      "iteration: 392990 loss: 0.0017 lr: 0.02\n",
      "iteration: 393000 loss: 0.0015 lr: 0.02\n",
      "iteration: 393010 loss: 0.0015 lr: 0.02\n",
      "iteration: 393020 loss: 0.0019 lr: 0.02\n",
      "iteration: 393030 loss: 0.0013 lr: 0.02\n",
      "iteration: 393040 loss: 0.0011 lr: 0.02\n",
      "iteration: 393050 loss: 0.0017 lr: 0.02\n",
      "iteration: 393060 loss: 0.0011 lr: 0.02\n",
      "iteration: 393070 loss: 0.0015 lr: 0.02\n",
      "iteration: 393080 loss: 0.0012 lr: 0.02\n",
      "iteration: 393090 loss: 0.0020 lr: 0.02\n",
      "iteration: 393100 loss: 0.0015 lr: 0.02\n",
      "iteration: 393110 loss: 0.0013 lr: 0.02\n",
      "iteration: 393120 loss: 0.0014 lr: 0.02\n",
      "iteration: 393130 loss: 0.0016 lr: 0.02\n",
      "iteration: 393140 loss: 0.0012 lr: 0.02\n",
      "iteration: 393150 loss: 0.0012 lr: 0.02\n",
      "iteration: 393160 loss: 0.0015 lr: 0.02\n",
      "iteration: 393170 loss: 0.0019 lr: 0.02\n",
      "iteration: 393180 loss: 0.0013 lr: 0.02\n",
      "iteration: 393190 loss: 0.0015 lr: 0.02\n",
      "iteration: 393200 loss: 0.0015 lr: 0.02\n",
      "iteration: 393210 loss: 0.0014 lr: 0.02\n",
      "iteration: 393220 loss: 0.0010 lr: 0.02\n",
      "iteration: 393230 loss: 0.0009 lr: 0.02\n",
      "iteration: 393240 loss: 0.0010 lr: 0.02\n",
      "iteration: 393250 loss: 0.0013 lr: 0.02\n",
      "iteration: 393260 loss: 0.0019 lr: 0.02\n",
      "iteration: 393270 loss: 0.0014 lr: 0.02\n",
      "iteration: 393280 loss: 0.0014 lr: 0.02\n",
      "iteration: 393290 loss: 0.0015 lr: 0.02\n",
      "iteration: 393300 loss: 0.0018 lr: 0.02\n",
      "iteration: 393310 loss: 0.0014 lr: 0.02\n",
      "iteration: 393320 loss: 0.0013 lr: 0.02\n",
      "iteration: 393330 loss: 0.0015 lr: 0.02\n",
      "iteration: 393340 loss: 0.0034 lr: 0.02\n",
      "iteration: 393350 loss: 0.0014 lr: 0.02\n",
      "iteration: 393360 loss: 0.0014 lr: 0.02\n",
      "iteration: 393370 loss: 0.0018 lr: 0.02\n",
      "iteration: 393380 loss: 0.0018 lr: 0.02\n",
      "iteration: 393390 loss: 0.0013 lr: 0.02\n",
      "iteration: 393400 loss: 0.0010 lr: 0.02\n",
      "iteration: 393410 loss: 0.0014 lr: 0.02\n",
      "iteration: 393420 loss: 0.0014 lr: 0.02\n",
      "iteration: 393430 loss: 0.0017 lr: 0.02\n",
      "iteration: 393440 loss: 0.0014 lr: 0.02\n",
      "iteration: 393450 loss: 0.0017 lr: 0.02\n",
      "iteration: 393460 loss: 0.0018 lr: 0.02\n",
      "iteration: 393470 loss: 0.0013 lr: 0.02\n",
      "iteration: 393480 loss: 0.0011 lr: 0.02\n",
      "iteration: 393490 loss: 0.0020 lr: 0.02\n",
      "iteration: 393500 loss: 0.0016 lr: 0.02\n",
      "iteration: 393510 loss: 0.0013 lr: 0.02\n",
      "iteration: 393520 loss: 0.0019 lr: 0.02\n",
      "iteration: 393530 loss: 0.0017 lr: 0.02\n",
      "iteration: 393540 loss: 0.0014 lr: 0.02\n",
      "iteration: 393550 loss: 0.0015 lr: 0.02\n",
      "iteration: 393560 loss: 0.0018 lr: 0.02\n",
      "iteration: 393570 loss: 0.0015 lr: 0.02\n",
      "iteration: 393580 loss: 0.0019 lr: 0.02\n",
      "iteration: 393590 loss: 0.0014 lr: 0.02\n",
      "iteration: 393600 loss: 0.0017 lr: 0.02\n",
      "iteration: 393610 loss: 0.0010 lr: 0.02\n",
      "iteration: 393620 loss: 0.0014 lr: 0.02\n",
      "iteration: 393630 loss: 0.0013 lr: 0.02\n",
      "iteration: 393640 loss: 0.0022 lr: 0.02\n",
      "iteration: 393650 loss: 0.0021 lr: 0.02\n",
      "iteration: 393660 loss: 0.0020 lr: 0.02\n",
      "iteration: 393670 loss: 0.0014 lr: 0.02\n",
      "iteration: 393680 loss: 0.0021 lr: 0.02\n",
      "iteration: 393690 loss: 0.0012 lr: 0.02\n",
      "iteration: 393700 loss: 0.0019 lr: 0.02\n",
      "iteration: 393710 loss: 0.0011 lr: 0.02\n",
      "iteration: 393720 loss: 0.0012 lr: 0.02\n",
      "iteration: 393730 loss: 0.0021 lr: 0.02\n",
      "iteration: 393740 loss: 0.0014 lr: 0.02\n",
      "iteration: 393750 loss: 0.0022 lr: 0.02\n",
      "iteration: 393760 loss: 0.0010 lr: 0.02\n",
      "iteration: 393770 loss: 0.0017 lr: 0.02\n",
      "iteration: 393780 loss: 0.0016 lr: 0.02\n",
      "iteration: 393790 loss: 0.0012 lr: 0.02\n",
      "iteration: 393800 loss: 0.0021 lr: 0.02\n",
      "iteration: 393810 loss: 0.0021 lr: 0.02\n",
      "iteration: 393820 loss: 0.0018 lr: 0.02\n",
      "iteration: 393830 loss: 0.0019 lr: 0.02\n",
      "iteration: 393840 loss: 0.0018 lr: 0.02\n",
      "iteration: 393850 loss: 0.0013 lr: 0.02\n",
      "iteration: 393860 loss: 0.0011 lr: 0.02\n",
      "iteration: 393870 loss: 0.0023 lr: 0.02\n",
      "iteration: 393880 loss: 0.0016 lr: 0.02\n",
      "iteration: 393890 loss: 0.0027 lr: 0.02\n",
      "iteration: 393900 loss: 0.0011 lr: 0.02\n",
      "iteration: 393910 loss: 0.0020 lr: 0.02\n",
      "iteration: 393920 loss: 0.0012 lr: 0.02\n",
      "iteration: 393930 loss: 0.0018 lr: 0.02\n",
      "iteration: 393940 loss: 0.0015 lr: 0.02\n",
      "iteration: 393950 loss: 0.0016 lr: 0.02\n",
      "iteration: 393960 loss: 0.0024 lr: 0.02\n",
      "iteration: 393970 loss: 0.0016 lr: 0.02\n",
      "iteration: 393980 loss: 0.0014 lr: 0.02\n",
      "iteration: 393990 loss: 0.0013 lr: 0.02\n",
      "iteration: 394000 loss: 0.0018 lr: 0.02\n",
      "iteration: 394010 loss: 0.0018 lr: 0.02\n",
      "iteration: 394020 loss: 0.0021 lr: 0.02\n",
      "iteration: 394030 loss: 0.0018 lr: 0.02\n",
      "iteration: 394040 loss: 0.0019 lr: 0.02\n",
      "iteration: 394050 loss: 0.0012 lr: 0.02\n",
      "iteration: 394060 loss: 0.0018 lr: 0.02\n",
      "iteration: 394070 loss: 0.0020 lr: 0.02\n",
      "iteration: 394080 loss: 0.0014 lr: 0.02\n",
      "iteration: 394090 loss: 0.0014 lr: 0.02\n",
      "iteration: 394100 loss: 0.0016 lr: 0.02\n",
      "iteration: 394110 loss: 0.0014 lr: 0.02\n",
      "iteration: 394120 loss: 0.0018 lr: 0.02\n",
      "iteration: 394130 loss: 0.0013 lr: 0.02\n",
      "iteration: 394140 loss: 0.0013 lr: 0.02\n",
      "iteration: 394150 loss: 0.0014 lr: 0.02\n",
      "iteration: 394160 loss: 0.0012 lr: 0.02\n",
      "iteration: 394170 loss: 0.0013 lr: 0.02\n",
      "iteration: 394180 loss: 0.0010 lr: 0.02\n",
      "iteration: 394190 loss: 0.0013 lr: 0.02\n",
      "iteration: 394200 loss: 0.0013 lr: 0.02\n",
      "iteration: 394210 loss: 0.0027 lr: 0.02\n",
      "iteration: 394220 loss: 0.0018 lr: 0.02\n",
      "iteration: 394230 loss: 0.0019 lr: 0.02\n",
      "iteration: 394240 loss: 0.0019 lr: 0.02\n",
      "iteration: 394250 loss: 0.0012 lr: 0.02\n",
      "iteration: 394260 loss: 0.0016 lr: 0.02\n",
      "iteration: 394270 loss: 0.0013 lr: 0.02\n",
      "iteration: 394280 loss: 0.0024 lr: 0.02\n",
      "iteration: 394290 loss: 0.0010 lr: 0.02\n",
      "iteration: 394300 loss: 0.0015 lr: 0.02\n",
      "iteration: 394310 loss: 0.0012 lr: 0.02\n",
      "iteration: 394320 loss: 0.0016 lr: 0.02\n",
      "iteration: 394330 loss: 0.0013 lr: 0.02\n",
      "iteration: 394340 loss: 0.0023 lr: 0.02\n",
      "iteration: 394350 loss: 0.0022 lr: 0.02\n",
      "iteration: 394360 loss: 0.0017 lr: 0.02\n",
      "iteration: 394370 loss: 0.0012 lr: 0.02\n",
      "iteration: 394380 loss: 0.0018 lr: 0.02\n",
      "iteration: 394390 loss: 0.0020 lr: 0.02\n",
      "iteration: 394400 loss: 0.0013 lr: 0.02\n",
      "iteration: 394410 loss: 0.0015 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 394420 loss: 0.0015 lr: 0.02\n",
      "iteration: 394430 loss: 0.0014 lr: 0.02\n",
      "iteration: 394440 loss: 0.0019 lr: 0.02\n",
      "iteration: 394450 loss: 0.0014 lr: 0.02\n",
      "iteration: 394460 loss: 0.0018 lr: 0.02\n",
      "iteration: 394470 loss: 0.0017 lr: 0.02\n",
      "iteration: 394480 loss: 0.0011 lr: 0.02\n",
      "iteration: 394490 loss: 0.0013 lr: 0.02\n",
      "iteration: 394500 loss: 0.0014 lr: 0.02\n",
      "iteration: 394510 loss: 0.0017 lr: 0.02\n",
      "iteration: 394520 loss: 0.0015 lr: 0.02\n",
      "iteration: 394530 loss: 0.0012 lr: 0.02\n",
      "iteration: 394540 loss: 0.0014 lr: 0.02\n",
      "iteration: 394550 loss: 0.0017 lr: 0.02\n",
      "iteration: 394560 loss: 0.0015 lr: 0.02\n",
      "iteration: 394570 loss: 0.0018 lr: 0.02\n",
      "iteration: 394580 loss: 0.0028 lr: 0.02\n",
      "iteration: 394590 loss: 0.0024 lr: 0.02\n",
      "iteration: 394600 loss: 0.0018 lr: 0.02\n",
      "iteration: 394610 loss: 0.0015 lr: 0.02\n",
      "iteration: 394620 loss: 0.0020 lr: 0.02\n",
      "iteration: 394630 loss: 0.0013 lr: 0.02\n",
      "iteration: 394640 loss: 0.0014 lr: 0.02\n",
      "iteration: 394650 loss: 0.0017 lr: 0.02\n",
      "iteration: 394660 loss: 0.0031 lr: 0.02\n",
      "iteration: 394670 loss: 0.0017 lr: 0.02\n",
      "iteration: 394680 loss: 0.0022 lr: 0.02\n",
      "iteration: 394690 loss: 0.0023 lr: 0.02\n",
      "iteration: 394700 loss: 0.0015 lr: 0.02\n",
      "iteration: 394710 loss: 0.0013 lr: 0.02\n",
      "iteration: 394720 loss: 0.0011 lr: 0.02\n",
      "iteration: 394730 loss: 0.0012 lr: 0.02\n",
      "iteration: 394740 loss: 0.0015 lr: 0.02\n",
      "iteration: 394750 loss: 0.0026 lr: 0.02\n",
      "iteration: 394760 loss: 0.0019 lr: 0.02\n",
      "iteration: 394770 loss: 0.0017 lr: 0.02\n",
      "iteration: 394780 loss: 0.0015 lr: 0.02\n",
      "iteration: 394790 loss: 0.0024 lr: 0.02\n",
      "iteration: 394800 loss: 0.0021 lr: 0.02\n",
      "iteration: 394810 loss: 0.0021 lr: 0.02\n",
      "iteration: 394820 loss: 0.0016 lr: 0.02\n",
      "iteration: 394830 loss: 0.0023 lr: 0.02\n",
      "iteration: 394840 loss: 0.0020 lr: 0.02\n",
      "iteration: 394850 loss: 0.0015 lr: 0.02\n",
      "iteration: 394860 loss: 0.0011 lr: 0.02\n",
      "iteration: 394870 loss: 0.0011 lr: 0.02\n",
      "iteration: 394880 loss: 0.0019 lr: 0.02\n",
      "iteration: 394890 loss: 0.0011 lr: 0.02\n",
      "iteration: 394900 loss: 0.0014 lr: 0.02\n",
      "iteration: 394910 loss: 0.0016 lr: 0.02\n",
      "iteration: 394920 loss: 0.0015 lr: 0.02\n",
      "iteration: 394930 loss: 0.0021 lr: 0.02\n",
      "iteration: 394940 loss: 0.0011 lr: 0.02\n",
      "iteration: 394950 loss: 0.0017 lr: 0.02\n",
      "iteration: 394960 loss: 0.0021 lr: 0.02\n",
      "iteration: 394970 loss: 0.0014 lr: 0.02\n",
      "iteration: 394980 loss: 0.0014 lr: 0.02\n",
      "iteration: 394990 loss: 0.0014 lr: 0.02\n",
      "iteration: 395000 loss: 0.0013 lr: 0.02\n",
      "iteration: 395010 loss: 0.0013 lr: 0.02\n",
      "iteration: 395020 loss: 0.0013 lr: 0.02\n",
      "iteration: 395030 loss: 0.0022 lr: 0.02\n",
      "iteration: 395040 loss: 0.0016 lr: 0.02\n",
      "iteration: 395050 loss: 0.0017 lr: 0.02\n",
      "iteration: 395060 loss: 0.0020 lr: 0.02\n",
      "iteration: 395070 loss: 0.0013 lr: 0.02\n",
      "iteration: 395080 loss: 0.0018 lr: 0.02\n",
      "iteration: 395090 loss: 0.0017 lr: 0.02\n",
      "iteration: 395100 loss: 0.0013 lr: 0.02\n",
      "iteration: 395110 loss: 0.0011 lr: 0.02\n",
      "iteration: 395120 loss: 0.0021 lr: 0.02\n",
      "iteration: 395130 loss: 0.0018 lr: 0.02\n",
      "iteration: 395140 loss: 0.0010 lr: 0.02\n",
      "iteration: 395150 loss: 0.0016 lr: 0.02\n",
      "iteration: 395160 loss: 0.0015 lr: 0.02\n",
      "iteration: 395170 loss: 0.0013 lr: 0.02\n",
      "iteration: 395180 loss: 0.0016 lr: 0.02\n",
      "iteration: 395190 loss: 0.0021 lr: 0.02\n",
      "iteration: 395200 loss: 0.0018 lr: 0.02\n",
      "iteration: 395210 loss: 0.0020 lr: 0.02\n",
      "iteration: 395220 loss: 0.0014 lr: 0.02\n",
      "iteration: 395230 loss: 0.0022 lr: 0.02\n",
      "iteration: 395240 loss: 0.0015 lr: 0.02\n",
      "iteration: 395250 loss: 0.0015 lr: 0.02\n",
      "iteration: 395260 loss: 0.0015 lr: 0.02\n",
      "iteration: 395270 loss: 0.0022 lr: 0.02\n",
      "iteration: 395280 loss: 0.0015 lr: 0.02\n",
      "iteration: 395290 loss: 0.0013 lr: 0.02\n",
      "iteration: 395300 loss: 0.0019 lr: 0.02\n",
      "iteration: 395310 loss: 0.0022 lr: 0.02\n",
      "iteration: 395320 loss: 0.0016 lr: 0.02\n",
      "iteration: 395330 loss: 0.0015 lr: 0.02\n",
      "iteration: 395340 loss: 0.0017 lr: 0.02\n",
      "iteration: 395350 loss: 0.0018 lr: 0.02\n",
      "iteration: 395360 loss: 0.0013 lr: 0.02\n",
      "iteration: 395370 loss: 0.0020 lr: 0.02\n",
      "iteration: 395380 loss: 0.0014 lr: 0.02\n",
      "iteration: 395390 loss: 0.0022 lr: 0.02\n",
      "iteration: 395400 loss: 0.0015 lr: 0.02\n",
      "iteration: 395410 loss: 0.0012 lr: 0.02\n",
      "iteration: 395420 loss: 0.0016 lr: 0.02\n",
      "iteration: 395430 loss: 0.0012 lr: 0.02\n",
      "iteration: 395440 loss: 0.0014 lr: 0.02\n",
      "iteration: 395450 loss: 0.0013 lr: 0.02\n",
      "iteration: 395460 loss: 0.0018 lr: 0.02\n",
      "iteration: 395470 loss: 0.0017 lr: 0.02\n",
      "iteration: 395480 loss: 0.0012 lr: 0.02\n",
      "iteration: 395490 loss: 0.0025 lr: 0.02\n",
      "iteration: 395500 loss: 0.0019 lr: 0.02\n",
      "iteration: 395510 loss: 0.0020 lr: 0.02\n",
      "iteration: 395520 loss: 0.0012 lr: 0.02\n",
      "iteration: 395530 loss: 0.0014 lr: 0.02\n",
      "iteration: 395540 loss: 0.0018 lr: 0.02\n",
      "iteration: 395550 loss: 0.0015 lr: 0.02\n",
      "iteration: 395560 loss: 0.0014 lr: 0.02\n",
      "iteration: 395570 loss: 0.0016 lr: 0.02\n",
      "iteration: 395580 loss: 0.0016 lr: 0.02\n",
      "iteration: 395590 loss: 0.0014 lr: 0.02\n",
      "iteration: 395600 loss: 0.0017 lr: 0.02\n",
      "iteration: 395610 loss: 0.0022 lr: 0.02\n",
      "iteration: 395620 loss: 0.0021 lr: 0.02\n",
      "iteration: 395630 loss: 0.0012 lr: 0.02\n",
      "iteration: 395640 loss: 0.0013 lr: 0.02\n",
      "iteration: 395650 loss: 0.0016 lr: 0.02\n",
      "iteration: 395660 loss: 0.0018 lr: 0.02\n",
      "iteration: 395670 loss: 0.0015 lr: 0.02\n",
      "iteration: 395680 loss: 0.0014 lr: 0.02\n",
      "iteration: 395690 loss: 0.0012 lr: 0.02\n",
      "iteration: 395700 loss: 0.0013 lr: 0.02\n",
      "iteration: 395710 loss: 0.0012 lr: 0.02\n",
      "iteration: 395720 loss: 0.0013 lr: 0.02\n",
      "iteration: 395730 loss: 0.0015 lr: 0.02\n",
      "iteration: 395740 loss: 0.0014 lr: 0.02\n",
      "iteration: 395750 loss: 0.0012 lr: 0.02\n",
      "iteration: 395760 loss: 0.0019 lr: 0.02\n",
      "iteration: 395770 loss: 0.0012 lr: 0.02\n",
      "iteration: 395780 loss: 0.0018 lr: 0.02\n",
      "iteration: 395790 loss: 0.0019 lr: 0.02\n",
      "iteration: 395800 loss: 0.0014 lr: 0.02\n",
      "iteration: 395810 loss: 0.0013 lr: 0.02\n",
      "iteration: 395820 loss: 0.0012 lr: 0.02\n",
      "iteration: 395830 loss: 0.0018 lr: 0.02\n",
      "iteration: 395840 loss: 0.0017 lr: 0.02\n",
      "iteration: 395850 loss: 0.0011 lr: 0.02\n",
      "iteration: 395860 loss: 0.0017 lr: 0.02\n",
      "iteration: 395870 loss: 0.0014 lr: 0.02\n",
      "iteration: 395880 loss: 0.0011 lr: 0.02\n",
      "iteration: 395890 loss: 0.0015 lr: 0.02\n",
      "iteration: 395900 loss: 0.0010 lr: 0.02\n",
      "iteration: 395910 loss: 0.0014 lr: 0.02\n",
      "iteration: 395920 loss: 0.0011 lr: 0.02\n",
      "iteration: 395930 loss: 0.0018 lr: 0.02\n",
      "iteration: 395940 loss: 0.0017 lr: 0.02\n",
      "iteration: 395950 loss: 0.0018 lr: 0.02\n",
      "iteration: 395960 loss: 0.0021 lr: 0.02\n",
      "iteration: 395970 loss: 0.0011 lr: 0.02\n",
      "iteration: 395980 loss: 0.0014 lr: 0.02\n",
      "iteration: 395990 loss: 0.0013 lr: 0.02\n",
      "iteration: 396000 loss: 0.0016 lr: 0.02\n",
      "iteration: 396010 loss: 0.0014 lr: 0.02\n",
      "iteration: 396020 loss: 0.0019 lr: 0.02\n",
      "iteration: 396030 loss: 0.0018 lr: 0.02\n",
      "iteration: 396040 loss: 0.0013 lr: 0.02\n",
      "iteration: 396050 loss: 0.0017 lr: 0.02\n",
      "iteration: 396060 loss: 0.0013 lr: 0.02\n",
      "iteration: 396070 loss: 0.0015 lr: 0.02\n",
      "iteration: 396080 loss: 0.0014 lr: 0.02\n",
      "iteration: 396090 loss: 0.0015 lr: 0.02\n",
      "iteration: 396100 loss: 0.0014 lr: 0.02\n",
      "iteration: 396110 loss: 0.0014 lr: 0.02\n",
      "iteration: 396120 loss: 0.0012 lr: 0.02\n",
      "iteration: 396130 loss: 0.0016 lr: 0.02\n",
      "iteration: 396140 loss: 0.0011 lr: 0.02\n",
      "iteration: 396150 loss: 0.0016 lr: 0.02\n",
      "iteration: 396160 loss: 0.0014 lr: 0.02\n",
      "iteration: 396170 loss: 0.0015 lr: 0.02\n",
      "iteration: 396180 loss: 0.0013 lr: 0.02\n",
      "iteration: 396190 loss: 0.0012 lr: 0.02\n",
      "iteration: 396200 loss: 0.0014 lr: 0.02\n",
      "iteration: 396210 loss: 0.0014 lr: 0.02\n",
      "iteration: 396220 loss: 0.0015 lr: 0.02\n",
      "iteration: 396230 loss: 0.0017 lr: 0.02\n",
      "iteration: 396240 loss: 0.0015 lr: 0.02\n",
      "iteration: 396250 loss: 0.0020 lr: 0.02\n",
      "iteration: 396260 loss: 0.0011 lr: 0.02\n",
      "iteration: 396270 loss: 0.0018 lr: 0.02\n",
      "iteration: 396280 loss: 0.0013 lr: 0.02\n",
      "iteration: 396290 loss: 0.0027 lr: 0.02\n",
      "iteration: 396300 loss: 0.0018 lr: 0.02\n",
      "iteration: 396310 loss: 0.0014 lr: 0.02\n",
      "iteration: 396320 loss: 0.0016 lr: 0.02\n",
      "iteration: 396330 loss: 0.0014 lr: 0.02\n",
      "iteration: 396340 loss: 0.0018 lr: 0.02\n",
      "iteration: 396350 loss: 0.0017 lr: 0.02\n",
      "iteration: 396360 loss: 0.0012 lr: 0.02\n",
      "iteration: 396370 loss: 0.0012 lr: 0.02\n",
      "iteration: 396380 loss: 0.0013 lr: 0.02\n",
      "iteration: 396390 loss: 0.0016 lr: 0.02\n",
      "iteration: 396400 loss: 0.0014 lr: 0.02\n",
      "iteration: 396410 loss: 0.0017 lr: 0.02\n",
      "iteration: 396420 loss: 0.0013 lr: 0.02\n",
      "iteration: 396430 loss: 0.0019 lr: 0.02\n",
      "iteration: 396440 loss: 0.0013 lr: 0.02\n",
      "iteration: 396450 loss: 0.0010 lr: 0.02\n",
      "iteration: 396460 loss: 0.0020 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 396470 loss: 0.0017 lr: 0.02\n",
      "iteration: 396480 loss: 0.0029 lr: 0.02\n",
      "iteration: 396490 loss: 0.0010 lr: 0.02\n",
      "iteration: 396500 loss: 0.0013 lr: 0.02\n",
      "iteration: 396510 loss: 0.0018 lr: 0.02\n",
      "iteration: 396520 loss: 0.0018 lr: 0.02\n",
      "iteration: 396530 loss: 0.0017 lr: 0.02\n",
      "iteration: 396540 loss: 0.0011 lr: 0.02\n",
      "iteration: 396550 loss: 0.0015 lr: 0.02\n",
      "iteration: 396560 loss: 0.0012 lr: 0.02\n",
      "iteration: 396570 loss: 0.0014 lr: 0.02\n",
      "iteration: 396580 loss: 0.0019 lr: 0.02\n",
      "iteration: 396590 loss: 0.0015 lr: 0.02\n",
      "iteration: 396600 loss: 0.0014 lr: 0.02\n",
      "iteration: 396610 loss: 0.0023 lr: 0.02\n",
      "iteration: 396620 loss: 0.0012 lr: 0.02\n",
      "iteration: 396630 loss: 0.0009 lr: 0.02\n",
      "iteration: 396640 loss: 0.0013 lr: 0.02\n",
      "iteration: 396650 loss: 0.0013 lr: 0.02\n",
      "iteration: 396660 loss: 0.0025 lr: 0.02\n",
      "iteration: 396670 loss: 0.0024 lr: 0.02\n",
      "iteration: 396680 loss: 0.0027 lr: 0.02\n",
      "iteration: 396690 loss: 0.0017 lr: 0.02\n",
      "iteration: 396700 loss: 0.0021 lr: 0.02\n",
      "iteration: 396710 loss: 0.0017 lr: 0.02\n",
      "iteration: 396720 loss: 0.0017 lr: 0.02\n",
      "iteration: 396730 loss: 0.0025 lr: 0.02\n",
      "iteration: 396740 loss: 0.0013 lr: 0.02\n",
      "iteration: 396750 loss: 0.0019 lr: 0.02\n",
      "iteration: 396760 loss: 0.0025 lr: 0.02\n",
      "iteration: 396770 loss: 0.0017 lr: 0.02\n",
      "iteration: 396780 loss: 0.0013 lr: 0.02\n",
      "iteration: 396790 loss: 0.0010 lr: 0.02\n",
      "iteration: 396800 loss: 0.0015 lr: 0.02\n",
      "iteration: 396810 loss: 0.0012 lr: 0.02\n",
      "iteration: 396820 loss: 0.0018 lr: 0.02\n",
      "iteration: 396830 loss: 0.0011 lr: 0.02\n",
      "iteration: 396840 loss: 0.0015 lr: 0.02\n",
      "iteration: 396850 loss: 0.0018 lr: 0.02\n",
      "iteration: 396860 loss: 0.0016 lr: 0.02\n",
      "iteration: 396870 loss: 0.0020 lr: 0.02\n",
      "iteration: 396880 loss: 0.0015 lr: 0.02\n",
      "iteration: 396890 loss: 0.0021 lr: 0.02\n",
      "iteration: 396900 loss: 0.0017 lr: 0.02\n",
      "iteration: 396910 loss: 0.0015 lr: 0.02\n",
      "iteration: 396920 loss: 0.0025 lr: 0.02\n",
      "iteration: 396930 loss: 0.0015 lr: 0.02\n",
      "iteration: 396940 loss: 0.0018 lr: 0.02\n",
      "iteration: 396950 loss: 0.0022 lr: 0.02\n",
      "iteration: 396960 loss: 0.0020 lr: 0.02\n",
      "iteration: 396970 loss: 0.0012 lr: 0.02\n",
      "iteration: 396980 loss: 0.0022 lr: 0.02\n",
      "iteration: 396990 loss: 0.0016 lr: 0.02\n",
      "iteration: 397000 loss: 0.0016 lr: 0.02\n",
      "iteration: 397010 loss: 0.0019 lr: 0.02\n",
      "iteration: 397020 loss: 0.0022 lr: 0.02\n",
      "iteration: 397030 loss: 0.0017 lr: 0.02\n",
      "iteration: 397040 loss: 0.0014 lr: 0.02\n",
      "iteration: 397050 loss: 0.0013 lr: 0.02\n",
      "iteration: 397060 loss: 0.0015 lr: 0.02\n",
      "iteration: 397070 loss: 0.0017 lr: 0.02\n",
      "iteration: 397080 loss: 0.0018 lr: 0.02\n",
      "iteration: 397090 loss: 0.0015 lr: 0.02\n",
      "iteration: 397100 loss: 0.0018 lr: 0.02\n",
      "iteration: 397110 loss: 0.0017 lr: 0.02\n",
      "iteration: 397120 loss: 0.0011 lr: 0.02\n",
      "iteration: 397130 loss: 0.0013 lr: 0.02\n",
      "iteration: 397140 loss: 0.0031 lr: 0.02\n",
      "iteration: 397150 loss: 0.0012 lr: 0.02\n",
      "iteration: 397160 loss: 0.0014 lr: 0.02\n",
      "iteration: 397170 loss: 0.0023 lr: 0.02\n",
      "iteration: 397180 loss: 0.0015 lr: 0.02\n",
      "iteration: 397190 loss: 0.0015 lr: 0.02\n",
      "iteration: 397200 loss: 0.0018 lr: 0.02\n",
      "iteration: 397210 loss: 0.0012 lr: 0.02\n",
      "iteration: 397220 loss: 0.0016 lr: 0.02\n",
      "iteration: 397230 loss: 0.0016 lr: 0.02\n",
      "iteration: 397240 loss: 0.0025 lr: 0.02\n",
      "iteration: 397250 loss: 0.0014 lr: 0.02\n",
      "iteration: 397260 loss: 0.0012 lr: 0.02\n",
      "iteration: 397270 loss: 0.0016 lr: 0.02\n",
      "iteration: 397280 loss: 0.0017 lr: 0.02\n",
      "iteration: 397290 loss: 0.0013 lr: 0.02\n",
      "iteration: 397300 loss: 0.0013 lr: 0.02\n",
      "iteration: 397310 loss: 0.0014 lr: 0.02\n",
      "iteration: 397320 loss: 0.0016 lr: 0.02\n",
      "iteration: 397330 loss: 0.0018 lr: 0.02\n",
      "iteration: 397340 loss: 0.0016 lr: 0.02\n",
      "iteration: 397350 loss: 0.0017 lr: 0.02\n",
      "iteration: 397360 loss: 0.0018 lr: 0.02\n",
      "iteration: 397370 loss: 0.0013 lr: 0.02\n",
      "iteration: 397380 loss: 0.0023 lr: 0.02\n",
      "iteration: 397390 loss: 0.0014 lr: 0.02\n",
      "iteration: 397400 loss: 0.0013 lr: 0.02\n",
      "iteration: 397410 loss: 0.0013 lr: 0.02\n",
      "iteration: 397420 loss: 0.0014 lr: 0.02\n",
      "iteration: 397430 loss: 0.0020 lr: 0.02\n",
      "iteration: 397440 loss: 0.0013 lr: 0.02\n",
      "iteration: 397450 loss: 0.0018 lr: 0.02\n",
      "iteration: 397460 loss: 0.0018 lr: 0.02\n",
      "iteration: 397470 loss: 0.0015 lr: 0.02\n",
      "iteration: 397480 loss: 0.0013 lr: 0.02\n",
      "iteration: 397490 loss: 0.0045 lr: 0.02\n",
      "iteration: 397500 loss: 0.0015 lr: 0.02\n",
      "iteration: 397510 loss: 0.0013 lr: 0.02\n",
      "iteration: 397520 loss: 0.0018 lr: 0.02\n",
      "iteration: 397530 loss: 0.0016 lr: 0.02\n",
      "iteration: 397540 loss: 0.0013 lr: 0.02\n",
      "iteration: 397550 loss: 0.0011 lr: 0.02\n",
      "iteration: 397560 loss: 0.0014 lr: 0.02\n",
      "iteration: 397570 loss: 0.0010 lr: 0.02\n",
      "iteration: 397580 loss: 0.0016 lr: 0.02\n",
      "iteration: 397590 loss: 0.0019 lr: 0.02\n",
      "iteration: 397600 loss: 0.0014 lr: 0.02\n",
      "iteration: 397610 loss: 0.0016 lr: 0.02\n",
      "iteration: 397620 loss: 0.0011 lr: 0.02\n",
      "iteration: 397630 loss: 0.0011 lr: 0.02\n",
      "iteration: 397640 loss: 0.0013 lr: 0.02\n",
      "iteration: 397650 loss: 0.0016 lr: 0.02\n",
      "iteration: 397660 loss: 0.0017 lr: 0.02\n",
      "iteration: 397670 loss: 0.0018 lr: 0.02\n",
      "iteration: 397680 loss: 0.0030 lr: 0.02\n",
      "iteration: 397690 loss: 0.0011 lr: 0.02\n",
      "iteration: 397700 loss: 0.0018 lr: 0.02\n",
      "iteration: 397710 loss: 0.0019 lr: 0.02\n",
      "iteration: 397720 loss: 0.0014 lr: 0.02\n",
      "iteration: 397730 loss: 0.0015 lr: 0.02\n",
      "iteration: 397740 loss: 0.0021 lr: 0.02\n",
      "iteration: 397750 loss: 0.0018 lr: 0.02\n",
      "iteration: 397760 loss: 0.0017 lr: 0.02\n",
      "iteration: 397770 loss: 0.0019 lr: 0.02\n",
      "iteration: 397780 loss: 0.0011 lr: 0.02\n",
      "iteration: 397790 loss: 0.0011 lr: 0.02\n",
      "iteration: 397800 loss: 0.0013 lr: 0.02\n",
      "iteration: 397810 loss: 0.0017 lr: 0.02\n",
      "iteration: 397820 loss: 0.0010 lr: 0.02\n",
      "iteration: 397830 loss: 0.0012 lr: 0.02\n",
      "iteration: 397840 loss: 0.0016 lr: 0.02\n",
      "iteration: 397850 loss: 0.0022 lr: 0.02\n",
      "iteration: 397860 loss: 0.0019 lr: 0.02\n",
      "iteration: 397870 loss: 0.0017 lr: 0.02\n",
      "iteration: 397880 loss: 0.0021 lr: 0.02\n",
      "iteration: 397890 loss: 0.0016 lr: 0.02\n",
      "iteration: 397900 loss: 0.0017 lr: 0.02\n",
      "iteration: 397910 loss: 0.0014 lr: 0.02\n",
      "iteration: 397920 loss: 0.0013 lr: 0.02\n",
      "iteration: 397930 loss: 0.0014 lr: 0.02\n",
      "iteration: 397940 loss: 0.0034 lr: 0.02\n",
      "iteration: 397950 loss: 0.0017 lr: 0.02\n",
      "iteration: 397960 loss: 0.0011 lr: 0.02\n",
      "iteration: 397970 loss: 0.0024 lr: 0.02\n",
      "iteration: 397980 loss: 0.0012 lr: 0.02\n",
      "iteration: 397990 loss: 0.0016 lr: 0.02\n",
      "iteration: 398000 loss: 0.0022 lr: 0.02\n",
      "iteration: 398010 loss: 0.0019 lr: 0.02\n",
      "iteration: 398020 loss: 0.0015 lr: 0.02\n",
      "iteration: 398030 loss: 0.0011 lr: 0.02\n",
      "iteration: 398040 loss: 0.0019 lr: 0.02\n",
      "iteration: 398050 loss: 0.0018 lr: 0.02\n",
      "iteration: 398060 loss: 0.0023 lr: 0.02\n",
      "iteration: 398070 loss: 0.0012 lr: 0.02\n",
      "iteration: 398080 loss: 0.0022 lr: 0.02\n",
      "iteration: 398090 loss: 0.0019 lr: 0.02\n",
      "iteration: 398100 loss: 0.0014 lr: 0.02\n",
      "iteration: 398110 loss: 0.0018 lr: 0.02\n",
      "iteration: 398120 loss: 0.0014 lr: 0.02\n",
      "iteration: 398130 loss: 0.0015 lr: 0.02\n",
      "iteration: 398140 loss: 0.0012 lr: 0.02\n",
      "iteration: 398150 loss: 0.0018 lr: 0.02\n",
      "iteration: 398160 loss: 0.0020 lr: 0.02\n",
      "iteration: 398170 loss: 0.0013 lr: 0.02\n",
      "iteration: 398180 loss: 0.0011 lr: 0.02\n",
      "iteration: 398190 loss: 0.0011 lr: 0.02\n",
      "iteration: 398200 loss: 0.0013 lr: 0.02\n",
      "iteration: 398210 loss: 0.0020 lr: 0.02\n",
      "iteration: 398220 loss: 0.0017 lr: 0.02\n",
      "iteration: 398230 loss: 0.0016 lr: 0.02\n",
      "iteration: 398240 loss: 0.0019 lr: 0.02\n",
      "iteration: 398250 loss: 0.0015 lr: 0.02\n",
      "iteration: 398260 loss: 0.0012 lr: 0.02\n",
      "iteration: 398270 loss: 0.0019 lr: 0.02\n",
      "iteration: 398280 loss: 0.0021 lr: 0.02\n",
      "iteration: 398290 loss: 0.0011 lr: 0.02\n",
      "iteration: 398300 loss: 0.0016 lr: 0.02\n",
      "iteration: 398310 loss: 0.0016 lr: 0.02\n",
      "iteration: 398320 loss: 0.0015 lr: 0.02\n",
      "iteration: 398330 loss: 0.0012 lr: 0.02\n",
      "iteration: 398340 loss: 0.0019 lr: 0.02\n",
      "iteration: 398350 loss: 0.0022 lr: 0.02\n",
      "iteration: 398360 loss: 0.0016 lr: 0.02\n",
      "iteration: 398370 loss: 0.0011 lr: 0.02\n",
      "iteration: 398380 loss: 0.0029 lr: 0.02\n",
      "iteration: 398390 loss: 0.0016 lr: 0.02\n",
      "iteration: 398400 loss: 0.0022 lr: 0.02\n",
      "iteration: 398410 loss: 0.0017 lr: 0.02\n",
      "iteration: 398420 loss: 0.0014 lr: 0.02\n",
      "iteration: 398430 loss: 0.0012 lr: 0.02\n",
      "iteration: 398440 loss: 0.0016 lr: 0.02\n",
      "iteration: 398450 loss: 0.0027 lr: 0.02\n",
      "iteration: 398460 loss: 0.0019 lr: 0.02\n",
      "iteration: 398470 loss: 0.0013 lr: 0.02\n",
      "iteration: 398480 loss: 0.0010 lr: 0.02\n",
      "iteration: 398490 loss: 0.0021 lr: 0.02\n",
      "iteration: 398500 loss: 0.0017 lr: 0.02\n",
      "iteration: 398510 loss: 0.0011 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 398520 loss: 0.0013 lr: 0.02\n",
      "iteration: 398530 loss: 0.0013 lr: 0.02\n",
      "iteration: 398540 loss: 0.0014 lr: 0.02\n",
      "iteration: 398550 loss: 0.0019 lr: 0.02\n",
      "iteration: 398560 loss: 0.0017 lr: 0.02\n",
      "iteration: 398570 loss: 0.0022 lr: 0.02\n",
      "iteration: 398580 loss: 0.0014 lr: 0.02\n",
      "iteration: 398590 loss: 0.0019 lr: 0.02\n",
      "iteration: 398600 loss: 0.0014 lr: 0.02\n",
      "iteration: 398610 loss: 0.0016 lr: 0.02\n",
      "iteration: 398620 loss: 0.0016 lr: 0.02\n",
      "iteration: 398630 loss: 0.0011 lr: 0.02\n",
      "iteration: 398640 loss: 0.0014 lr: 0.02\n",
      "iteration: 398650 loss: 0.0010 lr: 0.02\n",
      "iteration: 398660 loss: 0.0019 lr: 0.02\n",
      "iteration: 398670 loss: 0.0009 lr: 0.02\n",
      "iteration: 398680 loss: 0.0013 lr: 0.02\n",
      "iteration: 398690 loss: 0.0015 lr: 0.02\n",
      "iteration: 398700 loss: 0.0014 lr: 0.02\n",
      "iteration: 398710 loss: 0.0016 lr: 0.02\n",
      "iteration: 398720 loss: 0.0020 lr: 0.02\n",
      "iteration: 398730 loss: 0.0012 lr: 0.02\n",
      "iteration: 398740 loss: 0.0022 lr: 0.02\n",
      "iteration: 398750 loss: 0.0011 lr: 0.02\n",
      "iteration: 398760 loss: 0.0017 lr: 0.02\n",
      "iteration: 398770 loss: 0.0018 lr: 0.02\n",
      "iteration: 398780 loss: 0.0016 lr: 0.02\n",
      "iteration: 398790 loss: 0.0014 lr: 0.02\n",
      "iteration: 398800 loss: 0.0018 lr: 0.02\n",
      "iteration: 398810 loss: 0.0015 lr: 0.02\n",
      "iteration: 398820 loss: 0.0011 lr: 0.02\n",
      "iteration: 398830 loss: 0.0024 lr: 0.02\n",
      "iteration: 398840 loss: 0.0024 lr: 0.02\n",
      "iteration: 398850 loss: 0.0016 lr: 0.02\n",
      "iteration: 398860 loss: 0.0012 lr: 0.02\n",
      "iteration: 398870 loss: 0.0019 lr: 0.02\n",
      "iteration: 398880 loss: 0.0017 lr: 0.02\n",
      "iteration: 398890 loss: 0.0016 lr: 0.02\n",
      "iteration: 398900 loss: 0.0017 lr: 0.02\n",
      "iteration: 398910 loss: 0.0013 lr: 0.02\n",
      "iteration: 398920 loss: 0.0014 lr: 0.02\n",
      "iteration: 398930 loss: 0.0014 lr: 0.02\n",
      "iteration: 398940 loss: 0.0017 lr: 0.02\n",
      "iteration: 398950 loss: 0.0021 lr: 0.02\n",
      "iteration: 398960 loss: 0.0010 lr: 0.02\n",
      "iteration: 398970 loss: 0.0016 lr: 0.02\n",
      "iteration: 398980 loss: 0.0015 lr: 0.02\n",
      "iteration: 398990 loss: 0.0011 lr: 0.02\n",
      "iteration: 399000 loss: 0.0012 lr: 0.02\n",
      "iteration: 399010 loss: 0.0012 lr: 0.02\n",
      "iteration: 399020 loss: 0.0012 lr: 0.02\n",
      "iteration: 399030 loss: 0.0011 lr: 0.02\n",
      "iteration: 399040 loss: 0.0015 lr: 0.02\n",
      "iteration: 399050 loss: 0.0015 lr: 0.02\n",
      "iteration: 399060 loss: 0.0018 lr: 0.02\n",
      "iteration: 399070 loss: 0.0014 lr: 0.02\n",
      "iteration: 399080 loss: 0.0013 lr: 0.02\n",
      "iteration: 399090 loss: 0.0020 lr: 0.02\n",
      "iteration: 399100 loss: 0.0014 lr: 0.02\n",
      "iteration: 399110 loss: 0.0016 lr: 0.02\n",
      "iteration: 399120 loss: 0.0023 lr: 0.02\n",
      "iteration: 399130 loss: 0.0011 lr: 0.02\n",
      "iteration: 399140 loss: 0.0017 lr: 0.02\n",
      "iteration: 399150 loss: 0.0015 lr: 0.02\n",
      "iteration: 399160 loss: 0.0013 lr: 0.02\n",
      "iteration: 399170 loss: 0.0012 lr: 0.02\n",
      "iteration: 399180 loss: 0.0017 lr: 0.02\n",
      "iteration: 399190 loss: 0.0022 lr: 0.02\n",
      "iteration: 399200 loss: 0.0015 lr: 0.02\n",
      "iteration: 399210 loss: 0.0015 lr: 0.02\n",
      "iteration: 399220 loss: 0.0017 lr: 0.02\n",
      "iteration: 399230 loss: 0.0023 lr: 0.02\n",
      "iteration: 399240 loss: 0.0014 lr: 0.02\n",
      "iteration: 399250 loss: 0.0015 lr: 0.02\n",
      "iteration: 399260 loss: 0.0015 lr: 0.02\n",
      "iteration: 399270 loss: 0.0013 lr: 0.02\n",
      "iteration: 399280 loss: 0.0012 lr: 0.02\n",
      "iteration: 399290 loss: 0.0011 lr: 0.02\n",
      "iteration: 399300 loss: 0.0014 lr: 0.02\n",
      "iteration: 399310 loss: 0.0017 lr: 0.02\n",
      "iteration: 399320 loss: 0.0018 lr: 0.02\n",
      "iteration: 399330 loss: 0.0018 lr: 0.02\n",
      "iteration: 399340 loss: 0.0014 lr: 0.02\n",
      "iteration: 399350 loss: 0.0015 lr: 0.02\n",
      "iteration: 399360 loss: 0.0016 lr: 0.02\n",
      "iteration: 399370 loss: 0.0017 lr: 0.02\n",
      "iteration: 399380 loss: 0.0014 lr: 0.02\n",
      "iteration: 399390 loss: 0.0013 lr: 0.02\n",
      "iteration: 399400 loss: 0.0019 lr: 0.02\n",
      "iteration: 399410 loss: 0.0020 lr: 0.02\n",
      "iteration: 399420 loss: 0.0015 lr: 0.02\n",
      "iteration: 399430 loss: 0.0014 lr: 0.02\n",
      "iteration: 399440 loss: 0.0016 lr: 0.02\n",
      "iteration: 399450 loss: 0.0011 lr: 0.02\n",
      "iteration: 399460 loss: 0.0015 lr: 0.02\n",
      "iteration: 399470 loss: 0.0014 lr: 0.02\n",
      "iteration: 399480 loss: 0.0013 lr: 0.02\n",
      "iteration: 399490 loss: 0.0018 lr: 0.02\n",
      "iteration: 399500 loss: 0.0015 lr: 0.02\n",
      "iteration: 399510 loss: 0.0018 lr: 0.02\n",
      "iteration: 399520 loss: 0.0031 lr: 0.02\n",
      "iteration: 399530 loss: 0.0024 lr: 0.02\n",
      "iteration: 399540 loss: 0.0016 lr: 0.02\n",
      "iteration: 399550 loss: 0.0012 lr: 0.02\n",
      "iteration: 399560 loss: 0.0020 lr: 0.02\n",
      "iteration: 399570 loss: 0.0016 lr: 0.02\n",
      "iteration: 399580 loss: 0.0013 lr: 0.02\n",
      "iteration: 399590 loss: 0.0015 lr: 0.02\n",
      "iteration: 399600 loss: 0.0018 lr: 0.02\n",
      "iteration: 399610 loss: 0.0015 lr: 0.02\n",
      "iteration: 399620 loss: 0.0014 lr: 0.02\n",
      "iteration: 399630 loss: 0.0019 lr: 0.02\n",
      "iteration: 399640 loss: 0.0021 lr: 0.02\n",
      "iteration: 399650 loss: 0.0012 lr: 0.02\n",
      "iteration: 399660 loss: 0.0015 lr: 0.02\n",
      "iteration: 399670 loss: 0.0019 lr: 0.02\n",
      "iteration: 399680 loss: 0.0013 lr: 0.02\n",
      "iteration: 399690 loss: 0.0013 lr: 0.02\n",
      "iteration: 399700 loss: 0.0013 lr: 0.02\n",
      "iteration: 399710 loss: 0.0015 lr: 0.02\n",
      "iteration: 399720 loss: 0.0025 lr: 0.02\n",
      "iteration: 399730 loss: 0.0013 lr: 0.02\n",
      "iteration: 399740 loss: 0.0013 lr: 0.02\n",
      "iteration: 399750 loss: 0.0012 lr: 0.02\n",
      "iteration: 399760 loss: 0.0012 lr: 0.02\n",
      "iteration: 399770 loss: 0.0010 lr: 0.02\n",
      "iteration: 399780 loss: 0.0014 lr: 0.02\n",
      "iteration: 399790 loss: 0.0018 lr: 0.02\n",
      "iteration: 399800 loss: 0.0016 lr: 0.02\n",
      "iteration: 399810 loss: 0.0018 lr: 0.02\n",
      "iteration: 399820 loss: 0.0011 lr: 0.02\n",
      "iteration: 399830 loss: 0.0010 lr: 0.02\n",
      "iteration: 399840 loss: 0.0016 lr: 0.02\n",
      "iteration: 399850 loss: 0.0016 lr: 0.02\n",
      "iteration: 399860 loss: 0.0014 lr: 0.02\n",
      "iteration: 399870 loss: 0.0015 lr: 0.02\n",
      "iteration: 399880 loss: 0.0012 lr: 0.02\n",
      "iteration: 399890 loss: 0.0016 lr: 0.02\n",
      "iteration: 399900 loss: 0.0017 lr: 0.02\n",
      "iteration: 399910 loss: 0.0013 lr: 0.02\n",
      "iteration: 399920 loss: 0.0014 lr: 0.02\n",
      "iteration: 399930 loss: 0.0019 lr: 0.02\n",
      "iteration: 399940 loss: 0.0017 lr: 0.02\n",
      "iteration: 399950 loss: 0.0017 lr: 0.02\n",
      "iteration: 399960 loss: 0.0012 lr: 0.02\n",
      "iteration: 399970 loss: 0.0017 lr: 0.02\n",
      "iteration: 399980 loss: 0.0012 lr: 0.02\n",
      "iteration: 399990 loss: 0.0014 lr: 0.02\n",
      "iteration: 400000 loss: 0.0016 lr: 0.02\n",
      "iteration: 400010 loss: 0.0024 lr: 0.02\n",
      "iteration: 400020 loss: 0.0016 lr: 0.02\n",
      "iteration: 400030 loss: 0.0017 lr: 0.02\n",
      "iteration: 400040 loss: 0.0015 lr: 0.02\n",
      "iteration: 400050 loss: 0.0018 lr: 0.02\n",
      "iteration: 400060 loss: 0.0012 lr: 0.02\n",
      "iteration: 400070 loss: 0.0012 lr: 0.02\n",
      "iteration: 400080 loss: 0.0016 lr: 0.02\n",
      "iteration: 400090 loss: 0.0017 lr: 0.02\n",
      "iteration: 400100 loss: 0.0013 lr: 0.02\n",
      "iteration: 400110 loss: 0.0024 lr: 0.02\n",
      "iteration: 400120 loss: 0.0019 lr: 0.02\n",
      "iteration: 400130 loss: 0.0015 lr: 0.02\n",
      "iteration: 400140 loss: 0.0018 lr: 0.02\n",
      "iteration: 400150 loss: 0.0016 lr: 0.02\n",
      "iteration: 400160 loss: 0.0016 lr: 0.02\n",
      "iteration: 400170 loss: 0.0019 lr: 0.02\n",
      "iteration: 400180 loss: 0.0019 lr: 0.02\n",
      "iteration: 400190 loss: 0.0014 lr: 0.02\n",
      "iteration: 400200 loss: 0.0012 lr: 0.02\n",
      "iteration: 400210 loss: 0.0013 lr: 0.02\n",
      "iteration: 400220 loss: 0.0015 lr: 0.02\n",
      "iteration: 400230 loss: 0.0015 lr: 0.02\n",
      "iteration: 400240 loss: 0.0015 lr: 0.02\n",
      "iteration: 400250 loss: 0.0022 lr: 0.02\n",
      "iteration: 400260 loss: 0.0015 lr: 0.02\n",
      "iteration: 400270 loss: 0.0015 lr: 0.02\n",
      "iteration: 400280 loss: 0.0020 lr: 0.02\n",
      "iteration: 400290 loss: 0.0010 lr: 0.02\n",
      "iteration: 400300 loss: 0.0023 lr: 0.02\n",
      "iteration: 400310 loss: 0.0036 lr: 0.02\n",
      "iteration: 400320 loss: 0.0015 lr: 0.02\n",
      "iteration: 400330 loss: 0.0016 lr: 0.02\n",
      "iteration: 400340 loss: 0.0016 lr: 0.02\n",
      "iteration: 400350 loss: 0.0012 lr: 0.02\n",
      "iteration: 400360 loss: 0.0012 lr: 0.02\n",
      "iteration: 400370 loss: 0.0017 lr: 0.02\n",
      "iteration: 400380 loss: 0.0020 lr: 0.02\n",
      "iteration: 400390 loss: 0.0012 lr: 0.02\n",
      "iteration: 400400 loss: 0.0015 lr: 0.02\n",
      "iteration: 400410 loss: 0.0016 lr: 0.02\n",
      "iteration: 400420 loss: 0.0021 lr: 0.02\n",
      "iteration: 400430 loss: 0.0020 lr: 0.02\n",
      "iteration: 400440 loss: 0.0012 lr: 0.02\n",
      "iteration: 400450 loss: 0.0015 lr: 0.02\n",
      "iteration: 400460 loss: 0.0015 lr: 0.02\n",
      "iteration: 400470 loss: 0.0020 lr: 0.02\n",
      "iteration: 400480 loss: 0.0015 lr: 0.02\n",
      "iteration: 400490 loss: 0.0024 lr: 0.02\n",
      "iteration: 400500 loss: 0.0018 lr: 0.02\n",
      "iteration: 400510 loss: 0.0012 lr: 0.02\n",
      "iteration: 400520 loss: 0.0013 lr: 0.02\n",
      "iteration: 400530 loss: 0.0020 lr: 0.02\n",
      "iteration: 400540 loss: 0.0016 lr: 0.02\n",
      "iteration: 400550 loss: 0.0013 lr: 0.02\n",
      "iteration: 400560 loss: 0.0017 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 400570 loss: 0.0014 lr: 0.02\n",
      "iteration: 400580 loss: 0.0015 lr: 0.02\n",
      "iteration: 400590 loss: 0.0013 lr: 0.02\n",
      "iteration: 400600 loss: 0.0029 lr: 0.02\n",
      "iteration: 400610 loss: 0.0020 lr: 0.02\n",
      "iteration: 400620 loss: 0.0012 lr: 0.02\n",
      "iteration: 400630 loss: 0.0020 lr: 0.02\n",
      "iteration: 400640 loss: 0.0017 lr: 0.02\n",
      "iteration: 400650 loss: 0.0013 lr: 0.02\n",
      "iteration: 400660 loss: 0.0024 lr: 0.02\n",
      "iteration: 400670 loss: 0.0015 lr: 0.02\n",
      "iteration: 400680 loss: 0.0028 lr: 0.02\n",
      "iteration: 400690 loss: 0.0014 lr: 0.02\n",
      "iteration: 400700 loss: 0.0014 lr: 0.02\n",
      "iteration: 400710 loss: 0.0010 lr: 0.02\n",
      "iteration: 400720 loss: 0.0015 lr: 0.02\n",
      "iteration: 400730 loss: 0.0016 lr: 0.02\n",
      "iteration: 400740 loss: 0.0015 lr: 0.02\n",
      "iteration: 400750 loss: 0.0016 lr: 0.02\n",
      "iteration: 400760 loss: 0.0020 lr: 0.02\n",
      "iteration: 400770 loss: 0.0011 lr: 0.02\n",
      "iteration: 400780 loss: 0.0012 lr: 0.02\n",
      "iteration: 400790 loss: 0.0017 lr: 0.02\n",
      "iteration: 400800 loss: 0.0013 lr: 0.02\n",
      "iteration: 400810 loss: 0.0030 lr: 0.02\n",
      "iteration: 400820 loss: 0.0015 lr: 0.02\n",
      "iteration: 400830 loss: 0.0014 lr: 0.02\n",
      "iteration: 400840 loss: 0.0010 lr: 0.02\n",
      "iteration: 400850 loss: 0.0016 lr: 0.02\n",
      "iteration: 400860 loss: 0.0010 lr: 0.02\n",
      "iteration: 400870 loss: 0.0014 lr: 0.02\n",
      "iteration: 400880 loss: 0.0016 lr: 0.02\n",
      "iteration: 400890 loss: 0.0022 lr: 0.02\n",
      "iteration: 400900 loss: 0.0018 lr: 0.02\n",
      "iteration: 400910 loss: 0.0014 lr: 0.02\n",
      "iteration: 400920 loss: 0.0012 lr: 0.02\n",
      "iteration: 400930 loss: 0.0022 lr: 0.02\n",
      "iteration: 400940 loss: 0.0014 lr: 0.02\n",
      "iteration: 400950 loss: 0.0019 lr: 0.02\n",
      "iteration: 400960 loss: 0.0018 lr: 0.02\n",
      "iteration: 400970 loss: 0.0025 lr: 0.02\n",
      "iteration: 400980 loss: 0.0015 lr: 0.02\n",
      "iteration: 400990 loss: 0.0018 lr: 0.02\n",
      "iteration: 401000 loss: 0.0017 lr: 0.02\n",
      "iteration: 401010 loss: 0.0012 lr: 0.02\n",
      "iteration: 401020 loss: 0.0020 lr: 0.02\n",
      "iteration: 401030 loss: 0.0018 lr: 0.02\n",
      "iteration: 401040 loss: 0.0017 lr: 0.02\n",
      "iteration: 401050 loss: 0.0016 lr: 0.02\n",
      "iteration: 401060 loss: 0.0013 lr: 0.02\n",
      "iteration: 401070 loss: 0.0010 lr: 0.02\n",
      "iteration: 401080 loss: 0.0013 lr: 0.02\n",
      "iteration: 401090 loss: 0.0019 lr: 0.02\n",
      "iteration: 401100 loss: 0.0012 lr: 0.02\n",
      "iteration: 401110 loss: 0.0009 lr: 0.02\n",
      "iteration: 401120 loss: 0.0017 lr: 0.02\n",
      "iteration: 401130 loss: 0.0013 lr: 0.02\n",
      "iteration: 401140 loss: 0.0010 lr: 0.02\n",
      "iteration: 401150 loss: 0.0012 lr: 0.02\n",
      "iteration: 401160 loss: 0.0016 lr: 0.02\n",
      "iteration: 401170 loss: 0.0019 lr: 0.02\n",
      "iteration: 401180 loss: 0.0015 lr: 0.02\n",
      "iteration: 401190 loss: 0.0010 lr: 0.02\n",
      "iteration: 401200 loss: 0.0014 lr: 0.02\n",
      "iteration: 401210 loss: 0.0011 lr: 0.02\n",
      "iteration: 401220 loss: 0.0013 lr: 0.02\n",
      "iteration: 401230 loss: 0.0012 lr: 0.02\n",
      "iteration: 401240 loss: 0.0011 lr: 0.02\n",
      "iteration: 401250 loss: 0.0015 lr: 0.02\n",
      "iteration: 401260 loss: 0.0014 lr: 0.02\n",
      "iteration: 401270 loss: 0.0018 lr: 0.02\n",
      "iteration: 401280 loss: 0.0025 lr: 0.02\n",
      "iteration: 401290 loss: 0.0014 lr: 0.02\n",
      "iteration: 401300 loss: 0.0014 lr: 0.02\n",
      "iteration: 401310 loss: 0.0023 lr: 0.02\n",
      "iteration: 401320 loss: 0.0013 lr: 0.02\n",
      "iteration: 401330 loss: 0.0014 lr: 0.02\n",
      "iteration: 401340 loss: 0.0016 lr: 0.02\n",
      "iteration: 401350 loss: 0.0016 lr: 0.02\n",
      "iteration: 401360 loss: 0.0013 lr: 0.02\n",
      "iteration: 401370 loss: 0.0017 lr: 0.02\n",
      "iteration: 401380 loss: 0.0013 lr: 0.02\n",
      "iteration: 401390 loss: 0.0013 lr: 0.02\n",
      "iteration: 401400 loss: 0.0015 lr: 0.02\n",
      "iteration: 401410 loss: 0.0017 lr: 0.02\n",
      "iteration: 401420 loss: 0.0012 lr: 0.02\n",
      "iteration: 401430 loss: 0.0016 lr: 0.02\n",
      "iteration: 401440 loss: 0.0016 lr: 0.02\n",
      "iteration: 401450 loss: 0.0017 lr: 0.02\n",
      "iteration: 401460 loss: 0.0016 lr: 0.02\n",
      "iteration: 401470 loss: 0.0010 lr: 0.02\n",
      "iteration: 401480 loss: 0.0018 lr: 0.02\n",
      "iteration: 401490 loss: 0.0017 lr: 0.02\n",
      "iteration: 401500 loss: 0.0014 lr: 0.02\n",
      "iteration: 401510 loss: 0.0014 lr: 0.02\n",
      "iteration: 401520 loss: 0.0012 lr: 0.02\n",
      "iteration: 401530 loss: 0.0021 lr: 0.02\n",
      "iteration: 401540 loss: 0.0016 lr: 0.02\n",
      "iteration: 401550 loss: 0.0025 lr: 0.02\n",
      "iteration: 401560 loss: 0.0024 lr: 0.02\n",
      "iteration: 401570 loss: 0.0026 lr: 0.02\n",
      "iteration: 401580 loss: 0.0016 lr: 0.02\n",
      "iteration: 401590 loss: 0.0010 lr: 0.02\n",
      "iteration: 401600 loss: 0.0011 lr: 0.02\n",
      "iteration: 401610 loss: 0.0019 lr: 0.02\n",
      "iteration: 401620 loss: 0.0022 lr: 0.02\n",
      "iteration: 401630 loss: 0.0013 lr: 0.02\n",
      "iteration: 401640 loss: 0.0016 lr: 0.02\n",
      "iteration: 401650 loss: 0.0013 lr: 0.02\n",
      "iteration: 401660 loss: 0.0014 lr: 0.02\n",
      "iteration: 401670 loss: 0.0021 lr: 0.02\n",
      "iteration: 401680 loss: 0.0015 lr: 0.02\n",
      "iteration: 401690 loss: 0.0020 lr: 0.02\n",
      "iteration: 401700 loss: 0.0024 lr: 0.02\n",
      "iteration: 401710 loss: 0.0018 lr: 0.02\n",
      "iteration: 401720 loss: 0.0019 lr: 0.02\n",
      "iteration: 401730 loss: 0.0014 lr: 0.02\n",
      "iteration: 401740 loss: 0.0016 lr: 0.02\n",
      "iteration: 401750 loss: 0.0023 lr: 0.02\n",
      "iteration: 401760 loss: 0.0016 lr: 0.02\n",
      "iteration: 401770 loss: 0.0015 lr: 0.02\n",
      "iteration: 401780 loss: 0.0016 lr: 0.02\n",
      "iteration: 401790 loss: 0.0016 lr: 0.02\n",
      "iteration: 401800 loss: 0.0015 lr: 0.02\n",
      "iteration: 401810 loss: 0.0010 lr: 0.02\n",
      "iteration: 401820 loss: 0.0012 lr: 0.02\n",
      "iteration: 401830 loss: 0.0018 lr: 0.02\n",
      "iteration: 401840 loss: 0.0017 lr: 0.02\n",
      "iteration: 401850 loss: 0.0013 lr: 0.02\n",
      "iteration: 401860 loss: 0.0014 lr: 0.02\n",
      "iteration: 401870 loss: 0.0022 lr: 0.02\n",
      "iteration: 401880 loss: 0.0024 lr: 0.02\n",
      "iteration: 401890 loss: 0.0014 lr: 0.02\n",
      "iteration: 401900 loss: 0.0017 lr: 0.02\n",
      "iteration: 401910 loss: 0.0015 lr: 0.02\n",
      "iteration: 401920 loss: 0.0015 lr: 0.02\n",
      "iteration: 401930 loss: 0.0015 lr: 0.02\n",
      "iteration: 401940 loss: 0.0012 lr: 0.02\n",
      "iteration: 401950 loss: 0.0012 lr: 0.02\n",
      "iteration: 401960 loss: 0.0013 lr: 0.02\n",
      "iteration: 401970 loss: 0.0010 lr: 0.02\n",
      "iteration: 401980 loss: 0.0017 lr: 0.02\n",
      "iteration: 401990 loss: 0.0011 lr: 0.02\n",
      "iteration: 402000 loss: 0.0021 lr: 0.02\n",
      "iteration: 402010 loss: 0.0021 lr: 0.02\n",
      "iteration: 402020 loss: 0.0021 lr: 0.02\n",
      "iteration: 402030 loss: 0.0012 lr: 0.02\n",
      "iteration: 402040 loss: 0.0013 lr: 0.02\n",
      "iteration: 402050 loss: 0.0017 lr: 0.02\n",
      "iteration: 402060 loss: 0.0017 lr: 0.02\n",
      "iteration: 402070 loss: 0.0016 lr: 0.02\n",
      "iteration: 402080 loss: 0.0019 lr: 0.02\n",
      "iteration: 402090 loss: 0.0017 lr: 0.02\n",
      "iteration: 402100 loss: 0.0015 lr: 0.02\n",
      "iteration: 402110 loss: 0.0022 lr: 0.02\n",
      "iteration: 402120 loss: 0.0013 lr: 0.02\n",
      "iteration: 402130 loss: 0.0016 lr: 0.02\n",
      "iteration: 402140 loss: 0.0022 lr: 0.02\n",
      "iteration: 402150 loss: 0.0019 lr: 0.02\n",
      "iteration: 402160 loss: 0.0014 lr: 0.02\n",
      "iteration: 402170 loss: 0.0022 lr: 0.02\n",
      "iteration: 402180 loss: 0.0014 lr: 0.02\n",
      "iteration: 402190 loss: 0.0015 lr: 0.02\n",
      "iteration: 402200 loss: 0.0015 lr: 0.02\n",
      "iteration: 402210 loss: 0.0024 lr: 0.02\n",
      "iteration: 402220 loss: 0.0019 lr: 0.02\n",
      "iteration: 402230 loss: 0.0017 lr: 0.02\n",
      "iteration: 402240 loss: 0.0013 lr: 0.02\n",
      "iteration: 402250 loss: 0.0012 lr: 0.02\n",
      "iteration: 402260 loss: 0.0015 lr: 0.02\n",
      "iteration: 402270 loss: 0.0012 lr: 0.02\n",
      "iteration: 402280 loss: 0.0017 lr: 0.02\n",
      "iteration: 402290 loss: 0.0016 lr: 0.02\n",
      "iteration: 402300 loss: 0.0021 lr: 0.02\n",
      "iteration: 402310 loss: 0.0020 lr: 0.02\n",
      "iteration: 402320 loss: 0.0016 lr: 0.02\n",
      "iteration: 402330 loss: 0.0015 lr: 0.02\n",
      "iteration: 402340 loss: 0.0017 lr: 0.02\n",
      "iteration: 402350 loss: 0.0016 lr: 0.02\n",
      "iteration: 402360 loss: 0.0016 lr: 0.02\n",
      "iteration: 402370 loss: 0.0009 lr: 0.02\n",
      "iteration: 402380 loss: 0.0018 lr: 0.02\n",
      "iteration: 402390 loss: 0.0012 lr: 0.02\n",
      "iteration: 402400 loss: 0.0010 lr: 0.02\n",
      "iteration: 402410 loss: 0.0012 lr: 0.02\n",
      "iteration: 402420 loss: 0.0011 lr: 0.02\n",
      "iteration: 402430 loss: 0.0015 lr: 0.02\n",
      "iteration: 402440 loss: 0.0013 lr: 0.02\n",
      "iteration: 402450 loss: 0.0014 lr: 0.02\n",
      "iteration: 402460 loss: 0.0024 lr: 0.02\n",
      "iteration: 402470 loss: 0.0022 lr: 0.02\n",
      "iteration: 402480 loss: 0.0018 lr: 0.02\n",
      "iteration: 402490 loss: 0.0011 lr: 0.02\n",
      "iteration: 402500 loss: 0.0022 lr: 0.02\n",
      "iteration: 402510 loss: 0.0015 lr: 0.02\n",
      "iteration: 402520 loss: 0.0018 lr: 0.02\n",
      "iteration: 402530 loss: 0.0019 lr: 0.02\n",
      "iteration: 402540 loss: 0.0013 lr: 0.02\n",
      "iteration: 402550 loss: 0.0015 lr: 0.02\n",
      "iteration: 402560 loss: 0.0019 lr: 0.02\n",
      "iteration: 402570 loss: 0.0020 lr: 0.02\n",
      "iteration: 402580 loss: 0.0012 lr: 0.02\n",
      "iteration: 402590 loss: 0.0016 lr: 0.02\n",
      "iteration: 402600 loss: 0.0016 lr: 0.02\n",
      "iteration: 402610 loss: 0.0015 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 402620 loss: 0.0016 lr: 0.02\n",
      "iteration: 402630 loss: 0.0011 lr: 0.02\n",
      "iteration: 402640 loss: 0.0017 lr: 0.02\n",
      "iteration: 402650 loss: 0.0019 lr: 0.02\n",
      "iteration: 402660 loss: 0.0015 lr: 0.02\n",
      "iteration: 402670 loss: 0.0013 lr: 0.02\n",
      "iteration: 402680 loss: 0.0014 lr: 0.02\n",
      "iteration: 402690 loss: 0.0013 lr: 0.02\n",
      "iteration: 402700 loss: 0.0019 lr: 0.02\n",
      "iteration: 402710 loss: 0.0014 lr: 0.02\n",
      "iteration: 402720 loss: 0.0013 lr: 0.02\n",
      "iteration: 402730 loss: 0.0012 lr: 0.02\n",
      "iteration: 402740 loss: 0.0014 lr: 0.02\n",
      "iteration: 402750 loss: 0.0024 lr: 0.02\n",
      "iteration: 402760 loss: 0.0015 lr: 0.02\n",
      "iteration: 402770 loss: 0.0019 lr: 0.02\n",
      "iteration: 402780 loss: 0.0012 lr: 0.02\n",
      "iteration: 402790 loss: 0.0009 lr: 0.02\n",
      "iteration: 402800 loss: 0.0019 lr: 0.02\n",
      "iteration: 402810 loss: 0.0017 lr: 0.02\n",
      "iteration: 402820 loss: 0.0015 lr: 0.02\n",
      "iteration: 402830 loss: 0.0013 lr: 0.02\n",
      "iteration: 402840 loss: 0.0014 lr: 0.02\n",
      "iteration: 402850 loss: 0.0017 lr: 0.02\n",
      "iteration: 402860 loss: 0.0013 lr: 0.02\n",
      "iteration: 402870 loss: 0.0012 lr: 0.02\n",
      "iteration: 402880 loss: 0.0014 lr: 0.02\n",
      "iteration: 402890 loss: 0.0017 lr: 0.02\n",
      "iteration: 402900 loss: 0.0013 lr: 0.02\n",
      "iteration: 402910 loss: 0.0012 lr: 0.02\n",
      "iteration: 402920 loss: 0.0014 lr: 0.02\n",
      "iteration: 402930 loss: 0.0016 lr: 0.02\n",
      "iteration: 402940 loss: 0.0021 lr: 0.02\n",
      "iteration: 402950 loss: 0.0015 lr: 0.02\n",
      "iteration: 402960 loss: 0.0017 lr: 0.02\n",
      "iteration: 402970 loss: 0.0023 lr: 0.02\n",
      "iteration: 402980 loss: 0.0032 lr: 0.02\n",
      "iteration: 402990 loss: 0.0014 lr: 0.02\n",
      "iteration: 403000 loss: 0.0016 lr: 0.02\n",
      "iteration: 403010 loss: 0.0015 lr: 0.02\n",
      "iteration: 403020 loss: 0.0019 lr: 0.02\n",
      "iteration: 403030 loss: 0.0026 lr: 0.02\n",
      "iteration: 403040 loss: 0.0016 lr: 0.02\n",
      "iteration: 403050 loss: 0.0014 lr: 0.02\n",
      "iteration: 403060 loss: 0.0014 lr: 0.02\n",
      "iteration: 403070 loss: 0.0013 lr: 0.02\n",
      "iteration: 403080 loss: 0.0015 lr: 0.02\n",
      "iteration: 403090 loss: 0.0015 lr: 0.02\n",
      "iteration: 403100 loss: 0.0019 lr: 0.02\n",
      "iteration: 403110 loss: 0.0011 lr: 0.02\n",
      "iteration: 403120 loss: 0.0012 lr: 0.02\n",
      "iteration: 403130 loss: 0.0014 lr: 0.02\n",
      "iteration: 403140 loss: 0.0014 lr: 0.02\n",
      "iteration: 403150 loss: 0.0016 lr: 0.02\n",
      "iteration: 403160 loss: 0.0014 lr: 0.02\n",
      "iteration: 403170 loss: 0.0020 lr: 0.02\n",
      "iteration: 403180 loss: 0.0016 lr: 0.02\n",
      "iteration: 403190 loss: 0.0017 lr: 0.02\n",
      "iteration: 403200 loss: 0.0012 lr: 0.02\n",
      "iteration: 403210 loss: 0.0017 lr: 0.02\n",
      "iteration: 403220 loss: 0.0013 lr: 0.02\n",
      "iteration: 403230 loss: 0.0013 lr: 0.02\n",
      "iteration: 403240 loss: 0.0015 lr: 0.02\n",
      "iteration: 403250 loss: 0.0019 lr: 0.02\n",
      "iteration: 403260 loss: 0.0017 lr: 0.02\n",
      "iteration: 403270 loss: 0.0016 lr: 0.02\n",
      "iteration: 403280 loss: 0.0012 lr: 0.02\n",
      "iteration: 403290 loss: 0.0016 lr: 0.02\n",
      "iteration: 403300 loss: 0.0019 lr: 0.02\n",
      "iteration: 403310 loss: 0.0016 lr: 0.02\n",
      "iteration: 403320 loss: 0.0016 lr: 0.02\n",
      "iteration: 403330 loss: 0.0016 lr: 0.02\n",
      "iteration: 403340 loss: 0.0019 lr: 0.02\n",
      "iteration: 403350 loss: 0.0013 lr: 0.02\n",
      "iteration: 403360 loss: 0.0013 lr: 0.02\n",
      "iteration: 403370 loss: 0.0011 lr: 0.02\n",
      "iteration: 403380 loss: 0.0017 lr: 0.02\n",
      "iteration: 403390 loss: 0.0018 lr: 0.02\n",
      "iteration: 403400 loss: 0.0018 lr: 0.02\n",
      "iteration: 403410 loss: 0.0017 lr: 0.02\n",
      "iteration: 403420 loss: 0.0013 lr: 0.02\n",
      "iteration: 403430 loss: 0.0016 lr: 0.02\n",
      "iteration: 403440 loss: 0.0013 lr: 0.02\n",
      "iteration: 403450 loss: 0.0016 lr: 0.02\n",
      "iteration: 403460 loss: 0.0012 lr: 0.02\n",
      "iteration: 403470 loss: 0.0023 lr: 0.02\n",
      "iteration: 403480 loss: 0.0020 lr: 0.02\n",
      "iteration: 403490 loss: 0.0015 lr: 0.02\n",
      "iteration: 403500 loss: 0.0021 lr: 0.02\n",
      "iteration: 403510 loss: 0.0015 lr: 0.02\n",
      "iteration: 403520 loss: 0.0018 lr: 0.02\n",
      "iteration: 403530 loss: 0.0018 lr: 0.02\n",
      "iteration: 403540 loss: 0.0012 lr: 0.02\n",
      "iteration: 403550 loss: 0.0010 lr: 0.02\n",
      "iteration: 403560 loss: 0.0015 lr: 0.02\n",
      "iteration: 403570 loss: 0.0016 lr: 0.02\n",
      "iteration: 403580 loss: 0.0014 lr: 0.02\n",
      "iteration: 403590 loss: 0.0017 lr: 0.02\n",
      "iteration: 403600 loss: 0.0028 lr: 0.02\n",
      "iteration: 403610 loss: 0.0014 lr: 0.02\n",
      "iteration: 403620 loss: 0.0019 lr: 0.02\n",
      "iteration: 403630 loss: 0.0021 lr: 0.02\n",
      "iteration: 403640 loss: 0.0012 lr: 0.02\n",
      "iteration: 403650 loss: 0.0015 lr: 0.02\n",
      "iteration: 403660 loss: 0.0016 lr: 0.02\n",
      "iteration: 403670 loss: 0.0023 lr: 0.02\n",
      "iteration: 403680 loss: 0.0013 lr: 0.02\n",
      "iteration: 403690 loss: 0.0014 lr: 0.02\n",
      "iteration: 403700 loss: 0.0014 lr: 0.02\n",
      "iteration: 403710 loss: 0.0012 lr: 0.02\n",
      "iteration: 403720 loss: 0.0014 lr: 0.02\n",
      "iteration: 403730 loss: 0.0019 lr: 0.02\n",
      "iteration: 403740 loss: 0.0012 lr: 0.02\n",
      "iteration: 403750 loss: 0.0014 lr: 0.02\n",
      "iteration: 403760 loss: 0.0020 lr: 0.02\n",
      "iteration: 403770 loss: 0.0013 lr: 0.02\n",
      "iteration: 403780 loss: 0.0012 lr: 0.02\n",
      "iteration: 403790 loss: 0.0021 lr: 0.02\n",
      "iteration: 403800 loss: 0.0012 lr: 0.02\n",
      "iteration: 403810 loss: 0.0010 lr: 0.02\n",
      "iteration: 403820 loss: 0.0019 lr: 0.02\n",
      "iteration: 403830 loss: 0.0020 lr: 0.02\n",
      "iteration: 403840 loss: 0.0013 lr: 0.02\n",
      "iteration: 403850 loss: 0.0021 lr: 0.02\n",
      "iteration: 403860 loss: 0.0019 lr: 0.02\n",
      "iteration: 403870 loss: 0.0026 lr: 0.02\n",
      "iteration: 403880 loss: 0.0015 lr: 0.02\n",
      "iteration: 403890 loss: 0.0020 lr: 0.02\n",
      "iteration: 403900 loss: 0.0014 lr: 0.02\n",
      "iteration: 403910 loss: 0.0012 lr: 0.02\n",
      "iteration: 403920 loss: 0.0017 lr: 0.02\n",
      "iteration: 403930 loss: 0.0016 lr: 0.02\n",
      "iteration: 403940 loss: 0.0022 lr: 0.02\n",
      "iteration: 403950 loss: 0.0012 lr: 0.02\n",
      "iteration: 403960 loss: 0.0016 lr: 0.02\n",
      "iteration: 403970 loss: 0.0014 lr: 0.02\n",
      "iteration: 403980 loss: 0.0014 lr: 0.02\n",
      "iteration: 403990 loss: 0.0017 lr: 0.02\n",
      "iteration: 404000 loss: 0.0018 lr: 0.02\n",
      "iteration: 404010 loss: 0.0014 lr: 0.02\n",
      "iteration: 404020 loss: 0.0019 lr: 0.02\n",
      "iteration: 404030 loss: 0.0017 lr: 0.02\n",
      "iteration: 404040 loss: 0.0014 lr: 0.02\n",
      "iteration: 404050 loss: 0.0015 lr: 0.02\n",
      "iteration: 404060 loss: 0.0016 lr: 0.02\n",
      "iteration: 404070 loss: 0.0016 lr: 0.02\n",
      "iteration: 404080 loss: 0.0018 lr: 0.02\n",
      "iteration: 404090 loss: 0.0008 lr: 0.02\n",
      "iteration: 404100 loss: 0.0019 lr: 0.02\n",
      "iteration: 404110 loss: 0.0017 lr: 0.02\n",
      "iteration: 404120 loss: 0.0024 lr: 0.02\n",
      "iteration: 404130 loss: 0.0019 lr: 0.02\n",
      "iteration: 404140 loss: 0.0027 lr: 0.02\n",
      "iteration: 404150 loss: 0.0019 lr: 0.02\n",
      "iteration: 404160 loss: 0.0016 lr: 0.02\n",
      "iteration: 404170 loss: 0.0013 lr: 0.02\n",
      "iteration: 404180 loss: 0.0010 lr: 0.02\n",
      "iteration: 404190 loss: 0.0015 lr: 0.02\n",
      "iteration: 404200 loss: 0.0017 lr: 0.02\n",
      "iteration: 404210 loss: 0.0017 lr: 0.02\n",
      "iteration: 404220 loss: 0.0016 lr: 0.02\n",
      "iteration: 404230 loss: 0.0016 lr: 0.02\n",
      "iteration: 404240 loss: 0.0013 lr: 0.02\n",
      "iteration: 404250 loss: 0.0016 lr: 0.02\n",
      "iteration: 404260 loss: 0.0024 lr: 0.02\n",
      "iteration: 404270 loss: 0.0010 lr: 0.02\n",
      "iteration: 404280 loss: 0.0014 lr: 0.02\n",
      "iteration: 404290 loss: 0.0014 lr: 0.02\n",
      "iteration: 404300 loss: 0.0018 lr: 0.02\n",
      "iteration: 404310 loss: 0.0016 lr: 0.02\n",
      "iteration: 404320 loss: 0.0016 lr: 0.02\n",
      "iteration: 404330 loss: 0.0019 lr: 0.02\n",
      "iteration: 404340 loss: 0.0014 lr: 0.02\n",
      "iteration: 404350 loss: 0.0011 lr: 0.02\n",
      "iteration: 404360 loss: 0.0015 lr: 0.02\n",
      "iteration: 404370 loss: 0.0012 lr: 0.02\n",
      "iteration: 404380 loss: 0.0010 lr: 0.02\n",
      "iteration: 404390 loss: 0.0014 lr: 0.02\n",
      "iteration: 404400 loss: 0.0022 lr: 0.02\n",
      "iteration: 404410 loss: 0.0013 lr: 0.02\n",
      "iteration: 404420 loss: 0.0021 lr: 0.02\n",
      "iteration: 404430 loss: 0.0014 lr: 0.02\n",
      "iteration: 404440 loss: 0.0017 lr: 0.02\n",
      "iteration: 404450 loss: 0.0022 lr: 0.02\n",
      "iteration: 404460 loss: 0.0017 lr: 0.02\n",
      "iteration: 404470 loss: 0.0017 lr: 0.02\n",
      "iteration: 404480 loss: 0.0010 lr: 0.02\n",
      "iteration: 404490 loss: 0.0016 lr: 0.02\n",
      "iteration: 404500 loss: 0.0022 lr: 0.02\n",
      "iteration: 404510 loss: 0.0013 lr: 0.02\n",
      "iteration: 404520 loss: 0.0018 lr: 0.02\n",
      "iteration: 404530 loss: 0.0021 lr: 0.02\n",
      "iteration: 404540 loss: 0.0016 lr: 0.02\n",
      "iteration: 404550 loss: 0.0017 lr: 0.02\n",
      "iteration: 404560 loss: 0.0016 lr: 0.02\n",
      "iteration: 404570 loss: 0.0012 lr: 0.02\n",
      "iteration: 404580 loss: 0.0017 lr: 0.02\n",
      "iteration: 404590 loss: 0.0023 lr: 0.02\n",
      "iteration: 404600 loss: 0.0012 lr: 0.02\n",
      "iteration: 404610 loss: 0.0016 lr: 0.02\n",
      "iteration: 404620 loss: 0.0022 lr: 0.02\n",
      "iteration: 404630 loss: 0.0019 lr: 0.02\n",
      "iteration: 404640 loss: 0.0026 lr: 0.02\n",
      "iteration: 404650 loss: 0.0018 lr: 0.02\n",
      "iteration: 404660 loss: 0.0017 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 404670 loss: 0.0014 lr: 0.02\n",
      "iteration: 404680 loss: 0.0017 lr: 0.02\n",
      "iteration: 404690 loss: 0.0020 lr: 0.02\n",
      "iteration: 404700 loss: 0.0017 lr: 0.02\n",
      "iteration: 404710 loss: 0.0021 lr: 0.02\n",
      "iteration: 404720 loss: 0.0014 lr: 0.02\n",
      "iteration: 404730 loss: 0.0015 lr: 0.02\n",
      "iteration: 404740 loss: 0.0014 lr: 0.02\n",
      "iteration: 404750 loss: 0.0018 lr: 0.02\n",
      "iteration: 404760 loss: 0.0013 lr: 0.02\n",
      "iteration: 404770 loss: 0.0017 lr: 0.02\n",
      "iteration: 404780 loss: 0.0022 lr: 0.02\n",
      "iteration: 404790 loss: 0.0015 lr: 0.02\n",
      "iteration: 404800 loss: 0.0014 lr: 0.02\n",
      "iteration: 404810 loss: 0.0019 lr: 0.02\n",
      "iteration: 404820 loss: 0.0014 lr: 0.02\n",
      "iteration: 404830 loss: 0.0021 lr: 0.02\n",
      "iteration: 404840 loss: 0.0020 lr: 0.02\n",
      "iteration: 404850 loss: 0.0015 lr: 0.02\n",
      "iteration: 404860 loss: 0.0019 lr: 0.02\n",
      "iteration: 404870 loss: 0.0015 lr: 0.02\n",
      "iteration: 404880 loss: 0.0014 lr: 0.02\n",
      "iteration: 404890 loss: 0.0015 lr: 0.02\n",
      "iteration: 404900 loss: 0.0018 lr: 0.02\n",
      "iteration: 404910 loss: 0.0015 lr: 0.02\n",
      "iteration: 404920 loss: 0.0013 lr: 0.02\n",
      "iteration: 404930 loss: 0.0016 lr: 0.02\n",
      "iteration: 404940 loss: 0.0015 lr: 0.02\n",
      "iteration: 404950 loss: 0.0025 lr: 0.02\n",
      "iteration: 404960 loss: 0.0021 lr: 0.02\n",
      "iteration: 404970 loss: 0.0023 lr: 0.02\n",
      "iteration: 404980 loss: 0.0013 lr: 0.02\n",
      "iteration: 404990 loss: 0.0020 lr: 0.02\n",
      "iteration: 405000 loss: 0.0016 lr: 0.02\n",
      "iteration: 405010 loss: 0.0018 lr: 0.02\n",
      "iteration: 405020 loss: 0.0019 lr: 0.02\n",
      "iteration: 405030 loss: 0.0015 lr: 0.02\n",
      "iteration: 405040 loss: 0.0023 lr: 0.02\n",
      "iteration: 405050 loss: 0.0020 lr: 0.02\n",
      "iteration: 405060 loss: 0.0017 lr: 0.02\n",
      "iteration: 405070 loss: 0.0012 lr: 0.02\n",
      "iteration: 405080 loss: 0.0021 lr: 0.02\n",
      "iteration: 405090 loss: 0.0021 lr: 0.02\n",
      "iteration: 405100 loss: 0.0014 lr: 0.02\n",
      "iteration: 405110 loss: 0.0014 lr: 0.02\n",
      "iteration: 405120 loss: 0.0016 lr: 0.02\n",
      "iteration: 405130 loss: 0.0019 lr: 0.02\n",
      "iteration: 405140 loss: 0.0012 lr: 0.02\n",
      "iteration: 405150 loss: 0.0014 lr: 0.02\n",
      "iteration: 405160 loss: 0.0023 lr: 0.02\n",
      "iteration: 405170 loss: 0.0014 lr: 0.02\n",
      "iteration: 405180 loss: 0.0016 lr: 0.02\n",
      "iteration: 405190 loss: 0.0016 lr: 0.02\n",
      "iteration: 405200 loss: 0.0019 lr: 0.02\n",
      "iteration: 405210 loss: 0.0017 lr: 0.02\n",
      "iteration: 405220 loss: 0.0014 lr: 0.02\n",
      "iteration: 405230 loss: 0.0027 lr: 0.02\n",
      "iteration: 405240 loss: 0.0019 lr: 0.02\n",
      "iteration: 405250 loss: 0.0016 lr: 0.02\n",
      "iteration: 405260 loss: 0.0010 lr: 0.02\n",
      "iteration: 405270 loss: 0.0011 lr: 0.02\n",
      "iteration: 405280 loss: 0.0023 lr: 0.02\n",
      "iteration: 405290 loss: 0.0014 lr: 0.02\n",
      "iteration: 405300 loss: 0.0013 lr: 0.02\n",
      "iteration: 405310 loss: 0.0014 lr: 0.02\n",
      "iteration: 405320 loss: 0.0024 lr: 0.02\n",
      "iteration: 405330 loss: 0.0012 lr: 0.02\n",
      "iteration: 405340 loss: 0.0014 lr: 0.02\n",
      "iteration: 405350 loss: 0.0015 lr: 0.02\n",
      "iteration: 405360 loss: 0.0015 lr: 0.02\n",
      "iteration: 405370 loss: 0.0014 lr: 0.02\n",
      "iteration: 405380 loss: 0.0013 lr: 0.02\n",
      "iteration: 405390 loss: 0.0014 lr: 0.02\n",
      "iteration: 405400 loss: 0.0014 lr: 0.02\n",
      "iteration: 405410 loss: 0.0017 lr: 0.02\n",
      "iteration: 405420 loss: 0.0014 lr: 0.02\n",
      "iteration: 405430 loss: 0.0013 lr: 0.02\n",
      "iteration: 405440 loss: 0.0012 lr: 0.02\n",
      "iteration: 405450 loss: 0.0035 lr: 0.02\n",
      "iteration: 405460 loss: 0.0013 lr: 0.02\n",
      "iteration: 405470 loss: 0.0015 lr: 0.02\n",
      "iteration: 405480 loss: 0.0014 lr: 0.02\n",
      "iteration: 405490 loss: 0.0014 lr: 0.02\n",
      "iteration: 405500 loss: 0.0017 lr: 0.02\n",
      "iteration: 405510 loss: 0.0013 lr: 0.02\n",
      "iteration: 405520 loss: 0.0016 lr: 0.02\n",
      "iteration: 405530 loss: 0.0014 lr: 0.02\n",
      "iteration: 405540 loss: 0.0016 lr: 0.02\n",
      "iteration: 405550 loss: 0.0016 lr: 0.02\n",
      "iteration: 405560 loss: 0.0012 lr: 0.02\n",
      "iteration: 405570 loss: 0.0018 lr: 0.02\n",
      "iteration: 405580 loss: 0.0017 lr: 0.02\n",
      "iteration: 405590 loss: 0.0013 lr: 0.02\n",
      "iteration: 405600 loss: 0.0032 lr: 0.02\n",
      "iteration: 405610 loss: 0.0012 lr: 0.02\n",
      "iteration: 405620 loss: 0.0016 lr: 0.02\n",
      "iteration: 405630 loss: 0.0014 lr: 0.02\n",
      "iteration: 405640 loss: 0.0017 lr: 0.02\n",
      "iteration: 405650 loss: 0.0012 lr: 0.02\n",
      "iteration: 405660 loss: 0.0019 lr: 0.02\n",
      "iteration: 405670 loss: 0.0013 lr: 0.02\n",
      "iteration: 405680 loss: 0.0018 lr: 0.02\n",
      "iteration: 405690 loss: 0.0014 lr: 0.02\n",
      "iteration: 405700 loss: 0.0017 lr: 0.02\n",
      "iteration: 405710 loss: 0.0014 lr: 0.02\n",
      "iteration: 405720 loss: 0.0012 lr: 0.02\n",
      "iteration: 405730 loss: 0.0010 lr: 0.02\n",
      "iteration: 405740 loss: 0.0014 lr: 0.02\n",
      "iteration: 405750 loss: 0.0012 lr: 0.02\n",
      "iteration: 405760 loss: 0.0015 lr: 0.02\n",
      "iteration: 405770 loss: 0.0020 lr: 0.02\n",
      "iteration: 405780 loss: 0.0019 lr: 0.02\n",
      "iteration: 405790 loss: 0.0012 lr: 0.02\n",
      "iteration: 405800 loss: 0.0015 lr: 0.02\n",
      "iteration: 405810 loss: 0.0015 lr: 0.02\n",
      "iteration: 405820 loss: 0.0012 lr: 0.02\n",
      "iteration: 405830 loss: 0.0014 lr: 0.02\n",
      "iteration: 405840 loss: 0.0015 lr: 0.02\n",
      "iteration: 405850 loss: 0.0021 lr: 0.02\n",
      "iteration: 405860 loss: 0.0016 lr: 0.02\n",
      "iteration: 405870 loss: 0.0016 lr: 0.02\n",
      "iteration: 405880 loss: 0.0011 lr: 0.02\n",
      "iteration: 405890 loss: 0.0022 lr: 0.02\n",
      "iteration: 405900 loss: 0.0017 lr: 0.02\n",
      "iteration: 405910 loss: 0.0017 lr: 0.02\n",
      "iteration: 405920 loss: 0.0013 lr: 0.02\n",
      "iteration: 405930 loss: 0.0019 lr: 0.02\n",
      "iteration: 405940 loss: 0.0022 lr: 0.02\n",
      "iteration: 405950 loss: 0.0015 lr: 0.02\n",
      "iteration: 405960 loss: 0.0016 lr: 0.02\n",
      "iteration: 405970 loss: 0.0015 lr: 0.02\n",
      "iteration: 405980 loss: 0.0016 lr: 0.02\n",
      "iteration: 405990 loss: 0.0014 lr: 0.02\n",
      "iteration: 406000 loss: 0.0015 lr: 0.02\n",
      "iteration: 406010 loss: 0.0016 lr: 0.02\n",
      "iteration: 406020 loss: 0.0013 lr: 0.02\n",
      "iteration: 406030 loss: 0.0018 lr: 0.02\n",
      "iteration: 406040 loss: 0.0017 lr: 0.02\n",
      "iteration: 406050 loss: 0.0012 lr: 0.02\n",
      "iteration: 406060 loss: 0.0017 lr: 0.02\n",
      "iteration: 406070 loss: 0.0014 lr: 0.02\n",
      "iteration: 406080 loss: 0.0014 lr: 0.02\n",
      "iteration: 406090 loss: 0.0017 lr: 0.02\n",
      "iteration: 406100 loss: 0.0016 lr: 0.02\n",
      "iteration: 406110 loss: 0.0015 lr: 0.02\n",
      "iteration: 406120 loss: 0.0015 lr: 0.02\n",
      "iteration: 406130 loss: 0.0014 lr: 0.02\n",
      "iteration: 406140 loss: 0.0015 lr: 0.02\n",
      "iteration: 406150 loss: 0.0016 lr: 0.02\n",
      "iteration: 406160 loss: 0.0011 lr: 0.02\n",
      "iteration: 406170 loss: 0.0019 lr: 0.02\n",
      "iteration: 406180 loss: 0.0014 lr: 0.02\n",
      "iteration: 406190 loss: 0.0013 lr: 0.02\n",
      "iteration: 406200 loss: 0.0018 lr: 0.02\n",
      "iteration: 406210 loss: 0.0021 lr: 0.02\n",
      "iteration: 406220 loss: 0.0019 lr: 0.02\n",
      "iteration: 406230 loss: 0.0019 lr: 0.02\n",
      "iteration: 406240 loss: 0.0012 lr: 0.02\n",
      "iteration: 406250 loss: 0.0013 lr: 0.02\n",
      "iteration: 406260 loss: 0.0015 lr: 0.02\n",
      "iteration: 406270 loss: 0.0016 lr: 0.02\n",
      "iteration: 406280 loss: 0.0010 lr: 0.02\n",
      "iteration: 406290 loss: 0.0012 lr: 0.02\n",
      "iteration: 406300 loss: 0.0013 lr: 0.02\n",
      "iteration: 406310 loss: 0.0011 lr: 0.02\n",
      "iteration: 406320 loss: 0.0018 lr: 0.02\n",
      "iteration: 406330 loss: 0.0013 lr: 0.02\n",
      "iteration: 406340 loss: 0.0015 lr: 0.02\n",
      "iteration: 406350 loss: 0.0009 lr: 0.02\n",
      "iteration: 406360 loss: 0.0020 lr: 0.02\n",
      "iteration: 406370 loss: 0.0014 lr: 0.02\n",
      "iteration: 406380 loss: 0.0014 lr: 0.02\n",
      "iteration: 406390 loss: 0.0016 lr: 0.02\n",
      "iteration: 406400 loss: 0.0017 lr: 0.02\n",
      "iteration: 406410 loss: 0.0015 lr: 0.02\n",
      "iteration: 406420 loss: 0.0017 lr: 0.02\n",
      "iteration: 406430 loss: 0.0028 lr: 0.02\n",
      "iteration: 406440 loss: 0.0016 lr: 0.02\n",
      "iteration: 406450 loss: 0.0020 lr: 0.02\n",
      "iteration: 406460 loss: 0.0018 lr: 0.02\n",
      "iteration: 406470 loss: 0.0019 lr: 0.02\n",
      "iteration: 406480 loss: 0.0012 lr: 0.02\n",
      "iteration: 406490 loss: 0.0011 lr: 0.02\n",
      "iteration: 406500 loss: 0.0014 lr: 0.02\n",
      "iteration: 406510 loss: 0.0020 lr: 0.02\n",
      "iteration: 406520 loss: 0.0016 lr: 0.02\n",
      "iteration: 406530 loss: 0.0022 lr: 0.02\n",
      "iteration: 406540 loss: 0.0014 lr: 0.02\n",
      "iteration: 406550 loss: 0.0017 lr: 0.02\n",
      "iteration: 406560 loss: 0.0019 lr: 0.02\n",
      "iteration: 406570 loss: 0.0019 lr: 0.02\n",
      "iteration: 406580 loss: 0.0015 lr: 0.02\n",
      "iteration: 406590 loss: 0.0018 lr: 0.02\n",
      "iteration: 406600 loss: 0.0015 lr: 0.02\n",
      "iteration: 406610 loss: 0.0024 lr: 0.02\n",
      "iteration: 406620 loss: 0.0013 lr: 0.02\n",
      "iteration: 406630 loss: 0.0016 lr: 0.02\n",
      "iteration: 406640 loss: 0.0016 lr: 0.02\n",
      "iteration: 406650 loss: 0.0014 lr: 0.02\n",
      "iteration: 406660 loss: 0.0011 lr: 0.02\n",
      "iteration: 406670 loss: 0.0016 lr: 0.02\n",
      "iteration: 406680 loss: 0.0018 lr: 0.02\n",
      "iteration: 406690 loss: 0.0018 lr: 0.02\n",
      "iteration: 406700 loss: 0.0016 lr: 0.02\n",
      "iteration: 406710 loss: 0.0016 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 406720 loss: 0.0018 lr: 0.02\n",
      "iteration: 406730 loss: 0.0017 lr: 0.02\n",
      "iteration: 406740 loss: 0.0020 lr: 0.02\n",
      "iteration: 406750 loss: 0.0015 lr: 0.02\n",
      "iteration: 406760 loss: 0.0012 lr: 0.02\n",
      "iteration: 406770 loss: 0.0015 lr: 0.02\n",
      "iteration: 406780 loss: 0.0015 lr: 0.02\n",
      "iteration: 406790 loss: 0.0014 lr: 0.02\n",
      "iteration: 406800 loss: 0.0017 lr: 0.02\n",
      "iteration: 406810 loss: 0.0019 lr: 0.02\n",
      "iteration: 406820 loss: 0.0018 lr: 0.02\n",
      "iteration: 406830 loss: 0.0012 lr: 0.02\n",
      "iteration: 406840 loss: 0.0027 lr: 0.02\n",
      "iteration: 406850 loss: 0.0013 lr: 0.02\n",
      "iteration: 406860 loss: 0.0018 lr: 0.02\n",
      "iteration: 406870 loss: 0.0016 lr: 0.02\n",
      "iteration: 406880 loss: 0.0021 lr: 0.02\n",
      "iteration: 406890 loss: 0.0016 lr: 0.02\n",
      "iteration: 406900 loss: 0.0014 lr: 0.02\n",
      "iteration: 406910 loss: 0.0014 lr: 0.02\n",
      "iteration: 406920 loss: 0.0012 lr: 0.02\n",
      "iteration: 406930 loss: 0.0013 lr: 0.02\n",
      "iteration: 406940 loss: 0.0019 lr: 0.02\n",
      "iteration: 406950 loss: 0.0012 lr: 0.02\n",
      "iteration: 406960 loss: 0.0018 lr: 0.02\n",
      "iteration: 406970 loss: 0.0015 lr: 0.02\n",
      "iteration: 406980 loss: 0.0012 lr: 0.02\n",
      "iteration: 406990 loss: 0.0013 lr: 0.02\n",
      "iteration: 407000 loss: 0.0021 lr: 0.02\n",
      "iteration: 407010 loss: 0.0018 lr: 0.02\n",
      "iteration: 407020 loss: 0.0022 lr: 0.02\n",
      "iteration: 407030 loss: 0.0015 lr: 0.02\n",
      "iteration: 407040 loss: 0.0016 lr: 0.02\n",
      "iteration: 407050 loss: 0.0015 lr: 0.02\n",
      "iteration: 407060 loss: 0.0016 lr: 0.02\n",
      "iteration: 407070 loss: 0.0021 lr: 0.02\n",
      "iteration: 407080 loss: 0.0011 lr: 0.02\n",
      "iteration: 407090 loss: 0.0017 lr: 0.02\n",
      "iteration: 407100 loss: 0.0010 lr: 0.02\n",
      "iteration: 407110 loss: 0.0015 lr: 0.02\n",
      "iteration: 407120 loss: 0.0015 lr: 0.02\n",
      "iteration: 407130 loss: 0.0017 lr: 0.02\n",
      "iteration: 407140 loss: 0.0015 lr: 0.02\n",
      "iteration: 407150 loss: 0.0046 lr: 0.02\n",
      "iteration: 407160 loss: 0.0013 lr: 0.02\n",
      "iteration: 407170 loss: 0.0021 lr: 0.02\n",
      "iteration: 407180 loss: 0.0015 lr: 0.02\n",
      "iteration: 407190 loss: 0.0021 lr: 0.02\n",
      "iteration: 407200 loss: 0.0013 lr: 0.02\n",
      "iteration: 407210 loss: 0.0022 lr: 0.02\n",
      "iteration: 407220 loss: 0.0022 lr: 0.02\n",
      "iteration: 407230 loss: 0.0011 lr: 0.02\n",
      "iteration: 407240 loss: 0.0017 lr: 0.02\n",
      "iteration: 407250 loss: 0.0012 lr: 0.02\n",
      "iteration: 407260 loss: 0.0010 lr: 0.02\n",
      "iteration: 407270 loss: 0.0015 lr: 0.02\n",
      "iteration: 407280 loss: 0.0016 lr: 0.02\n",
      "iteration: 407290 loss: 0.0022 lr: 0.02\n",
      "iteration: 407300 loss: 0.0014 lr: 0.02\n",
      "iteration: 407310 loss: 0.0025 lr: 0.02\n",
      "iteration: 407320 loss: 0.0013 lr: 0.02\n",
      "iteration: 407330 loss: 0.0015 lr: 0.02\n",
      "iteration: 407340 loss: 0.0013 lr: 0.02\n",
      "iteration: 407350 loss: 0.0016 lr: 0.02\n",
      "iteration: 407360 loss: 0.0017 lr: 0.02\n",
      "iteration: 407370 loss: 0.0017 lr: 0.02\n",
      "iteration: 407380 loss: 0.0014 lr: 0.02\n",
      "iteration: 407390 loss: 0.0010 lr: 0.02\n",
      "iteration: 407400 loss: 0.0014 lr: 0.02\n",
      "iteration: 407410 loss: 0.0021 lr: 0.02\n",
      "iteration: 407420 loss: 0.0018 lr: 0.02\n",
      "iteration: 407430 loss: 0.0017 lr: 0.02\n",
      "iteration: 407440 loss: 0.0017 lr: 0.02\n",
      "iteration: 407450 loss: 0.0015 lr: 0.02\n",
      "iteration: 407460 loss: 0.0015 lr: 0.02\n",
      "iteration: 407470 loss: 0.0012 lr: 0.02\n",
      "iteration: 407480 loss: 0.0012 lr: 0.02\n",
      "iteration: 407490 loss: 0.0014 lr: 0.02\n",
      "iteration: 407500 loss: 0.0014 lr: 0.02\n",
      "iteration: 407510 loss: 0.0012 lr: 0.02\n",
      "iteration: 407520 loss: 0.0014 lr: 0.02\n",
      "iteration: 407530 loss: 0.0012 lr: 0.02\n",
      "iteration: 407540 loss: 0.0018 lr: 0.02\n",
      "iteration: 407550 loss: 0.0016 lr: 0.02\n",
      "iteration: 407560 loss: 0.0016 lr: 0.02\n",
      "iteration: 407570 loss: 0.0014 lr: 0.02\n",
      "iteration: 407580 loss: 0.0013 lr: 0.02\n",
      "iteration: 407590 loss: 0.0011 lr: 0.02\n",
      "iteration: 407600 loss: 0.0016 lr: 0.02\n",
      "iteration: 407610 loss: 0.0015 lr: 0.02\n",
      "iteration: 407620 loss: 0.0012 lr: 0.02\n",
      "iteration: 407630 loss: 0.0017 lr: 0.02\n",
      "iteration: 407640 loss: 0.0012 lr: 0.02\n",
      "iteration: 407650 loss: 0.0019 lr: 0.02\n",
      "iteration: 407660 loss: 0.0015 lr: 0.02\n",
      "iteration: 407670 loss: 0.0012 lr: 0.02\n",
      "iteration: 407680 loss: 0.0014 lr: 0.02\n",
      "iteration: 407690 loss: 0.0016 lr: 0.02\n",
      "iteration: 407700 loss: 0.0016 lr: 0.02\n",
      "iteration: 407710 loss: 0.0028 lr: 0.02\n",
      "iteration: 407720 loss: 0.0013 lr: 0.02\n",
      "iteration: 407730 loss: 0.0012 lr: 0.02\n",
      "iteration: 407740 loss: 0.0013 lr: 0.02\n",
      "iteration: 407750 loss: 0.0012 lr: 0.02\n",
      "iteration: 407760 loss: 0.0018 lr: 0.02\n",
      "iteration: 407770 loss: 0.0015 lr: 0.02\n",
      "iteration: 407780 loss: 0.0014 lr: 0.02\n",
      "iteration: 407790 loss: 0.0019 lr: 0.02\n",
      "iteration: 407800 loss: 0.0014 lr: 0.02\n",
      "iteration: 407810 loss: 0.0017 lr: 0.02\n",
      "iteration: 407820 loss: 0.0016 lr: 0.02\n",
      "iteration: 407830 loss: 0.0016 lr: 0.02\n",
      "iteration: 407840 loss: 0.0014 lr: 0.02\n",
      "iteration: 407850 loss: 0.0017 lr: 0.02\n",
      "iteration: 407860 loss: 0.0019 lr: 0.02\n",
      "iteration: 407870 loss: 0.0019 lr: 0.02\n",
      "iteration: 407880 loss: 0.0018 lr: 0.02\n",
      "iteration: 407890 loss: 0.0010 lr: 0.02\n",
      "iteration: 407900 loss: 0.0015 lr: 0.02\n",
      "iteration: 407910 loss: 0.0014 lr: 0.02\n",
      "iteration: 407920 loss: 0.0015 lr: 0.02\n",
      "iteration: 407930 loss: 0.0009 lr: 0.02\n",
      "iteration: 407940 loss: 0.0012 lr: 0.02\n",
      "iteration: 407950 loss: 0.0023 lr: 0.02\n",
      "iteration: 407960 loss: 0.0017 lr: 0.02\n",
      "iteration: 407970 loss: 0.0014 lr: 0.02\n",
      "iteration: 407980 loss: 0.0014 lr: 0.02\n",
      "iteration: 407990 loss: 0.0016 lr: 0.02\n",
      "iteration: 408000 loss: 0.0015 lr: 0.02\n",
      "iteration: 408010 loss: 0.0012 lr: 0.02\n",
      "iteration: 408020 loss: 0.0016 lr: 0.02\n",
      "iteration: 408030 loss: 0.0018 lr: 0.02\n",
      "iteration: 408040 loss: 0.0015 lr: 0.02\n",
      "iteration: 408050 loss: 0.0015 lr: 0.02\n",
      "iteration: 408060 loss: 0.0013 lr: 0.02\n",
      "iteration: 408070 loss: 0.0014 lr: 0.02\n",
      "iteration: 408080 loss: 0.0010 lr: 0.02\n",
      "iteration: 408090 loss: 0.0015 lr: 0.02\n",
      "iteration: 408100 loss: 0.0023 lr: 0.02\n",
      "iteration: 408110 loss: 0.0011 lr: 0.02\n",
      "iteration: 408120 loss: 0.0017 lr: 0.02\n",
      "iteration: 408130 loss: 0.0015 lr: 0.02\n",
      "iteration: 408140 loss: 0.0016 lr: 0.02\n",
      "iteration: 408150 loss: 0.0013 lr: 0.02\n",
      "iteration: 408160 loss: 0.0019 lr: 0.02\n",
      "iteration: 408170 loss: 0.0014 lr: 0.02\n",
      "iteration: 408180 loss: 0.0023 lr: 0.02\n",
      "iteration: 408190 loss: 0.0012 lr: 0.02\n",
      "iteration: 408200 loss: 0.0022 lr: 0.02\n",
      "iteration: 408210 loss: 0.0011 lr: 0.02\n",
      "iteration: 408220 loss: 0.0015 lr: 0.02\n",
      "iteration: 408230 loss: 0.0013 lr: 0.02\n",
      "iteration: 408240 loss: 0.0016 lr: 0.02\n",
      "iteration: 408250 loss: 0.0014 lr: 0.02\n",
      "iteration: 408260 loss: 0.0015 lr: 0.02\n",
      "iteration: 408270 loss: 0.0013 lr: 0.02\n",
      "iteration: 408280 loss: 0.0016 lr: 0.02\n",
      "iteration: 408290 loss: 0.0015 lr: 0.02\n",
      "iteration: 408300 loss: 0.0020 lr: 0.02\n",
      "iteration: 408310 loss: 0.0011 lr: 0.02\n",
      "iteration: 408320 loss: 0.0016 lr: 0.02\n",
      "iteration: 408330 loss: 0.0020 lr: 0.02\n",
      "iteration: 408340 loss: 0.0017 lr: 0.02\n",
      "iteration: 408350 loss: 0.0016 lr: 0.02\n",
      "iteration: 408360 loss: 0.0015 lr: 0.02\n",
      "iteration: 408370 loss: 0.0014 lr: 0.02\n",
      "iteration: 408380 loss: 0.0012 lr: 0.02\n",
      "iteration: 408390 loss: 0.0014 lr: 0.02\n",
      "iteration: 408400 loss: 0.0013 lr: 0.02\n",
      "iteration: 408410 loss: 0.0011 lr: 0.02\n",
      "iteration: 408420 loss: 0.0015 lr: 0.02\n",
      "iteration: 408430 loss: 0.0015 lr: 0.02\n",
      "iteration: 408440 loss: 0.0013 lr: 0.02\n",
      "iteration: 408450 loss: 0.0016 lr: 0.02\n",
      "iteration: 408460 loss: 0.0016 lr: 0.02\n",
      "iteration: 408470 loss: 0.0018 lr: 0.02\n",
      "iteration: 408480 loss: 0.0015 lr: 0.02\n",
      "iteration: 408490 loss: 0.0027 lr: 0.02\n",
      "iteration: 408500 loss: 0.0016 lr: 0.02\n",
      "iteration: 408510 loss: 0.0013 lr: 0.02\n",
      "iteration: 408520 loss: 0.0018 lr: 0.02\n",
      "iteration: 408530 loss: 0.0014 lr: 0.02\n",
      "iteration: 408540 loss: 0.0018 lr: 0.02\n",
      "iteration: 408550 loss: 0.0010 lr: 0.02\n",
      "iteration: 408560 loss: 0.0011 lr: 0.02\n",
      "iteration: 408570 loss: 0.0010 lr: 0.02\n",
      "iteration: 408580 loss: 0.0010 lr: 0.02\n",
      "iteration: 408590 loss: 0.0015 lr: 0.02\n",
      "iteration: 408600 loss: 0.0021 lr: 0.02\n",
      "iteration: 408610 loss: 0.0016 lr: 0.02\n",
      "iteration: 408620 loss: 0.0020 lr: 0.02\n",
      "iteration: 408630 loss: 0.0014 lr: 0.02\n",
      "iteration: 408640 loss: 0.0014 lr: 0.02\n",
      "iteration: 408650 loss: 0.0017 lr: 0.02\n",
      "iteration: 408660 loss: 0.0015 lr: 0.02\n",
      "iteration: 408670 loss: 0.0023 lr: 0.02\n",
      "iteration: 408680 loss: 0.0024 lr: 0.02\n",
      "iteration: 408690 loss: 0.0012 lr: 0.02\n",
      "iteration: 408700 loss: 0.0013 lr: 0.02\n",
      "iteration: 408710 loss: 0.0016 lr: 0.02\n",
      "iteration: 408720 loss: 0.0024 lr: 0.02\n",
      "iteration: 408730 loss: 0.0021 lr: 0.02\n",
      "iteration: 408740 loss: 0.0019 lr: 0.02\n",
      "iteration: 408750 loss: 0.0011 lr: 0.02\n",
      "iteration: 408760 loss: 0.0030 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 408770 loss: 0.0016 lr: 0.02\n",
      "iteration: 408780 loss: 0.0019 lr: 0.02\n",
      "iteration: 408790 loss: 0.0015 lr: 0.02\n",
      "iteration: 408800 loss: 0.0009 lr: 0.02\n",
      "iteration: 408810 loss: 0.0023 lr: 0.02\n",
      "iteration: 408820 loss: 0.0021 lr: 0.02\n",
      "iteration: 408830 loss: 0.0013 lr: 0.02\n",
      "iteration: 408840 loss: 0.0015 lr: 0.02\n",
      "iteration: 408850 loss: 0.0023 lr: 0.02\n",
      "iteration: 408860 loss: 0.0011 lr: 0.02\n",
      "iteration: 408870 loss: 0.0015 lr: 0.02\n",
      "iteration: 408880 loss: 0.0013 lr: 0.02\n",
      "iteration: 408890 loss: 0.0013 lr: 0.02\n",
      "iteration: 408900 loss: 0.0010 lr: 0.02\n",
      "iteration: 408910 loss: 0.0015 lr: 0.02\n",
      "iteration: 408920 loss: 0.0017 lr: 0.02\n",
      "iteration: 408930 loss: 0.0016 lr: 0.02\n",
      "iteration: 408940 loss: 0.0030 lr: 0.02\n",
      "iteration: 408950 loss: 0.0011 lr: 0.02\n",
      "iteration: 408960 loss: 0.0021 lr: 0.02\n",
      "iteration: 408970 loss: 0.0017 lr: 0.02\n",
      "iteration: 408980 loss: 0.0012 lr: 0.02\n",
      "iteration: 408990 loss: 0.0013 lr: 0.02\n",
      "iteration: 409000 loss: 0.0020 lr: 0.02\n",
      "iteration: 409010 loss: 0.0016 lr: 0.02\n",
      "iteration: 409020 loss: 0.0013 lr: 0.02\n",
      "iteration: 409030 loss: 0.0014 lr: 0.02\n",
      "iteration: 409040 loss: 0.0010 lr: 0.02\n",
      "iteration: 409050 loss: 0.0022 lr: 0.02\n",
      "iteration: 409060 loss: 0.0012 lr: 0.02\n",
      "iteration: 409070 loss: 0.0019 lr: 0.02\n",
      "iteration: 409080 loss: 0.0013 lr: 0.02\n",
      "iteration: 409090 loss: 0.0018 lr: 0.02\n",
      "iteration: 409100 loss: 0.0019 lr: 0.02\n",
      "iteration: 409110 loss: 0.0013 lr: 0.02\n",
      "iteration: 409120 loss: 0.0014 lr: 0.02\n",
      "iteration: 409130 loss: 0.0009 lr: 0.02\n",
      "iteration: 409140 loss: 0.0015 lr: 0.02\n",
      "iteration: 409150 loss: 0.0016 lr: 0.02\n",
      "iteration: 409160 loss: 0.0024 lr: 0.02\n",
      "iteration: 409170 loss: 0.0014 lr: 0.02\n",
      "iteration: 409180 loss: 0.0019 lr: 0.02\n",
      "iteration: 409190 loss: 0.0014 lr: 0.02\n",
      "iteration: 409200 loss: 0.0014 lr: 0.02\n",
      "iteration: 409210 loss: 0.0018 lr: 0.02\n",
      "iteration: 409220 loss: 0.0017 lr: 0.02\n",
      "iteration: 409230 loss: 0.0013 lr: 0.02\n",
      "iteration: 409240 loss: 0.0018 lr: 0.02\n",
      "iteration: 409250 loss: 0.0014 lr: 0.02\n",
      "iteration: 409260 loss: 0.0017 lr: 0.02\n",
      "iteration: 409270 loss: 0.0014 lr: 0.02\n",
      "iteration: 409280 loss: 0.0016 lr: 0.02\n",
      "iteration: 409290 loss: 0.0011 lr: 0.02\n",
      "iteration: 409300 loss: 0.0025 lr: 0.02\n",
      "iteration: 409310 loss: 0.0020 lr: 0.02\n",
      "iteration: 409320 loss: 0.0018 lr: 0.02\n",
      "iteration: 409330 loss: 0.0017 lr: 0.02\n",
      "iteration: 409340 loss: 0.0017 lr: 0.02\n",
      "iteration: 409350 loss: 0.0015 lr: 0.02\n",
      "iteration: 409360 loss: 0.0011 lr: 0.02\n",
      "iteration: 409370 loss: 0.0014 lr: 0.02\n",
      "iteration: 409380 loss: 0.0018 lr: 0.02\n",
      "iteration: 409390 loss: 0.0016 lr: 0.02\n",
      "iteration: 409400 loss: 0.0019 lr: 0.02\n",
      "iteration: 409410 loss: 0.0016 lr: 0.02\n",
      "iteration: 409420 loss: 0.0015 lr: 0.02\n",
      "iteration: 409430 loss: 0.0016 lr: 0.02\n",
      "iteration: 409440 loss: 0.0012 lr: 0.02\n",
      "iteration: 409450 loss: 0.0018 lr: 0.02\n",
      "iteration: 409460 loss: 0.0012 lr: 0.02\n",
      "iteration: 409470 loss: 0.0019 lr: 0.02\n",
      "iteration: 409480 loss: 0.0020 lr: 0.02\n",
      "iteration: 409490 loss: 0.0018 lr: 0.02\n",
      "iteration: 409500 loss: 0.0017 lr: 0.02\n",
      "iteration: 409510 loss: 0.0018 lr: 0.02\n",
      "iteration: 409520 loss: 0.0012 lr: 0.02\n",
      "iteration: 409530 loss: 0.0012 lr: 0.02\n",
      "iteration: 409540 loss: 0.0014 lr: 0.02\n",
      "iteration: 409550 loss: 0.0019 lr: 0.02\n",
      "iteration: 409560 loss: 0.0015 lr: 0.02\n",
      "iteration: 409570 loss: 0.0015 lr: 0.02\n",
      "iteration: 409580 loss: 0.0013 lr: 0.02\n",
      "iteration: 409590 loss: 0.0014 lr: 0.02\n",
      "iteration: 409600 loss: 0.0021 lr: 0.02\n",
      "iteration: 409610 loss: 0.0017 lr: 0.02\n",
      "iteration: 409620 loss: 0.0019 lr: 0.02\n",
      "iteration: 409630 loss: 0.0016 lr: 0.02\n",
      "iteration: 409640 loss: 0.0014 lr: 0.02\n",
      "iteration: 409650 loss: 0.0017 lr: 0.02\n",
      "iteration: 409660 loss: 0.0015 lr: 0.02\n",
      "iteration: 409670 loss: 0.0012 lr: 0.02\n",
      "iteration: 409680 loss: 0.0013 lr: 0.02\n",
      "iteration: 409690 loss: 0.0017 lr: 0.02\n",
      "iteration: 409700 loss: 0.0011 lr: 0.02\n",
      "iteration: 409710 loss: 0.0015 lr: 0.02\n",
      "iteration: 409720 loss: 0.0013 lr: 0.02\n",
      "iteration: 409730 loss: 0.0014 lr: 0.02\n",
      "iteration: 409740 loss: 0.0013 lr: 0.02\n",
      "iteration: 409750 loss: 0.0015 lr: 0.02\n",
      "iteration: 409760 loss: 0.0024 lr: 0.02\n",
      "iteration: 409770 loss: 0.0016 lr: 0.02\n",
      "iteration: 409780 loss: 0.0021 lr: 0.02\n",
      "iteration: 409790 loss: 0.0016 lr: 0.02\n",
      "iteration: 409800 loss: 0.0016 lr: 0.02\n",
      "iteration: 409810 loss: 0.0011 lr: 0.02\n",
      "iteration: 409820 loss: 0.0018 lr: 0.02\n",
      "iteration: 409830 loss: 0.0019 lr: 0.02\n",
      "iteration: 409840 loss: 0.0013 lr: 0.02\n",
      "iteration: 409850 loss: 0.0012 lr: 0.02\n",
      "iteration: 409860 loss: 0.0014 lr: 0.02\n",
      "iteration: 409870 loss: 0.0010 lr: 0.02\n",
      "iteration: 409880 loss: 0.0011 lr: 0.02\n",
      "iteration: 409890 loss: 0.0019 lr: 0.02\n",
      "iteration: 409900 loss: 0.0022 lr: 0.02\n",
      "iteration: 409910 loss: 0.0016 lr: 0.02\n",
      "iteration: 409920 loss: 0.0025 lr: 0.02\n",
      "iteration: 409930 loss: 0.0012 lr: 0.02\n",
      "iteration: 409940 loss: 0.0013 lr: 0.02\n",
      "iteration: 409950 loss: 0.0013 lr: 0.02\n",
      "iteration: 409960 loss: 0.0009 lr: 0.02\n",
      "iteration: 409970 loss: 0.0015 lr: 0.02\n",
      "iteration: 409980 loss: 0.0015 lr: 0.02\n",
      "iteration: 409990 loss: 0.0014 lr: 0.02\n",
      "iteration: 410000 loss: 0.0013 lr: 0.02\n",
      "iteration: 410010 loss: 0.0015 lr: 0.02\n",
      "iteration: 410020 loss: 0.0014 lr: 0.02\n",
      "iteration: 410030 loss: 0.0017 lr: 0.02\n",
      "iteration: 410040 loss: 0.0016 lr: 0.02\n",
      "iteration: 410050 loss: 0.0026 lr: 0.02\n",
      "iteration: 410060 loss: 0.0015 lr: 0.02\n",
      "iteration: 410070 loss: 0.0019 lr: 0.02\n",
      "iteration: 410080 loss: 0.0009 lr: 0.02\n",
      "iteration: 410090 loss: 0.0014 lr: 0.02\n",
      "iteration: 410100 loss: 0.0014 lr: 0.02\n",
      "iteration: 410110 loss: 0.0017 lr: 0.02\n",
      "iteration: 410120 loss: 0.0013 lr: 0.02\n",
      "iteration: 410130 loss: 0.0018 lr: 0.02\n",
      "iteration: 410140 loss: 0.0016 lr: 0.02\n",
      "iteration: 410150 loss: 0.0011 lr: 0.02\n",
      "iteration: 410160 loss: 0.0014 lr: 0.02\n",
      "iteration: 410170 loss: 0.0017 lr: 0.02\n",
      "iteration: 410180 loss: 0.0020 lr: 0.02\n",
      "iteration: 410190 loss: 0.0014 lr: 0.02\n",
      "iteration: 410200 loss: 0.0016 lr: 0.02\n",
      "iteration: 410210 loss: 0.0018 lr: 0.02\n",
      "iteration: 410220 loss: 0.0018 lr: 0.02\n",
      "iteration: 410230 loss: 0.0018 lr: 0.02\n",
      "iteration: 410240 loss: 0.0020 lr: 0.02\n",
      "iteration: 410250 loss: 0.0024 lr: 0.02\n",
      "iteration: 410260 loss: 0.0017 lr: 0.02\n",
      "iteration: 410270 loss: 0.0014 lr: 0.02\n",
      "iteration: 410280 loss: 0.0018 lr: 0.02\n",
      "iteration: 410290 loss: 0.0022 lr: 0.02\n",
      "iteration: 410300 loss: 0.0019 lr: 0.02\n",
      "iteration: 410310 loss: 0.0016 lr: 0.02\n",
      "iteration: 410320 loss: 0.0014 lr: 0.02\n",
      "iteration: 410330 loss: 0.0017 lr: 0.02\n",
      "iteration: 410340 loss: 0.0020 lr: 0.02\n",
      "iteration: 410350 loss: 0.0021 lr: 0.02\n",
      "iteration: 410360 loss: 0.0013 lr: 0.02\n",
      "iteration: 410370 loss: 0.0025 lr: 0.02\n",
      "iteration: 410380 loss: 0.0022 lr: 0.02\n",
      "iteration: 410390 loss: 0.0014 lr: 0.02\n",
      "iteration: 410400 loss: 0.0015 lr: 0.02\n",
      "iteration: 410410 loss: 0.0012 lr: 0.02\n",
      "iteration: 410420 loss: 0.0020 lr: 0.02\n",
      "iteration: 410430 loss: 0.0012 lr: 0.02\n",
      "iteration: 410440 loss: 0.0015 lr: 0.02\n",
      "iteration: 410450 loss: 0.0014 lr: 0.02\n",
      "iteration: 410460 loss: 0.0013 lr: 0.02\n",
      "iteration: 410470 loss: 0.0019 lr: 0.02\n",
      "iteration: 410480 loss: 0.0016 lr: 0.02\n",
      "iteration: 410490 loss: 0.0011 lr: 0.02\n",
      "iteration: 410500 loss: 0.0013 lr: 0.02\n",
      "iteration: 410510 loss: 0.0013 lr: 0.02\n",
      "iteration: 410520 loss: 0.0012 lr: 0.02\n",
      "iteration: 410530 loss: 0.0020 lr: 0.02\n",
      "iteration: 410540 loss: 0.0043 lr: 0.02\n",
      "iteration: 410550 loss: 0.0015 lr: 0.02\n",
      "iteration: 410560 loss: 0.0020 lr: 0.02\n",
      "iteration: 410570 loss: 0.0019 lr: 0.02\n",
      "iteration: 410580 loss: 0.0013 lr: 0.02\n",
      "iteration: 410590 loss: 0.0012 lr: 0.02\n",
      "iteration: 410600 loss: 0.0014 lr: 0.02\n",
      "iteration: 410610 loss: 0.0017 lr: 0.02\n",
      "iteration: 410620 loss: 0.0024 lr: 0.02\n",
      "iteration: 410630 loss: 0.0015 lr: 0.02\n",
      "iteration: 410640 loss: 0.0016 lr: 0.02\n",
      "iteration: 410650 loss: 0.0020 lr: 0.02\n",
      "iteration: 410660 loss: 0.0014 lr: 0.02\n",
      "iteration: 410670 loss: 0.0013 lr: 0.02\n",
      "iteration: 410680 loss: 0.0027 lr: 0.02\n",
      "iteration: 410690 loss: 0.0026 lr: 0.02\n",
      "iteration: 410700 loss: 0.0015 lr: 0.02\n",
      "iteration: 410710 loss: 0.0015 lr: 0.02\n",
      "iteration: 410720 loss: 0.0011 lr: 0.02\n",
      "iteration: 410730 loss: 0.0016 lr: 0.02\n",
      "iteration: 410740 loss: 0.0017 lr: 0.02\n",
      "iteration: 410750 loss: 0.0014 lr: 0.02\n",
      "iteration: 410760 loss: 0.0016 lr: 0.02\n",
      "iteration: 410770 loss: 0.0015 lr: 0.02\n",
      "iteration: 410780 loss: 0.0025 lr: 0.02\n",
      "iteration: 410790 loss: 0.0011 lr: 0.02\n",
      "iteration: 410800 loss: 0.0022 lr: 0.02\n",
      "iteration: 410810 loss: 0.0010 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 410820 loss: 0.0014 lr: 0.02\n",
      "iteration: 410830 loss: 0.0013 lr: 0.02\n",
      "iteration: 410840 loss: 0.0013 lr: 0.02\n",
      "iteration: 410850 loss: 0.0015 lr: 0.02\n",
      "iteration: 410860 loss: 0.0013 lr: 0.02\n",
      "iteration: 410870 loss: 0.0018 lr: 0.02\n",
      "iteration: 410880 loss: 0.0011 lr: 0.02\n",
      "iteration: 410890 loss: 0.0016 lr: 0.02\n",
      "iteration: 410900 loss: 0.0012 lr: 0.02\n",
      "iteration: 410910 loss: 0.0016 lr: 0.02\n",
      "iteration: 410920 loss: 0.0012 lr: 0.02\n",
      "iteration: 410930 loss: 0.0019 lr: 0.02\n",
      "iteration: 410940 loss: 0.0010 lr: 0.02\n",
      "iteration: 410950 loss: 0.0022 lr: 0.02\n",
      "iteration: 410960 loss: 0.0016 lr: 0.02\n",
      "iteration: 410970 loss: 0.0016 lr: 0.02\n",
      "iteration: 410980 loss: 0.0018 lr: 0.02\n",
      "iteration: 410990 loss: 0.0025 lr: 0.02\n",
      "iteration: 411000 loss: 0.0014 lr: 0.02\n",
      "iteration: 411010 loss: 0.0012 lr: 0.02\n",
      "iteration: 411020 loss: 0.0015 lr: 0.02\n",
      "iteration: 411030 loss: 0.0017 lr: 0.02\n",
      "iteration: 411040 loss: 0.0011 lr: 0.02\n",
      "iteration: 411050 loss: 0.0012 lr: 0.02\n",
      "iteration: 411060 loss: 0.0012 lr: 0.02\n",
      "iteration: 411070 loss: 0.0014 lr: 0.02\n",
      "iteration: 411080 loss: 0.0014 lr: 0.02\n",
      "iteration: 411090 loss: 0.0013 lr: 0.02\n",
      "iteration: 411100 loss: 0.0017 lr: 0.02\n",
      "iteration: 411110 loss: 0.0013 lr: 0.02\n",
      "iteration: 411120 loss: 0.0022 lr: 0.02\n",
      "iteration: 411130 loss: 0.0015 lr: 0.02\n",
      "iteration: 411140 loss: 0.0013 lr: 0.02\n",
      "iteration: 411150 loss: 0.0017 lr: 0.02\n",
      "iteration: 411160 loss: 0.0014 lr: 0.02\n",
      "iteration: 411170 loss: 0.0018 lr: 0.02\n",
      "iteration: 411180 loss: 0.0022 lr: 0.02\n",
      "iteration: 411190 loss: 0.0014 lr: 0.02\n",
      "iteration: 411200 loss: 0.0014 lr: 0.02\n",
      "iteration: 411210 loss: 0.0016 lr: 0.02\n",
      "iteration: 411220 loss: 0.0026 lr: 0.02\n",
      "iteration: 411230 loss: 0.0009 lr: 0.02\n",
      "iteration: 411240 loss: 0.0013 lr: 0.02\n",
      "iteration: 411250 loss: 0.0012 lr: 0.02\n",
      "iteration: 411260 loss: 0.0012 lr: 0.02\n",
      "iteration: 411270 loss: 0.0010 lr: 0.02\n",
      "iteration: 411280 loss: 0.0010 lr: 0.02\n",
      "iteration: 411290 loss: 0.0012 lr: 0.02\n",
      "iteration: 411300 loss: 0.0018 lr: 0.02\n",
      "iteration: 411310 loss: 0.0013 lr: 0.02\n",
      "iteration: 411320 loss: 0.0015 lr: 0.02\n",
      "iteration: 411330 loss: 0.0019 lr: 0.02\n",
      "iteration: 411340 loss: 0.0012 lr: 0.02\n",
      "iteration: 411350 loss: 0.0015 lr: 0.02\n",
      "iteration: 411360 loss: 0.0012 lr: 0.02\n",
      "iteration: 411370 loss: 0.0014 lr: 0.02\n",
      "iteration: 411380 loss: 0.0014 lr: 0.02\n",
      "iteration: 411390 loss: 0.0014 lr: 0.02\n",
      "iteration: 411400 loss: 0.0014 lr: 0.02\n",
      "iteration: 411410 loss: 0.0022 lr: 0.02\n",
      "iteration: 411420 loss: 0.0012 lr: 0.02\n",
      "iteration: 411430 loss: 0.0012 lr: 0.02\n",
      "iteration: 411440 loss: 0.0015 lr: 0.02\n",
      "iteration: 411450 loss: 0.0011 lr: 0.02\n",
      "iteration: 411460 loss: 0.0024 lr: 0.02\n",
      "iteration: 411470 loss: 0.0014 lr: 0.02\n",
      "iteration: 411480 loss: 0.0014 lr: 0.02\n",
      "iteration: 411490 loss: 0.0018 lr: 0.02\n",
      "iteration: 411500 loss: 0.0021 lr: 0.02\n",
      "iteration: 411510 loss: 0.0017 lr: 0.02\n",
      "iteration: 411520 loss: 0.0012 lr: 0.02\n",
      "iteration: 411530 loss: 0.0014 lr: 0.02\n",
      "iteration: 411540 loss: 0.0020 lr: 0.02\n",
      "iteration: 411550 loss: 0.0013 lr: 0.02\n",
      "iteration: 411560 loss: 0.0015 lr: 0.02\n",
      "iteration: 411570 loss: 0.0012 lr: 0.02\n",
      "iteration: 411580 loss: 0.0015 lr: 0.02\n",
      "iteration: 411590 loss: 0.0015 lr: 0.02\n",
      "iteration: 411600 loss: 0.0014 lr: 0.02\n",
      "iteration: 411610 loss: 0.0015 lr: 0.02\n",
      "iteration: 411620 loss: 0.0013 lr: 0.02\n",
      "iteration: 411630 loss: 0.0013 lr: 0.02\n",
      "iteration: 411640 loss: 0.0016 lr: 0.02\n",
      "iteration: 411650 loss: 0.0016 lr: 0.02\n",
      "iteration: 411660 loss: 0.0011 lr: 0.02\n",
      "iteration: 411670 loss: 0.0015 lr: 0.02\n",
      "iteration: 411680 loss: 0.0014 lr: 0.02\n",
      "iteration: 411690 loss: 0.0013 lr: 0.02\n",
      "iteration: 411700 loss: 0.0009 lr: 0.02\n",
      "iteration: 411710 loss: 0.0010 lr: 0.02\n",
      "iteration: 411720 loss: 0.0011 lr: 0.02\n",
      "iteration: 411730 loss: 0.0011 lr: 0.02\n",
      "iteration: 411740 loss: 0.0012 lr: 0.02\n",
      "iteration: 411750 loss: 0.0012 lr: 0.02\n",
      "iteration: 411760 loss: 0.0018 lr: 0.02\n",
      "iteration: 411770 loss: 0.0014 lr: 0.02\n",
      "iteration: 411780 loss: 0.0018 lr: 0.02\n",
      "iteration: 411790 loss: 0.0017 lr: 0.02\n",
      "iteration: 411800 loss: 0.0019 lr: 0.02\n",
      "iteration: 411810 loss: 0.0013 lr: 0.02\n",
      "iteration: 411820 loss: 0.0018 lr: 0.02\n",
      "iteration: 411830 loss: 0.0015 lr: 0.02\n",
      "iteration: 411840 loss: 0.0017 lr: 0.02\n",
      "iteration: 411850 loss: 0.0013 lr: 0.02\n",
      "iteration: 411860 loss: 0.0020 lr: 0.02\n",
      "iteration: 411870 loss: 0.0015 lr: 0.02\n",
      "iteration: 411880 loss: 0.0015 lr: 0.02\n",
      "iteration: 411890 loss: 0.0017 lr: 0.02\n",
      "iteration: 411900 loss: 0.0016 lr: 0.02\n",
      "iteration: 411910 loss: 0.0015 lr: 0.02\n",
      "iteration: 411920 loss: 0.0010 lr: 0.02\n",
      "iteration: 411930 loss: 0.0014 lr: 0.02\n",
      "iteration: 411940 loss: 0.0015 lr: 0.02\n",
      "iteration: 411950 loss: 0.0016 lr: 0.02\n",
      "iteration: 411960 loss: 0.0018 lr: 0.02\n",
      "iteration: 411970 loss: 0.0016 lr: 0.02\n",
      "iteration: 411980 loss: 0.0015 lr: 0.02\n",
      "iteration: 411990 loss: 0.0017 lr: 0.02\n",
      "iteration: 412000 loss: 0.0017 lr: 0.02\n",
      "iteration: 412010 loss: 0.0012 lr: 0.02\n",
      "iteration: 412020 loss: 0.0016 lr: 0.02\n",
      "iteration: 412030 loss: 0.0011 lr: 0.02\n",
      "iteration: 412040 loss: 0.0019 lr: 0.02\n",
      "iteration: 412050 loss: 0.0016 lr: 0.02\n",
      "iteration: 412060 loss: 0.0012 lr: 0.02\n",
      "iteration: 412070 loss: 0.0015 lr: 0.02\n",
      "iteration: 412080 loss: 0.0013 lr: 0.02\n",
      "iteration: 412090 loss: 0.0013 lr: 0.02\n",
      "iteration: 412100 loss: 0.0012 lr: 0.02\n",
      "iteration: 412110 loss: 0.0017 lr: 0.02\n",
      "iteration: 412120 loss: 0.0014 lr: 0.02\n",
      "iteration: 412130 loss: 0.0017 lr: 0.02\n",
      "iteration: 412140 loss: 0.0011 lr: 0.02\n",
      "iteration: 412150 loss: 0.0017 lr: 0.02\n",
      "iteration: 412160 loss: 0.0016 lr: 0.02\n",
      "iteration: 412170 loss: 0.0017 lr: 0.02\n",
      "iteration: 412180 loss: 0.0023 lr: 0.02\n",
      "iteration: 412190 loss: 0.0021 lr: 0.02\n",
      "iteration: 412200 loss: 0.0017 lr: 0.02\n",
      "iteration: 412210 loss: 0.0017 lr: 0.02\n",
      "iteration: 412220 loss: 0.0014 lr: 0.02\n",
      "iteration: 412230 loss: 0.0014 lr: 0.02\n",
      "iteration: 412240 loss: 0.0011 lr: 0.02\n",
      "iteration: 412250 loss: 0.0012 lr: 0.02\n",
      "iteration: 412260 loss: 0.0015 lr: 0.02\n",
      "iteration: 412270 loss: 0.0011 lr: 0.02\n",
      "iteration: 412280 loss: 0.0010 lr: 0.02\n",
      "iteration: 412290 loss: 0.0017 lr: 0.02\n",
      "iteration: 412300 loss: 0.0015 lr: 0.02\n",
      "iteration: 412310 loss: 0.0014 lr: 0.02\n",
      "iteration: 412320 loss: 0.0015 lr: 0.02\n",
      "iteration: 412330 loss: 0.0019 lr: 0.02\n",
      "iteration: 412340 loss: 0.0012 lr: 0.02\n",
      "iteration: 412350 loss: 0.0014 lr: 0.02\n",
      "iteration: 412360 loss: 0.0016 lr: 0.02\n",
      "iteration: 412370 loss: 0.0012 lr: 0.02\n",
      "iteration: 412380 loss: 0.0011 lr: 0.02\n",
      "iteration: 412390 loss: 0.0017 lr: 0.02\n",
      "iteration: 412400 loss: 0.0009 lr: 0.02\n",
      "iteration: 412410 loss: 0.0030 lr: 0.02\n",
      "iteration: 412420 loss: 0.0015 lr: 0.02\n",
      "iteration: 412430 loss: 0.0010 lr: 0.02\n",
      "iteration: 412440 loss: 0.0020 lr: 0.02\n",
      "iteration: 412450 loss: 0.0016 lr: 0.02\n",
      "iteration: 412460 loss: 0.0023 lr: 0.02\n",
      "iteration: 412470 loss: 0.0019 lr: 0.02\n",
      "iteration: 412480 loss: 0.0022 lr: 0.02\n",
      "iteration: 412490 loss: 0.0013 lr: 0.02\n",
      "iteration: 412500 loss: 0.0012 lr: 0.02\n",
      "iteration: 412510 loss: 0.0016 lr: 0.02\n",
      "iteration: 412520 loss: 0.0017 lr: 0.02\n",
      "iteration: 412530 loss: 0.0023 lr: 0.02\n",
      "iteration: 412540 loss: 0.0016 lr: 0.02\n",
      "iteration: 412550 loss: 0.0018 lr: 0.02\n",
      "iteration: 412560 loss: 0.0013 lr: 0.02\n",
      "iteration: 412570 loss: 0.0013 lr: 0.02\n",
      "iteration: 412580 loss: 0.0017 lr: 0.02\n",
      "iteration: 412590 loss: 0.0011 lr: 0.02\n",
      "iteration: 412600 loss: 0.0027 lr: 0.02\n",
      "iteration: 412610 loss: 0.0011 lr: 0.02\n",
      "iteration: 412620 loss: 0.0018 lr: 0.02\n",
      "iteration: 412630 loss: 0.0017 lr: 0.02\n",
      "iteration: 412640 loss: 0.0012 lr: 0.02\n",
      "iteration: 412650 loss: 0.0012 lr: 0.02\n",
      "iteration: 412660 loss: 0.0014 lr: 0.02\n",
      "iteration: 412670 loss: 0.0016 lr: 0.02\n",
      "iteration: 412680 loss: 0.0020 lr: 0.02\n",
      "iteration: 412690 loss: 0.0015 lr: 0.02\n",
      "iteration: 412700 loss: 0.0019 lr: 0.02\n",
      "iteration: 412710 loss: 0.0020 lr: 0.02\n",
      "iteration: 412720 loss: 0.0016 lr: 0.02\n",
      "iteration: 412730 loss: 0.0015 lr: 0.02\n",
      "iteration: 412740 loss: 0.0014 lr: 0.02\n",
      "iteration: 412750 loss: 0.0015 lr: 0.02\n",
      "iteration: 412760 loss: 0.0015 lr: 0.02\n",
      "iteration: 412770 loss: 0.0020 lr: 0.02\n",
      "iteration: 412780 loss: 0.0017 lr: 0.02\n",
      "iteration: 412790 loss: 0.0016 lr: 0.02\n",
      "iteration: 412800 loss: 0.0013 lr: 0.02\n",
      "iteration: 412810 loss: 0.0019 lr: 0.02\n",
      "iteration: 412820 loss: 0.0018 lr: 0.02\n",
      "iteration: 412830 loss: 0.0027 lr: 0.02\n",
      "iteration: 412840 loss: 0.0011 lr: 0.02\n",
      "iteration: 412850 loss: 0.0013 lr: 0.02\n",
      "iteration: 412860 loss: 0.0018 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 412870 loss: 0.0023 lr: 0.02\n",
      "iteration: 412880 loss: 0.0017 lr: 0.02\n",
      "iteration: 412890 loss: 0.0011 lr: 0.02\n",
      "iteration: 412900 loss: 0.0017 lr: 0.02\n",
      "iteration: 412910 loss: 0.0015 lr: 0.02\n",
      "iteration: 412920 loss: 0.0012 lr: 0.02\n",
      "iteration: 412930 loss: 0.0014 lr: 0.02\n",
      "iteration: 412940 loss: 0.0017 lr: 0.02\n",
      "iteration: 412950 loss: 0.0014 lr: 0.02\n",
      "iteration: 412960 loss: 0.0015 lr: 0.02\n",
      "iteration: 412970 loss: 0.0018 lr: 0.02\n",
      "iteration: 412980 loss: 0.0012 lr: 0.02\n",
      "iteration: 412990 loss: 0.0021 lr: 0.02\n",
      "iteration: 413000 loss: 0.0014 lr: 0.02\n",
      "iteration: 413010 loss: 0.0018 lr: 0.02\n",
      "iteration: 413020 loss: 0.0014 lr: 0.02\n",
      "iteration: 413030 loss: 0.0009 lr: 0.02\n",
      "iteration: 413040 loss: 0.0013 lr: 0.02\n",
      "iteration: 413050 loss: 0.0014 lr: 0.02\n",
      "iteration: 413060 loss: 0.0015 lr: 0.02\n",
      "iteration: 413070 loss: 0.0020 lr: 0.02\n",
      "iteration: 413080 loss: 0.0015 lr: 0.02\n",
      "iteration: 413090 loss: 0.0024 lr: 0.02\n",
      "iteration: 413100 loss: 0.0017 lr: 0.02\n",
      "iteration: 413110 loss: 0.0014 lr: 0.02\n",
      "iteration: 413120 loss: 0.0019 lr: 0.02\n",
      "iteration: 413130 loss: 0.0016 lr: 0.02\n",
      "iteration: 413140 loss: 0.0009 lr: 0.02\n",
      "iteration: 413150 loss: 0.0016 lr: 0.02\n",
      "iteration: 413160 loss: 0.0022 lr: 0.02\n",
      "iteration: 413170 loss: 0.0009 lr: 0.02\n",
      "iteration: 413180 loss: 0.0016 lr: 0.02\n",
      "iteration: 413190 loss: 0.0019 lr: 0.02\n",
      "iteration: 413200 loss: 0.0011 lr: 0.02\n",
      "iteration: 413210 loss: 0.0012 lr: 0.02\n",
      "iteration: 413220 loss: 0.0012 lr: 0.02\n",
      "iteration: 413230 loss: 0.0018 lr: 0.02\n",
      "iteration: 413240 loss: 0.0011 lr: 0.02\n",
      "iteration: 413250 loss: 0.0013 lr: 0.02\n",
      "iteration: 413260 loss: 0.0015 lr: 0.02\n",
      "iteration: 413270 loss: 0.0020 lr: 0.02\n",
      "iteration: 413280 loss: 0.0015 lr: 0.02\n",
      "iteration: 413290 loss: 0.0021 lr: 0.02\n",
      "iteration: 413300 loss: 0.0018 lr: 0.02\n",
      "iteration: 413310 loss: 0.0015 lr: 0.02\n",
      "iteration: 413320 loss: 0.0015 lr: 0.02\n",
      "iteration: 413330 loss: 0.0019 lr: 0.02\n",
      "iteration: 413340 loss: 0.0017 lr: 0.02\n",
      "iteration: 413350 loss: 0.0013 lr: 0.02\n",
      "iteration: 413360 loss: 0.0015 lr: 0.02\n",
      "iteration: 413370 loss: 0.0014 lr: 0.02\n",
      "iteration: 413380 loss: 0.0016 lr: 0.02\n",
      "iteration: 413390 loss: 0.0014 lr: 0.02\n",
      "iteration: 413400 loss: 0.0013 lr: 0.02\n",
      "iteration: 413410 loss: 0.0014 lr: 0.02\n",
      "iteration: 413420 loss: 0.0014 lr: 0.02\n",
      "iteration: 413430 loss: 0.0018 lr: 0.02\n",
      "iteration: 413440 loss: 0.0017 lr: 0.02\n",
      "iteration: 413450 loss: 0.0017 lr: 0.02\n",
      "iteration: 413460 loss: 0.0013 lr: 0.02\n",
      "iteration: 413470 loss: 0.0013 lr: 0.02\n",
      "iteration: 413480 loss: 0.0017 lr: 0.02\n",
      "iteration: 413490 loss: 0.0011 lr: 0.02\n",
      "iteration: 413500 loss: 0.0018 lr: 0.02\n",
      "iteration: 413510 loss: 0.0017 lr: 0.02\n",
      "iteration: 413520 loss: 0.0018 lr: 0.02\n",
      "iteration: 413530 loss: 0.0010 lr: 0.02\n",
      "iteration: 413540 loss: 0.0014 lr: 0.02\n",
      "iteration: 413550 loss: 0.0014 lr: 0.02\n",
      "iteration: 413560 loss: 0.0015 lr: 0.02\n",
      "iteration: 413570 loss: 0.0016 lr: 0.02\n",
      "iteration: 413580 loss: 0.0014 lr: 0.02\n",
      "iteration: 413590 loss: 0.0019 lr: 0.02\n",
      "iteration: 413600 loss: 0.0017 lr: 0.02\n",
      "iteration: 413610 loss: 0.0017 lr: 0.02\n",
      "iteration: 413620 loss: 0.0014 lr: 0.02\n",
      "iteration: 413630 loss: 0.0013 lr: 0.02\n",
      "iteration: 413640 loss: 0.0015 lr: 0.02\n",
      "iteration: 413650 loss: 0.0013 lr: 0.02\n",
      "iteration: 413660 loss: 0.0018 lr: 0.02\n",
      "iteration: 413670 loss: 0.0017 lr: 0.02\n",
      "iteration: 413680 loss: 0.0013 lr: 0.02\n",
      "iteration: 413690 loss: 0.0010 lr: 0.02\n",
      "iteration: 413700 loss: 0.0015 lr: 0.02\n",
      "iteration: 413710 loss: 0.0019 lr: 0.02\n",
      "iteration: 413720 loss: 0.0015 lr: 0.02\n",
      "iteration: 413730 loss: 0.0021 lr: 0.02\n",
      "iteration: 413740 loss: 0.0010 lr: 0.02\n",
      "iteration: 413750 loss: 0.0012 lr: 0.02\n",
      "iteration: 413760 loss: 0.0018 lr: 0.02\n",
      "iteration: 413770 loss: 0.0016 lr: 0.02\n",
      "iteration: 413780 loss: 0.0010 lr: 0.02\n",
      "iteration: 413790 loss: 0.0019 lr: 0.02\n",
      "iteration: 413800 loss: 0.0014 lr: 0.02\n",
      "iteration: 413810 loss: 0.0024 lr: 0.02\n",
      "iteration: 413820 loss: 0.0017 lr: 0.02\n",
      "iteration: 413830 loss: 0.0022 lr: 0.02\n",
      "iteration: 413840 loss: 0.0014 lr: 0.02\n",
      "iteration: 413850 loss: 0.0015 lr: 0.02\n",
      "iteration: 413860 loss: 0.0019 lr: 0.02\n",
      "iteration: 413870 loss: 0.0012 lr: 0.02\n",
      "iteration: 413880 loss: 0.0011 lr: 0.02\n",
      "iteration: 413890 loss: 0.0014 lr: 0.02\n",
      "iteration: 413900 loss: 0.0010 lr: 0.02\n",
      "iteration: 413910 loss: 0.0014 lr: 0.02\n",
      "iteration: 413920 loss: 0.0011 lr: 0.02\n",
      "iteration: 413930 loss: 0.0012 lr: 0.02\n",
      "iteration: 413940 loss: 0.0017 lr: 0.02\n",
      "iteration: 413950 loss: 0.0016 lr: 0.02\n",
      "iteration: 413960 loss: 0.0014 lr: 0.02\n",
      "iteration: 413970 loss: 0.0017 lr: 0.02\n",
      "iteration: 413980 loss: 0.0011 lr: 0.02\n",
      "iteration: 413990 loss: 0.0014 lr: 0.02\n",
      "iteration: 414000 loss: 0.0015 lr: 0.02\n",
      "iteration: 414010 loss: 0.0014 lr: 0.02\n",
      "iteration: 414020 loss: 0.0020 lr: 0.02\n",
      "iteration: 414030 loss: 0.0014 lr: 0.02\n",
      "iteration: 414040 loss: 0.0013 lr: 0.02\n",
      "iteration: 414050 loss: 0.0014 lr: 0.02\n",
      "iteration: 414060 loss: 0.0019 lr: 0.02\n",
      "iteration: 414070 loss: 0.0023 lr: 0.02\n",
      "iteration: 414080 loss: 0.0010 lr: 0.02\n",
      "iteration: 414090 loss: 0.0025 lr: 0.02\n",
      "iteration: 414100 loss: 0.0010 lr: 0.02\n",
      "iteration: 414110 loss: 0.0017 lr: 0.02\n",
      "iteration: 414120 loss: 0.0017 lr: 0.02\n",
      "iteration: 414130 loss: 0.0014 lr: 0.02\n",
      "iteration: 414140 loss: 0.0019 lr: 0.02\n",
      "iteration: 414150 loss: 0.0014 lr: 0.02\n",
      "iteration: 414160 loss: 0.0017 lr: 0.02\n",
      "iteration: 414170 loss: 0.0014 lr: 0.02\n",
      "iteration: 414180 loss: 0.0016 lr: 0.02\n",
      "iteration: 414190 loss: 0.0019 lr: 0.02\n",
      "iteration: 414200 loss: 0.0015 lr: 0.02\n",
      "iteration: 414210 loss: 0.0017 lr: 0.02\n",
      "iteration: 414220 loss: 0.0016 lr: 0.02\n",
      "iteration: 414230 loss: 0.0015 lr: 0.02\n",
      "iteration: 414240 loss: 0.0018 lr: 0.02\n",
      "iteration: 414250 loss: 0.0013 lr: 0.02\n",
      "iteration: 414260 loss: 0.0009 lr: 0.02\n",
      "iteration: 414270 loss: 0.0016 lr: 0.02\n",
      "iteration: 414280 loss: 0.0030 lr: 0.02\n",
      "iteration: 414290 loss: 0.0016 lr: 0.02\n",
      "iteration: 414300 loss: 0.0022 lr: 0.02\n",
      "iteration: 414310 loss: 0.0013 lr: 0.02\n",
      "iteration: 414320 loss: 0.0016 lr: 0.02\n",
      "iteration: 414330 loss: 0.0019 lr: 0.02\n",
      "iteration: 414340 loss: 0.0015 lr: 0.02\n",
      "iteration: 414350 loss: 0.0015 lr: 0.02\n",
      "iteration: 414360 loss: 0.0010 lr: 0.02\n",
      "iteration: 414370 loss: 0.0018 lr: 0.02\n",
      "iteration: 414380 loss: 0.0016 lr: 0.02\n",
      "iteration: 414390 loss: 0.0017 lr: 0.02\n",
      "iteration: 414400 loss: 0.0016 lr: 0.02\n",
      "iteration: 414410 loss: 0.0012 lr: 0.02\n",
      "iteration: 414420 loss: 0.0013 lr: 0.02\n",
      "iteration: 414430 loss: 0.0012 lr: 0.02\n",
      "iteration: 414440 loss: 0.0019 lr: 0.02\n",
      "iteration: 414450 loss: 0.0012 lr: 0.02\n",
      "iteration: 414460 loss: 0.0020 lr: 0.02\n",
      "iteration: 414470 loss: 0.0017 lr: 0.02\n",
      "iteration: 414480 loss: 0.0016 lr: 0.02\n",
      "iteration: 414490 loss: 0.0013 lr: 0.02\n",
      "iteration: 414500 loss: 0.0025 lr: 0.02\n",
      "iteration: 414510 loss: 0.0014 lr: 0.02\n",
      "iteration: 414520 loss: 0.0014 lr: 0.02\n",
      "iteration: 414530 loss: 0.0016 lr: 0.02\n",
      "iteration: 414540 loss: 0.0014 lr: 0.02\n",
      "iteration: 414550 loss: 0.0017 lr: 0.02\n",
      "iteration: 414560 loss: 0.0011 lr: 0.02\n",
      "iteration: 414570 loss: 0.0017 lr: 0.02\n",
      "iteration: 414580 loss: 0.0016 lr: 0.02\n",
      "iteration: 414590 loss: 0.0021 lr: 0.02\n",
      "iteration: 414600 loss: 0.0013 lr: 0.02\n",
      "iteration: 414610 loss: 0.0014 lr: 0.02\n",
      "iteration: 414620 loss: 0.0024 lr: 0.02\n",
      "iteration: 414630 loss: 0.0019 lr: 0.02\n",
      "iteration: 414640 loss: 0.0022 lr: 0.02\n",
      "iteration: 414650 loss: 0.0015 lr: 0.02\n",
      "iteration: 414660 loss: 0.0012 lr: 0.02\n",
      "iteration: 414670 loss: 0.0014 lr: 0.02\n",
      "iteration: 414680 loss: 0.0010 lr: 0.02\n",
      "iteration: 414690 loss: 0.0017 lr: 0.02\n",
      "iteration: 414700 loss: 0.0015 lr: 0.02\n",
      "iteration: 414710 loss: 0.0021 lr: 0.02\n",
      "iteration: 414720 loss: 0.0011 lr: 0.02\n",
      "iteration: 414730 loss: 0.0021 lr: 0.02\n",
      "iteration: 414740 loss: 0.0016 lr: 0.02\n",
      "iteration: 414750 loss: 0.0016 lr: 0.02\n",
      "iteration: 414760 loss: 0.0018 lr: 0.02\n",
      "iteration: 414770 loss: 0.0012 lr: 0.02\n",
      "iteration: 414780 loss: 0.0012 lr: 0.02\n",
      "iteration: 414790 loss: 0.0014 lr: 0.02\n",
      "iteration: 414800 loss: 0.0018 lr: 0.02\n",
      "iteration: 414810 loss: 0.0011 lr: 0.02\n",
      "iteration: 414820 loss: 0.0022 lr: 0.02\n",
      "iteration: 414830 loss: 0.0014 lr: 0.02\n",
      "iteration: 414840 loss: 0.0020 lr: 0.02\n",
      "iteration: 414850 loss: 0.0018 lr: 0.02\n",
      "iteration: 414860 loss: 0.0017 lr: 0.02\n",
      "iteration: 414870 loss: 0.0014 lr: 0.02\n",
      "iteration: 414880 loss: 0.0021 lr: 0.02\n",
      "iteration: 414890 loss: 0.0019 lr: 0.02\n",
      "iteration: 414900 loss: 0.0014 lr: 0.02\n",
      "iteration: 414910 loss: 0.0013 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 414920 loss: 0.0012 lr: 0.02\n",
      "iteration: 414930 loss: 0.0009 lr: 0.02\n",
      "iteration: 414940 loss: 0.0014 lr: 0.02\n",
      "iteration: 414950 loss: 0.0016 lr: 0.02\n",
      "iteration: 414960 loss: 0.0012 lr: 0.02\n",
      "iteration: 414970 loss: 0.0017 lr: 0.02\n",
      "iteration: 414980 loss: 0.0012 lr: 0.02\n",
      "iteration: 414990 loss: 0.0013 lr: 0.02\n",
      "iteration: 415000 loss: 0.0014 lr: 0.02\n",
      "iteration: 415010 loss: 0.0017 lr: 0.02\n",
      "iteration: 415020 loss: 0.0014 lr: 0.02\n",
      "iteration: 415030 loss: 0.0015 lr: 0.02\n",
      "iteration: 415040 loss: 0.0016 lr: 0.02\n",
      "iteration: 415050 loss: 0.0016 lr: 0.02\n",
      "iteration: 415060 loss: 0.0020 lr: 0.02\n",
      "iteration: 415070 loss: 0.0013 lr: 0.02\n",
      "iteration: 415080 loss: 0.0014 lr: 0.02\n",
      "iteration: 415090 loss: 0.0013 lr: 0.02\n",
      "iteration: 415100 loss: 0.0019 lr: 0.02\n",
      "iteration: 415110 loss: 0.0011 lr: 0.02\n",
      "iteration: 415120 loss: 0.0019 lr: 0.02\n",
      "iteration: 415130 loss: 0.0014 lr: 0.02\n",
      "iteration: 415140 loss: 0.0015 lr: 0.02\n",
      "iteration: 415150 loss: 0.0017 lr: 0.02\n",
      "iteration: 415160 loss: 0.0018 lr: 0.02\n",
      "iteration: 415170 loss: 0.0020 lr: 0.02\n",
      "iteration: 415180 loss: 0.0013 lr: 0.02\n",
      "iteration: 415190 loss: 0.0015 lr: 0.02\n",
      "iteration: 415200 loss: 0.0010 lr: 0.02\n",
      "iteration: 415210 loss: 0.0012 lr: 0.02\n",
      "iteration: 415220 loss: 0.0017 lr: 0.02\n",
      "iteration: 415230 loss: 0.0012 lr: 0.02\n",
      "iteration: 415240 loss: 0.0013 lr: 0.02\n",
      "iteration: 415250 loss: 0.0014 lr: 0.02\n",
      "iteration: 415260 loss: 0.0010 lr: 0.02\n",
      "iteration: 415270 loss: 0.0020 lr: 0.02\n",
      "iteration: 415280 loss: 0.0018 lr: 0.02\n",
      "iteration: 415290 loss: 0.0012 lr: 0.02\n",
      "iteration: 415300 loss: 0.0019 lr: 0.02\n",
      "iteration: 415310 loss: 0.0011 lr: 0.02\n",
      "iteration: 415320 loss: 0.0018 lr: 0.02\n",
      "iteration: 415330 loss: 0.0021 lr: 0.02\n",
      "iteration: 415340 loss: 0.0031 lr: 0.02\n",
      "iteration: 415350 loss: 0.0027 lr: 0.02\n",
      "iteration: 415360 loss: 0.0011 lr: 0.02\n",
      "iteration: 415370 loss: 0.0015 lr: 0.02\n",
      "iteration: 415380 loss: 0.0013 lr: 0.02\n",
      "iteration: 415390 loss: 0.0017 lr: 0.02\n",
      "iteration: 415400 loss: 0.0019 lr: 0.02\n",
      "iteration: 415410 loss: 0.0011 lr: 0.02\n",
      "iteration: 415420 loss: 0.0014 lr: 0.02\n",
      "iteration: 415430 loss: 0.0023 lr: 0.02\n",
      "iteration: 415440 loss: 0.0030 lr: 0.02\n",
      "iteration: 415450 loss: 0.0023 lr: 0.02\n",
      "iteration: 415460 loss: 0.0014 lr: 0.02\n",
      "iteration: 415470 loss: 0.0017 lr: 0.02\n",
      "iteration: 415480 loss: 0.0020 lr: 0.02\n",
      "iteration: 415490 loss: 0.0022 lr: 0.02\n",
      "iteration: 415500 loss: 0.0014 lr: 0.02\n",
      "iteration: 415510 loss: 0.0021 lr: 0.02\n",
      "iteration: 415520 loss: 0.0013 lr: 0.02\n",
      "iteration: 415530 loss: 0.0027 lr: 0.02\n",
      "iteration: 415540 loss: 0.0020 lr: 0.02\n",
      "iteration: 415550 loss: 0.0021 lr: 0.02\n",
      "iteration: 415560 loss: 0.0018 lr: 0.02\n",
      "iteration: 415570 loss: 0.0013 lr: 0.02\n",
      "iteration: 415580 loss: 0.0023 lr: 0.02\n",
      "iteration: 415590 loss: 0.0024 lr: 0.02\n",
      "iteration: 415600 loss: 0.0014 lr: 0.02\n",
      "iteration: 415610 loss: 0.0016 lr: 0.02\n",
      "iteration: 415620 loss: 0.0018 lr: 0.02\n",
      "iteration: 415630 loss: 0.0018 lr: 0.02\n",
      "iteration: 415640 loss: 0.0022 lr: 0.02\n",
      "iteration: 415650 loss: 0.0015 lr: 0.02\n",
      "iteration: 415660 loss: 0.0021 lr: 0.02\n",
      "iteration: 415670 loss: 0.0023 lr: 0.02\n",
      "iteration: 415680 loss: 0.0013 lr: 0.02\n",
      "iteration: 415690 loss: 0.0014 lr: 0.02\n",
      "iteration: 415700 loss: 0.0014 lr: 0.02\n",
      "iteration: 415710 loss: 0.0015 lr: 0.02\n",
      "iteration: 415720 loss: 0.0011 lr: 0.02\n",
      "iteration: 415730 loss: 0.0018 lr: 0.02\n",
      "iteration: 415740 loss: 0.0011 lr: 0.02\n",
      "iteration: 415750 loss: 0.0019 lr: 0.02\n",
      "iteration: 415760 loss: 0.0010 lr: 0.02\n",
      "iteration: 415770 loss: 0.0017 lr: 0.02\n",
      "iteration: 415780 loss: 0.0021 lr: 0.02\n",
      "iteration: 415790 loss: 0.0024 lr: 0.02\n",
      "iteration: 415800 loss: 0.0016 lr: 0.02\n",
      "iteration: 415810 loss: 0.0018 lr: 0.02\n",
      "iteration: 415820 loss: 0.0013 lr: 0.02\n",
      "iteration: 415830 loss: 0.0019 lr: 0.02\n",
      "iteration: 415840 loss: 0.0024 lr: 0.02\n",
      "iteration: 415850 loss: 0.0024 lr: 0.02\n",
      "iteration: 415860 loss: 0.0020 lr: 0.02\n",
      "iteration: 415870 loss: 0.0016 lr: 0.02\n",
      "iteration: 415880 loss: 0.0016 lr: 0.02\n",
      "iteration: 415890 loss: 0.0015 lr: 0.02\n",
      "iteration: 415900 loss: 0.0010 lr: 0.02\n",
      "iteration: 415910 loss: 0.0012 lr: 0.02\n",
      "iteration: 415920 loss: 0.0021 lr: 0.02\n",
      "iteration: 415930 loss: 0.0016 lr: 0.02\n",
      "iteration: 415940 loss: 0.0020 lr: 0.02\n",
      "iteration: 415950 loss: 0.0018 lr: 0.02\n",
      "iteration: 415960 loss: 0.0017 lr: 0.02\n",
      "iteration: 415970 loss: 0.0017 lr: 0.02\n",
      "iteration: 415980 loss: 0.0012 lr: 0.02\n",
      "iteration: 415990 loss: 0.0013 lr: 0.02\n",
      "iteration: 416000 loss: 0.0014 lr: 0.02\n",
      "iteration: 416010 loss: 0.0021 lr: 0.02\n",
      "iteration: 416020 loss: 0.0015 lr: 0.02\n",
      "iteration: 416030 loss: 0.0018 lr: 0.02\n",
      "iteration: 416040 loss: 0.0010 lr: 0.02\n",
      "iteration: 416050 loss: 0.0014 lr: 0.02\n",
      "iteration: 416060 loss: 0.0013 lr: 0.02\n",
      "iteration: 416070 loss: 0.0015 lr: 0.02\n",
      "iteration: 416080 loss: 0.0017 lr: 0.02\n",
      "iteration: 416090 loss: 0.0018 lr: 0.02\n",
      "iteration: 416100 loss: 0.0014 lr: 0.02\n",
      "iteration: 416110 loss: 0.0016 lr: 0.02\n",
      "iteration: 416120 loss: 0.0015 lr: 0.02\n",
      "iteration: 416130 loss: 0.0012 lr: 0.02\n",
      "iteration: 416140 loss: 0.0018 lr: 0.02\n",
      "iteration: 416150 loss: 0.0012 lr: 0.02\n",
      "iteration: 416160 loss: 0.0013 lr: 0.02\n",
      "iteration: 416170 loss: 0.0016 lr: 0.02\n",
      "iteration: 416180 loss: 0.0010 lr: 0.02\n",
      "iteration: 416190 loss: 0.0019 lr: 0.02\n",
      "iteration: 416200 loss: 0.0010 lr: 0.02\n",
      "iteration: 416210 loss: 0.0014 lr: 0.02\n",
      "iteration: 416220 loss: 0.0012 lr: 0.02\n",
      "iteration: 416230 loss: 0.0016 lr: 0.02\n",
      "iteration: 416240 loss: 0.0013 lr: 0.02\n",
      "iteration: 416250 loss: 0.0012 lr: 0.02\n",
      "iteration: 416260 loss: 0.0012 lr: 0.02\n",
      "iteration: 416270 loss: 0.0018 lr: 0.02\n",
      "iteration: 416280 loss: 0.0015 lr: 0.02\n",
      "iteration: 416290 loss: 0.0017 lr: 0.02\n",
      "iteration: 416300 loss: 0.0015 lr: 0.02\n",
      "iteration: 416310 loss: 0.0014 lr: 0.02\n",
      "iteration: 416320 loss: 0.0012 lr: 0.02\n",
      "iteration: 416330 loss: 0.0013 lr: 0.02\n",
      "iteration: 416340 loss: 0.0015 lr: 0.02\n",
      "iteration: 416350 loss: 0.0013 lr: 0.02\n",
      "iteration: 416360 loss: 0.0016 lr: 0.02\n",
      "iteration: 416370 loss: 0.0015 lr: 0.02\n",
      "iteration: 416380 loss: 0.0011 lr: 0.02\n",
      "iteration: 416390 loss: 0.0019 lr: 0.02\n",
      "iteration: 416400 loss: 0.0009 lr: 0.02\n",
      "iteration: 416410 loss: 0.0014 lr: 0.02\n",
      "iteration: 416420 loss: 0.0026 lr: 0.02\n",
      "iteration: 416430 loss: 0.0014 lr: 0.02\n",
      "iteration: 416440 loss: 0.0017 lr: 0.02\n",
      "iteration: 416450 loss: 0.0018 lr: 0.02\n",
      "iteration: 416460 loss: 0.0017 lr: 0.02\n",
      "iteration: 416470 loss: 0.0016 lr: 0.02\n",
      "iteration: 416480 loss: 0.0018 lr: 0.02\n",
      "iteration: 416490 loss: 0.0018 lr: 0.02\n",
      "iteration: 416500 loss: 0.0019 lr: 0.02\n",
      "iteration: 416510 loss: 0.0013 lr: 0.02\n",
      "iteration: 416520 loss: 0.0012 lr: 0.02\n",
      "iteration: 416530 loss: 0.0011 lr: 0.02\n",
      "iteration: 416540 loss: 0.0025 lr: 0.02\n",
      "iteration: 416550 loss: 0.0014 lr: 0.02\n",
      "iteration: 416560 loss: 0.0014 lr: 0.02\n",
      "iteration: 416570 loss: 0.0015 lr: 0.02\n",
      "iteration: 416580 loss: 0.0018 lr: 0.02\n",
      "iteration: 416590 loss: 0.0015 lr: 0.02\n",
      "iteration: 416600 loss: 0.0013 lr: 0.02\n",
      "iteration: 416610 loss: 0.0018 lr: 0.02\n",
      "iteration: 416620 loss: 0.0012 lr: 0.02\n",
      "iteration: 416630 loss: 0.0024 lr: 0.02\n",
      "iteration: 416640 loss: 0.0013 lr: 0.02\n",
      "iteration: 416650 loss: 0.0010 lr: 0.02\n",
      "iteration: 416660 loss: 0.0014 lr: 0.02\n",
      "iteration: 416670 loss: 0.0017 lr: 0.02\n",
      "iteration: 416680 loss: 0.0014 lr: 0.02\n",
      "iteration: 416690 loss: 0.0011 lr: 0.02\n",
      "iteration: 416700 loss: 0.0017 lr: 0.02\n",
      "iteration: 416710 loss: 0.0015 lr: 0.02\n",
      "iteration: 416720 loss: 0.0014 lr: 0.02\n",
      "iteration: 416730 loss: 0.0010 lr: 0.02\n",
      "iteration: 416740 loss: 0.0011 lr: 0.02\n",
      "iteration: 416750 loss: 0.0015 lr: 0.02\n",
      "iteration: 416760 loss: 0.0018 lr: 0.02\n",
      "iteration: 416770 loss: 0.0015 lr: 0.02\n",
      "iteration: 416780 loss: 0.0019 lr: 0.02\n",
      "iteration: 416790 loss: 0.0024 lr: 0.02\n",
      "iteration: 416800 loss: 0.0018 lr: 0.02\n",
      "iteration: 416810 loss: 0.0017 lr: 0.02\n",
      "iteration: 416820 loss: 0.0017 lr: 0.02\n",
      "iteration: 416830 loss: 0.0014 lr: 0.02\n",
      "iteration: 416840 loss: 0.0014 lr: 0.02\n",
      "iteration: 416850 loss: 0.0031 lr: 0.02\n",
      "iteration: 416860 loss: 0.0013 lr: 0.02\n",
      "iteration: 416870 loss: 0.0009 lr: 0.02\n",
      "iteration: 416880 loss: 0.0025 lr: 0.02\n",
      "iteration: 416890 loss: 0.0012 lr: 0.02\n",
      "iteration: 416900 loss: 0.0019 lr: 0.02\n",
      "iteration: 416910 loss: 0.0016 lr: 0.02\n",
      "iteration: 416920 loss: 0.0017 lr: 0.02\n",
      "iteration: 416930 loss: 0.0016 lr: 0.02\n",
      "iteration: 416940 loss: 0.0013 lr: 0.02\n",
      "iteration: 416950 loss: 0.0015 lr: 0.02\n",
      "iteration: 416960 loss: 0.0014 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 416970 loss: 0.0008 lr: 0.02\n",
      "iteration: 416980 loss: 0.0024 lr: 0.02\n",
      "iteration: 416990 loss: 0.0023 lr: 0.02\n",
      "iteration: 417000 loss: 0.0017 lr: 0.02\n",
      "iteration: 417010 loss: 0.0018 lr: 0.02\n",
      "iteration: 417020 loss: 0.0016 lr: 0.02\n",
      "iteration: 417030 loss: 0.0015 lr: 0.02\n",
      "iteration: 417040 loss: 0.0016 lr: 0.02\n",
      "iteration: 417050 loss: 0.0013 lr: 0.02\n",
      "iteration: 417060 loss: 0.0015 lr: 0.02\n",
      "iteration: 417070 loss: 0.0018 lr: 0.02\n",
      "iteration: 417080 loss: 0.0021 lr: 0.02\n",
      "iteration: 417090 loss: 0.0024 lr: 0.02\n",
      "iteration: 417100 loss: 0.0019 lr: 0.02\n",
      "iteration: 417110 loss: 0.0017 lr: 0.02\n",
      "iteration: 417120 loss: 0.0020 lr: 0.02\n",
      "iteration: 417130 loss: 0.0011 lr: 0.02\n",
      "iteration: 417140 loss: 0.0015 lr: 0.02\n",
      "iteration: 417150 loss: 0.0018 lr: 0.02\n",
      "iteration: 417160 loss: 0.0022 lr: 0.02\n",
      "iteration: 417170 loss: 0.0020 lr: 0.02\n",
      "iteration: 417180 loss: 0.0017 lr: 0.02\n",
      "iteration: 417190 loss: 0.0014 lr: 0.02\n",
      "iteration: 417200 loss: 0.0014 lr: 0.02\n",
      "iteration: 417210 loss: 0.0022 lr: 0.02\n",
      "iteration: 417220 loss: 0.0016 lr: 0.02\n",
      "iteration: 417230 loss: 0.0023 lr: 0.02\n",
      "iteration: 417240 loss: 0.0014 lr: 0.02\n",
      "iteration: 417250 loss: 0.0018 lr: 0.02\n",
      "iteration: 417260 loss: 0.0012 lr: 0.02\n",
      "iteration: 417270 loss: 0.0019 lr: 0.02\n",
      "iteration: 417280 loss: 0.0017 lr: 0.02\n",
      "iteration: 417290 loss: 0.0016 lr: 0.02\n",
      "iteration: 417300 loss: 0.0013 lr: 0.02\n",
      "iteration: 417310 loss: 0.0015 lr: 0.02\n",
      "iteration: 417320 loss: 0.0011 lr: 0.02\n",
      "iteration: 417330 loss: 0.0014 lr: 0.02\n",
      "iteration: 417340 loss: 0.0024 lr: 0.02\n",
      "iteration: 417350 loss: 0.0025 lr: 0.02\n",
      "iteration: 417360 loss: 0.0017 lr: 0.02\n",
      "iteration: 417370 loss: 0.0015 lr: 0.02\n",
      "iteration: 417380 loss: 0.0017 lr: 0.02\n",
      "iteration: 417390 loss: 0.0022 lr: 0.02\n",
      "iteration: 417400 loss: 0.0013 lr: 0.02\n",
      "iteration: 417410 loss: 0.0012 lr: 0.02\n",
      "iteration: 417420 loss: 0.0019 lr: 0.02\n",
      "iteration: 417430 loss: 0.0022 lr: 0.02\n",
      "iteration: 417440 loss: 0.0012 lr: 0.02\n",
      "iteration: 417450 loss: 0.0016 lr: 0.02\n",
      "iteration: 417460 loss: 0.0015 lr: 0.02\n",
      "iteration: 417470 loss: 0.0012 lr: 0.02\n",
      "iteration: 417480 loss: 0.0012 lr: 0.02\n",
      "iteration: 417490 loss: 0.0013 lr: 0.02\n",
      "iteration: 417500 loss: 0.0018 lr: 0.02\n",
      "iteration: 417510 loss: 0.0011 lr: 0.02\n",
      "iteration: 417520 loss: 0.0023 lr: 0.02\n",
      "iteration: 417530 loss: 0.0022 lr: 0.02\n",
      "iteration: 417540 loss: 0.0011 lr: 0.02\n",
      "iteration: 417550 loss: 0.0012 lr: 0.02\n",
      "iteration: 417560 loss: 0.0012 lr: 0.02\n",
      "iteration: 417570 loss: 0.0022 lr: 0.02\n",
      "iteration: 417580 loss: 0.0020 lr: 0.02\n",
      "iteration: 417590 loss: 0.0013 lr: 0.02\n",
      "iteration: 417600 loss: 0.0015 lr: 0.02\n",
      "iteration: 417610 loss: 0.0019 lr: 0.02\n",
      "iteration: 417620 loss: 0.0014 lr: 0.02\n",
      "iteration: 417630 loss: 0.0023 lr: 0.02\n",
      "iteration: 417640 loss: 0.0015 lr: 0.02\n",
      "iteration: 417650 loss: 0.0013 lr: 0.02\n",
      "iteration: 417660 loss: 0.0023 lr: 0.02\n",
      "iteration: 417670 loss: 0.0018 lr: 0.02\n",
      "iteration: 417680 loss: 0.0012 lr: 0.02\n",
      "iteration: 417690 loss: 0.0012 lr: 0.02\n",
      "iteration: 417700 loss: 0.0018 lr: 0.02\n",
      "iteration: 417710 loss: 0.0020 lr: 0.02\n",
      "iteration: 417720 loss: 0.0030 lr: 0.02\n",
      "iteration: 417730 loss: 0.0017 lr: 0.02\n",
      "iteration: 417740 loss: 0.0015 lr: 0.02\n",
      "iteration: 417750 loss: 0.0011 lr: 0.02\n",
      "iteration: 417760 loss: 0.0023 lr: 0.02\n",
      "iteration: 417770 loss: 0.0020 lr: 0.02\n",
      "iteration: 417780 loss: 0.0013 lr: 0.02\n",
      "iteration: 417790 loss: 0.0019 lr: 0.02\n",
      "iteration: 417800 loss: 0.0013 lr: 0.02\n",
      "iteration: 417810 loss: 0.0013 lr: 0.02\n",
      "iteration: 417820 loss: 0.0010 lr: 0.02\n",
      "iteration: 417830 loss: 0.0017 lr: 0.02\n",
      "iteration: 417840 loss: 0.0015 lr: 0.02\n",
      "iteration: 417850 loss: 0.0017 lr: 0.02\n",
      "iteration: 417860 loss: 0.0013 lr: 0.02\n",
      "iteration: 417870 loss: 0.0011 lr: 0.02\n",
      "iteration: 417880 loss: 0.0020 lr: 0.02\n",
      "iteration: 417890 loss: 0.0023 lr: 0.02\n",
      "iteration: 417900 loss: 0.0017 lr: 0.02\n",
      "iteration: 417910 loss: 0.0023 lr: 0.02\n",
      "iteration: 417920 loss: 0.0017 lr: 0.02\n",
      "iteration: 417930 loss: 0.0014 lr: 0.02\n",
      "iteration: 417940 loss: 0.0048 lr: 0.02\n",
      "iteration: 417950 loss: 0.0018 lr: 0.02\n",
      "iteration: 417960 loss: 0.0018 lr: 0.02\n",
      "iteration: 417970 loss: 0.0025 lr: 0.02\n",
      "iteration: 417980 loss: 0.0018 lr: 0.02\n",
      "iteration: 417990 loss: 0.0024 lr: 0.02\n",
      "iteration: 418000 loss: 0.0017 lr: 0.02\n",
      "iteration: 418010 loss: 0.0016 lr: 0.02\n",
      "iteration: 418020 loss: 0.0016 lr: 0.02\n",
      "iteration: 418030 loss: 0.0021 lr: 0.02\n",
      "iteration: 418040 loss: 0.0018 lr: 0.02\n",
      "iteration: 418050 loss: 0.0015 lr: 0.02\n",
      "iteration: 418060 loss: 0.0016 lr: 0.02\n",
      "iteration: 418070 loss: 0.0018 lr: 0.02\n",
      "iteration: 418080 loss: 0.0018 lr: 0.02\n",
      "iteration: 418090 loss: 0.0016 lr: 0.02\n",
      "iteration: 418100 loss: 0.0020 lr: 0.02\n",
      "iteration: 418110 loss: 0.0014 lr: 0.02\n",
      "iteration: 418120 loss: 0.0012 lr: 0.02\n",
      "iteration: 418130 loss: 0.0015 lr: 0.02\n",
      "iteration: 418140 loss: 0.0014 lr: 0.02\n",
      "iteration: 418150 loss: 0.0014 lr: 0.02\n",
      "iteration: 418160 loss: 0.0019 lr: 0.02\n",
      "iteration: 418170 loss: 0.0011 lr: 0.02\n",
      "iteration: 418180 loss: 0.0016 lr: 0.02\n",
      "iteration: 418190 loss: 0.0011 lr: 0.02\n",
      "iteration: 418200 loss: 0.0011 lr: 0.02\n",
      "iteration: 418210 loss: 0.0014 lr: 0.02\n",
      "iteration: 418220 loss: 0.0020 lr: 0.02\n",
      "iteration: 418230 loss: 0.0013 lr: 0.02\n",
      "iteration: 418240 loss: 0.0022 lr: 0.02\n",
      "iteration: 418250 loss: 0.0013 lr: 0.02\n",
      "iteration: 418260 loss: 0.0014 lr: 0.02\n",
      "iteration: 418270 loss: 0.0016 lr: 0.02\n",
      "iteration: 418280 loss: 0.0012 lr: 0.02\n",
      "iteration: 418290 loss: 0.0012 lr: 0.02\n",
      "iteration: 418300 loss: 0.0011 lr: 0.02\n",
      "iteration: 418310 loss: 0.0010 lr: 0.02\n",
      "iteration: 418320 loss: 0.0016 lr: 0.02\n",
      "iteration: 418330 loss: 0.0025 lr: 0.02\n",
      "iteration: 418340 loss: 0.0019 lr: 0.02\n",
      "iteration: 418350 loss: 0.0017 lr: 0.02\n",
      "iteration: 418360 loss: 0.0013 lr: 0.02\n",
      "iteration: 418370 loss: 0.0025 lr: 0.02\n",
      "iteration: 418380 loss: 0.0014 lr: 0.02\n",
      "iteration: 418390 loss: 0.0015 lr: 0.02\n",
      "iteration: 418400 loss: 0.0014 lr: 0.02\n",
      "iteration: 418410 loss: 0.0012 lr: 0.02\n",
      "iteration: 418420 loss: 0.0011 lr: 0.02\n",
      "iteration: 418430 loss: 0.0021 lr: 0.02\n",
      "iteration: 418440 loss: 0.0011 lr: 0.02\n",
      "iteration: 418450 loss: 0.0025 lr: 0.02\n",
      "iteration: 418460 loss: 0.0016 lr: 0.02\n",
      "iteration: 418470 loss: 0.0020 lr: 0.02\n",
      "iteration: 418480 loss: 0.0012 lr: 0.02\n",
      "iteration: 418490 loss: 0.0041 lr: 0.02\n",
      "iteration: 418500 loss: 0.0012 lr: 0.02\n",
      "iteration: 418510 loss: 0.0016 lr: 0.02\n",
      "iteration: 418520 loss: 0.0018 lr: 0.02\n",
      "iteration: 418530 loss: 0.0018 lr: 0.02\n",
      "iteration: 418540 loss: 0.0013 lr: 0.02\n",
      "iteration: 418550 loss: 0.0012 lr: 0.02\n",
      "iteration: 418560 loss: 0.0011 lr: 0.02\n",
      "iteration: 418570 loss: 0.0015 lr: 0.02\n",
      "iteration: 418580 loss: 0.0016 lr: 0.02\n",
      "iteration: 418590 loss: 0.0013 lr: 0.02\n",
      "iteration: 418600 loss: 0.0020 lr: 0.02\n",
      "iteration: 418610 loss: 0.0017 lr: 0.02\n",
      "iteration: 418620 loss: 0.0021 lr: 0.02\n",
      "iteration: 418630 loss: 0.0017 lr: 0.02\n",
      "iteration: 418640 loss: 0.0011 lr: 0.02\n",
      "iteration: 418650 loss: 0.0010 lr: 0.02\n",
      "iteration: 418660 loss: 0.0019 lr: 0.02\n",
      "iteration: 418670 loss: 0.0012 lr: 0.02\n",
      "iteration: 418680 loss: 0.0011 lr: 0.02\n",
      "iteration: 418690 loss: 0.0012 lr: 0.02\n",
      "iteration: 418700 loss: 0.0011 lr: 0.02\n",
      "iteration: 418710 loss: 0.0013 lr: 0.02\n",
      "iteration: 418720 loss: 0.0013 lr: 0.02\n",
      "iteration: 418730 loss: 0.0027 lr: 0.02\n",
      "iteration: 418740 loss: 0.0013 lr: 0.02\n",
      "iteration: 418750 loss: 0.0017 lr: 0.02\n",
      "iteration: 418760 loss: 0.0018 lr: 0.02\n",
      "iteration: 418770 loss: 0.0015 lr: 0.02\n",
      "iteration: 418780 loss: 0.0024 lr: 0.02\n",
      "iteration: 418790 loss: 0.0014 lr: 0.02\n",
      "iteration: 418800 loss: 0.0017 lr: 0.02\n",
      "iteration: 418810 loss: 0.0012 lr: 0.02\n",
      "iteration: 418820 loss: 0.0011 lr: 0.02\n",
      "iteration: 418830 loss: 0.0020 lr: 0.02\n",
      "iteration: 418840 loss: 0.0015 lr: 0.02\n",
      "iteration: 418850 loss: 0.0021 lr: 0.02\n",
      "iteration: 418860 loss: 0.0017 lr: 0.02\n",
      "iteration: 418870 loss: 0.0017 lr: 0.02\n",
      "iteration: 418880 loss: 0.0013 lr: 0.02\n",
      "iteration: 418890 loss: 0.0014 lr: 0.02\n",
      "iteration: 418900 loss: 0.0015 lr: 0.02\n",
      "iteration: 418910 loss: 0.0012 lr: 0.02\n",
      "iteration: 418920 loss: 0.0012 lr: 0.02\n",
      "iteration: 418930 loss: 0.0016 lr: 0.02\n",
      "iteration: 418940 loss: 0.0014 lr: 0.02\n",
      "iteration: 418950 loss: 0.0014 lr: 0.02\n",
      "iteration: 418960 loss: 0.0015 lr: 0.02\n",
      "iteration: 418970 loss: 0.0016 lr: 0.02\n",
      "iteration: 418980 loss: 0.0014 lr: 0.02\n",
      "iteration: 418990 loss: 0.0012 lr: 0.02\n",
      "iteration: 419000 loss: 0.0018 lr: 0.02\n",
      "iteration: 419010 loss: 0.0021 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 419020 loss: 0.0017 lr: 0.02\n",
      "iteration: 419030 loss: 0.0011 lr: 0.02\n",
      "iteration: 419040 loss: 0.0016 lr: 0.02\n",
      "iteration: 419050 loss: 0.0012 lr: 0.02\n",
      "iteration: 419060 loss: 0.0018 lr: 0.02\n",
      "iteration: 419070 loss: 0.0017 lr: 0.02\n",
      "iteration: 419080 loss: 0.0014 lr: 0.02\n",
      "iteration: 419090 loss: 0.0016 lr: 0.02\n",
      "iteration: 419100 loss: 0.0012 lr: 0.02\n",
      "iteration: 419110 loss: 0.0014 lr: 0.02\n",
      "iteration: 419120 loss: 0.0013 lr: 0.02\n",
      "iteration: 419130 loss: 0.0010 lr: 0.02\n",
      "iteration: 419140 loss: 0.0012 lr: 0.02\n",
      "iteration: 419150 loss: 0.0012 lr: 0.02\n",
      "iteration: 419160 loss: 0.0017 lr: 0.02\n",
      "iteration: 419170 loss: 0.0017 lr: 0.02\n",
      "iteration: 419180 loss: 0.0011 lr: 0.02\n",
      "iteration: 419190 loss: 0.0010 lr: 0.02\n",
      "iteration: 419200 loss: 0.0016 lr: 0.02\n",
      "iteration: 419210 loss: 0.0012 lr: 0.02\n",
      "iteration: 419220 loss: 0.0019 lr: 0.02\n",
      "iteration: 419230 loss: 0.0021 lr: 0.02\n",
      "iteration: 419240 loss: 0.0013 lr: 0.02\n",
      "iteration: 419250 loss: 0.0020 lr: 0.02\n",
      "iteration: 419260 loss: 0.0011 lr: 0.02\n",
      "iteration: 419270 loss: 0.0012 lr: 0.02\n",
      "iteration: 419280 loss: 0.0014 lr: 0.02\n",
      "iteration: 419290 loss: 0.0017 lr: 0.02\n",
      "iteration: 419300 loss: 0.0017 lr: 0.02\n",
      "iteration: 419310 loss: 0.0012 lr: 0.02\n",
      "iteration: 419320 loss: 0.0012 lr: 0.02\n",
      "iteration: 419330 loss: 0.0013 lr: 0.02\n",
      "iteration: 419340 loss: 0.0012 lr: 0.02\n",
      "iteration: 419350 loss: 0.0012 lr: 0.02\n",
      "iteration: 419360 loss: 0.0020 lr: 0.02\n",
      "iteration: 419370 loss: 0.0014 lr: 0.02\n",
      "iteration: 419380 loss: 0.0013 lr: 0.02\n",
      "iteration: 419390 loss: 0.0011 lr: 0.02\n",
      "iteration: 419400 loss: 0.0009 lr: 0.02\n",
      "iteration: 419410 loss: 0.0017 lr: 0.02\n",
      "iteration: 419420 loss: 0.0011 lr: 0.02\n",
      "iteration: 419430 loss: 0.0018 lr: 0.02\n",
      "iteration: 419440 loss: 0.0017 lr: 0.02\n",
      "iteration: 419450 loss: 0.0013 lr: 0.02\n",
      "iteration: 419460 loss: 0.0011 lr: 0.02\n",
      "iteration: 419470 loss: 0.0023 lr: 0.02\n",
      "iteration: 419480 loss: 0.0015 lr: 0.02\n",
      "iteration: 419490 loss: 0.0011 lr: 0.02\n",
      "iteration: 419500 loss: 0.0026 lr: 0.02\n",
      "iteration: 419510 loss: 0.0017 lr: 0.02\n",
      "iteration: 419520 loss: 0.0018 lr: 0.02\n",
      "iteration: 419530 loss: 0.0012 lr: 0.02\n",
      "iteration: 419540 loss: 0.0011 lr: 0.02\n",
      "iteration: 419550 loss: 0.0021 lr: 0.02\n",
      "iteration: 419560 loss: 0.0020 lr: 0.02\n",
      "iteration: 419570 loss: 0.0019 lr: 0.02\n",
      "iteration: 419580 loss: 0.0018 lr: 0.02\n",
      "iteration: 419590 loss: 0.0018 lr: 0.02\n",
      "iteration: 419600 loss: 0.0013 lr: 0.02\n",
      "iteration: 419610 loss: 0.0013 lr: 0.02\n",
      "iteration: 419620 loss: 0.0014 lr: 0.02\n",
      "iteration: 419630 loss: 0.0015 lr: 0.02\n",
      "iteration: 419640 loss: 0.0013 lr: 0.02\n",
      "iteration: 419650 loss: 0.0014 lr: 0.02\n",
      "iteration: 419660 loss: 0.0013 lr: 0.02\n",
      "iteration: 419670 loss: 0.0016 lr: 0.02\n",
      "iteration: 419680 loss: 0.0015 lr: 0.02\n",
      "iteration: 419690 loss: 0.0012 lr: 0.02\n",
      "iteration: 419700 loss: 0.0013 lr: 0.02\n",
      "iteration: 419710 loss: 0.0024 lr: 0.02\n",
      "iteration: 419720 loss: 0.0021 lr: 0.02\n",
      "iteration: 419730 loss: 0.0019 lr: 0.02\n",
      "iteration: 419740 loss: 0.0011 lr: 0.02\n",
      "iteration: 419750 loss: 0.0015 lr: 0.02\n",
      "iteration: 419760 loss: 0.0018 lr: 0.02\n",
      "iteration: 419770 loss: 0.0016 lr: 0.02\n",
      "iteration: 419780 loss: 0.0019 lr: 0.02\n",
      "iteration: 419790 loss: 0.0015 lr: 0.02\n",
      "iteration: 419800 loss: 0.0015 lr: 0.02\n",
      "iteration: 419810 loss: 0.0015 lr: 0.02\n",
      "iteration: 419820 loss: 0.0018 lr: 0.02\n",
      "iteration: 419830 loss: 0.0016 lr: 0.02\n",
      "iteration: 419840 loss: 0.0012 lr: 0.02\n",
      "iteration: 419850 loss: 0.0015 lr: 0.02\n",
      "iteration: 419860 loss: 0.0013 lr: 0.02\n",
      "iteration: 419870 loss: 0.0013 lr: 0.02\n",
      "iteration: 419880 loss: 0.0038 lr: 0.02\n",
      "iteration: 419890 loss: 0.0015 lr: 0.02\n",
      "iteration: 419900 loss: 0.0014 lr: 0.02\n",
      "iteration: 419910 loss: 0.0019 lr: 0.02\n",
      "iteration: 419920 loss: 0.0017 lr: 0.02\n",
      "iteration: 419930 loss: 0.0024 lr: 0.02\n",
      "iteration: 419940 loss: 0.0024 lr: 0.02\n",
      "iteration: 419950 loss: 0.0016 lr: 0.02\n",
      "iteration: 419960 loss: 0.0014 lr: 0.02\n",
      "iteration: 419970 loss: 0.0014 lr: 0.02\n",
      "iteration: 419980 loss: 0.0011 lr: 0.02\n",
      "iteration: 419990 loss: 0.0015 lr: 0.02\n",
      "iteration: 420000 loss: 0.0016 lr: 0.02\n",
      "iteration: 420010 loss: 0.0021 lr: 0.02\n",
      "iteration: 420020 loss: 0.0021 lr: 0.02\n",
      "iteration: 420030 loss: 0.0017 lr: 0.02\n",
      "iteration: 420040 loss: 0.0016 lr: 0.02\n",
      "iteration: 420050 loss: 0.0018 lr: 0.02\n",
      "iteration: 420060 loss: 0.0014 lr: 0.02\n",
      "iteration: 420070 loss: 0.0017 lr: 0.02\n",
      "iteration: 420080 loss: 0.0020 lr: 0.02\n",
      "iteration: 420090 loss: 0.0029 lr: 0.02\n",
      "iteration: 420100 loss: 0.0014 lr: 0.02\n",
      "iteration: 420110 loss: 0.0012 lr: 0.02\n",
      "iteration: 420120 loss: 0.0024 lr: 0.02\n",
      "iteration: 420130 loss: 0.0016 lr: 0.02\n",
      "iteration: 420140 loss: 0.0025 lr: 0.02\n",
      "iteration: 420150 loss: 0.0017 lr: 0.02\n",
      "iteration: 420160 loss: 0.0024 lr: 0.02\n",
      "iteration: 420170 loss: 0.0021 lr: 0.02\n",
      "iteration: 420180 loss: 0.0013 lr: 0.02\n",
      "iteration: 420190 loss: 0.0014 lr: 0.02\n",
      "iteration: 420200 loss: 0.0013 lr: 0.02\n",
      "iteration: 420210 loss: 0.0017 lr: 0.02\n",
      "iteration: 420220 loss: 0.0019 lr: 0.02\n",
      "iteration: 420230 loss: 0.0017 lr: 0.02\n",
      "iteration: 420240 loss: 0.0020 lr: 0.02\n",
      "iteration: 420250 loss: 0.0015 lr: 0.02\n",
      "iteration: 420260 loss: 0.0017 lr: 0.02\n",
      "iteration: 420270 loss: 0.0018 lr: 0.02\n",
      "iteration: 420280 loss: 0.0018 lr: 0.02\n",
      "iteration: 420290 loss: 0.0016 lr: 0.02\n",
      "iteration: 420300 loss: 0.0016 lr: 0.02\n",
      "iteration: 420310 loss: 0.0020 lr: 0.02\n",
      "iteration: 420320 loss: 0.0013 lr: 0.02\n",
      "iteration: 420330 loss: 0.0017 lr: 0.02\n",
      "iteration: 420340 loss: 0.0014 lr: 0.02\n",
      "iteration: 420350 loss: 0.0014 lr: 0.02\n",
      "iteration: 420360 loss: 0.0018 lr: 0.02\n",
      "iteration: 420370 loss: 0.0010 lr: 0.02\n",
      "iteration: 420380 loss: 0.0019 lr: 0.02\n",
      "iteration: 420390 loss: 0.0014 lr: 0.02\n",
      "iteration: 420400 loss: 0.0018 lr: 0.02\n",
      "iteration: 420410 loss: 0.0012 lr: 0.02\n",
      "iteration: 420420 loss: 0.0013 lr: 0.02\n",
      "iteration: 420430 loss: 0.0015 lr: 0.02\n",
      "iteration: 420440 loss: 0.0015 lr: 0.02\n",
      "iteration: 420450 loss: 0.0021 lr: 0.02\n",
      "iteration: 420460 loss: 0.0018 lr: 0.02\n",
      "iteration: 420470 loss: 0.0021 lr: 0.02\n",
      "iteration: 420480 loss: 0.0014 lr: 0.02\n",
      "iteration: 420490 loss: 0.0013 lr: 0.02\n",
      "iteration: 420500 loss: 0.0013 lr: 0.02\n",
      "iteration: 420510 loss: 0.0014 lr: 0.02\n",
      "iteration: 420520 loss: 0.0013 lr: 0.02\n",
      "iteration: 420530 loss: 0.0011 lr: 0.02\n",
      "iteration: 420540 loss: 0.0013 lr: 0.02\n",
      "iteration: 420550 loss: 0.0014 lr: 0.02\n",
      "iteration: 420560 loss: 0.0014 lr: 0.02\n",
      "iteration: 420570 loss: 0.0019 lr: 0.02\n",
      "iteration: 420580 loss: 0.0011 lr: 0.02\n",
      "iteration: 420590 loss: 0.0020 lr: 0.02\n",
      "iteration: 420600 loss: 0.0019 lr: 0.02\n",
      "iteration: 420610 loss: 0.0013 lr: 0.02\n",
      "iteration: 420620 loss: 0.0015 lr: 0.02\n",
      "iteration: 420630 loss: 0.0014 lr: 0.02\n",
      "iteration: 420640 loss: 0.0014 lr: 0.02\n",
      "iteration: 420650 loss: 0.0020 lr: 0.02\n",
      "iteration: 420660 loss: 0.0016 lr: 0.02\n",
      "iteration: 420670 loss: 0.0014 lr: 0.02\n",
      "iteration: 420680 loss: 0.0013 lr: 0.02\n",
      "iteration: 420690 loss: 0.0015 lr: 0.02\n",
      "iteration: 420700 loss: 0.0019 lr: 0.02\n",
      "iteration: 420710 loss: 0.0012 lr: 0.02\n",
      "iteration: 420720 loss: 0.0015 lr: 0.02\n",
      "iteration: 420730 loss: 0.0019 lr: 0.02\n",
      "iteration: 420740 loss: 0.0018 lr: 0.02\n",
      "iteration: 420750 loss: 0.0016 lr: 0.02\n",
      "iteration: 420760 loss: 0.0016 lr: 0.02\n",
      "iteration: 420770 loss: 0.0020 lr: 0.02\n",
      "iteration: 420780 loss: 0.0024 lr: 0.02\n",
      "iteration: 420790 loss: 0.0017 lr: 0.02\n",
      "iteration: 420800 loss: 0.0015 lr: 0.02\n",
      "iteration: 420810 loss: 0.0014 lr: 0.02\n",
      "iteration: 420820 loss: 0.0015 lr: 0.02\n",
      "iteration: 420830 loss: 0.0013 lr: 0.02\n",
      "iteration: 420840 loss: 0.0018 lr: 0.02\n",
      "iteration: 420850 loss: 0.0015 lr: 0.02\n",
      "iteration: 420860 loss: 0.0012 lr: 0.02\n",
      "iteration: 420870 loss: 0.0012 lr: 0.02\n",
      "iteration: 420880 loss: 0.0013 lr: 0.02\n",
      "iteration: 420890 loss: 0.0011 lr: 0.02\n",
      "iteration: 420900 loss: 0.0010 lr: 0.02\n",
      "iteration: 420910 loss: 0.0020 lr: 0.02\n",
      "iteration: 420920 loss: 0.0017 lr: 0.02\n",
      "iteration: 420930 loss: 0.0014 lr: 0.02\n",
      "iteration: 420940 loss: 0.0015 lr: 0.02\n",
      "iteration: 420950 loss: 0.0020 lr: 0.02\n",
      "iteration: 420960 loss: 0.0014 lr: 0.02\n",
      "iteration: 420970 loss: 0.0020 lr: 0.02\n",
      "iteration: 420980 loss: 0.0017 lr: 0.02\n",
      "iteration: 420990 loss: 0.0011 lr: 0.02\n",
      "iteration: 421000 loss: 0.0015 lr: 0.02\n",
      "iteration: 421010 loss: 0.0018 lr: 0.02\n",
      "iteration: 421020 loss: 0.0012 lr: 0.02\n",
      "iteration: 421030 loss: 0.0023 lr: 0.02\n",
      "iteration: 421040 loss: 0.0016 lr: 0.02\n",
      "iteration: 421050 loss: 0.0013 lr: 0.02\n",
      "iteration: 421060 loss: 0.0017 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 421070 loss: 0.0018 lr: 0.02\n",
      "iteration: 421080 loss: 0.0015 lr: 0.02\n",
      "iteration: 421090 loss: 0.0015 lr: 0.02\n",
      "iteration: 421100 loss: 0.0013 lr: 0.02\n",
      "iteration: 421110 loss: 0.0010 lr: 0.02\n",
      "iteration: 421120 loss: 0.0011 lr: 0.02\n",
      "iteration: 421130 loss: 0.0017 lr: 0.02\n",
      "iteration: 421140 loss: 0.0017 lr: 0.02\n",
      "iteration: 421150 loss: 0.0016 lr: 0.02\n",
      "iteration: 421160 loss: 0.0016 lr: 0.02\n",
      "iteration: 421170 loss: 0.0023 lr: 0.02\n",
      "iteration: 421180 loss: 0.0015 lr: 0.02\n",
      "iteration: 421190 loss: 0.0012 lr: 0.02\n",
      "iteration: 421200 loss: 0.0012 lr: 0.02\n",
      "iteration: 421210 loss: 0.0013 lr: 0.02\n",
      "iteration: 421220 loss: 0.0017 lr: 0.02\n",
      "iteration: 421230 loss: 0.0014 lr: 0.02\n",
      "iteration: 421240 loss: 0.0017 lr: 0.02\n",
      "iteration: 421250 loss: 0.0013 lr: 0.02\n",
      "iteration: 421260 loss: 0.0018 lr: 0.02\n",
      "iteration: 421270 loss: 0.0018 lr: 0.02\n",
      "iteration: 421280 loss: 0.0011 lr: 0.02\n",
      "iteration: 421290 loss: 0.0017 lr: 0.02\n",
      "iteration: 421300 loss: 0.0020 lr: 0.02\n",
      "iteration: 421310 loss: 0.0013 lr: 0.02\n",
      "iteration: 421320 loss: 0.0023 lr: 0.02\n",
      "iteration: 421330 loss: 0.0017 lr: 0.02\n",
      "iteration: 421340 loss: 0.0022 lr: 0.02\n",
      "iteration: 421350 loss: 0.0016 lr: 0.02\n",
      "iteration: 421360 loss: 0.0026 lr: 0.02\n",
      "iteration: 421370 loss: 0.0017 lr: 0.02\n",
      "iteration: 421380 loss: 0.0015 lr: 0.02\n",
      "iteration: 421390 loss: 0.0016 lr: 0.02\n",
      "iteration: 421400 loss: 0.0018 lr: 0.02\n",
      "iteration: 421410 loss: 0.0023 lr: 0.02\n",
      "iteration: 421420 loss: 0.0013 lr: 0.02\n",
      "iteration: 421430 loss: 0.0014 lr: 0.02\n",
      "iteration: 421440 loss: 0.0021 lr: 0.02\n",
      "iteration: 421450 loss: 0.0018 lr: 0.02\n",
      "iteration: 421460 loss: 0.0018 lr: 0.02\n",
      "iteration: 421470 loss: 0.0011 lr: 0.02\n",
      "iteration: 421480 loss: 0.0013 lr: 0.02\n",
      "iteration: 421490 loss: 0.0012 lr: 0.02\n",
      "iteration: 421500 loss: 0.0022 lr: 0.02\n",
      "iteration: 421510 loss: 0.0013 lr: 0.02\n",
      "iteration: 421520 loss: 0.0039 lr: 0.02\n",
      "iteration: 421530 loss: 0.0010 lr: 0.02\n",
      "iteration: 421540 loss: 0.0014 lr: 0.02\n",
      "iteration: 421550 loss: 0.0018 lr: 0.02\n",
      "iteration: 421560 loss: 0.0015 lr: 0.02\n",
      "iteration: 421570 loss: 0.0022 lr: 0.02\n",
      "iteration: 421580 loss: 0.0016 lr: 0.02\n",
      "iteration: 421590 loss: 0.0019 lr: 0.02\n",
      "iteration: 421600 loss: 0.0012 lr: 0.02\n",
      "iteration: 421610 loss: 0.0020 lr: 0.02\n",
      "iteration: 421620 loss: 0.0021 lr: 0.02\n",
      "iteration: 421630 loss: 0.0017 lr: 0.02\n",
      "iteration: 421640 loss: 0.0014 lr: 0.02\n",
      "iteration: 421650 loss: 0.0016 lr: 0.02\n",
      "iteration: 421660 loss: 0.0010 lr: 0.02\n",
      "iteration: 421670 loss: 0.0012 lr: 0.02\n",
      "iteration: 421680 loss: 0.0015 lr: 0.02\n",
      "iteration: 421690 loss: 0.0014 lr: 0.02\n",
      "iteration: 421700 loss: 0.0015 lr: 0.02\n",
      "iteration: 421710 loss: 0.0011 lr: 0.02\n",
      "iteration: 421720 loss: 0.0012 lr: 0.02\n",
      "iteration: 421730 loss: 0.0013 lr: 0.02\n",
      "iteration: 421740 loss: 0.0013 lr: 0.02\n",
      "iteration: 421750 loss: 0.0009 lr: 0.02\n",
      "iteration: 421760 loss: 0.0014 lr: 0.02\n",
      "iteration: 421770 loss: 0.0014 lr: 0.02\n",
      "iteration: 421780 loss: 0.0010 lr: 0.02\n",
      "iteration: 421790 loss: 0.0020 lr: 0.02\n",
      "iteration: 421800 loss: 0.0010 lr: 0.02\n",
      "iteration: 421810 loss: 0.0016 lr: 0.02\n",
      "iteration: 421820 loss: 0.0014 lr: 0.02\n",
      "iteration: 421830 loss: 0.0016 lr: 0.02\n",
      "iteration: 421840 loss: 0.0016 lr: 0.02\n",
      "iteration: 421850 loss: 0.0016 lr: 0.02\n",
      "iteration: 421860 loss: 0.0015 lr: 0.02\n",
      "iteration: 421870 loss: 0.0013 lr: 0.02\n",
      "iteration: 421880 loss: 0.0010 lr: 0.02\n",
      "iteration: 421890 loss: 0.0019 lr: 0.02\n",
      "iteration: 421900 loss: 0.0010 lr: 0.02\n",
      "iteration: 421910 loss: 0.0013 lr: 0.02\n",
      "iteration: 421920 loss: 0.0016 lr: 0.02\n",
      "iteration: 421930 loss: 0.0015 lr: 0.02\n",
      "iteration: 421940 loss: 0.0022 lr: 0.02\n",
      "iteration: 421950 loss: 0.0017 lr: 0.02\n",
      "iteration: 421960 loss: 0.0018 lr: 0.02\n",
      "iteration: 421970 loss: 0.0012 lr: 0.02\n",
      "iteration: 421980 loss: 0.0010 lr: 0.02\n",
      "iteration: 421990 loss: 0.0012 lr: 0.02\n",
      "iteration: 422000 loss: 0.0017 lr: 0.02\n",
      "iteration: 422010 loss: 0.0011 lr: 0.02\n",
      "iteration: 422020 loss: 0.0013 lr: 0.02\n",
      "iteration: 422030 loss: 0.0018 lr: 0.02\n",
      "iteration: 422040 loss: 0.0023 lr: 0.02\n",
      "iteration: 422050 loss: 0.0013 lr: 0.02\n",
      "iteration: 422060 loss: 0.0016 lr: 0.02\n",
      "iteration: 422070 loss: 0.0012 lr: 0.02\n",
      "iteration: 422080 loss: 0.0015 lr: 0.02\n",
      "iteration: 422090 loss: 0.0018 lr: 0.02\n",
      "iteration: 422100 loss: 0.0025 lr: 0.02\n",
      "iteration: 422110 loss: 0.0010 lr: 0.02\n",
      "iteration: 422120 loss: 0.0028 lr: 0.02\n",
      "iteration: 422130 loss: 0.0018 lr: 0.02\n",
      "iteration: 422140 loss: 0.0022 lr: 0.02\n",
      "iteration: 422150 loss: 0.0017 lr: 0.02\n",
      "iteration: 422160 loss: 0.0020 lr: 0.02\n",
      "iteration: 422170 loss: 0.0024 lr: 0.02\n",
      "iteration: 422180 loss: 0.0014 lr: 0.02\n",
      "iteration: 422190 loss: 0.0012 lr: 0.02\n",
      "iteration: 422200 loss: 0.0018 lr: 0.02\n",
      "iteration: 422210 loss: 0.0018 lr: 0.02\n",
      "iteration: 422220 loss: 0.0017 lr: 0.02\n",
      "iteration: 422230 loss: 0.0014 lr: 0.02\n",
      "iteration: 422240 loss: 0.0018 lr: 0.02\n",
      "iteration: 422250 loss: 0.0020 lr: 0.02\n",
      "iteration: 422260 loss: 0.0014 lr: 0.02\n",
      "iteration: 422270 loss: 0.0018 lr: 0.02\n",
      "iteration: 422280 loss: 0.0014 lr: 0.02\n",
      "iteration: 422290 loss: 0.0022 lr: 0.02\n",
      "iteration: 422300 loss: 0.0027 lr: 0.02\n",
      "iteration: 422310 loss: 0.0012 lr: 0.02\n",
      "iteration: 422320 loss: 0.0016 lr: 0.02\n",
      "iteration: 422330 loss: 0.0016 lr: 0.02\n",
      "iteration: 422340 loss: 0.0016 lr: 0.02\n",
      "iteration: 422350 loss: 0.0021 lr: 0.02\n",
      "iteration: 422360 loss: 0.0011 lr: 0.02\n",
      "iteration: 422370 loss: 0.0016 lr: 0.02\n",
      "iteration: 422380 loss: 0.0014 lr: 0.02\n",
      "iteration: 422390 loss: 0.0018 lr: 0.02\n",
      "iteration: 422400 loss: 0.0016 lr: 0.02\n",
      "iteration: 422410 loss: 0.0016 lr: 0.02\n",
      "iteration: 422420 loss: 0.0015 lr: 0.02\n",
      "iteration: 422430 loss: 0.0022 lr: 0.02\n",
      "iteration: 422440 loss: 0.0021 lr: 0.02\n",
      "iteration: 422450 loss: 0.0012 lr: 0.02\n",
      "iteration: 422460 loss: 0.0013 lr: 0.02\n",
      "iteration: 422470 loss: 0.0016 lr: 0.02\n",
      "iteration: 422480 loss: 0.0014 lr: 0.02\n",
      "iteration: 422490 loss: 0.0016 lr: 0.02\n",
      "iteration: 422500 loss: 0.0016 lr: 0.02\n",
      "iteration: 422510 loss: 0.0021 lr: 0.02\n",
      "iteration: 422520 loss: 0.0014 lr: 0.02\n",
      "iteration: 422530 loss: 0.0015 lr: 0.02\n",
      "iteration: 422540 loss: 0.0013 lr: 0.02\n",
      "iteration: 422550 loss: 0.0018 lr: 0.02\n",
      "iteration: 422560 loss: 0.0013 lr: 0.02\n",
      "iteration: 422570 loss: 0.0015 lr: 0.02\n",
      "iteration: 422580 loss: 0.0021 lr: 0.02\n",
      "iteration: 422590 loss: 0.0010 lr: 0.02\n",
      "iteration: 422600 loss: 0.0016 lr: 0.02\n",
      "iteration: 422610 loss: 0.0013 lr: 0.02\n",
      "iteration: 422620 loss: 0.0018 lr: 0.02\n",
      "iteration: 422630 loss: 0.0012 lr: 0.02\n",
      "iteration: 422640 loss: 0.0012 lr: 0.02\n",
      "iteration: 422650 loss: 0.0040 lr: 0.02\n",
      "iteration: 422660 loss: 0.0023 lr: 0.02\n",
      "iteration: 422670 loss: 0.0012 lr: 0.02\n",
      "iteration: 422680 loss: 0.0010 lr: 0.02\n",
      "iteration: 422690 loss: 0.0013 lr: 0.02\n",
      "iteration: 422700 loss: 0.0016 lr: 0.02\n",
      "iteration: 422710 loss: 0.0017 lr: 0.02\n",
      "iteration: 422720 loss: 0.0015 lr: 0.02\n",
      "iteration: 422730 loss: 0.0013 lr: 0.02\n",
      "iteration: 422740 loss: 0.0012 lr: 0.02\n",
      "iteration: 422750 loss: 0.0011 lr: 0.02\n",
      "iteration: 422760 loss: 0.0016 lr: 0.02\n",
      "iteration: 422770 loss: 0.0016 lr: 0.02\n",
      "iteration: 422780 loss: 0.0019 lr: 0.02\n",
      "iteration: 422790 loss: 0.0014 lr: 0.02\n",
      "iteration: 422800 loss: 0.0030 lr: 0.02\n",
      "iteration: 422810 loss: 0.0022 lr: 0.02\n",
      "iteration: 422820 loss: 0.0017 lr: 0.02\n",
      "iteration: 422830 loss: 0.0015 lr: 0.02\n",
      "iteration: 422840 loss: 0.0017 lr: 0.02\n",
      "iteration: 422850 loss: 0.0015 lr: 0.02\n",
      "iteration: 422860 loss: 0.0011 lr: 0.02\n",
      "iteration: 422870 loss: 0.0014 lr: 0.02\n",
      "iteration: 422880 loss: 0.0009 lr: 0.02\n",
      "iteration: 422890 loss: 0.0012 lr: 0.02\n",
      "iteration: 422900 loss: 0.0013 lr: 0.02\n",
      "iteration: 422910 loss: 0.0023 lr: 0.02\n",
      "iteration: 422920 loss: 0.0014 lr: 0.02\n",
      "iteration: 422930 loss: 0.0026 lr: 0.02\n",
      "iteration: 422940 loss: 0.0020 lr: 0.02\n",
      "iteration: 422950 loss: 0.0015 lr: 0.02\n",
      "iteration: 422960 loss: 0.0019 lr: 0.02\n",
      "iteration: 422970 loss: 0.0016 lr: 0.02\n",
      "iteration: 422980 loss: 0.0014 lr: 0.02\n",
      "iteration: 422990 loss: 0.0016 lr: 0.02\n",
      "iteration: 423000 loss: 0.0019 lr: 0.02\n",
      "iteration: 423010 loss: 0.0013 lr: 0.02\n",
      "iteration: 423020 loss: 0.0024 lr: 0.02\n",
      "iteration: 423030 loss: 0.0017 lr: 0.02\n",
      "iteration: 423040 loss: 0.0014 lr: 0.02\n",
      "iteration: 423050 loss: 0.0017 lr: 0.02\n",
      "iteration: 423060 loss: 0.0025 lr: 0.02\n",
      "iteration: 423070 loss: 0.0012 lr: 0.02\n",
      "iteration: 423080 loss: 0.0016 lr: 0.02\n",
      "iteration: 423090 loss: 0.0020 lr: 0.02\n",
      "iteration: 423100 loss: 0.0015 lr: 0.02\n",
      "iteration: 423110 loss: 0.0012 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 423120 loss: 0.0023 lr: 0.02\n",
      "iteration: 423130 loss: 0.0012 lr: 0.02\n",
      "iteration: 423140 loss: 0.0013 lr: 0.02\n",
      "iteration: 423150 loss: 0.0015 lr: 0.02\n",
      "iteration: 423160 loss: 0.0015 lr: 0.02\n",
      "iteration: 423170 loss: 0.0015 lr: 0.02\n",
      "iteration: 423180 loss: 0.0013 lr: 0.02\n",
      "iteration: 423190 loss: 0.0012 lr: 0.02\n",
      "iteration: 423200 loss: 0.0017 lr: 0.02\n",
      "iteration: 423210 loss: 0.0014 lr: 0.02\n",
      "iteration: 423220 loss: 0.0012 lr: 0.02\n",
      "iteration: 423230 loss: 0.0019 lr: 0.02\n",
      "iteration: 423240 loss: 0.0014 lr: 0.02\n",
      "iteration: 423250 loss: 0.0015 lr: 0.02\n",
      "iteration: 423260 loss: 0.0014 lr: 0.02\n",
      "iteration: 423270 loss: 0.0017 lr: 0.02\n",
      "iteration: 423280 loss: 0.0016 lr: 0.02\n",
      "iteration: 423290 loss: 0.0021 lr: 0.02\n",
      "iteration: 423300 loss: 0.0017 lr: 0.02\n",
      "iteration: 423310 loss: 0.0015 lr: 0.02\n",
      "iteration: 423320 loss: 0.0016 lr: 0.02\n",
      "iteration: 423330 loss: 0.0017 lr: 0.02\n",
      "iteration: 423340 loss: 0.0015 lr: 0.02\n",
      "iteration: 423350 loss: 0.0026 lr: 0.02\n",
      "iteration: 423360 loss: 0.0013 lr: 0.02\n",
      "iteration: 423370 loss: 0.0013 lr: 0.02\n",
      "iteration: 423380 loss: 0.0015 lr: 0.02\n",
      "iteration: 423390 loss: 0.0015 lr: 0.02\n",
      "iteration: 423400 loss: 0.0026 lr: 0.02\n",
      "iteration: 423410 loss: 0.0021 lr: 0.02\n",
      "iteration: 423420 loss: 0.0019 lr: 0.02\n",
      "iteration: 423430 loss: 0.0015 lr: 0.02\n",
      "iteration: 423440 loss: 0.0017 lr: 0.02\n",
      "iteration: 423450 loss: 0.0015 lr: 0.02\n",
      "iteration: 423460 loss: 0.0011 lr: 0.02\n",
      "iteration: 423470 loss: 0.0020 lr: 0.02\n",
      "iteration: 423480 loss: 0.0014 lr: 0.02\n",
      "iteration: 423490 loss: 0.0013 lr: 0.02\n",
      "iteration: 423500 loss: 0.0015 lr: 0.02\n",
      "iteration: 423510 loss: 0.0013 lr: 0.02\n",
      "iteration: 423520 loss: 0.0011 lr: 0.02\n",
      "iteration: 423530 loss: 0.0012 lr: 0.02\n",
      "iteration: 423540 loss: 0.0013 lr: 0.02\n",
      "iteration: 423550 loss: 0.0014 lr: 0.02\n",
      "iteration: 423560 loss: 0.0014 lr: 0.02\n",
      "iteration: 423570 loss: 0.0020 lr: 0.02\n",
      "iteration: 423580 loss: 0.0015 lr: 0.02\n",
      "iteration: 423590 loss: 0.0010 lr: 0.02\n",
      "iteration: 423600 loss: 0.0017 lr: 0.02\n",
      "iteration: 423610 loss: 0.0015 lr: 0.02\n",
      "iteration: 423620 loss: 0.0017 lr: 0.02\n",
      "iteration: 423630 loss: 0.0020 lr: 0.02\n",
      "iteration: 423640 loss: 0.0014 lr: 0.02\n",
      "iteration: 423650 loss: 0.0020 lr: 0.02\n",
      "iteration: 423660 loss: 0.0015 lr: 0.02\n",
      "iteration: 423670 loss: 0.0014 lr: 0.02\n",
      "iteration: 423680 loss: 0.0014 lr: 0.02\n",
      "iteration: 423690 loss: 0.0016 lr: 0.02\n",
      "iteration: 423700 loss: 0.0016 lr: 0.02\n",
      "iteration: 423710 loss: 0.0011 lr: 0.02\n",
      "iteration: 423720 loss: 0.0012 lr: 0.02\n",
      "iteration: 423730 loss: 0.0014 lr: 0.02\n",
      "iteration: 423740 loss: 0.0014 lr: 0.02\n",
      "iteration: 423750 loss: 0.0014 lr: 0.02\n",
      "iteration: 423760 loss: 0.0019 lr: 0.02\n",
      "iteration: 423770 loss: 0.0012 lr: 0.02\n",
      "iteration: 423780 loss: 0.0019 lr: 0.02\n",
      "iteration: 423790 loss: 0.0015 lr: 0.02\n",
      "iteration: 423800 loss: 0.0017 lr: 0.02\n",
      "iteration: 423810 loss: 0.0012 lr: 0.02\n",
      "iteration: 423820 loss: 0.0014 lr: 0.02\n",
      "iteration: 423830 loss: 0.0015 lr: 0.02\n",
      "iteration: 423840 loss: 0.0021 lr: 0.02\n",
      "iteration: 423850 loss: 0.0018 lr: 0.02\n",
      "iteration: 423860 loss: 0.0017 lr: 0.02\n",
      "iteration: 423870 loss: 0.0013 lr: 0.02\n",
      "iteration: 423880 loss: 0.0013 lr: 0.02\n",
      "iteration: 423890 loss: 0.0008 lr: 0.02\n",
      "iteration: 423900 loss: 0.0011 lr: 0.02\n",
      "iteration: 423910 loss: 0.0010 lr: 0.02\n",
      "iteration: 423920 loss: 0.0012 lr: 0.02\n",
      "iteration: 423930 loss: 0.0016 lr: 0.02\n",
      "iteration: 423940 loss: 0.0017 lr: 0.02\n",
      "iteration: 423950 loss: 0.0016 lr: 0.02\n",
      "iteration: 423960 loss: 0.0012 lr: 0.02\n",
      "iteration: 423970 loss: 0.0019 lr: 0.02\n",
      "iteration: 423980 loss: 0.0018 lr: 0.02\n",
      "iteration: 423990 loss: 0.0016 lr: 0.02\n",
      "iteration: 424000 loss: 0.0012 lr: 0.02\n",
      "iteration: 424010 loss: 0.0014 lr: 0.02\n",
      "iteration: 424020 loss: 0.0015 lr: 0.02\n",
      "iteration: 424030 loss: 0.0014 lr: 0.02\n",
      "iteration: 424040 loss: 0.0010 lr: 0.02\n",
      "iteration: 424050 loss: 0.0020 lr: 0.02\n",
      "iteration: 424060 loss: 0.0021 lr: 0.02\n",
      "iteration: 424070 loss: 0.0020 lr: 0.02\n",
      "iteration: 424080 loss: 0.0020 lr: 0.02\n",
      "iteration: 424090 loss: 0.0010 lr: 0.02\n",
      "iteration: 424100 loss: 0.0014 lr: 0.02\n",
      "iteration: 424110 loss: 0.0012 lr: 0.02\n",
      "iteration: 424120 loss: 0.0017 lr: 0.02\n",
      "iteration: 424130 loss: 0.0015 lr: 0.02\n",
      "iteration: 424140 loss: 0.0019 lr: 0.02\n",
      "iteration: 424150 loss: 0.0019 lr: 0.02\n",
      "iteration: 424160 loss: 0.0018 lr: 0.02\n",
      "iteration: 424170 loss: 0.0016 lr: 0.02\n",
      "iteration: 424180 loss: 0.0016 lr: 0.02\n",
      "iteration: 424190 loss: 0.0011 lr: 0.02\n",
      "iteration: 424200 loss: 0.0012 lr: 0.02\n",
      "iteration: 424210 loss: 0.0012 lr: 0.02\n",
      "iteration: 424220 loss: 0.0015 lr: 0.02\n",
      "iteration: 424230 loss: 0.0016 lr: 0.02\n",
      "iteration: 424240 loss: 0.0016 lr: 0.02\n",
      "iteration: 424250 loss: 0.0015 lr: 0.02\n",
      "iteration: 424260 loss: 0.0016 lr: 0.02\n",
      "iteration: 424270 loss: 0.0014 lr: 0.02\n",
      "iteration: 424280 loss: 0.0019 lr: 0.02\n",
      "iteration: 424290 loss: 0.0017 lr: 0.02\n",
      "iteration: 424300 loss: 0.0011 lr: 0.02\n",
      "iteration: 424310 loss: 0.0015 lr: 0.02\n",
      "iteration: 424320 loss: 0.0013 lr: 0.02\n",
      "iteration: 424330 loss: 0.0011 lr: 0.02\n",
      "iteration: 424340 loss: 0.0015 lr: 0.02\n",
      "iteration: 424350 loss: 0.0015 lr: 0.02\n",
      "iteration: 424360 loss: 0.0018 lr: 0.02\n",
      "iteration: 424370 loss: 0.0013 lr: 0.02\n",
      "iteration: 424380 loss: 0.0016 lr: 0.02\n",
      "iteration: 424390 loss: 0.0012 lr: 0.02\n",
      "iteration: 424400 loss: 0.0022 lr: 0.02\n",
      "iteration: 424410 loss: 0.0028 lr: 0.02\n",
      "iteration: 424420 loss: 0.0014 lr: 0.02\n",
      "iteration: 424430 loss: 0.0013 lr: 0.02\n",
      "iteration: 424440 loss: 0.0021 lr: 0.02\n",
      "iteration: 424450 loss: 0.0020 lr: 0.02\n",
      "iteration: 424460 loss: 0.0013 lr: 0.02\n",
      "iteration: 424470 loss: 0.0010 lr: 0.02\n",
      "iteration: 424480 loss: 0.0011 lr: 0.02\n",
      "iteration: 424490 loss: 0.0015 lr: 0.02\n",
      "iteration: 424500 loss: 0.0010 lr: 0.02\n",
      "iteration: 424510 loss: 0.0018 lr: 0.02\n",
      "iteration: 424520 loss: 0.0010 lr: 0.02\n",
      "iteration: 424530 loss: 0.0014 lr: 0.02\n",
      "iteration: 424540 loss: 0.0014 lr: 0.02\n",
      "iteration: 424550 loss: 0.0015 lr: 0.02\n",
      "iteration: 424560 loss: 0.0019 lr: 0.02\n",
      "iteration: 424570 loss: 0.0015 lr: 0.02\n",
      "iteration: 424580 loss: 0.0014 lr: 0.02\n",
      "iteration: 424590 loss: 0.0023 lr: 0.02\n",
      "iteration: 424600 loss: 0.0022 lr: 0.02\n",
      "iteration: 424610 loss: 0.0016 lr: 0.02\n",
      "iteration: 424620 loss: 0.0021 lr: 0.02\n",
      "iteration: 424630 loss: 0.0014 lr: 0.02\n",
      "iteration: 424640 loss: 0.0019 lr: 0.02\n",
      "iteration: 424650 loss: 0.0013 lr: 0.02\n",
      "iteration: 424660 loss: 0.0014 lr: 0.02\n",
      "iteration: 424670 loss: 0.0015 lr: 0.02\n",
      "iteration: 424680 loss: 0.0019 lr: 0.02\n",
      "iteration: 424690 loss: 0.0019 lr: 0.02\n",
      "iteration: 424700 loss: 0.0015 lr: 0.02\n",
      "iteration: 424710 loss: 0.0013 lr: 0.02\n",
      "iteration: 424720 loss: 0.0026 lr: 0.02\n",
      "iteration: 424730 loss: 0.0013 lr: 0.02\n",
      "iteration: 424740 loss: 0.0012 lr: 0.02\n",
      "iteration: 424750 loss: 0.0015 lr: 0.02\n",
      "iteration: 424760 loss: 0.0012 lr: 0.02\n",
      "iteration: 424770 loss: 0.0012 lr: 0.02\n",
      "iteration: 424780 loss: 0.0014 lr: 0.02\n",
      "iteration: 424790 loss: 0.0016 lr: 0.02\n",
      "iteration: 424800 loss: 0.0011 lr: 0.02\n",
      "iteration: 424810 loss: 0.0012 lr: 0.02\n",
      "iteration: 424820 loss: 0.0013 lr: 0.02\n",
      "iteration: 424830 loss: 0.0016 lr: 0.02\n",
      "iteration: 424840 loss: 0.0017 lr: 0.02\n",
      "iteration: 424850 loss: 0.0012 lr: 0.02\n",
      "iteration: 424860 loss: 0.0022 lr: 0.02\n",
      "iteration: 424870 loss: 0.0013 lr: 0.02\n",
      "iteration: 424880 loss: 0.0010 lr: 0.02\n",
      "iteration: 424890 loss: 0.0012 lr: 0.02\n",
      "iteration: 424900 loss: 0.0016 lr: 0.02\n",
      "iteration: 424910 loss: 0.0010 lr: 0.02\n",
      "iteration: 424920 loss: 0.0015 lr: 0.02\n",
      "iteration: 424930 loss: 0.0016 lr: 0.02\n",
      "iteration: 424940 loss: 0.0015 lr: 0.02\n",
      "iteration: 424950 loss: 0.0013 lr: 0.02\n",
      "iteration: 424960 loss: 0.0013 lr: 0.02\n",
      "iteration: 424970 loss: 0.0010 lr: 0.02\n",
      "iteration: 424980 loss: 0.0022 lr: 0.02\n",
      "iteration: 424990 loss: 0.0014 lr: 0.02\n",
      "iteration: 425000 loss: 0.0020 lr: 0.02\n",
      "iteration: 425010 loss: 0.0019 lr: 0.02\n",
      "iteration: 425020 loss: 0.0013 lr: 0.02\n",
      "iteration: 425030 loss: 0.0013 lr: 0.02\n",
      "iteration: 425040 loss: 0.0024 lr: 0.02\n",
      "iteration: 425050 loss: 0.0011 lr: 0.02\n",
      "iteration: 425060 loss: 0.0012 lr: 0.02\n",
      "iteration: 425070 loss: 0.0009 lr: 0.02\n",
      "iteration: 425080 loss: 0.0013 lr: 0.02\n",
      "iteration: 425090 loss: 0.0013 lr: 0.02\n",
      "iteration: 425100 loss: 0.0011 lr: 0.02\n",
      "iteration: 425110 loss: 0.0020 lr: 0.02\n",
      "iteration: 425120 loss: 0.0016 lr: 0.02\n",
      "iteration: 425130 loss: 0.0012 lr: 0.02\n",
      "iteration: 425140 loss: 0.0013 lr: 0.02\n",
      "iteration: 425150 loss: 0.0011 lr: 0.02\n",
      "iteration: 425160 loss: 0.0026 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 425170 loss: 0.0015 lr: 0.02\n",
      "iteration: 425180 loss: 0.0014 lr: 0.02\n",
      "iteration: 425190 loss: 0.0016 lr: 0.02\n",
      "iteration: 425200 loss: 0.0019 lr: 0.02\n",
      "iteration: 425210 loss: 0.0015 lr: 0.02\n",
      "iteration: 425220 loss: 0.0016 lr: 0.02\n",
      "iteration: 425230 loss: 0.0013 lr: 0.02\n",
      "iteration: 425240 loss: 0.0013 lr: 0.02\n",
      "iteration: 425250 loss: 0.0013 lr: 0.02\n",
      "iteration: 425260 loss: 0.0020 lr: 0.02\n",
      "iteration: 425270 loss: 0.0030 lr: 0.02\n",
      "iteration: 425280 loss: 0.0014 lr: 0.02\n",
      "iteration: 425290 loss: 0.0019 lr: 0.02\n",
      "iteration: 425300 loss: 0.0021 lr: 0.02\n",
      "iteration: 425310 loss: 0.0022 lr: 0.02\n",
      "iteration: 425320 loss: 0.0021 lr: 0.02\n",
      "iteration: 425330 loss: 0.0013 lr: 0.02\n",
      "iteration: 425340 loss: 0.0015 lr: 0.02\n",
      "iteration: 425350 loss: 0.0021 lr: 0.02\n",
      "iteration: 425360 loss: 0.0013 lr: 0.02\n",
      "iteration: 425370 loss: 0.0016 lr: 0.02\n",
      "iteration: 425380 loss: 0.0015 lr: 0.02\n",
      "iteration: 425390 loss: 0.0011 lr: 0.02\n",
      "iteration: 425400 loss: 0.0014 lr: 0.02\n",
      "iteration: 425410 loss: 0.0014 lr: 0.02\n",
      "iteration: 425420 loss: 0.0012 lr: 0.02\n",
      "iteration: 425430 loss: 0.0013 lr: 0.02\n",
      "iteration: 425440 loss: 0.0017 lr: 0.02\n",
      "iteration: 425450 loss: 0.0018 lr: 0.02\n",
      "iteration: 425460 loss: 0.0012 lr: 0.02\n",
      "iteration: 425470 loss: 0.0018 lr: 0.02\n",
      "iteration: 425480 loss: 0.0022 lr: 0.02\n",
      "iteration: 425490 loss: 0.0020 lr: 0.02\n",
      "iteration: 425500 loss: 0.0016 lr: 0.02\n",
      "iteration: 425510 loss: 0.0014 lr: 0.02\n",
      "iteration: 425520 loss: 0.0014 lr: 0.02\n",
      "iteration: 425530 loss: 0.0014 lr: 0.02\n",
      "iteration: 425540 loss: 0.0015 lr: 0.02\n",
      "iteration: 425550 loss: 0.0011 lr: 0.02\n",
      "iteration: 425560 loss: 0.0012 lr: 0.02\n",
      "iteration: 425570 loss: 0.0013 lr: 0.02\n",
      "iteration: 425580 loss: 0.0017 lr: 0.02\n",
      "iteration: 425590 loss: 0.0015 lr: 0.02\n",
      "iteration: 425600 loss: 0.0011 lr: 0.02\n",
      "iteration: 425610 loss: 0.0011 lr: 0.02\n",
      "iteration: 425620 loss: 0.0018 lr: 0.02\n",
      "iteration: 425630 loss: 0.0017 lr: 0.02\n",
      "iteration: 425640 loss: 0.0012 lr: 0.02\n",
      "iteration: 425650 loss: 0.0015 lr: 0.02\n",
      "iteration: 425660 loss: 0.0013 lr: 0.02\n",
      "iteration: 425670 loss: 0.0010 lr: 0.02\n",
      "iteration: 425680 loss: 0.0011 lr: 0.02\n",
      "iteration: 425690 loss: 0.0016 lr: 0.02\n",
      "iteration: 425700 loss: 0.0014 lr: 0.02\n",
      "iteration: 425710 loss: 0.0012 lr: 0.02\n",
      "iteration: 425720 loss: 0.0021 lr: 0.02\n",
      "iteration: 425730 loss: 0.0021 lr: 0.02\n",
      "iteration: 425740 loss: 0.0020 lr: 0.02\n",
      "iteration: 425750 loss: 0.0022 lr: 0.02\n",
      "iteration: 425760 loss: 0.0015 lr: 0.02\n",
      "iteration: 425770 loss: 0.0014 lr: 0.02\n",
      "iteration: 425780 loss: 0.0015 lr: 0.02\n",
      "iteration: 425790 loss: 0.0018 lr: 0.02\n",
      "iteration: 425800 loss: 0.0019 lr: 0.02\n",
      "iteration: 425810 loss: 0.0012 lr: 0.02\n",
      "iteration: 425820 loss: 0.0014 lr: 0.02\n",
      "iteration: 425830 loss: 0.0014 lr: 0.02\n",
      "iteration: 425840 loss: 0.0013 lr: 0.02\n",
      "iteration: 425850 loss: 0.0018 lr: 0.02\n",
      "iteration: 425860 loss: 0.0018 lr: 0.02\n",
      "iteration: 425870 loss: 0.0016 lr: 0.02\n",
      "iteration: 425880 loss: 0.0015 lr: 0.02\n",
      "iteration: 425890 loss: 0.0016 lr: 0.02\n",
      "iteration: 425900 loss: 0.0014 lr: 0.02\n",
      "iteration: 425910 loss: 0.0023 lr: 0.02\n",
      "iteration: 425920 loss: 0.0017 lr: 0.02\n",
      "iteration: 425930 loss: 0.0013 lr: 0.02\n",
      "iteration: 425940 loss: 0.0012 lr: 0.02\n",
      "iteration: 425950 loss: 0.0014 lr: 0.02\n",
      "iteration: 425960 loss: 0.0015 lr: 0.02\n",
      "iteration: 425970 loss: 0.0014 lr: 0.02\n",
      "iteration: 425980 loss: 0.0014 lr: 0.02\n",
      "iteration: 425990 loss: 0.0011 lr: 0.02\n",
      "iteration: 426000 loss: 0.0016 lr: 0.02\n",
      "iteration: 426010 loss: 0.0017 lr: 0.02\n",
      "iteration: 426020 loss: 0.0018 lr: 0.02\n",
      "iteration: 426030 loss: 0.0017 lr: 0.02\n",
      "iteration: 426040 loss: 0.0012 lr: 0.02\n",
      "iteration: 426050 loss: 0.0014 lr: 0.02\n",
      "iteration: 426060 loss: 0.0011 lr: 0.02\n",
      "iteration: 426070 loss: 0.0018 lr: 0.02\n",
      "iteration: 426080 loss: 0.0015 lr: 0.02\n",
      "iteration: 426090 loss: 0.0016 lr: 0.02\n",
      "iteration: 426100 loss: 0.0020 lr: 0.02\n",
      "iteration: 426110 loss: 0.0019 lr: 0.02\n",
      "iteration: 426120 loss: 0.0018 lr: 0.02\n",
      "iteration: 426130 loss: 0.0019 lr: 0.02\n",
      "iteration: 426140 loss: 0.0013 lr: 0.02\n",
      "iteration: 426150 loss: 0.0024 lr: 0.02\n",
      "iteration: 426160 loss: 0.0016 lr: 0.02\n",
      "iteration: 426170 loss: 0.0027 lr: 0.02\n",
      "iteration: 426180 loss: 0.0023 lr: 0.02\n",
      "iteration: 426190 loss: 0.0018 lr: 0.02\n",
      "iteration: 426200 loss: 0.0014 lr: 0.02\n",
      "iteration: 426210 loss: 0.0019 lr: 0.02\n",
      "iteration: 426220 loss: 0.0015 lr: 0.02\n",
      "iteration: 426230 loss: 0.0014 lr: 0.02\n",
      "iteration: 426240 loss: 0.0014 lr: 0.02\n",
      "iteration: 426250 loss: 0.0014 lr: 0.02\n",
      "iteration: 426260 loss: 0.0019 lr: 0.02\n",
      "iteration: 426270 loss: 0.0016 lr: 0.02\n",
      "iteration: 426280 loss: 0.0012 lr: 0.02\n",
      "iteration: 426290 loss: 0.0015 lr: 0.02\n",
      "iteration: 426300 loss: 0.0010 lr: 0.02\n",
      "iteration: 426310 loss: 0.0022 lr: 0.02\n",
      "iteration: 426320 loss: 0.0014 lr: 0.02\n",
      "iteration: 426330 loss: 0.0017 lr: 0.02\n",
      "iteration: 426340 loss: 0.0013 lr: 0.02\n",
      "iteration: 426350 loss: 0.0011 lr: 0.02\n",
      "iteration: 426360 loss: 0.0010 lr: 0.02\n",
      "iteration: 426370 loss: 0.0014 lr: 0.02\n",
      "iteration: 426380 loss: 0.0011 lr: 0.02\n",
      "iteration: 426390 loss: 0.0022 lr: 0.02\n",
      "iteration: 426400 loss: 0.0012 lr: 0.02\n",
      "iteration: 426410 loss: 0.0016 lr: 0.02\n",
      "iteration: 426420 loss: 0.0015 lr: 0.02\n",
      "iteration: 426430 loss: 0.0017 lr: 0.02\n",
      "iteration: 426440 loss: 0.0016 lr: 0.02\n",
      "iteration: 426450 loss: 0.0018 lr: 0.02\n",
      "iteration: 426460 loss: 0.0015 lr: 0.02\n",
      "iteration: 426470 loss: 0.0011 lr: 0.02\n",
      "iteration: 426480 loss: 0.0017 lr: 0.02\n",
      "iteration: 426490 loss: 0.0011 lr: 0.02\n",
      "iteration: 426500 loss: 0.0016 lr: 0.02\n",
      "iteration: 426510 loss: 0.0019 lr: 0.02\n",
      "iteration: 426520 loss: 0.0021 lr: 0.02\n",
      "iteration: 426530 loss: 0.0013 lr: 0.02\n",
      "iteration: 426540 loss: 0.0020 lr: 0.02\n",
      "iteration: 426550 loss: 0.0016 lr: 0.02\n",
      "iteration: 426560 loss: 0.0014 lr: 0.02\n",
      "iteration: 426570 loss: 0.0014 lr: 0.02\n",
      "iteration: 426580 loss: 0.0009 lr: 0.02\n",
      "iteration: 426590 loss: 0.0014 lr: 0.02\n",
      "iteration: 426600 loss: 0.0016 lr: 0.02\n",
      "iteration: 426610 loss: 0.0017 lr: 0.02\n",
      "iteration: 426620 loss: 0.0014 lr: 0.02\n",
      "iteration: 426630 loss: 0.0012 lr: 0.02\n",
      "iteration: 426640 loss: 0.0015 lr: 0.02\n",
      "iteration: 426650 loss: 0.0016 lr: 0.02\n",
      "iteration: 426660 loss: 0.0019 lr: 0.02\n",
      "iteration: 426670 loss: 0.0015 lr: 0.02\n",
      "iteration: 426680 loss: 0.0016 lr: 0.02\n",
      "iteration: 426690 loss: 0.0014 lr: 0.02\n",
      "iteration: 426700 loss: 0.0016 lr: 0.02\n",
      "iteration: 426710 loss: 0.0012 lr: 0.02\n",
      "iteration: 426720 loss: 0.0017 lr: 0.02\n",
      "iteration: 426730 loss: 0.0025 lr: 0.02\n",
      "iteration: 426740 loss: 0.0021 lr: 0.02\n",
      "iteration: 426750 loss: 0.0018 lr: 0.02\n",
      "iteration: 426760 loss: 0.0013 lr: 0.02\n",
      "iteration: 426770 loss: 0.0022 lr: 0.02\n",
      "iteration: 426780 loss: 0.0013 lr: 0.02\n",
      "iteration: 426790 loss: 0.0013 lr: 0.02\n",
      "iteration: 426800 loss: 0.0019 lr: 0.02\n",
      "iteration: 426810 loss: 0.0018 lr: 0.02\n",
      "iteration: 426820 loss: 0.0016 lr: 0.02\n",
      "iteration: 426830 loss: 0.0017 lr: 0.02\n",
      "iteration: 426840 loss: 0.0017 lr: 0.02\n",
      "iteration: 426850 loss: 0.0011 lr: 0.02\n",
      "iteration: 426860 loss: 0.0014 lr: 0.02\n",
      "iteration: 426870 loss: 0.0010 lr: 0.02\n",
      "iteration: 426880 loss: 0.0021 lr: 0.02\n",
      "iteration: 426890 loss: 0.0017 lr: 0.02\n",
      "iteration: 426900 loss: 0.0015 lr: 0.02\n",
      "iteration: 426910 loss: 0.0013 lr: 0.02\n",
      "iteration: 426920 loss: 0.0012 lr: 0.02\n",
      "iteration: 426930 loss: 0.0019 lr: 0.02\n",
      "iteration: 426940 loss: 0.0013 lr: 0.02\n",
      "iteration: 426950 loss: 0.0019 lr: 0.02\n",
      "iteration: 426960 loss: 0.0019 lr: 0.02\n",
      "iteration: 426970 loss: 0.0012 lr: 0.02\n",
      "iteration: 426980 loss: 0.0011 lr: 0.02\n",
      "iteration: 426990 loss: 0.0023 lr: 0.02\n",
      "iteration: 427000 loss: 0.0014 lr: 0.02\n",
      "iteration: 427010 loss: 0.0019 lr: 0.02\n",
      "iteration: 427020 loss: 0.0017 lr: 0.02\n",
      "iteration: 427030 loss: 0.0018 lr: 0.02\n",
      "iteration: 427040 loss: 0.0015 lr: 0.02\n",
      "iteration: 427050 loss: 0.0014 lr: 0.02\n",
      "iteration: 427060 loss: 0.0026 lr: 0.02\n",
      "iteration: 427070 loss: 0.0015 lr: 0.02\n",
      "iteration: 427080 loss: 0.0016 lr: 0.02\n",
      "iteration: 427090 loss: 0.0019 lr: 0.02\n",
      "iteration: 427100 loss: 0.0019 lr: 0.02\n",
      "iteration: 427110 loss: 0.0013 lr: 0.02\n",
      "iteration: 427120 loss: 0.0016 lr: 0.02\n",
      "iteration: 427130 loss: 0.0014 lr: 0.02\n",
      "iteration: 427140 loss: 0.0017 lr: 0.02\n",
      "iteration: 427150 loss: 0.0013 lr: 0.02\n",
      "iteration: 427160 loss: 0.0016 lr: 0.02\n",
      "iteration: 427170 loss: 0.0014 lr: 0.02\n",
      "iteration: 427180 loss: 0.0022 lr: 0.02\n",
      "iteration: 427190 loss: 0.0011 lr: 0.02\n",
      "iteration: 427200 loss: 0.0014 lr: 0.02\n",
      "iteration: 427210 loss: 0.0019 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 427220 loss: 0.0013 lr: 0.02\n",
      "iteration: 427230 loss: 0.0011 lr: 0.02\n",
      "iteration: 427240 loss: 0.0016 lr: 0.02\n",
      "iteration: 427250 loss: 0.0020 lr: 0.02\n",
      "iteration: 427260 loss: 0.0013 lr: 0.02\n",
      "iteration: 427270 loss: 0.0016 lr: 0.02\n",
      "iteration: 427280 loss: 0.0015 lr: 0.02\n",
      "iteration: 427290 loss: 0.0023 lr: 0.02\n",
      "iteration: 427300 loss: 0.0015 lr: 0.02\n",
      "iteration: 427310 loss: 0.0013 lr: 0.02\n",
      "iteration: 427320 loss: 0.0015 lr: 0.02\n",
      "iteration: 427330 loss: 0.0016 lr: 0.02\n",
      "iteration: 427340 loss: 0.0013 lr: 0.02\n",
      "iteration: 427350 loss: 0.0013 lr: 0.02\n",
      "iteration: 427360 loss: 0.0010 lr: 0.02\n",
      "iteration: 427370 loss: 0.0016 lr: 0.02\n",
      "iteration: 427380 loss: 0.0018 lr: 0.02\n",
      "iteration: 427390 loss: 0.0014 lr: 0.02\n",
      "iteration: 427400 loss: 0.0012 lr: 0.02\n",
      "iteration: 427410 loss: 0.0016 lr: 0.02\n",
      "iteration: 427420 loss: 0.0012 lr: 0.02\n",
      "iteration: 427430 loss: 0.0010 lr: 0.02\n",
      "iteration: 427440 loss: 0.0015 lr: 0.02\n",
      "iteration: 427450 loss: 0.0023 lr: 0.02\n",
      "iteration: 427460 loss: 0.0014 lr: 0.02\n",
      "iteration: 427470 loss: 0.0013 lr: 0.02\n",
      "iteration: 427480 loss: 0.0018 lr: 0.02\n",
      "iteration: 427490 loss: 0.0019 lr: 0.02\n",
      "iteration: 427500 loss: 0.0012 lr: 0.02\n",
      "iteration: 427510 loss: 0.0016 lr: 0.02\n",
      "iteration: 427520 loss: 0.0016 lr: 0.02\n",
      "iteration: 427530 loss: 0.0016 lr: 0.02\n",
      "iteration: 427540 loss: 0.0018 lr: 0.02\n",
      "iteration: 427550 loss: 0.0023 lr: 0.02\n",
      "iteration: 427560 loss: 0.0012 lr: 0.02\n",
      "iteration: 427570 loss: 0.0018 lr: 0.02\n",
      "iteration: 427580 loss: 0.0024 lr: 0.02\n",
      "iteration: 427590 loss: 0.0022 lr: 0.02\n",
      "iteration: 427600 loss: 0.0018 lr: 0.02\n",
      "iteration: 427610 loss: 0.0020 lr: 0.02\n",
      "iteration: 427620 loss: 0.0015 lr: 0.02\n",
      "iteration: 427630 loss: 0.0018 lr: 0.02\n",
      "iteration: 427640 loss: 0.0015 lr: 0.02\n",
      "iteration: 427650 loss: 0.0020 lr: 0.02\n",
      "iteration: 427660 loss: 0.0022 lr: 0.02\n",
      "iteration: 427670 loss: 0.0010 lr: 0.02\n",
      "iteration: 427680 loss: 0.0021 lr: 0.02\n",
      "iteration: 427690 loss: 0.0013 lr: 0.02\n",
      "iteration: 427700 loss: 0.0014 lr: 0.02\n",
      "iteration: 427710 loss: 0.0010 lr: 0.02\n",
      "iteration: 427720 loss: 0.0015 lr: 0.02\n",
      "iteration: 427730 loss: 0.0020 lr: 0.02\n",
      "iteration: 427740 loss: 0.0013 lr: 0.02\n",
      "iteration: 427750 loss: 0.0011 lr: 0.02\n",
      "iteration: 427760 loss: 0.0014 lr: 0.02\n",
      "iteration: 427770 loss: 0.0021 lr: 0.02\n",
      "iteration: 427780 loss: 0.0016 lr: 0.02\n",
      "iteration: 427790 loss: 0.0018 lr: 0.02\n",
      "iteration: 427800 loss: 0.0016 lr: 0.02\n",
      "iteration: 427810 loss: 0.0016 lr: 0.02\n",
      "iteration: 427820 loss: 0.0013 lr: 0.02\n",
      "iteration: 427830 loss: 0.0012 lr: 0.02\n",
      "iteration: 427840 loss: 0.0017 lr: 0.02\n",
      "iteration: 427850 loss: 0.0013 lr: 0.02\n",
      "iteration: 427860 loss: 0.0018 lr: 0.02\n",
      "iteration: 427870 loss: 0.0012 lr: 0.02\n",
      "iteration: 427880 loss: 0.0015 lr: 0.02\n",
      "iteration: 427890 loss: 0.0014 lr: 0.02\n",
      "iteration: 427900 loss: 0.0012 lr: 0.02\n",
      "iteration: 427910 loss: 0.0023 lr: 0.02\n",
      "iteration: 427920 loss: 0.0015 lr: 0.02\n",
      "iteration: 427930 loss: 0.0012 lr: 0.02\n",
      "iteration: 427940 loss: 0.0019 lr: 0.02\n",
      "iteration: 427950 loss: 0.0018 lr: 0.02\n",
      "iteration: 427960 loss: 0.0015 lr: 0.02\n",
      "iteration: 427970 loss: 0.0015 lr: 0.02\n",
      "iteration: 427980 loss: 0.0012 lr: 0.02\n",
      "iteration: 427990 loss: 0.0016 lr: 0.02\n",
      "iteration: 428000 loss: 0.0014 lr: 0.02\n",
      "iteration: 428010 loss: 0.0012 lr: 0.02\n",
      "iteration: 428020 loss: 0.0017 lr: 0.02\n",
      "iteration: 428030 loss: 0.0012 lr: 0.02\n",
      "iteration: 428040 loss: 0.0012 lr: 0.02\n",
      "iteration: 428050 loss: 0.0029 lr: 0.02\n",
      "iteration: 428060 loss: 0.0027 lr: 0.02\n",
      "iteration: 428070 loss: 0.0011 lr: 0.02\n",
      "iteration: 428080 loss: 0.0013 lr: 0.02\n",
      "iteration: 428090 loss: 0.0016 lr: 0.02\n",
      "iteration: 428100 loss: 0.0013 lr: 0.02\n",
      "iteration: 428110 loss: 0.0021 lr: 0.02\n",
      "iteration: 428120 loss: 0.0015 lr: 0.02\n",
      "iteration: 428130 loss: 0.0018 lr: 0.02\n",
      "iteration: 428140 loss: 0.0014 lr: 0.02\n",
      "iteration: 428150 loss: 0.0018 lr: 0.02\n",
      "iteration: 428160 loss: 0.0018 lr: 0.02\n",
      "iteration: 428170 loss: 0.0019 lr: 0.02\n",
      "iteration: 428180 loss: 0.0010 lr: 0.02\n",
      "iteration: 428190 loss: 0.0022 lr: 0.02\n",
      "iteration: 428200 loss: 0.0015 lr: 0.02\n",
      "iteration: 428210 loss: 0.0024 lr: 0.02\n",
      "iteration: 428220 loss: 0.0017 lr: 0.02\n",
      "iteration: 428230 loss: 0.0013 lr: 0.02\n",
      "iteration: 428240 loss: 0.0015 lr: 0.02\n",
      "iteration: 428250 loss: 0.0013 lr: 0.02\n",
      "iteration: 428260 loss: 0.0018 lr: 0.02\n",
      "iteration: 428270 loss: 0.0018 lr: 0.02\n",
      "iteration: 428280 loss: 0.0026 lr: 0.02\n",
      "iteration: 428290 loss: 0.0017 lr: 0.02\n",
      "iteration: 428300 loss: 0.0026 lr: 0.02\n",
      "iteration: 428310 loss: 0.0016 lr: 0.02\n",
      "iteration: 428320 loss: 0.0013 lr: 0.02\n",
      "iteration: 428330 loss: 0.0011 lr: 0.02\n",
      "iteration: 428340 loss: 0.0017 lr: 0.02\n",
      "iteration: 428350 loss: 0.0010 lr: 0.02\n",
      "iteration: 428360 loss: 0.0028 lr: 0.02\n",
      "iteration: 428370 loss: 0.0015 lr: 0.02\n",
      "iteration: 428380 loss: 0.0011 lr: 0.02\n",
      "iteration: 428390 loss: 0.0014 lr: 0.02\n",
      "iteration: 428400 loss: 0.0017 lr: 0.02\n",
      "iteration: 428410 loss: 0.0013 lr: 0.02\n",
      "iteration: 428420 loss: 0.0010 lr: 0.02\n",
      "iteration: 428430 loss: 0.0013 lr: 0.02\n",
      "iteration: 428440 loss: 0.0014 lr: 0.02\n",
      "iteration: 428450 loss: 0.0013 lr: 0.02\n",
      "iteration: 428460 loss: 0.0013 lr: 0.02\n",
      "iteration: 428470 loss: 0.0016 lr: 0.02\n",
      "iteration: 428480 loss: 0.0013 lr: 0.02\n",
      "iteration: 428490 loss: 0.0016 lr: 0.02\n",
      "iteration: 428500 loss: 0.0012 lr: 0.02\n",
      "iteration: 428510 loss: 0.0011 lr: 0.02\n",
      "iteration: 428520 loss: 0.0016 lr: 0.02\n",
      "iteration: 428530 loss: 0.0014 lr: 0.02\n",
      "iteration: 428540 loss: 0.0017 lr: 0.02\n",
      "iteration: 428550 loss: 0.0012 lr: 0.02\n",
      "iteration: 428560 loss: 0.0015 lr: 0.02\n",
      "iteration: 428570 loss: 0.0013 lr: 0.02\n",
      "iteration: 428580 loss: 0.0029 lr: 0.02\n",
      "iteration: 428590 loss: 0.0011 lr: 0.02\n",
      "iteration: 428600 loss: 0.0019 lr: 0.02\n",
      "iteration: 428610 loss: 0.0013 lr: 0.02\n",
      "iteration: 428620 loss: 0.0012 lr: 0.02\n",
      "iteration: 428630 loss: 0.0013 lr: 0.02\n",
      "iteration: 428640 loss: 0.0013 lr: 0.02\n",
      "iteration: 428650 loss: 0.0018 lr: 0.02\n",
      "iteration: 428660 loss: 0.0021 lr: 0.02\n",
      "iteration: 428670 loss: 0.0013 lr: 0.02\n",
      "iteration: 428680 loss: 0.0014 lr: 0.02\n",
      "iteration: 428690 loss: 0.0016 lr: 0.02\n",
      "iteration: 428700 loss: 0.0019 lr: 0.02\n",
      "iteration: 428710 loss: 0.0017 lr: 0.02\n",
      "iteration: 428720 loss: 0.0014 lr: 0.02\n",
      "iteration: 428730 loss: 0.0015 lr: 0.02\n",
      "iteration: 428740 loss: 0.0016 lr: 0.02\n",
      "iteration: 428750 loss: 0.0010 lr: 0.02\n",
      "iteration: 428760 loss: 0.0017 lr: 0.02\n",
      "iteration: 428770 loss: 0.0017 lr: 0.02\n",
      "iteration: 428780 loss: 0.0016 lr: 0.02\n",
      "iteration: 428790 loss: 0.0028 lr: 0.02\n",
      "iteration: 428800 loss: 0.0013 lr: 0.02\n",
      "iteration: 428810 loss: 0.0018 lr: 0.02\n",
      "iteration: 428820 loss: 0.0016 lr: 0.02\n",
      "iteration: 428830 loss: 0.0013 lr: 0.02\n",
      "iteration: 428840 loss: 0.0016 lr: 0.02\n",
      "iteration: 428850 loss: 0.0016 lr: 0.02\n",
      "iteration: 428860 loss: 0.0016 lr: 0.02\n",
      "iteration: 428870 loss: 0.0019 lr: 0.02\n",
      "iteration: 428880 loss: 0.0018 lr: 0.02\n",
      "iteration: 428890 loss: 0.0012 lr: 0.02\n",
      "iteration: 428900 loss: 0.0020 lr: 0.02\n",
      "iteration: 428910 loss: 0.0011 lr: 0.02\n",
      "iteration: 428920 loss: 0.0014 lr: 0.02\n",
      "iteration: 428930 loss: 0.0013 lr: 0.02\n",
      "iteration: 428940 loss: 0.0014 lr: 0.02\n",
      "iteration: 428950 loss: 0.0021 lr: 0.02\n",
      "iteration: 428960 loss: 0.0014 lr: 0.02\n",
      "iteration: 428970 loss: 0.0016 lr: 0.02\n",
      "iteration: 428980 loss: 0.0013 lr: 0.02\n",
      "iteration: 428990 loss: 0.0015 lr: 0.02\n",
      "iteration: 429000 loss: 0.0021 lr: 0.02\n",
      "iteration: 429010 loss: 0.0016 lr: 0.02\n",
      "iteration: 429020 loss: 0.0014 lr: 0.02\n",
      "iteration: 429030 loss: 0.0016 lr: 0.02\n",
      "iteration: 429040 loss: 0.0017 lr: 0.02\n",
      "iteration: 429050 loss: 0.0022 lr: 0.02\n",
      "iteration: 429060 loss: 0.0016 lr: 0.02\n",
      "iteration: 429070 loss: 0.0025 lr: 0.02\n",
      "iteration: 429080 loss: 0.0016 lr: 0.02\n",
      "iteration: 429090 loss: 0.0013 lr: 0.02\n",
      "iteration: 429100 loss: 0.0017 lr: 0.02\n",
      "iteration: 429110 loss: 0.0017 lr: 0.02\n",
      "iteration: 429120 loss: 0.0017 lr: 0.02\n",
      "iteration: 429130 loss: 0.0014 lr: 0.02\n",
      "iteration: 429140 loss: 0.0016 lr: 0.02\n",
      "iteration: 429150 loss: 0.0009 lr: 0.02\n",
      "iteration: 429160 loss: 0.0016 lr: 0.02\n",
      "iteration: 429170 loss: 0.0019 lr: 0.02\n",
      "iteration: 429180 loss: 0.0018 lr: 0.02\n",
      "iteration: 429190 loss: 0.0017 lr: 0.02\n",
      "iteration: 429200 loss: 0.0014 lr: 0.02\n",
      "iteration: 429210 loss: 0.0018 lr: 0.02\n",
      "iteration: 429220 loss: 0.0011 lr: 0.02\n",
      "iteration: 429230 loss: 0.0013 lr: 0.02\n",
      "iteration: 429240 loss: 0.0019 lr: 0.02\n",
      "iteration: 429250 loss: 0.0015 lr: 0.02\n",
      "iteration: 429260 loss: 0.0021 lr: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 429270 loss: 0.0014 lr: 0.02\n",
      "iteration: 429280 loss: 0.0016 lr: 0.02\n",
      "iteration: 429290 loss: 0.0012 lr: 0.02\n",
      "iteration: 429300 loss: 0.0022 lr: 0.02\n",
      "iteration: 429310 loss: 0.0016 lr: 0.02\n",
      "iteration: 429320 loss: 0.0017 lr: 0.02\n",
      "iteration: 429330 loss: 0.0021 lr: 0.02\n",
      "iteration: 429340 loss: 0.0012 lr: 0.02\n",
      "iteration: 429350 loss: 0.0020 lr: 0.02\n",
      "iteration: 429360 loss: 0.0012 lr: 0.02\n",
      "iteration: 429370 loss: 0.0029 lr: 0.02\n",
      "iteration: 429380 loss: 0.0019 lr: 0.02\n",
      "iteration: 429390 loss: 0.0013 lr: 0.02\n",
      "iteration: 429400 loss: 0.0016 lr: 0.02\n",
      "iteration: 429410 loss: 0.0016 lr: 0.02\n",
      "iteration: 429420 loss: 0.0024 lr: 0.02\n",
      "iteration: 429430 loss: 0.0025 lr: 0.02\n",
      "iteration: 429440 loss: 0.0014 lr: 0.02\n",
      "iteration: 429450 loss: 0.0011 lr: 0.02\n",
      "iteration: 429460 loss: 0.0017 lr: 0.02\n",
      "iteration: 429470 loss: 0.0017 lr: 0.02\n",
      "iteration: 429480 loss: 0.0014 lr: 0.02\n",
      "iteration: 429490 loss: 0.0012 lr: 0.02\n",
      "iteration: 429500 loss: 0.0019 lr: 0.02\n",
      "iteration: 429510 loss: 0.0016 lr: 0.02\n",
      "iteration: 429520 loss: 0.0017 lr: 0.02\n",
      "iteration: 429530 loss: 0.0018 lr: 0.02\n",
      "iteration: 429540 loss: 0.0014 lr: 0.02\n",
      "iteration: 429550 loss: 0.0018 lr: 0.02\n",
      "iteration: 429560 loss: 0.0015 lr: 0.02\n",
      "iteration: 429570 loss: 0.0015 lr: 0.02\n",
      "iteration: 429580 loss: 0.0016 lr: 0.02\n",
      "iteration: 429590 loss: 0.0012 lr: 0.02\n",
      "iteration: 429600 loss: 0.0018 lr: 0.02\n",
      "iteration: 429610 loss: 0.0009 lr: 0.02\n",
      "iteration: 429620 loss: 0.0013 lr: 0.02\n",
      "iteration: 429630 loss: 0.0015 lr: 0.02\n",
      "iteration: 429640 loss: 0.0016 lr: 0.02\n",
      "iteration: 429650 loss: 0.0014 lr: 0.02\n",
      "iteration: 429660 loss: 0.0013 lr: 0.02\n",
      "iteration: 429670 loss: 0.0017 lr: 0.02\n",
      "iteration: 429680 loss: 0.0009 lr: 0.02\n",
      "iteration: 429690 loss: 0.0013 lr: 0.02\n",
      "iteration: 429700 loss: 0.0017 lr: 0.02\n",
      "iteration: 429710 loss: 0.0017 lr: 0.02\n",
      "iteration: 429720 loss: 0.0013 lr: 0.02\n",
      "iteration: 429730 loss: 0.0013 lr: 0.02\n",
      "iteration: 429740 loss: 0.0012 lr: 0.02\n",
      "iteration: 429750 loss: 0.0010 lr: 0.02\n",
      "iteration: 429760 loss: 0.0017 lr: 0.02\n",
      "iteration: 429770 loss: 0.0011 lr: 0.02\n",
      "iteration: 429780 loss: 0.0016 lr: 0.02\n",
      "iteration: 429790 loss: 0.0016 lr: 0.02\n",
      "iteration: 429800 loss: 0.0013 lr: 0.02\n",
      "iteration: 429810 loss: 0.0011 lr: 0.02\n",
      "iteration: 429820 loss: 0.0014 lr: 0.02\n",
      "iteration: 429830 loss: 0.0014 lr: 0.02\n",
      "iteration: 429840 loss: 0.0014 lr: 0.02\n",
      "iteration: 429850 loss: 0.0012 lr: 0.02\n",
      "iteration: 429860 loss: 0.0014 lr: 0.02\n",
      "iteration: 429870 loss: 0.0039 lr: 0.02\n",
      "iteration: 429880 loss: 0.0016 lr: 0.02\n",
      "iteration: 429890 loss: 0.0014 lr: 0.02\n",
      "iteration: 429900 loss: 0.0010 lr: 0.02\n",
      "iteration: 429910 loss: 0.0020 lr: 0.02\n",
      "iteration: 429920 loss: 0.0015 lr: 0.02\n",
      "iteration: 429930 loss: 0.0018 lr: 0.02\n",
      "iteration: 429940 loss: 0.0012 lr: 0.02\n",
      "iteration: 429950 loss: 0.0014 lr: 0.02\n",
      "iteration: 429960 loss: 0.0014 lr: 0.02\n",
      "iteration: 429970 loss: 0.0012 lr: 0.02\n",
      "iteration: 429980 loss: 0.0016 lr: 0.02\n",
      "iteration: 429990 loss: 0.0018 lr: 0.02\n",
      "iteration: 430000 loss: 0.0014 lr: 0.02\n",
      "iteration: 430010 loss: 0.0024 lr: 0.002\n",
      "iteration: 430020 loss: 0.0014 lr: 0.002\n",
      "iteration: 430030 loss: 0.0010 lr: 0.002\n",
      "iteration: 430040 loss: 0.0010 lr: 0.002\n",
      "iteration: 430050 loss: 0.0015 lr: 0.002\n",
      "iteration: 430060 loss: 0.0016 lr: 0.002\n",
      "iteration: 430070 loss: 0.0014 lr: 0.002\n",
      "iteration: 430080 loss: 0.0022 lr: 0.002\n",
      "iteration: 430090 loss: 0.0014 lr: 0.002\n",
      "iteration: 430100 loss: 0.0011 lr: 0.002\n",
      "iteration: 430110 loss: 0.0014 lr: 0.002\n",
      "iteration: 430120 loss: 0.0013 lr: 0.002\n",
      "iteration: 430130 loss: 0.0012 lr: 0.002\n",
      "iteration: 430140 loss: 0.0019 lr: 0.002\n",
      "iteration: 430150 loss: 0.0010 lr: 0.002\n",
      "iteration: 430160 loss: 0.0014 lr: 0.002\n",
      "iteration: 430170 loss: 0.0008 lr: 0.002\n",
      "iteration: 430180 loss: 0.0015 lr: 0.002\n",
      "iteration: 430190 loss: 0.0012 lr: 0.002\n",
      "iteration: 430200 loss: 0.0016 lr: 0.002\n",
      "iteration: 430210 loss: 0.0012 lr: 0.002\n",
      "iteration: 430220 loss: 0.0011 lr: 0.002\n",
      "iteration: 430230 loss: 0.0017 lr: 0.002\n",
      "iteration: 430240 loss: 0.0014 lr: 0.002\n",
      "iteration: 430250 loss: 0.0018 lr: 0.002\n",
      "iteration: 430260 loss: 0.0011 lr: 0.002\n",
      "iteration: 430270 loss: 0.0015 lr: 0.002\n",
      "iteration: 430280 loss: 0.0016 lr: 0.002\n",
      "iteration: 430290 loss: 0.0017 lr: 0.002\n",
      "iteration: 430300 loss: 0.0014 lr: 0.002\n",
      "iteration: 430310 loss: 0.0011 lr: 0.002\n",
      "iteration: 430320 loss: 0.0027 lr: 0.002\n",
      "iteration: 430330 loss: 0.0011 lr: 0.002\n",
      "iteration: 430340 loss: 0.0017 lr: 0.002\n",
      "iteration: 430350 loss: 0.0012 lr: 0.002\n",
      "iteration: 430360 loss: 0.0014 lr: 0.002\n",
      "iteration: 430370 loss: 0.0012 lr: 0.002\n",
      "iteration: 430380 loss: 0.0009 lr: 0.002\n",
      "iteration: 430390 loss: 0.0016 lr: 0.002\n",
      "iteration: 430400 loss: 0.0011 lr: 0.002\n",
      "iteration: 430410 loss: 0.0022 lr: 0.002\n",
      "iteration: 430420 loss: 0.0011 lr: 0.002\n",
      "iteration: 430430 loss: 0.0015 lr: 0.002\n",
      "iteration: 430440 loss: 0.0018 lr: 0.002\n",
      "iteration: 430450 loss: 0.0012 lr: 0.002\n",
      "iteration: 430460 loss: 0.0012 lr: 0.002\n",
      "iteration: 430470 loss: 0.0014 lr: 0.002\n",
      "iteration: 430480 loss: 0.0012 lr: 0.002\n",
      "iteration: 430490 loss: 0.0012 lr: 0.002\n",
      "iteration: 430500 loss: 0.0014 lr: 0.002\n",
      "iteration: 430510 loss: 0.0013 lr: 0.002\n",
      "iteration: 430520 loss: 0.0018 lr: 0.002\n",
      "iteration: 430530 loss: 0.0013 lr: 0.002\n",
      "iteration: 430540 loss: 0.0011 lr: 0.002\n",
      "iteration: 430550 loss: 0.0012 lr: 0.002\n",
      "iteration: 430560 loss: 0.0016 lr: 0.002\n",
      "iteration: 430570 loss: 0.0013 lr: 0.002\n",
      "iteration: 430580 loss: 0.0011 lr: 0.002\n",
      "iteration: 430590 loss: 0.0009 lr: 0.002\n",
      "iteration: 430600 loss: 0.0015 lr: 0.002\n",
      "iteration: 430610 loss: 0.0010 lr: 0.002\n",
      "iteration: 430620 loss: 0.0016 lr: 0.002\n",
      "iteration: 430630 loss: 0.0015 lr: 0.002\n",
      "iteration: 430640 loss: 0.0009 lr: 0.002\n",
      "iteration: 430650 loss: 0.0019 lr: 0.002\n",
      "iteration: 430660 loss: 0.0012 lr: 0.002\n",
      "iteration: 430670 loss: 0.0013 lr: 0.002\n",
      "iteration: 430680 loss: 0.0010 lr: 0.002\n",
      "iteration: 430690 loss: 0.0017 lr: 0.002\n",
      "iteration: 430700 loss: 0.0011 lr: 0.002\n",
      "iteration: 430710 loss: 0.0012 lr: 0.002\n",
      "iteration: 430720 loss: 0.0014 lr: 0.002\n",
      "iteration: 430730 loss: 0.0017 lr: 0.002\n",
      "iteration: 430740 loss: 0.0013 lr: 0.002\n",
      "iteration: 430750 loss: 0.0015 lr: 0.002\n",
      "iteration: 430760 loss: 0.0011 lr: 0.002\n",
      "iteration: 430770 loss: 0.0016 lr: 0.002\n",
      "iteration: 430780 loss: 0.0022 lr: 0.002\n",
      "iteration: 430790 loss: 0.0011 lr: 0.002\n",
      "iteration: 430800 loss: 0.0014 lr: 0.002\n",
      "iteration: 430810 loss: 0.0010 lr: 0.002\n",
      "iteration: 430820 loss: 0.0011 lr: 0.002\n",
      "iteration: 430830 loss: 0.0015 lr: 0.002\n",
      "iteration: 430840 loss: 0.0011 lr: 0.002\n",
      "iteration: 430850 loss: 0.0018 lr: 0.002\n",
      "iteration: 430860 loss: 0.0010 lr: 0.002\n",
      "iteration: 430870 loss: 0.0011 lr: 0.002\n",
      "iteration: 430880 loss: 0.0012 lr: 0.002\n",
      "iteration: 430890 loss: 0.0009 lr: 0.002\n",
      "iteration: 430900 loss: 0.0011 lr: 0.002\n",
      "iteration: 430910 loss: 0.0016 lr: 0.002\n",
      "iteration: 430920 loss: 0.0014 lr: 0.002\n",
      "iteration: 430930 loss: 0.0014 lr: 0.002\n",
      "iteration: 430940 loss: 0.0018 lr: 0.002\n",
      "iteration: 430950 loss: 0.0011 lr: 0.002\n",
      "iteration: 430960 loss: 0.0010 lr: 0.002\n",
      "iteration: 430970 loss: 0.0014 lr: 0.002\n",
      "iteration: 430980 loss: 0.0020 lr: 0.002\n",
      "iteration: 430990 loss: 0.0012 lr: 0.002\n",
      "iteration: 431000 loss: 0.0010 lr: 0.002\n",
      "iteration: 431010 loss: 0.0010 lr: 0.002\n",
      "iteration: 431020 loss: 0.0011 lr: 0.002\n",
      "iteration: 431030 loss: 0.0015 lr: 0.002\n",
      "iteration: 431040 loss: 0.0018 lr: 0.002\n",
      "iteration: 431050 loss: 0.0011 lr: 0.002\n",
      "iteration: 431060 loss: 0.0013 lr: 0.002\n",
      "iteration: 431070 loss: 0.0011 lr: 0.002\n",
      "iteration: 431080 loss: 0.0010 lr: 0.002\n",
      "iteration: 431090 loss: 0.0009 lr: 0.002\n",
      "iteration: 431100 loss: 0.0011 lr: 0.002\n",
      "iteration: 431110 loss: 0.0015 lr: 0.002\n",
      "iteration: 431120 loss: 0.0013 lr: 0.002\n",
      "iteration: 431130 loss: 0.0009 lr: 0.002\n",
      "iteration: 431140 loss: 0.0010 lr: 0.002\n",
      "iteration: 431150 loss: 0.0015 lr: 0.002\n",
      "iteration: 431160 loss: 0.0010 lr: 0.002\n",
      "iteration: 431170 loss: 0.0020 lr: 0.002\n",
      "iteration: 431180 loss: 0.0008 lr: 0.002\n",
      "iteration: 431190 loss: 0.0012 lr: 0.002\n",
      "iteration: 431200 loss: 0.0029 lr: 0.002\n",
      "iteration: 431210 loss: 0.0015 lr: 0.002\n",
      "iteration: 431220 loss: 0.0019 lr: 0.002\n",
      "iteration: 431230 loss: 0.0009 lr: 0.002\n",
      "iteration: 431240 loss: 0.0010 lr: 0.002\n",
      "iteration: 431250 loss: 0.0011 lr: 0.002\n",
      "iteration: 431260 loss: 0.0011 lr: 0.002\n",
      "iteration: 431270 loss: 0.0012 lr: 0.002\n",
      "iteration: 431280 loss: 0.0010 lr: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 431290 loss: 0.0015 lr: 0.002\n",
      "iteration: 431300 loss: 0.0033 lr: 0.002\n",
      "iteration: 431310 loss: 0.0013 lr: 0.002\n",
      "iteration: 431320 loss: 0.0011 lr: 0.002\n",
      "iteration: 431330 loss: 0.0008 lr: 0.002\n",
      "iteration: 431340 loss: 0.0012 lr: 0.002\n",
      "iteration: 431350 loss: 0.0012 lr: 0.002\n",
      "iteration: 431360 loss: 0.0011 lr: 0.002\n",
      "iteration: 431370 loss: 0.0016 lr: 0.002\n",
      "iteration: 431380 loss: 0.0010 lr: 0.002\n",
      "iteration: 431390 loss: 0.0008 lr: 0.002\n",
      "iteration: 431400 loss: 0.0012 lr: 0.002\n",
      "iteration: 431410 loss: 0.0010 lr: 0.002\n",
      "iteration: 431420 loss: 0.0010 lr: 0.002\n",
      "iteration: 431430 loss: 0.0015 lr: 0.002\n",
      "iteration: 431440 loss: 0.0012 lr: 0.002\n",
      "iteration: 431450 loss: 0.0015 lr: 0.002\n",
      "iteration: 431460 loss: 0.0013 lr: 0.002\n",
      "iteration: 431470 loss: 0.0010 lr: 0.002\n",
      "iteration: 431480 loss: 0.0016 lr: 0.002\n",
      "iteration: 431490 loss: 0.0021 lr: 0.002\n",
      "iteration: 431500 loss: 0.0014 lr: 0.002\n",
      "iteration: 431510 loss: 0.0015 lr: 0.002\n",
      "iteration: 431520 loss: 0.0010 lr: 0.002\n",
      "iteration: 431530 loss: 0.0009 lr: 0.002\n",
      "iteration: 431540 loss: 0.0018 lr: 0.002\n",
      "iteration: 431550 loss: 0.0010 lr: 0.002\n",
      "iteration: 431560 loss: 0.0012 lr: 0.002\n",
      "iteration: 431570 loss: 0.0017 lr: 0.002\n",
      "iteration: 431580 loss: 0.0015 lr: 0.002\n",
      "iteration: 431590 loss: 0.0011 lr: 0.002\n",
      "iteration: 431600 loss: 0.0012 lr: 0.002\n",
      "iteration: 431610 loss: 0.0013 lr: 0.002\n",
      "iteration: 431620 loss: 0.0021 lr: 0.002\n",
      "iteration: 431630 loss: 0.0014 lr: 0.002\n",
      "iteration: 431640 loss: 0.0012 lr: 0.002\n",
      "iteration: 431650 loss: 0.0009 lr: 0.002\n",
      "iteration: 431660 loss: 0.0014 lr: 0.002\n",
      "iteration: 431670 loss: 0.0015 lr: 0.002\n",
      "iteration: 431680 loss: 0.0013 lr: 0.002\n",
      "iteration: 431690 loss: 0.0016 lr: 0.002\n",
      "iteration: 431700 loss: 0.0011 lr: 0.002\n",
      "iteration: 431710 loss: 0.0011 lr: 0.002\n",
      "iteration: 431720 loss: 0.0012 lr: 0.002\n",
      "iteration: 431730 loss: 0.0011 lr: 0.002\n",
      "iteration: 431740 loss: 0.0010 lr: 0.002\n",
      "iteration: 431750 loss: 0.0012 lr: 0.002\n",
      "iteration: 431760 loss: 0.0014 lr: 0.002\n",
      "iteration: 431770 loss: 0.0015 lr: 0.002\n",
      "iteration: 431780 loss: 0.0010 lr: 0.002\n",
      "iteration: 431790 loss: 0.0012 lr: 0.002\n",
      "iteration: 431800 loss: 0.0011 lr: 0.002\n",
      "iteration: 431810 loss: 0.0023 lr: 0.002\n",
      "iteration: 431820 loss: 0.0012 lr: 0.002\n",
      "iteration: 431830 loss: 0.0014 lr: 0.002\n",
      "iteration: 431840 loss: 0.0010 lr: 0.002\n",
      "iteration: 431850 loss: 0.0015 lr: 0.002\n",
      "iteration: 431860 loss: 0.0015 lr: 0.002\n",
      "iteration: 431870 loss: 0.0018 lr: 0.002\n",
      "iteration: 431880 loss: 0.0015 lr: 0.002\n",
      "iteration: 431890 loss: 0.0012 lr: 0.002\n",
      "iteration: 431900 loss: 0.0014 lr: 0.002\n",
      "iteration: 431910 loss: 0.0018 lr: 0.002\n",
      "iteration: 431920 loss: 0.0017 lr: 0.002\n",
      "iteration: 431930 loss: 0.0015 lr: 0.002\n",
      "iteration: 431940 loss: 0.0016 lr: 0.002\n",
      "iteration: 431950 loss: 0.0019 lr: 0.002\n",
      "iteration: 431960 loss: 0.0010 lr: 0.002\n",
      "iteration: 431970 loss: 0.0010 lr: 0.002\n",
      "iteration: 431980 loss: 0.0012 lr: 0.002\n",
      "iteration: 431990 loss: 0.0013 lr: 0.002\n",
      "iteration: 432000 loss: 0.0011 lr: 0.002\n",
      "iteration: 432010 loss: 0.0009 lr: 0.002\n",
      "iteration: 432020 loss: 0.0012 lr: 0.002\n",
      "iteration: 432030 loss: 0.0019 lr: 0.002\n",
      "iteration: 432040 loss: 0.0013 lr: 0.002\n",
      "iteration: 432050 loss: 0.0012 lr: 0.002\n",
      "iteration: 432060 loss: 0.0018 lr: 0.002\n",
      "iteration: 432070 loss: 0.0011 lr: 0.002\n",
      "iteration: 432080 loss: 0.0011 lr: 0.002\n",
      "iteration: 432090 loss: 0.0014 lr: 0.002\n",
      "iteration: 432100 loss: 0.0011 lr: 0.002\n",
      "iteration: 432110 loss: 0.0016 lr: 0.002\n",
      "iteration: 432120 loss: 0.0012 lr: 0.002\n",
      "iteration: 432130 loss: 0.0019 lr: 0.002\n",
      "iteration: 432140 loss: 0.0026 lr: 0.002\n",
      "iteration: 432150 loss: 0.0018 lr: 0.002\n",
      "iteration: 432160 loss: 0.0014 lr: 0.002\n",
      "iteration: 432170 loss: 0.0008 lr: 0.002\n",
      "iteration: 432180 loss: 0.0010 lr: 0.002\n",
      "iteration: 432190 loss: 0.0010 lr: 0.002\n",
      "iteration: 432200 loss: 0.0013 lr: 0.002\n",
      "iteration: 432210 loss: 0.0011 lr: 0.002\n",
      "iteration: 432220 loss: 0.0019 lr: 0.002\n",
      "iteration: 432230 loss: 0.0012 lr: 0.002\n",
      "iteration: 432240 loss: 0.0014 lr: 0.002\n",
      "iteration: 432250 loss: 0.0010 lr: 0.002\n",
      "iteration: 432260 loss: 0.0015 lr: 0.002\n",
      "iteration: 432270 loss: 0.0010 lr: 0.002\n",
      "iteration: 432280 loss: 0.0011 lr: 0.002\n",
      "iteration: 432290 loss: 0.0013 lr: 0.002\n",
      "iteration: 432300 loss: 0.0012 lr: 0.002\n",
      "iteration: 432310 loss: 0.0011 lr: 0.002\n",
      "iteration: 432320 loss: 0.0011 lr: 0.002\n",
      "iteration: 432330 loss: 0.0017 lr: 0.002\n",
      "iteration: 432340 loss: 0.0013 lr: 0.002\n",
      "iteration: 432350 loss: 0.0012 lr: 0.002\n",
      "iteration: 432360 loss: 0.0015 lr: 0.002\n",
      "iteration: 432370 loss: 0.0011 lr: 0.002\n",
      "iteration: 432380 loss: 0.0010 lr: 0.002\n",
      "iteration: 432390 loss: 0.0015 lr: 0.002\n",
      "iteration: 432400 loss: 0.0025 lr: 0.002\n",
      "iteration: 432410 loss: 0.0012 lr: 0.002\n",
      "iteration: 432420 loss: 0.0011 lr: 0.002\n",
      "iteration: 432430 loss: 0.0008 lr: 0.002\n",
      "iteration: 432440 loss: 0.0008 lr: 0.002\n",
      "iteration: 432450 loss: 0.0009 lr: 0.002\n",
      "iteration: 432460 loss: 0.0015 lr: 0.002\n",
      "iteration: 432470 loss: 0.0013 lr: 0.002\n",
      "iteration: 432480 loss: 0.0009 lr: 0.002\n",
      "iteration: 432490 loss: 0.0022 lr: 0.002\n",
      "iteration: 432500 loss: 0.0009 lr: 0.002\n",
      "iteration: 432510 loss: 0.0014 lr: 0.002\n",
      "iteration: 432520 loss: 0.0010 lr: 0.002\n",
      "iteration: 432530 loss: 0.0020 lr: 0.002\n",
      "iteration: 432540 loss: 0.0011 lr: 0.002\n",
      "iteration: 432550 loss: 0.0012 lr: 0.002\n",
      "iteration: 432560 loss: 0.0015 lr: 0.002\n",
      "iteration: 432570 loss: 0.0015 lr: 0.002\n",
      "iteration: 432580 loss: 0.0012 lr: 0.002\n",
      "iteration: 432590 loss: 0.0010 lr: 0.002\n",
      "iteration: 432600 loss: 0.0011 lr: 0.002\n",
      "iteration: 432610 loss: 0.0017 lr: 0.002\n",
      "iteration: 432620 loss: 0.0010 lr: 0.002\n",
      "iteration: 432630 loss: 0.0010 lr: 0.002\n",
      "iteration: 432640 loss: 0.0015 lr: 0.002\n",
      "iteration: 432650 loss: 0.0015 lr: 0.002\n",
      "iteration: 432660 loss: 0.0015 lr: 0.002\n",
      "iteration: 432670 loss: 0.0016 lr: 0.002\n",
      "iteration: 432680 loss: 0.0012 lr: 0.002\n",
      "iteration: 432690 loss: 0.0011 lr: 0.002\n",
      "iteration: 432700 loss: 0.0013 lr: 0.002\n",
      "iteration: 432710 loss: 0.0014 lr: 0.002\n",
      "iteration: 432720 loss: 0.0011 lr: 0.002\n",
      "iteration: 432730 loss: 0.0011 lr: 0.002\n",
      "iteration: 432740 loss: 0.0011 lr: 0.002\n",
      "iteration: 432750 loss: 0.0021 lr: 0.002\n",
      "iteration: 432760 loss: 0.0013 lr: 0.002\n",
      "iteration: 432770 loss: 0.0019 lr: 0.002\n",
      "iteration: 432780 loss: 0.0008 lr: 0.002\n",
      "iteration: 432790 loss: 0.0010 lr: 0.002\n",
      "iteration: 432800 loss: 0.0020 lr: 0.002\n",
      "iteration: 432810 loss: 0.0013 lr: 0.002\n",
      "iteration: 432820 loss: 0.0009 lr: 0.002\n",
      "iteration: 432830 loss: 0.0018 lr: 0.002\n",
      "iteration: 432840 loss: 0.0010 lr: 0.002\n",
      "iteration: 432850 loss: 0.0010 lr: 0.002\n",
      "iteration: 432860 loss: 0.0019 lr: 0.002\n",
      "iteration: 432870 loss: 0.0011 lr: 0.002\n",
      "iteration: 432880 loss: 0.0014 lr: 0.002\n",
      "iteration: 432890 loss: 0.0013 lr: 0.002\n",
      "iteration: 432900 loss: 0.0010 lr: 0.002\n",
      "iteration: 432910 loss: 0.0019 lr: 0.002\n",
      "iteration: 432920 loss: 0.0009 lr: 0.002\n",
      "iteration: 432930 loss: 0.0015 lr: 0.002\n",
      "iteration: 432940 loss: 0.0010 lr: 0.002\n",
      "iteration: 432950 loss: 0.0011 lr: 0.002\n",
      "iteration: 432960 loss: 0.0010 lr: 0.002\n",
      "iteration: 432970 loss: 0.0017 lr: 0.002\n",
      "iteration: 432980 loss: 0.0012 lr: 0.002\n",
      "iteration: 432990 loss: 0.0012 lr: 0.002\n",
      "iteration: 433000 loss: 0.0011 lr: 0.002\n",
      "iteration: 433010 loss: 0.0010 lr: 0.002\n",
      "iteration: 433020 loss: 0.0019 lr: 0.002\n",
      "iteration: 433030 loss: 0.0010 lr: 0.002\n",
      "iteration: 433040 loss: 0.0014 lr: 0.002\n",
      "iteration: 433050 loss: 0.0012 lr: 0.002\n",
      "iteration: 433060 loss: 0.0015 lr: 0.002\n",
      "iteration: 433070 loss: 0.0010 lr: 0.002\n",
      "iteration: 433080 loss: 0.0014 lr: 0.002\n",
      "iteration: 433090 loss: 0.0013 lr: 0.002\n",
      "iteration: 433100 loss: 0.0008 lr: 0.002\n",
      "iteration: 433110 loss: 0.0014 lr: 0.002\n",
      "iteration: 433120 loss: 0.0011 lr: 0.002\n",
      "iteration: 433130 loss: 0.0018 lr: 0.002\n",
      "iteration: 433140 loss: 0.0015 lr: 0.002\n",
      "iteration: 433150 loss: 0.0009 lr: 0.002\n",
      "iteration: 433160 loss: 0.0011 lr: 0.002\n",
      "iteration: 433170 loss: 0.0011 lr: 0.002\n",
      "iteration: 433180 loss: 0.0018 lr: 0.002\n",
      "iteration: 433190 loss: 0.0011 lr: 0.002\n",
      "iteration: 433200 loss: 0.0007 lr: 0.002\n",
      "iteration: 433210 loss: 0.0012 lr: 0.002\n",
      "iteration: 433220 loss: 0.0011 lr: 0.002\n",
      "iteration: 433230 loss: 0.0011 lr: 0.002\n",
      "iteration: 433240 loss: 0.0010 lr: 0.002\n",
      "iteration: 433250 loss: 0.0015 lr: 0.002\n",
      "iteration: 433260 loss: 0.0011 lr: 0.002\n",
      "iteration: 433270 loss: 0.0013 lr: 0.002\n",
      "iteration: 433280 loss: 0.0014 lr: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 433290 loss: 0.0010 lr: 0.002\n",
      "iteration: 433300 loss: 0.0014 lr: 0.002\n",
      "iteration: 433310 loss: 0.0013 lr: 0.002\n",
      "iteration: 433320 loss: 0.0035 lr: 0.002\n",
      "iteration: 433330 loss: 0.0013 lr: 0.002\n",
      "iteration: 433340 loss: 0.0012 lr: 0.002\n",
      "iteration: 433350 loss: 0.0012 lr: 0.002\n",
      "iteration: 433360 loss: 0.0012 lr: 0.002\n",
      "iteration: 433370 loss: 0.0011 lr: 0.002\n",
      "iteration: 433380 loss: 0.0010 lr: 0.002\n",
      "iteration: 433390 loss: 0.0013 lr: 0.002\n",
      "iteration: 433400 loss: 0.0014 lr: 0.002\n",
      "iteration: 433410 loss: 0.0014 lr: 0.002\n",
      "iteration: 433420 loss: 0.0013 lr: 0.002\n",
      "iteration: 433430 loss: 0.0009 lr: 0.002\n",
      "iteration: 433440 loss: 0.0013 lr: 0.002\n",
      "iteration: 433450 loss: 0.0011 lr: 0.002\n",
      "iteration: 433460 loss: 0.0013 lr: 0.002\n",
      "iteration: 433470 loss: 0.0018 lr: 0.002\n",
      "iteration: 433480 loss: 0.0009 lr: 0.002\n",
      "iteration: 433490 loss: 0.0022 lr: 0.002\n",
      "iteration: 433500 loss: 0.0012 lr: 0.002\n",
      "iteration: 433510 loss: 0.0016 lr: 0.002\n",
      "iteration: 433520 loss: 0.0011 lr: 0.002\n",
      "iteration: 433530 loss: 0.0009 lr: 0.002\n",
      "iteration: 433540 loss: 0.0016 lr: 0.002\n",
      "iteration: 433550 loss: 0.0010 lr: 0.002\n",
      "iteration: 433560 loss: 0.0013 lr: 0.002\n",
      "iteration: 433570 loss: 0.0016 lr: 0.002\n",
      "iteration: 433580 loss: 0.0011 lr: 0.002\n",
      "iteration: 433590 loss: 0.0015 lr: 0.002\n",
      "iteration: 433600 loss: 0.0012 lr: 0.002\n",
      "iteration: 433610 loss: 0.0011 lr: 0.002\n",
      "iteration: 433620 loss: 0.0010 lr: 0.002\n",
      "iteration: 433630 loss: 0.0009 lr: 0.002\n",
      "iteration: 433640 loss: 0.0010 lr: 0.002\n",
      "iteration: 433650 loss: 0.0009 lr: 0.002\n",
      "iteration: 433660 loss: 0.0014 lr: 0.002\n",
      "iteration: 433670 loss: 0.0018 lr: 0.002\n",
      "iteration: 433680 loss: 0.0011 lr: 0.002\n",
      "iteration: 433690 loss: 0.0013 lr: 0.002\n",
      "iteration: 433700 loss: 0.0012 lr: 0.002\n",
      "iteration: 433710 loss: 0.0014 lr: 0.002\n",
      "iteration: 433720 loss: 0.0011 lr: 0.002\n",
      "iteration: 433730 loss: 0.0026 lr: 0.002\n",
      "iteration: 433740 loss: 0.0008 lr: 0.002\n",
      "iteration: 433750 loss: 0.0015 lr: 0.002\n",
      "iteration: 433760 loss: 0.0013 lr: 0.002\n",
      "iteration: 433770 loss: 0.0012 lr: 0.002\n",
      "iteration: 433780 loss: 0.0013 lr: 0.002\n",
      "iteration: 433790 loss: 0.0017 lr: 0.002\n",
      "iteration: 433800 loss: 0.0010 lr: 0.002\n",
      "iteration: 433810 loss: 0.0010 lr: 0.002\n",
      "iteration: 433820 loss: 0.0013 lr: 0.002\n",
      "iteration: 433830 loss: 0.0011 lr: 0.002\n",
      "iteration: 433840 loss: 0.0013 lr: 0.002\n",
      "iteration: 433850 loss: 0.0011 lr: 0.002\n",
      "iteration: 433860 loss: 0.0020 lr: 0.002\n",
      "iteration: 433870 loss: 0.0012 lr: 0.002\n",
      "iteration: 433880 loss: 0.0011 lr: 0.002\n",
      "iteration: 433890 loss: 0.0016 lr: 0.002\n",
      "iteration: 433900 loss: 0.0015 lr: 0.002\n",
      "iteration: 433910 loss: 0.0008 lr: 0.002\n",
      "iteration: 433920 loss: 0.0013 lr: 0.002\n",
      "iteration: 433930 loss: 0.0014 lr: 0.002\n",
      "iteration: 433940 loss: 0.0010 lr: 0.002\n",
      "iteration: 433950 loss: 0.0015 lr: 0.002\n",
      "iteration: 433960 loss: 0.0009 lr: 0.002\n",
      "iteration: 433970 loss: 0.0013 lr: 0.002\n",
      "iteration: 433980 loss: 0.0023 lr: 0.002\n",
      "iteration: 433990 loss: 0.0012 lr: 0.002\n",
      "iteration: 434000 loss: 0.0011 lr: 0.002\n",
      "iteration: 434010 loss: 0.0012 lr: 0.002\n",
      "iteration: 434020 loss: 0.0016 lr: 0.002\n",
      "iteration: 434030 loss: 0.0009 lr: 0.002\n",
      "iteration: 434040 loss: 0.0014 lr: 0.002\n",
      "iteration: 434050 loss: 0.0015 lr: 0.002\n",
      "iteration: 434060 loss: 0.0014 lr: 0.002\n",
      "iteration: 434070 loss: 0.0015 lr: 0.002\n",
      "iteration: 434080 loss: 0.0011 lr: 0.002\n",
      "iteration: 434090 loss: 0.0011 lr: 0.002\n",
      "iteration: 434100 loss: 0.0011 lr: 0.002\n",
      "iteration: 434110 loss: 0.0012 lr: 0.002\n",
      "iteration: 434120 loss: 0.0009 lr: 0.002\n",
      "iteration: 434130 loss: 0.0013 lr: 0.002\n",
      "iteration: 434140 loss: 0.0016 lr: 0.002\n",
      "iteration: 434150 loss: 0.0012 lr: 0.002\n",
      "iteration: 434160 loss: 0.0012 lr: 0.002\n",
      "iteration: 434170 loss: 0.0008 lr: 0.002\n",
      "iteration: 434180 loss: 0.0013 lr: 0.002\n",
      "iteration: 434190 loss: 0.0010 lr: 0.002\n",
      "iteration: 434200 loss: 0.0013 lr: 0.002\n",
      "iteration: 434210 loss: 0.0012 lr: 0.002\n",
      "iteration: 434220 loss: 0.0014 lr: 0.002\n",
      "iteration: 434230 loss: 0.0012 lr: 0.002\n",
      "iteration: 434240 loss: 0.0008 lr: 0.002\n",
      "iteration: 434250 loss: 0.0011 lr: 0.002\n",
      "iteration: 434260 loss: 0.0012 lr: 0.002\n",
      "iteration: 434270 loss: 0.0016 lr: 0.002\n",
      "iteration: 434280 loss: 0.0011 lr: 0.002\n",
      "iteration: 434290 loss: 0.0012 lr: 0.002\n",
      "iteration: 434300 loss: 0.0014 lr: 0.002\n",
      "iteration: 434310 loss: 0.0012 lr: 0.002\n",
      "iteration: 434320 loss: 0.0010 lr: 0.002\n",
      "iteration: 434330 loss: 0.0011 lr: 0.002\n",
      "iteration: 434340 loss: 0.0013 lr: 0.002\n",
      "iteration: 434350 loss: 0.0028 lr: 0.002\n",
      "iteration: 434360 loss: 0.0020 lr: 0.002\n",
      "iteration: 434370 loss: 0.0011 lr: 0.002\n",
      "iteration: 434380 loss: 0.0013 lr: 0.002\n",
      "iteration: 434390 loss: 0.0014 lr: 0.002\n",
      "iteration: 434400 loss: 0.0012 lr: 0.002\n",
      "iteration: 434410 loss: 0.0014 lr: 0.002\n",
      "iteration: 434420 loss: 0.0010 lr: 0.002\n",
      "iteration: 434430 loss: 0.0009 lr: 0.002\n",
      "iteration: 434440 loss: 0.0011 lr: 0.002\n",
      "iteration: 434450 loss: 0.0007 lr: 0.002\n",
      "iteration: 434460 loss: 0.0020 lr: 0.002\n",
      "iteration: 434470 loss: 0.0010 lr: 0.002\n",
      "iteration: 434480 loss: 0.0012 lr: 0.002\n",
      "iteration: 434490 loss: 0.0009 lr: 0.002\n",
      "iteration: 434500 loss: 0.0011 lr: 0.002\n",
      "iteration: 434510 loss: 0.0012 lr: 0.002\n",
      "iteration: 434520 loss: 0.0008 lr: 0.002\n",
      "iteration: 434530 loss: 0.0019 lr: 0.002\n",
      "iteration: 434540 loss: 0.0018 lr: 0.002\n",
      "iteration: 434550 loss: 0.0018 lr: 0.002\n",
      "iteration: 434560 loss: 0.0015 lr: 0.002\n",
      "iteration: 434570 loss: 0.0016 lr: 0.002\n",
      "iteration: 434580 loss: 0.0019 lr: 0.002\n",
      "iteration: 434590 loss: 0.0014 lr: 0.002\n",
      "iteration: 434600 loss: 0.0013 lr: 0.002\n",
      "iteration: 434610 loss: 0.0019 lr: 0.002\n",
      "iteration: 434620 loss: 0.0011 lr: 0.002\n",
      "iteration: 434630 loss: 0.0013 lr: 0.002\n",
      "iteration: 434640 loss: 0.0021 lr: 0.002\n",
      "iteration: 434650 loss: 0.0012 lr: 0.002\n",
      "iteration: 434660 loss: 0.0018 lr: 0.002\n",
      "iteration: 434670 loss: 0.0008 lr: 0.002\n",
      "iteration: 434680 loss: 0.0009 lr: 0.002\n",
      "iteration: 434690 loss: 0.0013 lr: 0.002\n",
      "iteration: 434700 loss: 0.0011 lr: 0.002\n",
      "iteration: 434710 loss: 0.0009 lr: 0.002\n",
      "iteration: 434720 loss: 0.0008 lr: 0.002\n",
      "iteration: 434730 loss: 0.0012 lr: 0.002\n",
      "iteration: 434740 loss: 0.0013 lr: 0.002\n",
      "iteration: 434750 loss: 0.0015 lr: 0.002\n",
      "iteration: 434760 loss: 0.0009 lr: 0.002\n",
      "iteration: 434770 loss: 0.0015 lr: 0.002\n",
      "iteration: 434780 loss: 0.0013 lr: 0.002\n",
      "iteration: 434790 loss: 0.0013 lr: 0.002\n",
      "iteration: 434800 loss: 0.0009 lr: 0.002\n",
      "iteration: 434810 loss: 0.0019 lr: 0.002\n",
      "iteration: 434820 loss: 0.0015 lr: 0.002\n",
      "iteration: 434830 loss: 0.0018 lr: 0.002\n",
      "iteration: 434840 loss: 0.0009 lr: 0.002\n",
      "iteration: 434850 loss: 0.0011 lr: 0.002\n",
      "iteration: 434860 loss: 0.0010 lr: 0.002\n",
      "iteration: 434870 loss: 0.0009 lr: 0.002\n",
      "iteration: 434880 loss: 0.0011 lr: 0.002\n",
      "iteration: 434890 loss: 0.0009 lr: 0.002\n",
      "iteration: 434900 loss: 0.0012 lr: 0.002\n",
      "iteration: 434910 loss: 0.0014 lr: 0.002\n",
      "iteration: 434920 loss: 0.0013 lr: 0.002\n",
      "iteration: 434930 loss: 0.0022 lr: 0.002\n",
      "iteration: 434940 loss: 0.0013 lr: 0.002\n",
      "iteration: 434950 loss: 0.0018 lr: 0.002\n",
      "iteration: 434960 loss: 0.0009 lr: 0.002\n",
      "iteration: 434970 loss: 0.0010 lr: 0.002\n",
      "iteration: 434980 loss: 0.0011 lr: 0.002\n",
      "iteration: 434990 loss: 0.0009 lr: 0.002\n",
      "iteration: 435000 loss: 0.0013 lr: 0.002\n",
      "iteration: 435010 loss: 0.0012 lr: 0.002\n",
      "iteration: 435020 loss: 0.0014 lr: 0.002\n",
      "iteration: 435030 loss: 0.0010 lr: 0.002\n",
      "iteration: 435040 loss: 0.0013 lr: 0.002\n",
      "iteration: 435050 loss: 0.0012 lr: 0.002\n",
      "iteration: 435060 loss: 0.0010 lr: 0.002\n",
      "iteration: 435070 loss: 0.0018 lr: 0.002\n",
      "iteration: 435080 loss: 0.0015 lr: 0.002\n",
      "iteration: 435090 loss: 0.0012 lr: 0.002\n",
      "iteration: 435100 loss: 0.0009 lr: 0.002\n",
      "iteration: 435110 loss: 0.0016 lr: 0.002\n",
      "iteration: 435120 loss: 0.0011 lr: 0.002\n",
      "iteration: 435130 loss: 0.0013 lr: 0.002\n",
      "iteration: 435140 loss: 0.0012 lr: 0.002\n",
      "iteration: 435150 loss: 0.0012 lr: 0.002\n",
      "iteration: 435160 loss: 0.0015 lr: 0.002\n",
      "iteration: 435170 loss: 0.0015 lr: 0.002\n",
      "iteration: 435180 loss: 0.0008 lr: 0.002\n",
      "iteration: 435190 loss: 0.0012 lr: 0.002\n",
      "iteration: 435200 loss: 0.0021 lr: 0.002\n",
      "iteration: 435210 loss: 0.0016 lr: 0.002\n",
      "iteration: 435220 loss: 0.0012 lr: 0.002\n",
      "iteration: 435230 loss: 0.0013 lr: 0.002\n",
      "iteration: 435240 loss: 0.0011 lr: 0.002\n",
      "iteration: 435250 loss: 0.0012 lr: 0.002\n",
      "iteration: 435260 loss: 0.0010 lr: 0.002\n",
      "iteration: 435270 loss: 0.0014 lr: 0.002\n",
      "iteration: 435280 loss: 0.0014 lr: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 435290 loss: 0.0012 lr: 0.002\n",
      "iteration: 435300 loss: 0.0012 lr: 0.002\n",
      "iteration: 435310 loss: 0.0013 lr: 0.002\n",
      "iteration: 435320 loss: 0.0012 lr: 0.002\n",
      "iteration: 435330 loss: 0.0015 lr: 0.002\n",
      "iteration: 435340 loss: 0.0013 lr: 0.002\n",
      "iteration: 435350 loss: 0.0012 lr: 0.002\n",
      "iteration: 435360 loss: 0.0012 lr: 0.002\n",
      "iteration: 435370 loss: 0.0013 lr: 0.002\n",
      "iteration: 435380 loss: 0.0016 lr: 0.002\n",
      "iteration: 435390 loss: 0.0009 lr: 0.002\n",
      "iteration: 435400 loss: 0.0013 lr: 0.002\n",
      "iteration: 435410 loss: 0.0013 lr: 0.002\n",
      "iteration: 435420 loss: 0.0015 lr: 0.002\n",
      "iteration: 435430 loss: 0.0014 lr: 0.002\n",
      "iteration: 435440 loss: 0.0010 lr: 0.002\n",
      "iteration: 435450 loss: 0.0009 lr: 0.002\n",
      "iteration: 435460 loss: 0.0012 lr: 0.002\n",
      "iteration: 435470 loss: 0.0010 lr: 0.002\n",
      "iteration: 435480 loss: 0.0015 lr: 0.002\n",
      "iteration: 435490 loss: 0.0013 lr: 0.002\n",
      "iteration: 435500 loss: 0.0010 lr: 0.002\n",
      "iteration: 435510 loss: 0.0012 lr: 0.002\n",
      "iteration: 435520 loss: 0.0012 lr: 0.002\n",
      "iteration: 435530 loss: 0.0014 lr: 0.002\n",
      "iteration: 435540 loss: 0.0023 lr: 0.002\n",
      "iteration: 435550 loss: 0.0014 lr: 0.002\n",
      "iteration: 435560 loss: 0.0009 lr: 0.002\n",
      "iteration: 435570 loss: 0.0012 lr: 0.002\n",
      "iteration: 435580 loss: 0.0014 lr: 0.002\n",
      "iteration: 435590 loss: 0.0014 lr: 0.002\n",
      "iteration: 435600 loss: 0.0011 lr: 0.002\n",
      "iteration: 435610 loss: 0.0012 lr: 0.002\n",
      "iteration: 435620 loss: 0.0015 lr: 0.002\n",
      "iteration: 435630 loss: 0.0010 lr: 0.002\n",
      "iteration: 435640 loss: 0.0015 lr: 0.002\n",
      "iteration: 435650 loss: 0.0017 lr: 0.002\n",
      "iteration: 435660 loss: 0.0011 lr: 0.002\n",
      "iteration: 435670 loss: 0.0014 lr: 0.002\n",
      "iteration: 435680 loss: 0.0013 lr: 0.002\n",
      "iteration: 435690 loss: 0.0012 lr: 0.002\n",
      "iteration: 435700 loss: 0.0014 lr: 0.002\n",
      "iteration: 435710 loss: 0.0021 lr: 0.002\n",
      "iteration: 435720 loss: 0.0011 lr: 0.002\n",
      "iteration: 435730 loss: 0.0011 lr: 0.002\n",
      "iteration: 435740 loss: 0.0008 lr: 0.002\n",
      "iteration: 435750 loss: 0.0013 lr: 0.002\n",
      "iteration: 435760 loss: 0.0017 lr: 0.002\n",
      "iteration: 435770 loss: 0.0012 lr: 0.002\n",
      "iteration: 435780 loss: 0.0012 lr: 0.002\n",
      "iteration: 435790 loss: 0.0014 lr: 0.002\n",
      "iteration: 435800 loss: 0.0014 lr: 0.002\n",
      "iteration: 435810 loss: 0.0012 lr: 0.002\n",
      "iteration: 435820 loss: 0.0011 lr: 0.002\n",
      "iteration: 435830 loss: 0.0008 lr: 0.002\n",
      "iteration: 435840 loss: 0.0012 lr: 0.002\n",
      "iteration: 435850 loss: 0.0015 lr: 0.002\n",
      "iteration: 435860 loss: 0.0014 lr: 0.002\n",
      "iteration: 435870 loss: 0.0014 lr: 0.002\n",
      "iteration: 435880 loss: 0.0015 lr: 0.002\n",
      "iteration: 435890 loss: 0.0016 lr: 0.002\n",
      "iteration: 435900 loss: 0.0015 lr: 0.002\n",
      "iteration: 435910 loss: 0.0011 lr: 0.002\n",
      "iteration: 435920 loss: 0.0009 lr: 0.002\n",
      "iteration: 435930 loss: 0.0012 lr: 0.002\n",
      "iteration: 435940 loss: 0.0011 lr: 0.002\n",
      "iteration: 435950 loss: 0.0011 lr: 0.002\n",
      "iteration: 435960 loss: 0.0010 lr: 0.002\n",
      "iteration: 435970 loss: 0.0016 lr: 0.002\n",
      "iteration: 435980 loss: 0.0014 lr: 0.002\n",
      "iteration: 435990 loss: 0.0008 lr: 0.002\n",
      "iteration: 436000 loss: 0.0009 lr: 0.002\n",
      "iteration: 436010 loss: 0.0015 lr: 0.002\n",
      "iteration: 436020 loss: 0.0012 lr: 0.002\n",
      "iteration: 436030 loss: 0.0013 lr: 0.002\n",
      "iteration: 436040 loss: 0.0013 lr: 0.002\n",
      "iteration: 436050 loss: 0.0013 lr: 0.002\n",
      "iteration: 436060 loss: 0.0008 lr: 0.002\n",
      "iteration: 436070 loss: 0.0016 lr: 0.002\n",
      "iteration: 436080 loss: 0.0008 lr: 0.002\n",
      "iteration: 436090 loss: 0.0027 lr: 0.002\n",
      "iteration: 436100 loss: 0.0010 lr: 0.002\n",
      "iteration: 436110 loss: 0.0015 lr: 0.002\n",
      "iteration: 436120 loss: 0.0013 lr: 0.002\n",
      "iteration: 436130 loss: 0.0007 lr: 0.002\n",
      "iteration: 436140 loss: 0.0015 lr: 0.002\n",
      "iteration: 436150 loss: 0.0013 lr: 0.002\n",
      "iteration: 436160 loss: 0.0013 lr: 0.002\n",
      "iteration: 436170 loss: 0.0008 lr: 0.002\n",
      "iteration: 436180 loss: 0.0010 lr: 0.002\n",
      "iteration: 436190 loss: 0.0012 lr: 0.002\n",
      "iteration: 436200 loss: 0.0021 lr: 0.002\n",
      "iteration: 436210 loss: 0.0017 lr: 0.002\n",
      "iteration: 436220 loss: 0.0011 lr: 0.002\n",
      "iteration: 436230 loss: 0.0012 lr: 0.002\n",
      "iteration: 436240 loss: 0.0010 lr: 0.002\n",
      "iteration: 436250 loss: 0.0010 lr: 0.002\n",
      "iteration: 436260 loss: 0.0010 lr: 0.002\n",
      "iteration: 436270 loss: 0.0014 lr: 0.002\n",
      "iteration: 436280 loss: 0.0025 lr: 0.002\n",
      "iteration: 436290 loss: 0.0013 lr: 0.002\n",
      "iteration: 436300 loss: 0.0015 lr: 0.002\n",
      "iteration: 436310 loss: 0.0015 lr: 0.002\n",
      "iteration: 436320 loss: 0.0016 lr: 0.002\n",
      "iteration: 436330 loss: 0.0013 lr: 0.002\n",
      "iteration: 436340 loss: 0.0017 lr: 0.002\n",
      "iteration: 436350 loss: 0.0013 lr: 0.002\n",
      "iteration: 436360 loss: 0.0014 lr: 0.002\n",
      "iteration: 436370 loss: 0.0012 lr: 0.002\n",
      "iteration: 436380 loss: 0.0017 lr: 0.002\n",
      "iteration: 436390 loss: 0.0011 lr: 0.002\n",
      "iteration: 436400 loss: 0.0013 lr: 0.002\n",
      "iteration: 436410 loss: 0.0010 lr: 0.002\n",
      "iteration: 436420 loss: 0.0015 lr: 0.002\n",
      "iteration: 436430 loss: 0.0011 lr: 0.002\n",
      "iteration: 436440 loss: 0.0009 lr: 0.002\n",
      "iteration: 436450 loss: 0.0025 lr: 0.002\n",
      "iteration: 436460 loss: 0.0009 lr: 0.002\n",
      "iteration: 436470 loss: 0.0013 lr: 0.002\n",
      "iteration: 436480 loss: 0.0013 lr: 0.002\n",
      "iteration: 436490 loss: 0.0009 lr: 0.002\n",
      "iteration: 436500 loss: 0.0016 lr: 0.002\n",
      "iteration: 436510 loss: 0.0011 lr: 0.002\n",
      "iteration: 436520 loss: 0.0012 lr: 0.002\n",
      "iteration: 436530 loss: 0.0010 lr: 0.002\n",
      "iteration: 436540 loss: 0.0009 lr: 0.002\n",
      "iteration: 436550 loss: 0.0012 lr: 0.002\n",
      "iteration: 436560 loss: 0.0012 lr: 0.002\n",
      "iteration: 436570 loss: 0.0009 lr: 0.002\n",
      "iteration: 436580 loss: 0.0010 lr: 0.002\n",
      "iteration: 436590 loss: 0.0011 lr: 0.002\n",
      "iteration: 436600 loss: 0.0014 lr: 0.002\n",
      "iteration: 436610 loss: 0.0011 lr: 0.002\n",
      "iteration: 436620 loss: 0.0013 lr: 0.002\n",
      "iteration: 436630 loss: 0.0013 lr: 0.002\n",
      "iteration: 436640 loss: 0.0012 lr: 0.002\n",
      "iteration: 436650 loss: 0.0009 lr: 0.002\n",
      "iteration: 436660 loss: 0.0012 lr: 0.002\n",
      "iteration: 436670 loss: 0.0014 lr: 0.002\n",
      "iteration: 436680 loss: 0.0015 lr: 0.002\n",
      "iteration: 436690 loss: 0.0012 lr: 0.002\n",
      "iteration: 436700 loss: 0.0013 lr: 0.002\n",
      "iteration: 436710 loss: 0.0016 lr: 0.002\n",
      "iteration: 436720 loss: 0.0010 lr: 0.002\n",
      "iteration: 436730 loss: 0.0011 lr: 0.002\n",
      "iteration: 436740 loss: 0.0011 lr: 0.002\n",
      "iteration: 436750 loss: 0.0011 lr: 0.002\n",
      "iteration: 436760 loss: 0.0010 lr: 0.002\n",
      "iteration: 436770 loss: 0.0012 lr: 0.002\n",
      "iteration: 436780 loss: 0.0012 lr: 0.002\n",
      "iteration: 436790 loss: 0.0010 lr: 0.002\n",
      "iteration: 436800 loss: 0.0012 lr: 0.002\n",
      "iteration: 436810 loss: 0.0019 lr: 0.002\n",
      "iteration: 436820 loss: 0.0012 lr: 0.002\n",
      "iteration: 436830 loss: 0.0018 lr: 0.002\n",
      "iteration: 436840 loss: 0.0025 lr: 0.002\n",
      "iteration: 436850 loss: 0.0007 lr: 0.002\n",
      "iteration: 436860 loss: 0.0014 lr: 0.002\n",
      "iteration: 436870 loss: 0.0014 lr: 0.002\n",
      "iteration: 436880 loss: 0.0011 lr: 0.002\n",
      "iteration: 436890 loss: 0.0017 lr: 0.002\n",
      "iteration: 436900 loss: 0.0010 lr: 0.002\n",
      "iteration: 436910 loss: 0.0011 lr: 0.002\n",
      "iteration: 436920 loss: 0.0016 lr: 0.002\n",
      "iteration: 436930 loss: 0.0017 lr: 0.002\n",
      "iteration: 436940 loss: 0.0010 lr: 0.002\n",
      "iteration: 436950 loss: 0.0011 lr: 0.002\n",
      "iteration: 436960 loss: 0.0013 lr: 0.002\n",
      "iteration: 436970 loss: 0.0018 lr: 0.002\n",
      "iteration: 436980 loss: 0.0011 lr: 0.002\n",
      "iteration: 436990 loss: 0.0020 lr: 0.002\n",
      "iteration: 437000 loss: 0.0009 lr: 0.002\n",
      "iteration: 437010 loss: 0.0019 lr: 0.002\n",
      "iteration: 437020 loss: 0.0015 lr: 0.002\n",
      "iteration: 437030 loss: 0.0010 lr: 0.002\n",
      "iteration: 437040 loss: 0.0013 lr: 0.002\n",
      "iteration: 437050 loss: 0.0012 lr: 0.002\n",
      "iteration: 437060 loss: 0.0011 lr: 0.002\n",
      "iteration: 437070 loss: 0.0010 lr: 0.002\n",
      "iteration: 437080 loss: 0.0017 lr: 0.002\n",
      "iteration: 437090 loss: 0.0013 lr: 0.002\n",
      "iteration: 437100 loss: 0.0009 lr: 0.002\n",
      "iteration: 437110 loss: 0.0015 lr: 0.002\n",
      "iteration: 437120 loss: 0.0014 lr: 0.002\n",
      "iteration: 437130 loss: 0.0014 lr: 0.002\n",
      "iteration: 437140 loss: 0.0010 lr: 0.002\n",
      "iteration: 437150 loss: 0.0013 lr: 0.002\n",
      "iteration: 437160 loss: 0.0010 lr: 0.002\n",
      "iteration: 437170 loss: 0.0017 lr: 0.002\n",
      "iteration: 437180 loss: 0.0013 lr: 0.002\n",
      "iteration: 437190 loss: 0.0011 lr: 0.002\n",
      "iteration: 437200 loss: 0.0015 lr: 0.002\n",
      "iteration: 437210 loss: 0.0010 lr: 0.002\n",
      "iteration: 437220 loss: 0.0014 lr: 0.002\n",
      "iteration: 437230 loss: 0.0009 lr: 0.002\n",
      "iteration: 437240 loss: 0.0016 lr: 0.002\n",
      "iteration: 437250 loss: 0.0016 lr: 0.002\n",
      "iteration: 437260 loss: 0.0012 lr: 0.002\n",
      "iteration: 437270 loss: 0.0020 lr: 0.002\n",
      "iteration: 437280 loss: 0.0014 lr: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 437290 loss: 0.0010 lr: 0.002\n",
      "iteration: 437300 loss: 0.0013 lr: 0.002\n",
      "iteration: 437310 loss: 0.0012 lr: 0.002\n",
      "iteration: 437320 loss: 0.0020 lr: 0.002\n",
      "iteration: 437330 loss: 0.0012 lr: 0.002\n",
      "iteration: 437340 loss: 0.0020 lr: 0.002\n",
      "iteration: 437350 loss: 0.0011 lr: 0.002\n",
      "iteration: 437360 loss: 0.0014 lr: 0.002\n",
      "iteration: 437370 loss: 0.0010 lr: 0.002\n",
      "iteration: 437380 loss: 0.0013 lr: 0.002\n",
      "iteration: 437390 loss: 0.0009 lr: 0.002\n",
      "iteration: 437400 loss: 0.0009 lr: 0.002\n",
      "iteration: 437410 loss: 0.0012 lr: 0.002\n",
      "iteration: 437420 loss: 0.0013 lr: 0.002\n",
      "iteration: 437430 loss: 0.0012 lr: 0.002\n",
      "iteration: 437440 loss: 0.0014 lr: 0.002\n",
      "iteration: 437450 loss: 0.0009 lr: 0.002\n",
      "iteration: 437460 loss: 0.0016 lr: 0.002\n",
      "iteration: 437470 loss: 0.0011 lr: 0.002\n",
      "iteration: 437480 loss: 0.0017 lr: 0.002\n",
      "iteration: 437490 loss: 0.0008 lr: 0.002\n",
      "iteration: 437500 loss: 0.0012 lr: 0.002\n",
      "iteration: 437510 loss: 0.0014 lr: 0.002\n",
      "iteration: 437520 loss: 0.0009 lr: 0.002\n",
      "iteration: 437530 loss: 0.0009 lr: 0.002\n",
      "iteration: 437540 loss: 0.0009 lr: 0.002\n",
      "iteration: 437550 loss: 0.0009 lr: 0.002\n",
      "iteration: 437560 loss: 0.0009 lr: 0.002\n",
      "iteration: 437570 loss: 0.0010 lr: 0.002\n",
      "iteration: 437580 loss: 0.0013 lr: 0.002\n",
      "iteration: 437590 loss: 0.0015 lr: 0.002\n",
      "iteration: 437600 loss: 0.0009 lr: 0.002\n",
      "iteration: 437610 loss: 0.0011 lr: 0.002\n",
      "iteration: 437620 loss: 0.0011 lr: 0.002\n",
      "iteration: 437630 loss: 0.0013 lr: 0.002\n",
      "iteration: 437640 loss: 0.0011 lr: 0.002\n",
      "iteration: 437650 loss: 0.0012 lr: 0.002\n",
      "iteration: 437660 loss: 0.0009 lr: 0.002\n",
      "iteration: 437670 loss: 0.0012 lr: 0.002\n",
      "iteration: 437680 loss: 0.0019 lr: 0.002\n",
      "iteration: 437690 loss: 0.0011 lr: 0.002\n",
      "iteration: 437700 loss: 0.0011 lr: 0.002\n",
      "iteration: 437710 loss: 0.0009 lr: 0.002\n",
      "iteration: 437720 loss: 0.0010 lr: 0.002\n",
      "iteration: 437730 loss: 0.0011 lr: 0.002\n",
      "iteration: 437740 loss: 0.0017 lr: 0.002\n",
      "iteration: 437750 loss: 0.0009 lr: 0.002\n",
      "iteration: 437760 loss: 0.0011 lr: 0.002\n",
      "iteration: 437770 loss: 0.0013 lr: 0.002\n",
      "iteration: 437780 loss: 0.0013 lr: 0.002\n",
      "iteration: 437790 loss: 0.0013 lr: 0.002\n",
      "iteration: 437800 loss: 0.0014 lr: 0.002\n",
      "iteration: 437810 loss: 0.0007 lr: 0.002\n",
      "iteration: 437820 loss: 0.0015 lr: 0.002\n",
      "iteration: 437830 loss: 0.0014 lr: 0.002\n",
      "iteration: 437840 loss: 0.0014 lr: 0.002\n",
      "iteration: 437850 loss: 0.0011 lr: 0.002\n",
      "iteration: 437860 loss: 0.0013 lr: 0.002\n",
      "iteration: 437870 loss: 0.0010 lr: 0.002\n",
      "iteration: 437880 loss: 0.0010 lr: 0.002\n",
      "iteration: 437890 loss: 0.0012 lr: 0.002\n",
      "iteration: 437900 loss: 0.0011 lr: 0.002\n",
      "iteration: 437910 loss: 0.0011 lr: 0.002\n",
      "iteration: 437920 loss: 0.0012 lr: 0.002\n",
      "iteration: 437930 loss: 0.0015 lr: 0.002\n",
      "iteration: 437940 loss: 0.0013 lr: 0.002\n",
      "iteration: 437950 loss: 0.0011 lr: 0.002\n",
      "iteration: 437960 loss: 0.0012 lr: 0.002\n",
      "iteration: 437970 loss: 0.0013 lr: 0.002\n",
      "iteration: 437980 loss: 0.0015 lr: 0.002\n",
      "iteration: 437990 loss: 0.0013 lr: 0.002\n",
      "iteration: 438000 loss: 0.0014 lr: 0.002\n",
      "iteration: 438010 loss: 0.0013 lr: 0.002\n",
      "iteration: 438020 loss: 0.0013 lr: 0.002\n",
      "iteration: 438030 loss: 0.0014 lr: 0.002\n",
      "iteration: 438040 loss: 0.0011 lr: 0.002\n",
      "iteration: 438050 loss: 0.0014 lr: 0.002\n",
      "iteration: 438060 loss: 0.0012 lr: 0.002\n",
      "iteration: 438070 loss: 0.0010 lr: 0.002\n",
      "iteration: 438080 loss: 0.0011 lr: 0.002\n",
      "iteration: 438090 loss: 0.0012 lr: 0.002\n",
      "iteration: 438100 loss: 0.0013 lr: 0.002\n",
      "iteration: 438110 loss: 0.0016 lr: 0.002\n",
      "iteration: 438120 loss: 0.0012 lr: 0.002\n",
      "iteration: 438130 loss: 0.0010 lr: 0.002\n",
      "iteration: 438140 loss: 0.0013 lr: 0.002\n",
      "iteration: 438150 loss: 0.0011 lr: 0.002\n",
      "iteration: 438160 loss: 0.0008 lr: 0.002\n",
      "iteration: 438170 loss: 0.0012 lr: 0.002\n",
      "iteration: 438180 loss: 0.0011 lr: 0.002\n",
      "iteration: 438190 loss: 0.0010 lr: 0.002\n",
      "iteration: 438200 loss: 0.0012 lr: 0.002\n",
      "iteration: 438210 loss: 0.0008 lr: 0.002\n",
      "iteration: 438220 loss: 0.0021 lr: 0.002\n",
      "iteration: 438230 loss: 0.0016 lr: 0.002\n",
      "iteration: 438240 loss: 0.0014 lr: 0.002\n",
      "iteration: 438250 loss: 0.0019 lr: 0.002\n",
      "iteration: 438260 loss: 0.0015 lr: 0.002\n",
      "iteration: 438270 loss: 0.0013 lr: 0.002\n",
      "iteration: 438280 loss: 0.0017 lr: 0.002\n",
      "iteration: 438290 loss: 0.0014 lr: 0.002\n",
      "iteration: 438300 loss: 0.0015 lr: 0.002\n",
      "iteration: 438310 loss: 0.0012 lr: 0.002\n",
      "iteration: 438320 loss: 0.0015 lr: 0.002\n",
      "iteration: 438330 loss: 0.0009 lr: 0.002\n",
      "iteration: 438340 loss: 0.0019 lr: 0.002\n",
      "iteration: 438350 loss: 0.0010 lr: 0.002\n",
      "iteration: 438360 loss: 0.0010 lr: 0.002\n",
      "iteration: 438370 loss: 0.0014 lr: 0.002\n",
      "iteration: 438380 loss: 0.0011 lr: 0.002\n",
      "iteration: 438390 loss: 0.0014 lr: 0.002\n",
      "iteration: 438400 loss: 0.0008 lr: 0.002\n",
      "iteration: 438410 loss: 0.0014 lr: 0.002\n",
      "iteration: 438420 loss: 0.0011 lr: 0.002\n",
      "iteration: 438430 loss: 0.0012 lr: 0.002\n",
      "iteration: 438440 loss: 0.0013 lr: 0.002\n",
      "iteration: 438450 loss: 0.0012 lr: 0.002\n",
      "iteration: 438460 loss: 0.0014 lr: 0.002\n",
      "iteration: 438470 loss: 0.0009 lr: 0.002\n",
      "iteration: 438480 loss: 0.0013 lr: 0.002\n",
      "iteration: 438490 loss: 0.0021 lr: 0.002\n",
      "iteration: 438500 loss: 0.0013 lr: 0.002\n",
      "iteration: 438510 loss: 0.0012 lr: 0.002\n",
      "iteration: 438520 loss: 0.0015 lr: 0.002\n",
      "iteration: 438530 loss: 0.0010 lr: 0.002\n",
      "iteration: 438540 loss: 0.0011 lr: 0.002\n",
      "iteration: 438550 loss: 0.0009 lr: 0.002\n",
      "iteration: 438560 loss: 0.0013 lr: 0.002\n",
      "iteration: 438570 loss: 0.0013 lr: 0.002\n",
      "iteration: 438580 loss: 0.0011 lr: 0.002\n",
      "iteration: 438590 loss: 0.0012 lr: 0.002\n",
      "iteration: 438600 loss: 0.0020 lr: 0.002\n",
      "iteration: 438610 loss: 0.0008 lr: 0.002\n",
      "iteration: 438620 loss: 0.0012 lr: 0.002\n",
      "iteration: 438630 loss: 0.0013 lr: 0.002\n",
      "iteration: 438640 loss: 0.0013 lr: 0.002\n",
      "iteration: 438650 loss: 0.0014 lr: 0.002\n",
      "iteration: 438660 loss: 0.0013 lr: 0.002\n",
      "iteration: 438670 loss: 0.0014 lr: 0.002\n",
      "iteration: 438680 loss: 0.0013 lr: 0.002\n",
      "iteration: 438690 loss: 0.0015 lr: 0.002\n",
      "iteration: 438700 loss: 0.0012 lr: 0.002\n",
      "iteration: 438710 loss: 0.0010 lr: 0.002\n",
      "iteration: 438720 loss: 0.0009 lr: 0.002\n",
      "iteration: 438730 loss: 0.0008 lr: 0.002\n",
      "iteration: 438740 loss: 0.0013 lr: 0.002\n",
      "iteration: 438750 loss: 0.0012 lr: 0.002\n",
      "iteration: 438760 loss: 0.0014 lr: 0.002\n",
      "iteration: 438770 loss: 0.0013 lr: 0.002\n",
      "iteration: 438780 loss: 0.0023 lr: 0.002\n",
      "iteration: 438790 loss: 0.0017 lr: 0.002\n",
      "iteration: 438800 loss: 0.0011 lr: 0.002\n",
      "iteration: 438810 loss: 0.0013 lr: 0.002\n",
      "iteration: 438820 loss: 0.0011 lr: 0.002\n",
      "iteration: 438830 loss: 0.0010 lr: 0.002\n",
      "iteration: 438840 loss: 0.0010 lr: 0.002\n",
      "iteration: 438850 loss: 0.0015 lr: 0.002\n",
      "iteration: 438860 loss: 0.0012 lr: 0.002\n",
      "iteration: 438870 loss: 0.0011 lr: 0.002\n",
      "iteration: 438880 loss: 0.0011 lr: 0.002\n",
      "iteration: 438890 loss: 0.0012 lr: 0.002\n",
      "iteration: 438900 loss: 0.0016 lr: 0.002\n",
      "iteration: 438910 loss: 0.0012 lr: 0.002\n",
      "iteration: 438920 loss: 0.0012 lr: 0.002\n",
      "iteration: 438930 loss: 0.0015 lr: 0.002\n",
      "iteration: 438940 loss: 0.0011 lr: 0.002\n",
      "iteration: 438950 loss: 0.0012 lr: 0.002\n",
      "iteration: 438960 loss: 0.0010 lr: 0.002\n",
      "iteration: 438970 loss: 0.0014 lr: 0.002\n",
      "iteration: 438980 loss: 0.0017 lr: 0.002\n",
      "iteration: 438990 loss: 0.0013 lr: 0.002\n",
      "iteration: 439000 loss: 0.0011 lr: 0.002\n",
      "iteration: 439010 loss: 0.0034 lr: 0.002\n",
      "iteration: 439020 loss: 0.0018 lr: 0.002\n",
      "iteration: 439030 loss: 0.0009 lr: 0.002\n",
      "iteration: 439040 loss: 0.0012 lr: 0.002\n",
      "iteration: 439050 loss: 0.0013 lr: 0.002\n",
      "iteration: 439060 loss: 0.0011 lr: 0.002\n",
      "iteration: 439070 loss: 0.0015 lr: 0.002\n",
      "iteration: 439080 loss: 0.0017 lr: 0.002\n",
      "iteration: 439090 loss: 0.0011 lr: 0.002\n",
      "iteration: 439100 loss: 0.0015 lr: 0.002\n",
      "iteration: 439110 loss: 0.0019 lr: 0.002\n",
      "iteration: 439120 loss: 0.0008 lr: 0.002\n",
      "iteration: 439130 loss: 0.0012 lr: 0.002\n",
      "iteration: 439140 loss: 0.0011 lr: 0.002\n",
      "iteration: 439150 loss: 0.0019 lr: 0.002\n",
      "iteration: 439160 loss: 0.0014 lr: 0.002\n",
      "iteration: 439170 loss: 0.0014 lr: 0.002\n",
      "iteration: 439180 loss: 0.0012 lr: 0.002\n",
      "iteration: 439190 loss: 0.0019 lr: 0.002\n",
      "iteration: 439200 loss: 0.0012 lr: 0.002\n",
      "iteration: 439210 loss: 0.0014 lr: 0.002\n",
      "iteration: 439220 loss: 0.0010 lr: 0.002\n",
      "iteration: 439230 loss: 0.0014 lr: 0.002\n",
      "iteration: 439240 loss: 0.0014 lr: 0.002\n",
      "iteration: 439250 loss: 0.0014 lr: 0.002\n",
      "iteration: 439260 loss: 0.0013 lr: 0.002\n",
      "iteration: 439270 loss: 0.0008 lr: 0.002\n",
      "iteration: 439280 loss: 0.0010 lr: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 439290 loss: 0.0011 lr: 0.002\n",
      "iteration: 439300 loss: 0.0017 lr: 0.002\n",
      "iteration: 439310 loss: 0.0016 lr: 0.002\n",
      "iteration: 439320 loss: 0.0011 lr: 0.002\n",
      "iteration: 439330 loss: 0.0012 lr: 0.002\n",
      "iteration: 439340 loss: 0.0019 lr: 0.002\n",
      "iteration: 439350 loss: 0.0010 lr: 0.002\n",
      "iteration: 439360 loss: 0.0012 lr: 0.002\n",
      "iteration: 439370 loss: 0.0017 lr: 0.002\n",
      "iteration: 439380 loss: 0.0019 lr: 0.002\n",
      "iteration: 439390 loss: 0.0011 lr: 0.002\n",
      "iteration: 439400 loss: 0.0014 lr: 0.002\n",
      "iteration: 439410 loss: 0.0008 lr: 0.002\n",
      "iteration: 439420 loss: 0.0011 lr: 0.002\n",
      "iteration: 439430 loss: 0.0014 lr: 0.002\n",
      "iteration: 439440 loss: 0.0011 lr: 0.002\n",
      "iteration: 439450 loss: 0.0011 lr: 0.002\n",
      "iteration: 439460 loss: 0.0012 lr: 0.002\n",
      "iteration: 439470 loss: 0.0016 lr: 0.002\n",
      "iteration: 439480 loss: 0.0009 lr: 0.002\n",
      "iteration: 439490 loss: 0.0012 lr: 0.002\n",
      "iteration: 439500 loss: 0.0008 lr: 0.002\n",
      "iteration: 439510 loss: 0.0016 lr: 0.002\n",
      "iteration: 439520 loss: 0.0009 lr: 0.002\n",
      "iteration: 439530 loss: 0.0009 lr: 0.002\n",
      "iteration: 439540 loss: 0.0009 lr: 0.002\n",
      "iteration: 439550 loss: 0.0014 lr: 0.002\n",
      "iteration: 439560 loss: 0.0014 lr: 0.002\n",
      "iteration: 439570 loss: 0.0012 lr: 0.002\n",
      "iteration: 439580 loss: 0.0017 lr: 0.002\n",
      "iteration: 439590 loss: 0.0016 lr: 0.002\n",
      "iteration: 439600 loss: 0.0017 lr: 0.002\n",
      "iteration: 439610 loss: 0.0008 lr: 0.002\n",
      "iteration: 439620 loss: 0.0017 lr: 0.002\n",
      "iteration: 439630 loss: 0.0009 lr: 0.002\n",
      "iteration: 439640 loss: 0.0010 lr: 0.002\n",
      "iteration: 439650 loss: 0.0012 lr: 0.002\n",
      "iteration: 439660 loss: 0.0013 lr: 0.002\n",
      "iteration: 439670 loss: 0.0010 lr: 0.002\n",
      "iteration: 439680 loss: 0.0008 lr: 0.002\n",
      "iteration: 439690 loss: 0.0011 lr: 0.002\n",
      "iteration: 439700 loss: 0.0013 lr: 0.002\n",
      "iteration: 439710 loss: 0.0013 lr: 0.002\n",
      "iteration: 439720 loss: 0.0012 lr: 0.002\n",
      "iteration: 439730 loss: 0.0013 lr: 0.002\n",
      "iteration: 439740 loss: 0.0011 lr: 0.002\n",
      "iteration: 439750 loss: 0.0012 lr: 0.002\n",
      "iteration: 439760 loss: 0.0010 lr: 0.002\n",
      "iteration: 439770 loss: 0.0017 lr: 0.002\n",
      "iteration: 439780 loss: 0.0023 lr: 0.002\n",
      "iteration: 439790 loss: 0.0011 lr: 0.002\n",
      "iteration: 439800 loss: 0.0018 lr: 0.002\n",
      "iteration: 439810 loss: 0.0013 lr: 0.002\n",
      "iteration: 439820 loss: 0.0009 lr: 0.002\n",
      "iteration: 439830 loss: 0.0018 lr: 0.002\n",
      "iteration: 439840 loss: 0.0012 lr: 0.002\n",
      "iteration: 439850 loss: 0.0013 lr: 0.002\n",
      "iteration: 439860 loss: 0.0014 lr: 0.002\n",
      "iteration: 439870 loss: 0.0015 lr: 0.002\n",
      "iteration: 439880 loss: 0.0012 lr: 0.002\n",
      "iteration: 439890 loss: 0.0011 lr: 0.002\n",
      "iteration: 439900 loss: 0.0009 lr: 0.002\n",
      "iteration: 439910 loss: 0.0022 lr: 0.002\n",
      "iteration: 439920 loss: 0.0010 lr: 0.002\n",
      "iteration: 439930 loss: 0.0012 lr: 0.002\n",
      "iteration: 439940 loss: 0.0014 lr: 0.002\n",
      "iteration: 439950 loss: 0.0012 lr: 0.002\n",
      "iteration: 439960 loss: 0.0009 lr: 0.002\n",
      "iteration: 439970 loss: 0.0017 lr: 0.002\n",
      "iteration: 439980 loss: 0.0014 lr: 0.002\n",
      "iteration: 439990 loss: 0.0013 lr: 0.002\n",
      "iteration: 440000 loss: 0.0011 lr: 0.002\n",
      "iteration: 440010 loss: 0.0012 lr: 0.002\n",
      "iteration: 440020 loss: 0.0009 lr: 0.002\n",
      "iteration: 440030 loss: 0.0012 lr: 0.002\n",
      "iteration: 440040 loss: 0.0012 lr: 0.002\n",
      "iteration: 440050 loss: 0.0010 lr: 0.002\n",
      "iteration: 440060 loss: 0.0014 lr: 0.002\n",
      "iteration: 440070 loss: 0.0016 lr: 0.002\n",
      "iteration: 440080 loss: 0.0009 lr: 0.002\n",
      "iteration: 440090 loss: 0.0010 lr: 0.002\n",
      "iteration: 440100 loss: 0.0009 lr: 0.002\n",
      "iteration: 440110 loss: 0.0010 lr: 0.002\n",
      "iteration: 440120 loss: 0.0014 lr: 0.002\n",
      "iteration: 440130 loss: 0.0016 lr: 0.002\n",
      "iteration: 440140 loss: 0.0012 lr: 0.002\n",
      "iteration: 440150 loss: 0.0015 lr: 0.002\n",
      "iteration: 440160 loss: 0.0015 lr: 0.002\n",
      "iteration: 440170 loss: 0.0010 lr: 0.002\n",
      "iteration: 440180 loss: 0.0009 lr: 0.002\n",
      "iteration: 440190 loss: 0.0009 lr: 0.002\n",
      "iteration: 440200 loss: 0.0014 lr: 0.002\n",
      "iteration: 440210 loss: 0.0011 lr: 0.002\n",
      "iteration: 440220 loss: 0.0009 lr: 0.002\n",
      "iteration: 440230 loss: 0.0017 lr: 0.002\n",
      "iteration: 440240 loss: 0.0011 lr: 0.002\n",
      "iteration: 440250 loss: 0.0012 lr: 0.002\n",
      "iteration: 440260 loss: 0.0016 lr: 0.002\n",
      "iteration: 440270 loss: 0.0011 lr: 0.002\n",
      "iteration: 440280 loss: 0.0011 lr: 0.002\n",
      "iteration: 440290 loss: 0.0011 lr: 0.002\n",
      "iteration: 440300 loss: 0.0018 lr: 0.002\n",
      "iteration: 440310 loss: 0.0011 lr: 0.002\n",
      "iteration: 440320 loss: 0.0008 lr: 0.002\n",
      "iteration: 440330 loss: 0.0013 lr: 0.002\n",
      "iteration: 440340 loss: 0.0014 lr: 0.002\n",
      "iteration: 440350 loss: 0.0013 lr: 0.002\n",
      "iteration: 440360 loss: 0.0009 lr: 0.002\n",
      "iteration: 440370 loss: 0.0020 lr: 0.002\n",
      "iteration: 440380 loss: 0.0009 lr: 0.002\n",
      "iteration: 440390 loss: 0.0011 lr: 0.002\n",
      "iteration: 440400 loss: 0.0012 lr: 0.002\n",
      "iteration: 440410 loss: 0.0010 lr: 0.002\n",
      "iteration: 440420 loss: 0.0013 lr: 0.002\n",
      "iteration: 440430 loss: 0.0009 lr: 0.002\n",
      "iteration: 440440 loss: 0.0014 lr: 0.002\n",
      "iteration: 440450 loss: 0.0011 lr: 0.002\n",
      "iteration: 440460 loss: 0.0010 lr: 0.002\n",
      "iteration: 440470 loss: 0.0015 lr: 0.002\n",
      "iteration: 440480 loss: 0.0014 lr: 0.002\n",
      "iteration: 440490 loss: 0.0012 lr: 0.002\n",
      "iteration: 440500 loss: 0.0013 lr: 0.002\n",
      "iteration: 440510 loss: 0.0013 lr: 0.002\n",
      "iteration: 440520 loss: 0.0010 lr: 0.002\n",
      "iteration: 440530 loss: 0.0011 lr: 0.002\n",
      "iteration: 440540 loss: 0.0011 lr: 0.002\n",
      "iteration: 440550 loss: 0.0008 lr: 0.002\n",
      "iteration: 440560 loss: 0.0014 lr: 0.002\n",
      "iteration: 440570 loss: 0.0008 lr: 0.002\n",
      "iteration: 440580 loss: 0.0012 lr: 0.002\n",
      "iteration: 440590 loss: 0.0020 lr: 0.002\n",
      "iteration: 440600 loss: 0.0008 lr: 0.002\n",
      "iteration: 440610 loss: 0.0011 lr: 0.002\n",
      "iteration: 440620 loss: 0.0015 lr: 0.002\n",
      "iteration: 440630 loss: 0.0017 lr: 0.002\n",
      "iteration: 440640 loss: 0.0017 lr: 0.002\n",
      "iteration: 440650 loss: 0.0012 lr: 0.002\n",
      "iteration: 440660 loss: 0.0015 lr: 0.002\n",
      "iteration: 440670 loss: 0.0018 lr: 0.002\n",
      "iteration: 440680 loss: 0.0013 lr: 0.002\n",
      "iteration: 440690 loss: 0.0012 lr: 0.002\n",
      "iteration: 440700 loss: 0.0014 lr: 0.002\n",
      "iteration: 440710 loss: 0.0015 lr: 0.002\n",
      "iteration: 440720 loss: 0.0010 lr: 0.002\n",
      "iteration: 440730 loss: 0.0010 lr: 0.002\n",
      "iteration: 440740 loss: 0.0009 lr: 0.002\n",
      "iteration: 440750 loss: 0.0018 lr: 0.002\n",
      "iteration: 440760 loss: 0.0013 lr: 0.002\n",
      "iteration: 440770 loss: 0.0010 lr: 0.002\n",
      "iteration: 440780 loss: 0.0009 lr: 0.002\n",
      "iteration: 440790 loss: 0.0019 lr: 0.002\n",
      "iteration: 440800 loss: 0.0012 lr: 0.002\n",
      "iteration: 440810 loss: 0.0014 lr: 0.002\n",
      "iteration: 440820 loss: 0.0011 lr: 0.002\n",
      "iteration: 440830 loss: 0.0014 lr: 0.002\n",
      "iteration: 440840 loss: 0.0015 lr: 0.002\n",
      "iteration: 440850 loss: 0.0016 lr: 0.002\n",
      "iteration: 440860 loss: 0.0012 lr: 0.002\n",
      "iteration: 440870 loss: 0.0014 lr: 0.002\n",
      "iteration: 440880 loss: 0.0014 lr: 0.002\n",
      "iteration: 440890 loss: 0.0011 lr: 0.002\n",
      "iteration: 440900 loss: 0.0015 lr: 0.002\n",
      "iteration: 440910 loss: 0.0011 lr: 0.002\n",
      "iteration: 440920 loss: 0.0012 lr: 0.002\n",
      "iteration: 440930 loss: 0.0010 lr: 0.002\n",
      "iteration: 440940 loss: 0.0008 lr: 0.002\n",
      "iteration: 440950 loss: 0.0012 lr: 0.002\n",
      "iteration: 440960 loss: 0.0016 lr: 0.002\n",
      "iteration: 440970 loss: 0.0009 lr: 0.002\n",
      "iteration: 440980 loss: 0.0015 lr: 0.002\n",
      "iteration: 440990 loss: 0.0010 lr: 0.002\n",
      "iteration: 441000 loss: 0.0019 lr: 0.002\n",
      "iteration: 441010 loss: 0.0010 lr: 0.002\n",
      "iteration: 441020 loss: 0.0013 lr: 0.002\n",
      "iteration: 441030 loss: 0.0011 lr: 0.002\n",
      "iteration: 441040 loss: 0.0014 lr: 0.002\n",
      "iteration: 441050 loss: 0.0020 lr: 0.002\n",
      "iteration: 441060 loss: 0.0019 lr: 0.002\n",
      "iteration: 441070 loss: 0.0010 lr: 0.002\n",
      "iteration: 441080 loss: 0.0013 lr: 0.002\n",
      "iteration: 441090 loss: 0.0014 lr: 0.002\n",
      "iteration: 441100 loss: 0.0016 lr: 0.002\n",
      "iteration: 441110 loss: 0.0010 lr: 0.002\n",
      "iteration: 441120 loss: 0.0014 lr: 0.002\n",
      "iteration: 441130 loss: 0.0014 lr: 0.002\n",
      "iteration: 441140 loss: 0.0010 lr: 0.002\n",
      "iteration: 441150 loss: 0.0009 lr: 0.002\n",
      "iteration: 441160 loss: 0.0012 lr: 0.002\n",
      "iteration: 441170 loss: 0.0012 lr: 0.002\n",
      "iteration: 441180 loss: 0.0014 lr: 0.002\n",
      "iteration: 441190 loss: 0.0011 lr: 0.002\n",
      "iteration: 441200 loss: 0.0012 lr: 0.002\n",
      "iteration: 441210 loss: 0.0011 lr: 0.002\n",
      "iteration: 441220 loss: 0.0009 lr: 0.002\n",
      "iteration: 441230 loss: 0.0017 lr: 0.002\n",
      "iteration: 441240 loss: 0.0016 lr: 0.002\n",
      "iteration: 441250 loss: 0.0008 lr: 0.002\n",
      "iteration: 441260 loss: 0.0016 lr: 0.002\n",
      "iteration: 441270 loss: 0.0012 lr: 0.002\n",
      "iteration: 441280 loss: 0.0011 lr: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 441290 loss: 0.0011 lr: 0.002\n",
      "iteration: 441300 loss: 0.0009 lr: 0.002\n",
      "iteration: 441310 loss: 0.0011 lr: 0.002\n",
      "iteration: 441320 loss: 0.0011 lr: 0.002\n",
      "iteration: 441330 loss: 0.0013 lr: 0.002\n",
      "iteration: 441340 loss: 0.0013 lr: 0.002\n",
      "iteration: 441350 loss: 0.0014 lr: 0.002\n",
      "iteration: 441360 loss: 0.0009 lr: 0.002\n",
      "iteration: 441370 loss: 0.0016 lr: 0.002\n",
      "iteration: 441380 loss: 0.0010 lr: 0.002\n",
      "iteration: 441390 loss: 0.0018 lr: 0.002\n",
      "iteration: 441400 loss: 0.0016 lr: 0.002\n",
      "iteration: 441410 loss: 0.0010 lr: 0.002\n",
      "iteration: 441420 loss: 0.0010 lr: 0.002\n",
      "iteration: 441430 loss: 0.0013 lr: 0.002\n",
      "iteration: 441440 loss: 0.0014 lr: 0.002\n",
      "iteration: 441450 loss: 0.0015 lr: 0.002\n",
      "iteration: 441460 loss: 0.0010 lr: 0.002\n",
      "iteration: 441470 loss: 0.0008 lr: 0.002\n",
      "iteration: 441480 loss: 0.0013 lr: 0.002\n",
      "iteration: 441490 loss: 0.0011 lr: 0.002\n",
      "iteration: 441500 loss: 0.0010 lr: 0.002\n",
      "iteration: 441510 loss: 0.0012 lr: 0.002\n",
      "iteration: 441520 loss: 0.0013 lr: 0.002\n",
      "iteration: 441530 loss: 0.0011 lr: 0.002\n",
      "iteration: 441540 loss: 0.0010 lr: 0.002\n",
      "iteration: 441550 loss: 0.0027 lr: 0.002\n",
      "iteration: 441560 loss: 0.0014 lr: 0.002\n",
      "iteration: 441570 loss: 0.0015 lr: 0.002\n",
      "iteration: 441580 loss: 0.0011 lr: 0.002\n",
      "iteration: 441590 loss: 0.0012 lr: 0.002\n",
      "iteration: 441600 loss: 0.0012 lr: 0.002\n",
      "iteration: 441610 loss: 0.0010 lr: 0.002\n",
      "iteration: 441620 loss: 0.0010 lr: 0.002\n",
      "iteration: 441630 loss: 0.0009 lr: 0.002\n",
      "iteration: 441640 loss: 0.0011 lr: 0.002\n",
      "iteration: 441650 loss: 0.0011 lr: 0.002\n",
      "iteration: 441660 loss: 0.0015 lr: 0.002\n",
      "iteration: 441670 loss: 0.0020 lr: 0.002\n",
      "iteration: 441680 loss: 0.0010 lr: 0.002\n",
      "iteration: 441690 loss: 0.0009 lr: 0.002\n",
      "iteration: 441700 loss: 0.0012 lr: 0.002\n",
      "iteration: 441710 loss: 0.0008 lr: 0.002\n",
      "iteration: 441720 loss: 0.0014 lr: 0.002\n",
      "iteration: 441730 loss: 0.0008 lr: 0.002\n",
      "iteration: 441740 loss: 0.0013 lr: 0.002\n",
      "iteration: 441750 loss: 0.0013 lr: 0.002\n",
      "iteration: 441760 loss: 0.0012 lr: 0.002\n",
      "iteration: 441770 loss: 0.0008 lr: 0.002\n",
      "iteration: 441780 loss: 0.0014 lr: 0.002\n",
      "iteration: 441790 loss: 0.0012 lr: 0.002\n",
      "iteration: 441800 loss: 0.0014 lr: 0.002\n",
      "iteration: 441810 loss: 0.0010 lr: 0.002\n",
      "iteration: 441820 loss: 0.0016 lr: 0.002\n",
      "iteration: 441830 loss: 0.0010 lr: 0.002\n",
      "iteration: 441840 loss: 0.0030 lr: 0.002\n",
      "iteration: 441850 loss: 0.0008 lr: 0.002\n",
      "iteration: 441860 loss: 0.0011 lr: 0.002\n",
      "iteration: 441870 loss: 0.0011 lr: 0.002\n",
      "iteration: 441880 loss: 0.0014 lr: 0.002\n",
      "iteration: 441890 loss: 0.0010 lr: 0.002\n",
      "iteration: 441900 loss: 0.0014 lr: 0.002\n",
      "iteration: 441910 loss: 0.0011 lr: 0.002\n",
      "iteration: 441920 loss: 0.0011 lr: 0.002\n",
      "iteration: 441930 loss: 0.0011 lr: 0.002\n",
      "iteration: 441940 loss: 0.0017 lr: 0.002\n",
      "iteration: 441950 loss: 0.0016 lr: 0.002\n",
      "iteration: 441960 loss: 0.0016 lr: 0.002\n",
      "iteration: 441970 loss: 0.0013 lr: 0.002\n",
      "iteration: 441980 loss: 0.0013 lr: 0.002\n",
      "iteration: 441990 loss: 0.0018 lr: 0.002\n",
      "iteration: 442000 loss: 0.0015 lr: 0.002\n",
      "iteration: 442010 loss: 0.0012 lr: 0.002\n",
      "iteration: 442020 loss: 0.0012 lr: 0.002\n",
      "iteration: 442030 loss: 0.0011 lr: 0.002\n",
      "iteration: 442040 loss: 0.0014 lr: 0.002\n",
      "iteration: 442050 loss: 0.0012 lr: 0.002\n",
      "iteration: 442060 loss: 0.0010 lr: 0.002\n",
      "iteration: 442070 loss: 0.0010 lr: 0.002\n",
      "iteration: 442080 loss: 0.0014 lr: 0.002\n",
      "iteration: 442090 loss: 0.0010 lr: 0.002\n",
      "iteration: 442100 loss: 0.0013 lr: 0.002\n",
      "iteration: 442110 loss: 0.0011 lr: 0.002\n",
      "iteration: 442120 loss: 0.0010 lr: 0.002\n",
      "iteration: 442130 loss: 0.0011 lr: 0.002\n",
      "iteration: 442140 loss: 0.0010 lr: 0.002\n",
      "iteration: 442150 loss: 0.0014 lr: 0.002\n",
      "iteration: 442160 loss: 0.0023 lr: 0.002\n",
      "iteration: 442170 loss: 0.0010 lr: 0.002\n",
      "iteration: 442180 loss: 0.0012 lr: 0.002\n",
      "iteration: 442190 loss: 0.0014 lr: 0.002\n",
      "iteration: 442200 loss: 0.0009 lr: 0.002\n",
      "iteration: 442210 loss: 0.0021 lr: 0.002\n",
      "iteration: 442220 loss: 0.0010 lr: 0.002\n",
      "iteration: 442230 loss: 0.0008 lr: 0.002\n",
      "iteration: 442240 loss: 0.0010 lr: 0.002\n",
      "iteration: 442250 loss: 0.0014 lr: 0.002\n",
      "iteration: 442260 loss: 0.0010 lr: 0.002\n",
      "iteration: 442270 loss: 0.0018 lr: 0.002\n",
      "iteration: 442280 loss: 0.0012 lr: 0.002\n",
      "iteration: 442290 loss: 0.0009 lr: 0.002\n",
      "iteration: 442300 loss: 0.0016 lr: 0.002\n",
      "iteration: 442310 loss: 0.0011 lr: 0.002\n",
      "iteration: 442320 loss: 0.0017 lr: 0.002\n",
      "iteration: 442330 loss: 0.0012 lr: 0.002\n",
      "iteration: 442340 loss: 0.0012 lr: 0.002\n",
      "iteration: 442350 loss: 0.0008 lr: 0.002\n",
      "iteration: 442360 loss: 0.0010 lr: 0.002\n",
      "iteration: 442370 loss: 0.0013 lr: 0.002\n",
      "iteration: 442380 loss: 0.0016 lr: 0.002\n",
      "iteration: 442390 loss: 0.0013 lr: 0.002\n",
      "iteration: 442400 loss: 0.0009 lr: 0.002\n",
      "iteration: 442410 loss: 0.0014 lr: 0.002\n",
      "iteration: 442420 loss: 0.0012 lr: 0.002\n",
      "iteration: 442430 loss: 0.0011 lr: 0.002\n",
      "iteration: 442440 loss: 0.0016 lr: 0.002\n",
      "iteration: 442450 loss: 0.0012 lr: 0.002\n",
      "iteration: 442460 loss: 0.0012 lr: 0.002\n",
      "iteration: 442470 loss: 0.0013 lr: 0.002\n",
      "iteration: 442480 loss: 0.0014 lr: 0.002\n",
      "iteration: 442490 loss: 0.0015 lr: 0.002\n",
      "iteration: 442500 loss: 0.0011 lr: 0.002\n",
      "iteration: 442510 loss: 0.0009 lr: 0.002\n",
      "iteration: 442520 loss: 0.0007 lr: 0.002\n",
      "iteration: 442530 loss: 0.0010 lr: 0.002\n",
      "iteration: 442540 loss: 0.0011 lr: 0.002\n",
      "iteration: 442550 loss: 0.0014 lr: 0.002\n",
      "iteration: 442560 loss: 0.0008 lr: 0.002\n",
      "iteration: 442570 loss: 0.0014 lr: 0.002\n",
      "iteration: 442580 loss: 0.0012 lr: 0.002\n",
      "iteration: 442590 loss: 0.0008 lr: 0.002\n",
      "iteration: 442600 loss: 0.0011 lr: 0.002\n",
      "iteration: 442610 loss: 0.0012 lr: 0.002\n",
      "iteration: 442620 loss: 0.0012 lr: 0.002\n",
      "iteration: 442630 loss: 0.0012 lr: 0.002\n",
      "iteration: 442640 loss: 0.0010 lr: 0.002\n",
      "iteration: 442650 loss: 0.0011 lr: 0.002\n",
      "iteration: 442660 loss: 0.0008 lr: 0.002\n",
      "iteration: 442670 loss: 0.0011 lr: 0.002\n",
      "iteration: 442680 loss: 0.0011 lr: 0.002\n",
      "iteration: 442690 loss: 0.0009 lr: 0.002\n",
      "iteration: 442700 loss: 0.0007 lr: 0.002\n",
      "iteration: 442710 loss: 0.0011 lr: 0.002\n",
      "iteration: 442720 loss: 0.0010 lr: 0.002\n",
      "iteration: 442730 loss: 0.0013 lr: 0.002\n",
      "iteration: 442740 loss: 0.0012 lr: 0.002\n",
      "iteration: 442750 loss: 0.0010 lr: 0.002\n",
      "iteration: 442760 loss: 0.0008 lr: 0.002\n",
      "iteration: 442770 loss: 0.0015 lr: 0.002\n",
      "iteration: 442780 loss: 0.0014 lr: 0.002\n",
      "iteration: 442790 loss: 0.0010 lr: 0.002\n",
      "iteration: 442800 loss: 0.0010 lr: 0.002\n",
      "iteration: 442810 loss: 0.0013 lr: 0.002\n",
      "iteration: 442820 loss: 0.0014 lr: 0.002\n",
      "iteration: 442830 loss: 0.0015 lr: 0.002\n",
      "iteration: 442840 loss: 0.0020 lr: 0.002\n",
      "iteration: 442850 loss: 0.0011 lr: 0.002\n",
      "iteration: 442860 loss: 0.0007 lr: 0.002\n",
      "iteration: 442870 loss: 0.0009 lr: 0.002\n",
      "iteration: 442880 loss: 0.0013 lr: 0.002\n",
      "iteration: 442890 loss: 0.0010 lr: 0.002\n",
      "iteration: 442900 loss: 0.0011 lr: 0.002\n",
      "iteration: 442910 loss: 0.0012 lr: 0.002\n",
      "iteration: 442920 loss: 0.0017 lr: 0.002\n",
      "iteration: 442930 loss: 0.0010 lr: 0.002\n",
      "iteration: 442940 loss: 0.0014 lr: 0.002\n",
      "iteration: 442950 loss: 0.0012 lr: 0.002\n",
      "iteration: 442960 loss: 0.0010 lr: 0.002\n",
      "iteration: 442970 loss: 0.0011 lr: 0.002\n",
      "iteration: 442980 loss: 0.0014 lr: 0.002\n",
      "iteration: 442990 loss: 0.0012 lr: 0.002\n",
      "iteration: 443000 loss: 0.0021 lr: 0.002\n",
      "iteration: 443010 loss: 0.0010 lr: 0.002\n",
      "iteration: 443020 loss: 0.0013 lr: 0.002\n",
      "iteration: 443030 loss: 0.0009 lr: 0.002\n",
      "iteration: 443040 loss: 0.0010 lr: 0.002\n",
      "iteration: 443050 loss: 0.0014 lr: 0.002\n",
      "iteration: 443060 loss: 0.0011 lr: 0.002\n",
      "iteration: 443070 loss: 0.0015 lr: 0.002\n",
      "iteration: 443080 loss: 0.0009 lr: 0.002\n",
      "iteration: 443090 loss: 0.0011 lr: 0.002\n",
      "iteration: 443100 loss: 0.0013 lr: 0.002\n",
      "iteration: 443110 loss: 0.0013 lr: 0.002\n",
      "iteration: 443120 loss: 0.0008 lr: 0.002\n",
      "iteration: 443130 loss: 0.0018 lr: 0.002\n",
      "iteration: 443140 loss: 0.0010 lr: 0.002\n",
      "iteration: 443150 loss: 0.0010 lr: 0.002\n",
      "iteration: 443160 loss: 0.0014 lr: 0.002\n",
      "iteration: 443170 loss: 0.0014 lr: 0.002\n",
      "iteration: 443180 loss: 0.0009 lr: 0.002\n",
      "iteration: 443190 loss: 0.0012 lr: 0.002\n",
      "iteration: 443200 loss: 0.0009 lr: 0.002\n",
      "iteration: 443210 loss: 0.0011 lr: 0.002\n",
      "iteration: 443220 loss: 0.0016 lr: 0.002\n",
      "iteration: 443230 loss: 0.0010 lr: 0.002\n",
      "iteration: 443240 loss: 0.0015 lr: 0.002\n",
      "iteration: 443250 loss: 0.0007 lr: 0.002\n",
      "iteration: 443260 loss: 0.0012 lr: 0.002\n",
      "iteration: 443270 loss: 0.0017 lr: 0.002\n",
      "iteration: 443280 loss: 0.0013 lr: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 443290 loss: 0.0016 lr: 0.002\n",
      "iteration: 443300 loss: 0.0012 lr: 0.002\n",
      "iteration: 443310 loss: 0.0010 lr: 0.002\n",
      "iteration: 443320 loss: 0.0013 lr: 0.002\n",
      "iteration: 443330 loss: 0.0012 lr: 0.002\n",
      "iteration: 443340 loss: 0.0012 lr: 0.002\n",
      "iteration: 443350 loss: 0.0014 lr: 0.002\n",
      "iteration: 443360 loss: 0.0010 lr: 0.002\n",
      "iteration: 443370 loss: 0.0010 lr: 0.002\n",
      "iteration: 443380 loss: 0.0010 lr: 0.002\n",
      "iteration: 443390 loss: 0.0017 lr: 0.002\n",
      "iteration: 443400 loss: 0.0010 lr: 0.002\n",
      "iteration: 443410 loss: 0.0013 lr: 0.002\n",
      "iteration: 443420 loss: 0.0011 lr: 0.002\n",
      "iteration: 443430 loss: 0.0013 lr: 0.002\n",
      "iteration: 443440 loss: 0.0015 lr: 0.002\n",
      "iteration: 443450 loss: 0.0017 lr: 0.002\n",
      "iteration: 443460 loss: 0.0013 lr: 0.002\n",
      "iteration: 443470 loss: 0.0012 lr: 0.002\n",
      "iteration: 443480 loss: 0.0017 lr: 0.002\n",
      "iteration: 443490 loss: 0.0009 lr: 0.002\n",
      "iteration: 443500 loss: 0.0013 lr: 0.002\n",
      "iteration: 443510 loss: 0.0011 lr: 0.002\n",
      "iteration: 443520 loss: 0.0014 lr: 0.002\n",
      "iteration: 443530 loss: 0.0010 lr: 0.002\n",
      "iteration: 443540 loss: 0.0011 lr: 0.002\n",
      "iteration: 443550 loss: 0.0014 lr: 0.002\n",
      "iteration: 443560 loss: 0.0014 lr: 0.002\n",
      "iteration: 443570 loss: 0.0011 lr: 0.002\n",
      "iteration: 443580 loss: 0.0018 lr: 0.002\n",
      "iteration: 443590 loss: 0.0011 lr: 0.002\n",
      "iteration: 443600 loss: 0.0016 lr: 0.002\n",
      "iteration: 443610 loss: 0.0007 lr: 0.002\n",
      "iteration: 443620 loss: 0.0007 lr: 0.002\n",
      "iteration: 443630 loss: 0.0018 lr: 0.002\n",
      "iteration: 443640 loss: 0.0012 lr: 0.002\n",
      "iteration: 443650 loss: 0.0009 lr: 0.002\n",
      "iteration: 443660 loss: 0.0015 lr: 0.002\n",
      "iteration: 443670 loss: 0.0014 lr: 0.002\n",
      "iteration: 443680 loss: 0.0010 lr: 0.002\n",
      "iteration: 443690 loss: 0.0018 lr: 0.002\n",
      "iteration: 443700 loss: 0.0015 lr: 0.002\n",
      "iteration: 443710 loss: 0.0009 lr: 0.002\n",
      "iteration: 443720 loss: 0.0013 lr: 0.002\n",
      "iteration: 443730 loss: 0.0016 lr: 0.002\n",
      "iteration: 443740 loss: 0.0013 lr: 0.002\n",
      "iteration: 443750 loss: 0.0016 lr: 0.002\n",
      "iteration: 443760 loss: 0.0010 lr: 0.002\n",
      "iteration: 443770 loss: 0.0015 lr: 0.002\n",
      "iteration: 443780 loss: 0.0010 lr: 0.002\n",
      "iteration: 443790 loss: 0.0015 lr: 0.002\n",
      "iteration: 443800 loss: 0.0011 lr: 0.002\n",
      "iteration: 443810 loss: 0.0014 lr: 0.002\n",
      "iteration: 443820 loss: 0.0013 lr: 0.002\n",
      "iteration: 443830 loss: 0.0012 lr: 0.002\n",
      "iteration: 443840 loss: 0.0017 lr: 0.002\n",
      "iteration: 443850 loss: 0.0016 lr: 0.002\n",
      "iteration: 443860 loss: 0.0019 lr: 0.002\n",
      "iteration: 443870 loss: 0.0010 lr: 0.002\n",
      "iteration: 443880 loss: 0.0010 lr: 0.002\n",
      "iteration: 443890 loss: 0.0010 lr: 0.002\n",
      "iteration: 443900 loss: 0.0010 lr: 0.002\n",
      "iteration: 443910 loss: 0.0010 lr: 0.002\n",
      "iteration: 443920 loss: 0.0019 lr: 0.002\n",
      "iteration: 443930 loss: 0.0011 lr: 0.002\n",
      "iteration: 443940 loss: 0.0014 lr: 0.002\n",
      "iteration: 443950 loss: 0.0013 lr: 0.002\n",
      "iteration: 443960 loss: 0.0011 lr: 0.002\n",
      "iteration: 443970 loss: 0.0012 lr: 0.002\n",
      "iteration: 443980 loss: 0.0011 lr: 0.002\n",
      "iteration: 443990 loss: 0.0010 lr: 0.002\n",
      "iteration: 444000 loss: 0.0012 lr: 0.002\n",
      "iteration: 444010 loss: 0.0011 lr: 0.002\n",
      "iteration: 444020 loss: 0.0012 lr: 0.002\n",
      "iteration: 444030 loss: 0.0015 lr: 0.002\n",
      "iteration: 444040 loss: 0.0010 lr: 0.002\n",
      "iteration: 444050 loss: 0.0018 lr: 0.002\n",
      "iteration: 444060 loss: 0.0012 lr: 0.002\n",
      "iteration: 444070 loss: 0.0013 lr: 0.002\n",
      "iteration: 444080 loss: 0.0012 lr: 0.002\n",
      "iteration: 444090 loss: 0.0012 lr: 0.002\n",
      "iteration: 444100 loss: 0.0014 lr: 0.002\n",
      "iteration: 444110 loss: 0.0012 lr: 0.002\n",
      "iteration: 444120 loss: 0.0012 lr: 0.002\n",
      "iteration: 444130 loss: 0.0015 lr: 0.002\n",
      "iteration: 444140 loss: 0.0007 lr: 0.002\n",
      "iteration: 444150 loss: 0.0013 lr: 0.002\n",
      "iteration: 444160 loss: 0.0012 lr: 0.002\n",
      "iteration: 444170 loss: 0.0019 lr: 0.002\n",
      "iteration: 444180 loss: 0.0011 lr: 0.002\n",
      "iteration: 444190 loss: 0.0012 lr: 0.002\n",
      "iteration: 444200 loss: 0.0013 lr: 0.002\n",
      "iteration: 444210 loss: 0.0012 lr: 0.002\n",
      "iteration: 444220 loss: 0.0014 lr: 0.002\n",
      "iteration: 444230 loss: 0.0012 lr: 0.002\n",
      "iteration: 444240 loss: 0.0010 lr: 0.002\n",
      "iteration: 444250 loss: 0.0012 lr: 0.002\n",
      "iteration: 444260 loss: 0.0012 lr: 0.002\n",
      "iteration: 444270 loss: 0.0013 lr: 0.002\n",
      "iteration: 444280 loss: 0.0020 lr: 0.002\n",
      "iteration: 444290 loss: 0.0010 lr: 0.002\n",
      "iteration: 444300 loss: 0.0011 lr: 0.002\n",
      "iteration: 444310 loss: 0.0009 lr: 0.002\n",
      "iteration: 444320 loss: 0.0015 lr: 0.002\n",
      "iteration: 444330 loss: 0.0008 lr: 0.002\n",
      "iteration: 444340 loss: 0.0011 lr: 0.002\n",
      "iteration: 444350 loss: 0.0011 lr: 0.002\n",
      "iteration: 444360 loss: 0.0011 lr: 0.002\n",
      "iteration: 444370 loss: 0.0009 lr: 0.002\n",
      "iteration: 444380 loss: 0.0013 lr: 0.002\n",
      "iteration: 444390 loss: 0.0012 lr: 0.002\n",
      "iteration: 444400 loss: 0.0013 lr: 0.002\n",
      "iteration: 444410 loss: 0.0008 lr: 0.002\n",
      "iteration: 444420 loss: 0.0009 lr: 0.002\n",
      "iteration: 444430 loss: 0.0010 lr: 0.002\n",
      "iteration: 444440 loss: 0.0011 lr: 0.002\n",
      "iteration: 444450 loss: 0.0012 lr: 0.002\n",
      "iteration: 444460 loss: 0.0009 lr: 0.002\n",
      "iteration: 444470 loss: 0.0011 lr: 0.002\n",
      "iteration: 444480 loss: 0.0010 lr: 0.002\n",
      "iteration: 444490 loss: 0.0013 lr: 0.002\n",
      "iteration: 444500 loss: 0.0012 lr: 0.002\n",
      "iteration: 444510 loss: 0.0010 lr: 0.002\n",
      "iteration: 444520 loss: 0.0010 lr: 0.002\n",
      "iteration: 444530 loss: 0.0008 lr: 0.002\n",
      "iteration: 444540 loss: 0.0018 lr: 0.002\n",
      "iteration: 444550 loss: 0.0022 lr: 0.002\n",
      "iteration: 444560 loss: 0.0011 lr: 0.002\n",
      "iteration: 444570 loss: 0.0021 lr: 0.002\n",
      "iteration: 444580 loss: 0.0012 lr: 0.002\n",
      "iteration: 444590 loss: 0.0012 lr: 0.002\n",
      "iteration: 444600 loss: 0.0012 lr: 0.002\n",
      "iteration: 444610 loss: 0.0006 lr: 0.002\n",
      "iteration: 444620 loss: 0.0011 lr: 0.002\n",
      "iteration: 444630 loss: 0.0016 lr: 0.002\n",
      "iteration: 444640 loss: 0.0020 lr: 0.002\n",
      "iteration: 444650 loss: 0.0014 lr: 0.002\n",
      "iteration: 444660 loss: 0.0015 lr: 0.002\n",
      "iteration: 444670 loss: 0.0011 lr: 0.002\n",
      "iteration: 444680 loss: 0.0015 lr: 0.002\n",
      "iteration: 444690 loss: 0.0014 lr: 0.002\n",
      "iteration: 444700 loss: 0.0010 lr: 0.002\n",
      "iteration: 444710 loss: 0.0011 lr: 0.002\n",
      "iteration: 444720 loss: 0.0011 lr: 0.002\n",
      "iteration: 444730 loss: 0.0011 lr: 0.002\n",
      "iteration: 444740 loss: 0.0014 lr: 0.002\n",
      "iteration: 444750 loss: 0.0014 lr: 0.002\n",
      "iteration: 444760 loss: 0.0015 lr: 0.002\n",
      "iteration: 444770 loss: 0.0010 lr: 0.002\n",
      "iteration: 444780 loss: 0.0010 lr: 0.002\n",
      "iteration: 444790 loss: 0.0008 lr: 0.002\n",
      "iteration: 444800 loss: 0.0013 lr: 0.002\n",
      "iteration: 444810 loss: 0.0012 lr: 0.002\n",
      "iteration: 444820 loss: 0.0011 lr: 0.002\n",
      "iteration: 444830 loss: 0.0016 lr: 0.002\n",
      "iteration: 444840 loss: 0.0010 lr: 0.002\n",
      "iteration: 444850 loss: 0.0012 lr: 0.002\n",
      "iteration: 444860 loss: 0.0014 lr: 0.002\n",
      "iteration: 444870 loss: 0.0016 lr: 0.002\n",
      "iteration: 444880 loss: 0.0011 lr: 0.002\n",
      "iteration: 444890 loss: 0.0013 lr: 0.002\n",
      "iteration: 444900 loss: 0.0007 lr: 0.002\n",
      "iteration: 444910 loss: 0.0010 lr: 0.002\n",
      "iteration: 444920 loss: 0.0017 lr: 0.002\n",
      "iteration: 444930 loss: 0.0013 lr: 0.002\n",
      "iteration: 444940 loss: 0.0014 lr: 0.002\n",
      "iteration: 444950 loss: 0.0011 lr: 0.002\n",
      "iteration: 444960 loss: 0.0013 lr: 0.002\n",
      "iteration: 444970 loss: 0.0011 lr: 0.002\n",
      "iteration: 444980 loss: 0.0009 lr: 0.002\n",
      "iteration: 444990 loss: 0.0011 lr: 0.002\n",
      "iteration: 445000 loss: 0.0019 lr: 0.002\n",
      "iteration: 445010 loss: 0.0013 lr: 0.002\n",
      "iteration: 445020 loss: 0.0014 lr: 0.002\n",
      "iteration: 445030 loss: 0.0014 lr: 0.002\n",
      "iteration: 445040 loss: 0.0015 lr: 0.002\n",
      "iteration: 445050 loss: 0.0014 lr: 0.002\n",
      "iteration: 445060 loss: 0.0011 lr: 0.002\n",
      "iteration: 445070 loss: 0.0014 lr: 0.002\n",
      "iteration: 445080 loss: 0.0016 lr: 0.002\n",
      "iteration: 445090 loss: 0.0015 lr: 0.002\n",
      "iteration: 445100 loss: 0.0010 lr: 0.002\n",
      "iteration: 445110 loss: 0.0009 lr: 0.002\n",
      "iteration: 445120 loss: 0.0015 lr: 0.002\n",
      "iteration: 445130 loss: 0.0012 lr: 0.002\n",
      "iteration: 445140 loss: 0.0009 lr: 0.002\n",
      "iteration: 445150 loss: 0.0026 lr: 0.002\n",
      "iteration: 445160 loss: 0.0013 lr: 0.002\n",
      "iteration: 445170 loss: 0.0024 lr: 0.002\n",
      "iteration: 445180 loss: 0.0009 lr: 0.002\n",
      "iteration: 445190 loss: 0.0010 lr: 0.002\n",
      "iteration: 445200 loss: 0.0011 lr: 0.002\n",
      "iteration: 445210 loss: 0.0009 lr: 0.002\n",
      "iteration: 445220 loss: 0.0010 lr: 0.002\n",
      "iteration: 445230 loss: 0.0014 lr: 0.002\n",
      "iteration: 445240 loss: 0.0015 lr: 0.002\n",
      "iteration: 445250 loss: 0.0009 lr: 0.002\n",
      "iteration: 445260 loss: 0.0016 lr: 0.002\n",
      "iteration: 445270 loss: 0.0011 lr: 0.002\n",
      "iteration: 445280 loss: 0.0012 lr: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 445290 loss: 0.0012 lr: 0.002\n",
      "iteration: 445300 loss: 0.0014 lr: 0.002\n",
      "iteration: 445310 loss: 0.0012 lr: 0.002\n",
      "iteration: 445320 loss: 0.0011 lr: 0.002\n",
      "iteration: 445330 loss: 0.0015 lr: 0.002\n",
      "iteration: 445340 loss: 0.0010 lr: 0.002\n",
      "iteration: 445350 loss: 0.0013 lr: 0.002\n",
      "iteration: 445360 loss: 0.0016 lr: 0.002\n",
      "iteration: 445370 loss: 0.0012 lr: 0.002\n",
      "iteration: 445380 loss: 0.0010 lr: 0.002\n",
      "iteration: 445390 loss: 0.0008 lr: 0.002\n",
      "iteration: 445400 loss: 0.0012 lr: 0.002\n",
      "iteration: 445410 loss: 0.0014 lr: 0.002\n",
      "iteration: 445420 loss: 0.0011 lr: 0.002\n",
      "iteration: 445430 loss: 0.0008 lr: 0.002\n",
      "iteration: 445440 loss: 0.0033 lr: 0.002\n",
      "iteration: 445450 loss: 0.0009 lr: 0.002\n",
      "iteration: 445460 loss: 0.0010 lr: 0.002\n",
      "iteration: 445470 loss: 0.0009 lr: 0.002\n",
      "iteration: 445480 loss: 0.0013 lr: 0.002\n",
      "iteration: 445490 loss: 0.0012 lr: 0.002\n",
      "iteration: 445500 loss: 0.0019 lr: 0.002\n",
      "iteration: 445510 loss: 0.0013 lr: 0.002\n",
      "iteration: 445520 loss: 0.0015 lr: 0.002\n",
      "iteration: 445530 loss: 0.0013 lr: 0.002\n",
      "iteration: 445540 loss: 0.0010 lr: 0.002\n",
      "iteration: 445550 loss: 0.0015 lr: 0.002\n",
      "iteration: 445560 loss: 0.0008 lr: 0.002\n",
      "iteration: 445570 loss: 0.0007 lr: 0.002\n",
      "iteration: 445580 loss: 0.0014 lr: 0.002\n",
      "iteration: 445590 loss: 0.0011 lr: 0.002\n",
      "iteration: 445600 loss: 0.0018 lr: 0.002\n",
      "iteration: 445610 loss: 0.0010 lr: 0.002\n",
      "iteration: 445620 loss: 0.0015 lr: 0.002\n",
      "iteration: 445630 loss: 0.0011 lr: 0.002\n",
      "iteration: 445640 loss: 0.0014 lr: 0.002\n",
      "iteration: 445650 loss: 0.0008 lr: 0.002\n",
      "iteration: 445660 loss: 0.0021 lr: 0.002\n",
      "iteration: 445670 loss: 0.0022 lr: 0.002\n",
      "iteration: 445680 loss: 0.0013 lr: 0.002\n",
      "iteration: 445690 loss: 0.0011 lr: 0.002\n",
      "iteration: 445700 loss: 0.0021 lr: 0.002\n",
      "iteration: 445710 loss: 0.0010 lr: 0.002\n",
      "iteration: 445720 loss: 0.0011 lr: 0.002\n",
      "iteration: 445730 loss: 0.0009 lr: 0.002\n",
      "iteration: 445740 loss: 0.0012 lr: 0.002\n",
      "iteration: 445750 loss: 0.0012 lr: 0.002\n",
      "iteration: 445760 loss: 0.0016 lr: 0.002\n",
      "iteration: 445770 loss: 0.0012 lr: 0.002\n",
      "iteration: 445780 loss: 0.0009 lr: 0.002\n",
      "iteration: 445790 loss: 0.0011 lr: 0.002\n",
      "iteration: 445800 loss: 0.0009 lr: 0.002\n",
      "iteration: 445810 loss: 0.0015 lr: 0.002\n",
      "iteration: 445820 loss: 0.0013 lr: 0.002\n",
      "iteration: 445830 loss: 0.0010 lr: 0.002\n",
      "iteration: 445840 loss: 0.0012 lr: 0.002\n",
      "iteration: 445850 loss: 0.0009 lr: 0.002\n",
      "iteration: 445860 loss: 0.0010 lr: 0.002\n",
      "iteration: 445870 loss: 0.0015 lr: 0.002\n",
      "iteration: 445880 loss: 0.0014 lr: 0.002\n",
      "iteration: 445890 loss: 0.0013 lr: 0.002\n",
      "iteration: 445900 loss: 0.0011 lr: 0.002\n",
      "iteration: 445910 loss: 0.0015 lr: 0.002\n",
      "iteration: 445920 loss: 0.0008 lr: 0.002\n",
      "iteration: 445930 loss: 0.0017 lr: 0.002\n",
      "iteration: 445940 loss: 0.0011 lr: 0.002\n",
      "iteration: 445950 loss: 0.0015 lr: 0.002\n",
      "iteration: 445960 loss: 0.0012 lr: 0.002\n",
      "iteration: 445970 loss: 0.0011 lr: 0.002\n",
      "iteration: 445980 loss: 0.0010 lr: 0.002\n",
      "iteration: 445990 loss: 0.0010 lr: 0.002\n",
      "iteration: 446000 loss: 0.0012 lr: 0.002\n",
      "iteration: 446010 loss: 0.0012 lr: 0.002\n",
      "iteration: 446020 loss: 0.0021 lr: 0.002\n",
      "iteration: 446030 loss: 0.0011 lr: 0.002\n",
      "iteration: 446040 loss: 0.0010 lr: 0.002\n",
      "iteration: 446050 loss: 0.0015 lr: 0.002\n",
      "iteration: 446060 loss: 0.0016 lr: 0.002\n",
      "iteration: 446070 loss: 0.0013 lr: 0.002\n",
      "iteration: 446080 loss: 0.0014 lr: 0.002\n",
      "iteration: 446090 loss: 0.0013 lr: 0.002\n",
      "iteration: 446100 loss: 0.0009 lr: 0.002\n",
      "iteration: 446110 loss: 0.0013 lr: 0.002\n",
      "iteration: 446120 loss: 0.0013 lr: 0.002\n",
      "iteration: 446130 loss: 0.0010 lr: 0.002\n",
      "iteration: 446140 loss: 0.0011 lr: 0.002\n",
      "iteration: 446150 loss: 0.0010 lr: 0.002\n",
      "iteration: 446160 loss: 0.0016 lr: 0.002\n",
      "iteration: 446170 loss: 0.0011 lr: 0.002\n",
      "iteration: 446180 loss: 0.0007 lr: 0.002\n",
      "iteration: 446190 loss: 0.0012 lr: 0.002\n",
      "iteration: 446200 loss: 0.0010 lr: 0.002\n",
      "iteration: 446210 loss: 0.0011 lr: 0.002\n",
      "iteration: 446220 loss: 0.0010 lr: 0.002\n",
      "iteration: 446230 loss: 0.0009 lr: 0.002\n",
      "iteration: 446240 loss: 0.0011 lr: 0.002\n",
      "iteration: 446250 loss: 0.0010 lr: 0.002\n",
      "iteration: 446260 loss: 0.0014 lr: 0.002\n",
      "iteration: 446270 loss: 0.0008 lr: 0.002\n",
      "iteration: 446280 loss: 0.0013 lr: 0.002\n",
      "iteration: 446290 loss: 0.0012 lr: 0.002\n",
      "iteration: 446300 loss: 0.0017 lr: 0.002\n",
      "iteration: 446310 loss: 0.0011 lr: 0.002\n",
      "iteration: 446320 loss: 0.0009 lr: 0.002\n",
      "iteration: 446330 loss: 0.0007 lr: 0.002\n",
      "iteration: 446340 loss: 0.0011 lr: 0.002\n",
      "iteration: 446350 loss: 0.0017 lr: 0.002\n",
      "iteration: 446360 loss: 0.0010 lr: 0.002\n",
      "iteration: 446370 loss: 0.0011 lr: 0.002\n",
      "iteration: 446380 loss: 0.0016 lr: 0.002\n",
      "iteration: 446390 loss: 0.0017 lr: 0.002\n",
      "iteration: 446400 loss: 0.0012 lr: 0.002\n",
      "iteration: 446410 loss: 0.0016 lr: 0.002\n",
      "iteration: 446420 loss: 0.0010 lr: 0.002\n",
      "iteration: 446430 loss: 0.0018 lr: 0.002\n",
      "iteration: 446440 loss: 0.0008 lr: 0.002\n",
      "iteration: 446450 loss: 0.0013 lr: 0.002\n",
      "iteration: 446460 loss: 0.0015 lr: 0.002\n",
      "iteration: 446470 loss: 0.0014 lr: 0.002\n",
      "iteration: 446480 loss: 0.0014 lr: 0.002\n",
      "iteration: 446490 loss: 0.0010 lr: 0.002\n",
      "iteration: 446500 loss: 0.0017 lr: 0.002\n",
      "iteration: 446510 loss: 0.0013 lr: 0.002\n",
      "iteration: 446520 loss: 0.0014 lr: 0.002\n",
      "iteration: 446530 loss: 0.0015 lr: 0.002\n",
      "iteration: 446540 loss: 0.0012 lr: 0.002\n",
      "iteration: 446550 loss: 0.0027 lr: 0.002\n",
      "iteration: 446560 loss: 0.0013 lr: 0.002\n",
      "iteration: 446570 loss: 0.0013 lr: 0.002\n",
      "iteration: 446580 loss: 0.0021 lr: 0.002\n",
      "iteration: 446590 loss: 0.0014 lr: 0.002\n",
      "iteration: 446600 loss: 0.0010 lr: 0.002\n",
      "iteration: 446610 loss: 0.0008 lr: 0.002\n",
      "iteration: 446620 loss: 0.0014 lr: 0.002\n",
      "iteration: 446630 loss: 0.0015 lr: 0.002\n",
      "iteration: 446640 loss: 0.0013 lr: 0.002\n",
      "iteration: 446650 loss: 0.0013 lr: 0.002\n",
      "iteration: 446660 loss: 0.0010 lr: 0.002\n",
      "iteration: 446670 loss: 0.0014 lr: 0.002\n",
      "iteration: 446680 loss: 0.0021 lr: 0.002\n",
      "iteration: 446690 loss: 0.0012 lr: 0.002\n",
      "iteration: 446700 loss: 0.0011 lr: 0.002\n",
      "iteration: 446710 loss: 0.0011 lr: 0.002\n",
      "iteration: 446720 loss: 0.0018 lr: 0.002\n",
      "iteration: 446730 loss: 0.0010 lr: 0.002\n",
      "iteration: 446740 loss: 0.0013 lr: 0.002\n",
      "iteration: 446750 loss: 0.0011 lr: 0.002\n",
      "iteration: 446760 loss: 0.0017 lr: 0.002\n",
      "iteration: 446770 loss: 0.0010 lr: 0.002\n",
      "iteration: 446780 loss: 0.0013 lr: 0.002\n",
      "iteration: 446790 loss: 0.0013 lr: 0.002\n",
      "iteration: 446800 loss: 0.0012 lr: 0.002\n",
      "iteration: 446810 loss: 0.0009 lr: 0.002\n",
      "iteration: 446820 loss: 0.0013 lr: 0.002\n",
      "iteration: 446830 loss: 0.0009 lr: 0.002\n",
      "iteration: 446840 loss: 0.0015 lr: 0.002\n",
      "iteration: 446850 loss: 0.0010 lr: 0.002\n",
      "iteration: 446860 loss: 0.0012 lr: 0.002\n",
      "iteration: 446870 loss: 0.0014 lr: 0.002\n",
      "iteration: 446880 loss: 0.0011 lr: 0.002\n",
      "iteration: 446890 loss: 0.0012 lr: 0.002\n",
      "iteration: 446900 loss: 0.0010 lr: 0.002\n",
      "iteration: 446910 loss: 0.0011 lr: 0.002\n",
      "iteration: 446920 loss: 0.0018 lr: 0.002\n",
      "iteration: 446930 loss: 0.0016 lr: 0.002\n",
      "iteration: 446940 loss: 0.0010 lr: 0.002\n",
      "iteration: 446950 loss: 0.0009 lr: 0.002\n",
      "iteration: 446960 loss: 0.0014 lr: 0.002\n",
      "iteration: 446970 loss: 0.0011 lr: 0.002\n",
      "iteration: 446980 loss: 0.0011 lr: 0.002\n",
      "iteration: 446990 loss: 0.0012 lr: 0.002\n",
      "iteration: 447000 loss: 0.0008 lr: 0.002\n",
      "iteration: 447010 loss: 0.0010 lr: 0.002\n",
      "iteration: 447020 loss: 0.0015 lr: 0.002\n",
      "iteration: 447030 loss: 0.0009 lr: 0.002\n",
      "iteration: 447040 loss: 0.0013 lr: 0.002\n",
      "iteration: 447050 loss: 0.0008 lr: 0.002\n",
      "iteration: 447060 loss: 0.0010 lr: 0.002\n",
      "iteration: 447070 loss: 0.0010 lr: 0.002\n",
      "iteration: 447080 loss: 0.0016 lr: 0.002\n",
      "iteration: 447090 loss: 0.0020 lr: 0.002\n",
      "iteration: 447100 loss: 0.0012 lr: 0.002\n",
      "iteration: 447110 loss: 0.0009 lr: 0.002\n",
      "iteration: 447120 loss: 0.0013 lr: 0.002\n",
      "iteration: 447130 loss: 0.0012 lr: 0.002\n",
      "iteration: 447140 loss: 0.0015 lr: 0.002\n",
      "iteration: 447150 loss: 0.0011 lr: 0.002\n",
      "iteration: 447160 loss: 0.0009 lr: 0.002\n",
      "iteration: 447170 loss: 0.0011 lr: 0.002\n",
      "iteration: 447180 loss: 0.0017 lr: 0.002\n",
      "iteration: 447190 loss: 0.0009 lr: 0.002\n",
      "iteration: 447200 loss: 0.0014 lr: 0.002\n",
      "iteration: 447210 loss: 0.0010 lr: 0.002\n",
      "iteration: 447220 loss: 0.0025 lr: 0.002\n",
      "iteration: 447230 loss: 0.0012 lr: 0.002\n",
      "iteration: 447240 loss: 0.0010 lr: 0.002\n",
      "iteration: 447250 loss: 0.0018 lr: 0.002\n",
      "iteration: 447260 loss: 0.0010 lr: 0.002\n",
      "iteration: 447270 loss: 0.0014 lr: 0.002\n",
      "iteration: 447280 loss: 0.0013 lr: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 447290 loss: 0.0013 lr: 0.002\n",
      "iteration: 447300 loss: 0.0017 lr: 0.002\n",
      "iteration: 447310 loss: 0.0012 lr: 0.002\n",
      "iteration: 447320 loss: 0.0015 lr: 0.002\n",
      "iteration: 447330 loss: 0.0022 lr: 0.002\n",
      "iteration: 447340 loss: 0.0011 lr: 0.002\n",
      "iteration: 447350 loss: 0.0016 lr: 0.002\n",
      "iteration: 447360 loss: 0.0008 lr: 0.002\n",
      "iteration: 447370 loss: 0.0015 lr: 0.002\n",
      "iteration: 447380 loss: 0.0013 lr: 0.002\n",
      "iteration: 447390 loss: 0.0011 lr: 0.002\n",
      "iteration: 447400 loss: 0.0009 lr: 0.002\n",
      "iteration: 447410 loss: 0.0011 lr: 0.002\n",
      "iteration: 447420 loss: 0.0011 lr: 0.002\n",
      "iteration: 447430 loss: 0.0013 lr: 0.002\n",
      "iteration: 447440 loss: 0.0019 lr: 0.002\n",
      "iteration: 447450 loss: 0.0010 lr: 0.002\n",
      "iteration: 447460 loss: 0.0018 lr: 0.002\n",
      "iteration: 447470 loss: 0.0017 lr: 0.002\n",
      "iteration: 447480 loss: 0.0016 lr: 0.002\n",
      "iteration: 447490 loss: 0.0027 lr: 0.002\n",
      "iteration: 447500 loss: 0.0010 lr: 0.002\n",
      "iteration: 447510 loss: 0.0009 lr: 0.002\n",
      "iteration: 447520 loss: 0.0006 lr: 0.002\n",
      "iteration: 447530 loss: 0.0008 lr: 0.002\n",
      "iteration: 447540 loss: 0.0012 lr: 0.002\n",
      "iteration: 447550 loss: 0.0009 lr: 0.002\n",
      "iteration: 447560 loss: 0.0013 lr: 0.002\n",
      "iteration: 447570 loss: 0.0015 lr: 0.002\n",
      "iteration: 447580 loss: 0.0011 lr: 0.002\n",
      "iteration: 447590 loss: 0.0011 lr: 0.002\n",
      "iteration: 447600 loss: 0.0016 lr: 0.002\n",
      "iteration: 447610 loss: 0.0018 lr: 0.002\n",
      "iteration: 447620 loss: 0.0012 lr: 0.002\n",
      "iteration: 447630 loss: 0.0015 lr: 0.002\n",
      "iteration: 447640 loss: 0.0014 lr: 0.002\n",
      "iteration: 447650 loss: 0.0016 lr: 0.002\n",
      "iteration: 447660 loss: 0.0011 lr: 0.002\n",
      "iteration: 447670 loss: 0.0013 lr: 0.002\n",
      "iteration: 447680 loss: 0.0011 lr: 0.002\n",
      "iteration: 447690 loss: 0.0010 lr: 0.002\n",
      "iteration: 447700 loss: 0.0012 lr: 0.002\n",
      "iteration: 447710 loss: 0.0011 lr: 0.002\n",
      "iteration: 447720 loss: 0.0018 lr: 0.002\n",
      "iteration: 447730 loss: 0.0012 lr: 0.002\n",
      "iteration: 447740 loss: 0.0009 lr: 0.002\n",
      "iteration: 447750 loss: 0.0014 lr: 0.002\n",
      "iteration: 447760 loss: 0.0017 lr: 0.002\n",
      "iteration: 447770 loss: 0.0012 lr: 0.002\n",
      "iteration: 447780 loss: 0.0012 lr: 0.002\n",
      "iteration: 447790 loss: 0.0014 lr: 0.002\n",
      "iteration: 447800 loss: 0.0011 lr: 0.002\n",
      "iteration: 447810 loss: 0.0017 lr: 0.002\n",
      "iteration: 447820 loss: 0.0008 lr: 0.002\n",
      "iteration: 447830 loss: 0.0008 lr: 0.002\n",
      "iteration: 447840 loss: 0.0010 lr: 0.002\n",
      "iteration: 447850 loss: 0.0011 lr: 0.002\n",
      "iteration: 447860 loss: 0.0010 lr: 0.002\n",
      "iteration: 447870 loss: 0.0014 lr: 0.002\n",
      "iteration: 447880 loss: 0.0012 lr: 0.002\n",
      "iteration: 447890 loss: 0.0009 lr: 0.002\n",
      "iteration: 447900 loss: 0.0028 lr: 0.002\n",
      "iteration: 447910 loss: 0.0011 lr: 0.002\n",
      "iteration: 447920 loss: 0.0022 lr: 0.002\n",
      "iteration: 447930 loss: 0.0013 lr: 0.002\n",
      "iteration: 447940 loss: 0.0023 lr: 0.002\n",
      "iteration: 447950 loss: 0.0013 lr: 0.002\n",
      "iteration: 447960 loss: 0.0016 lr: 0.002\n",
      "iteration: 447970 loss: 0.0017 lr: 0.002\n",
      "iteration: 447980 loss: 0.0012 lr: 0.002\n",
      "iteration: 447990 loss: 0.0010 lr: 0.002\n",
      "iteration: 448000 loss: 0.0010 lr: 0.002\n",
      "iteration: 448010 loss: 0.0014 lr: 0.002\n",
      "iteration: 448020 loss: 0.0010 lr: 0.002\n",
      "iteration: 448030 loss: 0.0008 lr: 0.002\n",
      "iteration: 448040 loss: 0.0015 lr: 0.002\n",
      "iteration: 448050 loss: 0.0007 lr: 0.002\n",
      "iteration: 448060 loss: 0.0010 lr: 0.002\n",
      "iteration: 448070 loss: 0.0011 lr: 0.002\n",
      "iteration: 448080 loss: 0.0012 lr: 0.002\n",
      "iteration: 448090 loss: 0.0008 lr: 0.002\n",
      "iteration: 448100 loss: 0.0010 lr: 0.002\n",
      "iteration: 448110 loss: 0.0010 lr: 0.002\n",
      "iteration: 448120 loss: 0.0022 lr: 0.002\n",
      "iteration: 448130 loss: 0.0010 lr: 0.002\n",
      "iteration: 448140 loss: 0.0018 lr: 0.002\n",
      "iteration: 448150 loss: 0.0016 lr: 0.002\n",
      "iteration: 448160 loss: 0.0009 lr: 0.002\n",
      "iteration: 448170 loss: 0.0007 lr: 0.002\n",
      "iteration: 448180 loss: 0.0013 lr: 0.002\n",
      "iteration: 448190 loss: 0.0015 lr: 0.002\n",
      "iteration: 448200 loss: 0.0010 lr: 0.002\n",
      "iteration: 448210 loss: 0.0012 lr: 0.002\n",
      "iteration: 448220 loss: 0.0013 lr: 0.002\n",
      "iteration: 448230 loss: 0.0017 lr: 0.002\n",
      "iteration: 448240 loss: 0.0017 lr: 0.002\n",
      "iteration: 448250 loss: 0.0013 lr: 0.002\n",
      "iteration: 448260 loss: 0.0007 lr: 0.002\n",
      "iteration: 448270 loss: 0.0011 lr: 0.002\n",
      "iteration: 448280 loss: 0.0011 lr: 0.002\n",
      "iteration: 448290 loss: 0.0010 lr: 0.002\n",
      "iteration: 448300 loss: 0.0010 lr: 0.002\n",
      "iteration: 448310 loss: 0.0010 lr: 0.002\n",
      "iteration: 448320 loss: 0.0014 lr: 0.002\n",
      "iteration: 448330 loss: 0.0010 lr: 0.002\n",
      "iteration: 448340 loss: 0.0011 lr: 0.002\n",
      "iteration: 448350 loss: 0.0011 lr: 0.002\n",
      "iteration: 448360 loss: 0.0014 lr: 0.002\n",
      "iteration: 448370 loss: 0.0009 lr: 0.002\n",
      "iteration: 448380 loss: 0.0027 lr: 0.002\n",
      "iteration: 448390 loss: 0.0015 lr: 0.002\n",
      "iteration: 448400 loss: 0.0014 lr: 0.002\n",
      "iteration: 448410 loss: 0.0015 lr: 0.002\n",
      "iteration: 448420 loss: 0.0014 lr: 0.002\n",
      "iteration: 448430 loss: 0.0011 lr: 0.002\n",
      "iteration: 448440 loss: 0.0013 lr: 0.002\n",
      "iteration: 448450 loss: 0.0012 lr: 0.002\n",
      "iteration: 448460 loss: 0.0014 lr: 0.002\n",
      "iteration: 448470 loss: 0.0010 lr: 0.002\n",
      "iteration: 448480 loss: 0.0012 lr: 0.002\n",
      "iteration: 448490 loss: 0.0008 lr: 0.002\n",
      "iteration: 448500 loss: 0.0013 lr: 0.002\n",
      "iteration: 448510 loss: 0.0012 lr: 0.002\n",
      "iteration: 448520 loss: 0.0009 lr: 0.002\n",
      "iteration: 448530 loss: 0.0010 lr: 0.002\n",
      "iteration: 448540 loss: 0.0009 lr: 0.002\n",
      "iteration: 448550 loss: 0.0015 lr: 0.002\n",
      "iteration: 448560 loss: 0.0014 lr: 0.002\n",
      "iteration: 448570 loss: 0.0011 lr: 0.002\n",
      "iteration: 448580 loss: 0.0014 lr: 0.002\n",
      "iteration: 448590 loss: 0.0009 lr: 0.002\n",
      "iteration: 448600 loss: 0.0009 lr: 0.002\n",
      "iteration: 448610 loss: 0.0013 lr: 0.002\n",
      "iteration: 448620 loss: 0.0011 lr: 0.002\n",
      "iteration: 448630 loss: 0.0013 lr: 0.002\n",
      "iteration: 448640 loss: 0.0016 lr: 0.002\n",
      "iteration: 448650 loss: 0.0011 lr: 0.002\n",
      "iteration: 448660 loss: 0.0027 lr: 0.002\n",
      "iteration: 448670 loss: 0.0012 lr: 0.002\n",
      "iteration: 448680 loss: 0.0012 lr: 0.002\n",
      "iteration: 448690 loss: 0.0007 lr: 0.002\n",
      "iteration: 448700 loss: 0.0009 lr: 0.002\n",
      "iteration: 448710 loss: 0.0008 lr: 0.002\n",
      "iteration: 448720 loss: 0.0015 lr: 0.002\n",
      "iteration: 448730 loss: 0.0017 lr: 0.002\n",
      "iteration: 448740 loss: 0.0011 lr: 0.002\n",
      "iteration: 448750 loss: 0.0010 lr: 0.002\n",
      "iteration: 448760 loss: 0.0014 lr: 0.002\n",
      "iteration: 448770 loss: 0.0014 lr: 0.002\n",
      "iteration: 448780 loss: 0.0016 lr: 0.002\n",
      "iteration: 448790 loss: 0.0010 lr: 0.002\n",
      "iteration: 448800 loss: 0.0010 lr: 0.002\n",
      "iteration: 448810 loss: 0.0011 lr: 0.002\n",
      "iteration: 448820 loss: 0.0013 lr: 0.002\n",
      "iteration: 448830 loss: 0.0012 lr: 0.002\n",
      "iteration: 448840 loss: 0.0013 lr: 0.002\n",
      "iteration: 448850 loss: 0.0011 lr: 0.002\n",
      "iteration: 448860 loss: 0.0011 lr: 0.002\n",
      "iteration: 448870 loss: 0.0012 lr: 0.002\n",
      "iteration: 448880 loss: 0.0021 lr: 0.002\n",
      "iteration: 448890 loss: 0.0011 lr: 0.002\n",
      "iteration: 448900 loss: 0.0015 lr: 0.002\n",
      "iteration: 448910 loss: 0.0009 lr: 0.002\n",
      "iteration: 448920 loss: 0.0013 lr: 0.002\n",
      "iteration: 448930 loss: 0.0010 lr: 0.002\n",
      "iteration: 448940 loss: 0.0013 lr: 0.002\n",
      "iteration: 448950 loss: 0.0008 lr: 0.002\n",
      "iteration: 448960 loss: 0.0014 lr: 0.002\n",
      "iteration: 448970 loss: 0.0012 lr: 0.002\n",
      "iteration: 448980 loss: 0.0007 lr: 0.002\n",
      "iteration: 448990 loss: 0.0012 lr: 0.002\n",
      "iteration: 449000 loss: 0.0012 lr: 0.002\n",
      "iteration: 449010 loss: 0.0012 lr: 0.002\n",
      "iteration: 449020 loss: 0.0009 lr: 0.002\n",
      "iteration: 449030 loss: 0.0017 lr: 0.002\n",
      "iteration: 449040 loss: 0.0011 lr: 0.002\n",
      "iteration: 449050 loss: 0.0008 lr: 0.002\n",
      "iteration: 449060 loss: 0.0011 lr: 0.002\n",
      "iteration: 449070 loss: 0.0011 lr: 0.002\n",
      "iteration: 449080 loss: 0.0034 lr: 0.002\n",
      "iteration: 449090 loss: 0.0015 lr: 0.002\n",
      "iteration: 449100 loss: 0.0014 lr: 0.002\n",
      "iteration: 449110 loss: 0.0024 lr: 0.002\n",
      "iteration: 449120 loss: 0.0017 lr: 0.002\n",
      "iteration: 449130 loss: 0.0009 lr: 0.002\n",
      "iteration: 449140 loss: 0.0013 lr: 0.002\n",
      "iteration: 449150 loss: 0.0011 lr: 0.002\n",
      "iteration: 449160 loss: 0.0011 lr: 0.002\n",
      "iteration: 449170 loss: 0.0018 lr: 0.002\n",
      "iteration: 449180 loss: 0.0013 lr: 0.002\n",
      "iteration: 449190 loss: 0.0016 lr: 0.002\n",
      "iteration: 449200 loss: 0.0019 lr: 0.002\n",
      "iteration: 449210 loss: 0.0016 lr: 0.002\n",
      "iteration: 449220 loss: 0.0011 lr: 0.002\n",
      "iteration: 449230 loss: 0.0021 lr: 0.002\n",
      "iteration: 449240 loss: 0.0011 lr: 0.002\n",
      "iteration: 449250 loss: 0.0010 lr: 0.002\n",
      "iteration: 449260 loss: 0.0009 lr: 0.002\n",
      "iteration: 449270 loss: 0.0014 lr: 0.002\n",
      "iteration: 449280 loss: 0.0015 lr: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 449290 loss: 0.0009 lr: 0.002\n",
      "iteration: 449300 loss: 0.0010 lr: 0.002\n",
      "iteration: 449310 loss: 0.0008 lr: 0.002\n",
      "iteration: 449320 loss: 0.0019 lr: 0.002\n",
      "iteration: 449330 loss: 0.0011 lr: 0.002\n",
      "iteration: 449340 loss: 0.0009 lr: 0.002\n",
      "iteration: 449350 loss: 0.0013 lr: 0.002\n",
      "iteration: 449360 loss: 0.0010 lr: 0.002\n",
      "iteration: 449370 loss: 0.0012 lr: 0.002\n",
      "iteration: 449380 loss: 0.0010 lr: 0.002\n",
      "iteration: 449390 loss: 0.0016 lr: 0.002\n",
      "iteration: 449400 loss: 0.0012 lr: 0.002\n",
      "iteration: 449410 loss: 0.0008 lr: 0.002\n",
      "iteration: 449420 loss: 0.0010 lr: 0.002\n",
      "iteration: 449430 loss: 0.0017 lr: 0.002\n",
      "iteration: 449440 loss: 0.0012 lr: 0.002\n",
      "iteration: 449450 loss: 0.0013 lr: 0.002\n",
      "iteration: 449460 loss: 0.0013 lr: 0.002\n",
      "iteration: 449470 loss: 0.0014 lr: 0.002\n",
      "iteration: 449480 loss: 0.0014 lr: 0.002\n",
      "iteration: 449490 loss: 0.0012 lr: 0.002\n",
      "iteration: 449500 loss: 0.0012 lr: 0.002\n",
      "iteration: 449510 loss: 0.0015 lr: 0.002\n",
      "iteration: 449520 loss: 0.0011 lr: 0.002\n",
      "iteration: 449530 loss: 0.0012 lr: 0.002\n",
      "iteration: 449540 loss: 0.0012 lr: 0.002\n",
      "iteration: 449550 loss: 0.0013 lr: 0.002\n",
      "iteration: 449560 loss: 0.0010 lr: 0.002\n",
      "iteration: 449570 loss: 0.0012 lr: 0.002\n",
      "iteration: 449580 loss: 0.0016 lr: 0.002\n",
      "iteration: 449590 loss: 0.0008 lr: 0.002\n",
      "iteration: 449600 loss: 0.0009 lr: 0.002\n",
      "iteration: 449610 loss: 0.0021 lr: 0.002\n",
      "iteration: 449620 loss: 0.0008 lr: 0.002\n",
      "iteration: 449630 loss: 0.0012 lr: 0.002\n",
      "iteration: 449640 loss: 0.0018 lr: 0.002\n",
      "iteration: 449650 loss: 0.0010 lr: 0.002\n",
      "iteration: 449660 loss: 0.0014 lr: 0.002\n",
      "iteration: 449670 loss: 0.0012 lr: 0.002\n",
      "iteration: 449680 loss: 0.0009 lr: 0.002\n",
      "iteration: 449690 loss: 0.0011 lr: 0.002\n",
      "iteration: 449700 loss: 0.0012 lr: 0.002\n",
      "iteration: 449710 loss: 0.0013 lr: 0.002\n",
      "iteration: 449720 loss: 0.0013 lr: 0.002\n",
      "iteration: 449730 loss: 0.0009 lr: 0.002\n",
      "iteration: 449740 loss: 0.0011 lr: 0.002\n",
      "iteration: 449750 loss: 0.0012 lr: 0.002\n",
      "iteration: 449760 loss: 0.0017 lr: 0.002\n",
      "iteration: 449770 loss: 0.0012 lr: 0.002\n",
      "iteration: 449780 loss: 0.0011 lr: 0.002\n",
      "iteration: 449790 loss: 0.0010 lr: 0.002\n",
      "iteration: 449800 loss: 0.0017 lr: 0.002\n",
      "iteration: 449810 loss: 0.0014 lr: 0.002\n",
      "iteration: 449820 loss: 0.0008 lr: 0.002\n",
      "iteration: 449830 loss: 0.0009 lr: 0.002\n",
      "iteration: 449840 loss: 0.0009 lr: 0.002\n",
      "iteration: 449850 loss: 0.0013 lr: 0.002\n",
      "iteration: 449860 loss: 0.0014 lr: 0.002\n",
      "iteration: 449870 loss: 0.0009 lr: 0.002\n",
      "iteration: 449880 loss: 0.0011 lr: 0.002\n",
      "iteration: 449890 loss: 0.0015 lr: 0.002\n",
      "iteration: 449900 loss: 0.0013 lr: 0.002\n",
      "iteration: 449910 loss: 0.0008 lr: 0.002\n",
      "iteration: 449920 loss: 0.0009 lr: 0.002\n",
      "iteration: 449930 loss: 0.0011 lr: 0.002\n",
      "iteration: 449940 loss: 0.0016 lr: 0.002\n",
      "iteration: 449950 loss: 0.0013 lr: 0.002\n",
      "iteration: 449960 loss: 0.0012 lr: 0.002\n",
      "iteration: 449970 loss: 0.0011 lr: 0.002\n",
      "iteration: 449980 loss: 0.0016 lr: 0.002\n",
      "iteration: 449990 loss: 0.0016 lr: 0.002\n",
      "iteration: 450000 loss: 0.0014 lr: 0.002\n",
      "iteration: 450010 loss: 0.0013 lr: 0.002\n",
      "iteration: 450020 loss: 0.0017 lr: 0.002\n",
      "iteration: 450030 loss: 0.0015 lr: 0.002\n",
      "iteration: 450040 loss: 0.0018 lr: 0.002\n",
      "iteration: 450050 loss: 0.0010 lr: 0.002\n",
      "iteration: 450060 loss: 0.0012 lr: 0.002\n",
      "iteration: 450070 loss: 0.0014 lr: 0.002\n",
      "iteration: 450080 loss: 0.0010 lr: 0.002\n",
      "iteration: 450090 loss: 0.0010 lr: 0.002\n",
      "iteration: 450100 loss: 0.0017 lr: 0.002\n",
      "iteration: 450110 loss: 0.0010 lr: 0.002\n",
      "iteration: 450120 loss: 0.0010 lr: 0.002\n",
      "iteration: 450130 loss: 0.0009 lr: 0.002\n",
      "iteration: 450140 loss: 0.0011 lr: 0.002\n",
      "iteration: 450150 loss: 0.0011 lr: 0.002\n",
      "iteration: 450160 loss: 0.0020 lr: 0.002\n",
      "iteration: 450170 loss: 0.0009 lr: 0.002\n",
      "iteration: 450180 loss: 0.0011 lr: 0.002\n",
      "iteration: 450190 loss: 0.0020 lr: 0.002\n",
      "iteration: 450200 loss: 0.0010 lr: 0.002\n",
      "iteration: 450210 loss: 0.0011 lr: 0.002\n",
      "iteration: 450220 loss: 0.0012 lr: 0.002\n",
      "iteration: 450230 loss: 0.0012 lr: 0.002\n",
      "iteration: 450240 loss: 0.0012 lr: 0.002\n",
      "iteration: 450250 loss: 0.0011 lr: 0.002\n",
      "iteration: 450260 loss: 0.0011 lr: 0.002\n",
      "iteration: 450270 loss: 0.0008 lr: 0.002\n",
      "iteration: 450280 loss: 0.0012 lr: 0.002\n",
      "iteration: 450290 loss: 0.0010 lr: 0.002\n",
      "iteration: 450300 loss: 0.0017 lr: 0.002\n",
      "iteration: 450310 loss: 0.0012 lr: 0.002\n",
      "iteration: 450320 loss: 0.0011 lr: 0.002\n",
      "iteration: 450330 loss: 0.0009 lr: 0.002\n",
      "iteration: 450340 loss: 0.0011 lr: 0.002\n",
      "iteration: 450350 loss: 0.0008 lr: 0.002\n",
      "iteration: 450360 loss: 0.0013 lr: 0.002\n",
      "iteration: 450370 loss: 0.0014 lr: 0.002\n",
      "iteration: 450380 loss: 0.0014 lr: 0.002\n",
      "iteration: 450390 loss: 0.0018 lr: 0.002\n",
      "iteration: 450400 loss: 0.0009 lr: 0.002\n",
      "iteration: 450410 loss: 0.0012 lr: 0.002\n",
      "iteration: 450420 loss: 0.0011 lr: 0.002\n",
      "iteration: 450430 loss: 0.0012 lr: 0.002\n",
      "iteration: 450440 loss: 0.0008 lr: 0.002\n",
      "iteration: 450450 loss: 0.0009 lr: 0.002\n",
      "iteration: 450460 loss: 0.0009 lr: 0.002\n",
      "iteration: 450470 loss: 0.0014 lr: 0.002\n",
      "iteration: 450480 loss: 0.0017 lr: 0.002\n",
      "iteration: 450490 loss: 0.0018 lr: 0.002\n",
      "iteration: 450500 loss: 0.0009 lr: 0.002\n",
      "iteration: 450510 loss: 0.0012 lr: 0.002\n",
      "iteration: 450520 loss: 0.0013 lr: 0.002\n",
      "iteration: 450530 loss: 0.0011 lr: 0.002\n",
      "iteration: 450540 loss: 0.0007 lr: 0.002\n",
      "iteration: 450550 loss: 0.0011 lr: 0.002\n",
      "iteration: 450560 loss: 0.0014 lr: 0.002\n",
      "iteration: 450570 loss: 0.0018 lr: 0.002\n",
      "iteration: 450580 loss: 0.0010 lr: 0.002\n",
      "iteration: 450590 loss: 0.0011 lr: 0.002\n",
      "iteration: 450600 loss: 0.0041 lr: 0.002\n",
      "iteration: 450610 loss: 0.0012 lr: 0.002\n",
      "iteration: 450620 loss: 0.0009 lr: 0.002\n",
      "iteration: 450630 loss: 0.0010 lr: 0.002\n",
      "iteration: 450640 loss: 0.0013 lr: 0.002\n",
      "iteration: 450650 loss: 0.0010 lr: 0.002\n",
      "iteration: 450660 loss: 0.0010 lr: 0.002\n",
      "iteration: 450670 loss: 0.0013 lr: 0.002\n",
      "iteration: 450680 loss: 0.0014 lr: 0.002\n",
      "iteration: 450690 loss: 0.0013 lr: 0.002\n",
      "iteration: 450700 loss: 0.0012 lr: 0.002\n",
      "iteration: 450710 loss: 0.0011 lr: 0.002\n",
      "iteration: 450720 loss: 0.0011 lr: 0.002\n",
      "iteration: 450730 loss: 0.0012 lr: 0.002\n",
      "iteration: 450740 loss: 0.0010 lr: 0.002\n",
      "iteration: 450750 loss: 0.0011 lr: 0.002\n",
      "iteration: 450760 loss: 0.0014 lr: 0.002\n",
      "iteration: 450770 loss: 0.0009 lr: 0.002\n",
      "iteration: 450780 loss: 0.0011 lr: 0.002\n",
      "iteration: 450790 loss: 0.0019 lr: 0.002\n",
      "iteration: 450800 loss: 0.0008 lr: 0.002\n",
      "iteration: 450810 loss: 0.0011 lr: 0.002\n",
      "iteration: 450820 loss: 0.0010 lr: 0.002\n",
      "iteration: 450830 loss: 0.0009 lr: 0.002\n",
      "iteration: 450840 loss: 0.0010 lr: 0.002\n",
      "iteration: 450850 loss: 0.0008 lr: 0.002\n",
      "iteration: 450860 loss: 0.0017 lr: 0.002\n",
      "iteration: 450870 loss: 0.0014 lr: 0.002\n",
      "iteration: 450880 loss: 0.0008 lr: 0.002\n",
      "iteration: 450890 loss: 0.0009 lr: 0.002\n",
      "iteration: 450900 loss: 0.0009 lr: 0.002\n",
      "iteration: 450910 loss: 0.0011 lr: 0.002\n",
      "iteration: 450920 loss: 0.0011 lr: 0.002\n",
      "iteration: 450930 loss: 0.0014 lr: 0.002\n",
      "iteration: 450940 loss: 0.0011 lr: 0.002\n",
      "iteration: 450950 loss: 0.0010 lr: 0.002\n",
      "iteration: 450960 loss: 0.0014 lr: 0.002\n",
      "iteration: 450970 loss: 0.0012 lr: 0.002\n",
      "iteration: 450980 loss: 0.0009 lr: 0.002\n",
      "iteration: 450990 loss: 0.0014 lr: 0.002\n",
      "iteration: 451000 loss: 0.0008 lr: 0.002\n",
      "iteration: 451010 loss: 0.0007 lr: 0.002\n",
      "iteration: 451020 loss: 0.0011 lr: 0.002\n",
      "iteration: 451030 loss: 0.0013 lr: 0.002\n",
      "iteration: 451040 loss: 0.0014 lr: 0.002\n",
      "iteration: 451050 loss: 0.0018 lr: 0.002\n",
      "iteration: 451060 loss: 0.0014 lr: 0.002\n",
      "iteration: 451070 loss: 0.0019 lr: 0.002\n",
      "iteration: 451080 loss: 0.0014 lr: 0.002\n",
      "iteration: 451090 loss: 0.0014 lr: 0.002\n",
      "iteration: 451100 loss: 0.0010 lr: 0.002\n",
      "iteration: 451110 loss: 0.0007 lr: 0.002\n",
      "iteration: 451120 loss: 0.0012 lr: 0.002\n",
      "iteration: 451130 loss: 0.0038 lr: 0.002\n",
      "iteration: 451140 loss: 0.0019 lr: 0.002\n",
      "iteration: 451150 loss: 0.0012 lr: 0.002\n",
      "iteration: 451160 loss: 0.0015 lr: 0.002\n",
      "iteration: 451170 loss: 0.0011 lr: 0.002\n",
      "iteration: 451180 loss: 0.0011 lr: 0.002\n",
      "iteration: 451190 loss: 0.0011 lr: 0.002\n",
      "iteration: 451200 loss: 0.0010 lr: 0.002\n",
      "iteration: 451210 loss: 0.0009 lr: 0.002\n",
      "iteration: 451220 loss: 0.0009 lr: 0.002\n",
      "iteration: 451230 loss: 0.0033 lr: 0.002\n",
      "iteration: 451240 loss: 0.0012 lr: 0.002\n",
      "iteration: 451250 loss: 0.0014 lr: 0.002\n",
      "iteration: 451260 loss: 0.0008 lr: 0.002\n",
      "iteration: 451270 loss: 0.0014 lr: 0.002\n",
      "iteration: 451280 loss: 0.0012 lr: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 451290 loss: 0.0011 lr: 0.002\n",
      "iteration: 451300 loss: 0.0009 lr: 0.002\n",
      "iteration: 451310 loss: 0.0013 lr: 0.002\n",
      "iteration: 451320 loss: 0.0011 lr: 0.002\n",
      "iteration: 451330 loss: 0.0011 lr: 0.002\n",
      "iteration: 451340 loss: 0.0011 lr: 0.002\n",
      "iteration: 451350 loss: 0.0010 lr: 0.002\n",
      "iteration: 451360 loss: 0.0019 lr: 0.002\n",
      "iteration: 451370 loss: 0.0009 lr: 0.002\n",
      "iteration: 451380 loss: 0.0013 lr: 0.002\n",
      "iteration: 451390 loss: 0.0010 lr: 0.002\n",
      "iteration: 451400 loss: 0.0011 lr: 0.002\n",
      "iteration: 451410 loss: 0.0019 lr: 0.002\n",
      "iteration: 451420 loss: 0.0013 lr: 0.002\n",
      "iteration: 451430 loss: 0.0009 lr: 0.002\n",
      "iteration: 451440 loss: 0.0012 lr: 0.002\n",
      "iteration: 451450 loss: 0.0011 lr: 0.002\n",
      "iteration: 451460 loss: 0.0013 lr: 0.002\n",
      "iteration: 451470 loss: 0.0011 lr: 0.002\n",
      "iteration: 451480 loss: 0.0007 lr: 0.002\n",
      "iteration: 451490 loss: 0.0011 lr: 0.002\n",
      "iteration: 451500 loss: 0.0013 lr: 0.002\n",
      "iteration: 451510 loss: 0.0008 lr: 0.002\n",
      "iteration: 451520 loss: 0.0016 lr: 0.002\n",
      "iteration: 451530 loss: 0.0017 lr: 0.002\n",
      "iteration: 451540 loss: 0.0009 lr: 0.002\n",
      "iteration: 451550 loss: 0.0015 lr: 0.002\n",
      "iteration: 451560 loss: 0.0008 lr: 0.002\n",
      "iteration: 451570 loss: 0.0012 lr: 0.002\n",
      "iteration: 451580 loss: 0.0009 lr: 0.002\n",
      "iteration: 451590 loss: 0.0013 lr: 0.002\n",
      "iteration: 451600 loss: 0.0015 lr: 0.002\n",
      "iteration: 451610 loss: 0.0010 lr: 0.002\n",
      "iteration: 451620 loss: 0.0012 lr: 0.002\n",
      "iteration: 451630 loss: 0.0010 lr: 0.002\n",
      "iteration: 451640 loss: 0.0008 lr: 0.002\n",
      "iteration: 451650 loss: 0.0007 lr: 0.002\n",
      "iteration: 451660 loss: 0.0008 lr: 0.002\n",
      "iteration: 451670 loss: 0.0015 lr: 0.002\n",
      "iteration: 451680 loss: 0.0014 lr: 0.002\n",
      "iteration: 451690 loss: 0.0014 lr: 0.002\n",
      "iteration: 451700 loss: 0.0009 lr: 0.002\n",
      "iteration: 451710 loss: 0.0012 lr: 0.002\n",
      "iteration: 451720 loss: 0.0015 lr: 0.002\n",
      "iteration: 451730 loss: 0.0015 lr: 0.002\n",
      "iteration: 451740 loss: 0.0011 lr: 0.002\n",
      "iteration: 451750 loss: 0.0008 lr: 0.002\n",
      "iteration: 451760 loss: 0.0014 lr: 0.002\n",
      "iteration: 451770 loss: 0.0013 lr: 0.002\n",
      "iteration: 451780 loss: 0.0018 lr: 0.002\n",
      "iteration: 451790 loss: 0.0017 lr: 0.002\n",
      "iteration: 451800 loss: 0.0007 lr: 0.002\n",
      "iteration: 451810 loss: 0.0013 lr: 0.002\n",
      "iteration: 451820 loss: 0.0010 lr: 0.002\n",
      "iteration: 451830 loss: 0.0018 lr: 0.002\n",
      "iteration: 451840 loss: 0.0013 lr: 0.002\n",
      "iteration: 451850 loss: 0.0010 lr: 0.002\n",
      "iteration: 451860 loss: 0.0008 lr: 0.002\n",
      "iteration: 451870 loss: 0.0013 lr: 0.002\n",
      "iteration: 451880 loss: 0.0011 lr: 0.002\n",
      "iteration: 451890 loss: 0.0010 lr: 0.002\n",
      "iteration: 451900 loss: 0.0008 lr: 0.002\n",
      "iteration: 451910 loss: 0.0011 lr: 0.002\n",
      "iteration: 451920 loss: 0.0013 lr: 0.002\n",
      "iteration: 451930 loss: 0.0011 lr: 0.002\n",
      "iteration: 451940 loss: 0.0025 lr: 0.002\n",
      "iteration: 451950 loss: 0.0011 lr: 0.002\n",
      "iteration: 451960 loss: 0.0014 lr: 0.002\n",
      "iteration: 451970 loss: 0.0009 lr: 0.002\n",
      "iteration: 451980 loss: 0.0011 lr: 0.002\n",
      "iteration: 451990 loss: 0.0012 lr: 0.002\n",
      "iteration: 452000 loss: 0.0013 lr: 0.002\n",
      "iteration: 452010 loss: 0.0010 lr: 0.002\n",
      "iteration: 452020 loss: 0.0017 lr: 0.002\n",
      "iteration: 452030 loss: 0.0017 lr: 0.002\n",
      "iteration: 452040 loss: 0.0013 lr: 0.002\n",
      "iteration: 452050 loss: 0.0010 lr: 0.002\n",
      "iteration: 452060 loss: 0.0010 lr: 0.002\n",
      "iteration: 452070 loss: 0.0012 lr: 0.002\n",
      "iteration: 452080 loss: 0.0011 lr: 0.002\n",
      "iteration: 452090 loss: 0.0016 lr: 0.002\n",
      "iteration: 452100 loss: 0.0013 lr: 0.002\n",
      "iteration: 452110 loss: 0.0012 lr: 0.002\n",
      "iteration: 452120 loss: 0.0015 lr: 0.002\n",
      "iteration: 452130 loss: 0.0011 lr: 0.002\n",
      "iteration: 452140 loss: 0.0011 lr: 0.002\n",
      "iteration: 452150 loss: 0.0012 lr: 0.002\n",
      "iteration: 452160 loss: 0.0012 lr: 0.002\n",
      "iteration: 452170 loss: 0.0010 lr: 0.002\n",
      "iteration: 452180 loss: 0.0010 lr: 0.002\n",
      "iteration: 452190 loss: 0.0015 lr: 0.002\n",
      "iteration: 452200 loss: 0.0008 lr: 0.002\n",
      "iteration: 452210 loss: 0.0010 lr: 0.002\n",
      "iteration: 452220 loss: 0.0015 lr: 0.002\n",
      "iteration: 452230 loss: 0.0013 lr: 0.002\n",
      "iteration: 452240 loss: 0.0013 lr: 0.002\n",
      "iteration: 452250 loss: 0.0018 lr: 0.002\n",
      "iteration: 452260 loss: 0.0013 lr: 0.002\n",
      "iteration: 452270 loss: 0.0010 lr: 0.002\n",
      "iteration: 452280 loss: 0.0012 lr: 0.002\n",
      "iteration: 452290 loss: 0.0010 lr: 0.002\n",
      "iteration: 452300 loss: 0.0013 lr: 0.002\n",
      "iteration: 452310 loss: 0.0013 lr: 0.002\n",
      "iteration: 452320 loss: 0.0013 lr: 0.002\n",
      "iteration: 452330 loss: 0.0012 lr: 0.002\n",
      "iteration: 452340 loss: 0.0013 lr: 0.002\n",
      "iteration: 452350 loss: 0.0011 lr: 0.002\n",
      "iteration: 452360 loss: 0.0012 lr: 0.002\n",
      "iteration: 452370 loss: 0.0012 lr: 0.002\n",
      "iteration: 452380 loss: 0.0012 lr: 0.002\n",
      "iteration: 452390 loss: 0.0007 lr: 0.002\n",
      "iteration: 452400 loss: 0.0019 lr: 0.002\n",
      "iteration: 452410 loss: 0.0011 lr: 0.002\n",
      "iteration: 452420 loss: 0.0017 lr: 0.002\n",
      "iteration: 452430 loss: 0.0010 lr: 0.002\n",
      "iteration: 452440 loss: 0.0015 lr: 0.002\n",
      "iteration: 452450 loss: 0.0011 lr: 0.002\n",
      "iteration: 452460 loss: 0.0013 lr: 0.002\n",
      "iteration: 452470 loss: 0.0012 lr: 0.002\n",
      "iteration: 452480 loss: 0.0012 lr: 0.002\n",
      "iteration: 452490 loss: 0.0015 lr: 0.002\n",
      "iteration: 452500 loss: 0.0012 lr: 0.002\n",
      "iteration: 452510 loss: 0.0009 lr: 0.002\n",
      "iteration: 452520 loss: 0.0011 lr: 0.002\n",
      "iteration: 452530 loss: 0.0018 lr: 0.002\n",
      "iteration: 452540 loss: 0.0010 lr: 0.002\n",
      "iteration: 452550 loss: 0.0013 lr: 0.002\n",
      "iteration: 452560 loss: 0.0020 lr: 0.002\n",
      "iteration: 452570 loss: 0.0013 lr: 0.002\n",
      "iteration: 452580 loss: 0.0010 lr: 0.002\n",
      "iteration: 452590 loss: 0.0013 lr: 0.002\n",
      "iteration: 452600 loss: 0.0012 lr: 0.002\n",
      "iteration: 452610 loss: 0.0013 lr: 0.002\n",
      "iteration: 452620 loss: 0.0011 lr: 0.002\n",
      "iteration: 452630 loss: 0.0017 lr: 0.002\n",
      "iteration: 452640 loss: 0.0017 lr: 0.002\n",
      "iteration: 452650 loss: 0.0008 lr: 0.002\n",
      "iteration: 452660 loss: 0.0013 lr: 0.002\n",
      "iteration: 452670 loss: 0.0010 lr: 0.002\n",
      "iteration: 452680 loss: 0.0010 lr: 0.002\n",
      "iteration: 452690 loss: 0.0014 lr: 0.002\n",
      "iteration: 452700 loss: 0.0010 lr: 0.002\n",
      "iteration: 452710 loss: 0.0016 lr: 0.002\n",
      "iteration: 452720 loss: 0.0011 lr: 0.002\n",
      "iteration: 452730 loss: 0.0010 lr: 0.002\n",
      "iteration: 452740 loss: 0.0014 lr: 0.002\n",
      "iteration: 452750 loss: 0.0015 lr: 0.002\n",
      "iteration: 452760 loss: 0.0010 lr: 0.002\n",
      "iteration: 452770 loss: 0.0011 lr: 0.002\n",
      "iteration: 452780 loss: 0.0011 lr: 0.002\n",
      "iteration: 452790 loss: 0.0010 lr: 0.002\n",
      "iteration: 452800 loss: 0.0017 lr: 0.002\n",
      "iteration: 452810 loss: 0.0010 lr: 0.002\n",
      "iteration: 452820 loss: 0.0014 lr: 0.002\n",
      "iteration: 452830 loss: 0.0014 lr: 0.002\n",
      "iteration: 452840 loss: 0.0010 lr: 0.002\n",
      "iteration: 452850 loss: 0.0011 lr: 0.002\n",
      "iteration: 452860 loss: 0.0007 lr: 0.002\n",
      "iteration: 452870 loss: 0.0008 lr: 0.002\n",
      "iteration: 452880 loss: 0.0016 lr: 0.002\n",
      "iteration: 452890 loss: 0.0012 lr: 0.002\n",
      "iteration: 452900 loss: 0.0011 lr: 0.002\n",
      "iteration: 452910 loss: 0.0014 lr: 0.002\n",
      "iteration: 452920 loss: 0.0009 lr: 0.002\n",
      "iteration: 452930 loss: 0.0014 lr: 0.002\n",
      "iteration: 452940 loss: 0.0013 lr: 0.002\n",
      "iteration: 452950 loss: 0.0017 lr: 0.002\n",
      "iteration: 452960 loss: 0.0012 lr: 0.002\n",
      "iteration: 452970 loss: 0.0013 lr: 0.002\n",
      "iteration: 452980 loss: 0.0013 lr: 0.002\n",
      "iteration: 452990 loss: 0.0017 lr: 0.002\n",
      "iteration: 453000 loss: 0.0016 lr: 0.002\n",
      "iteration: 453010 loss: 0.0009 lr: 0.002\n",
      "iteration: 453020 loss: 0.0021 lr: 0.002\n",
      "iteration: 453030 loss: 0.0016 lr: 0.002\n",
      "iteration: 453040 loss: 0.0017 lr: 0.002\n",
      "iteration: 453050 loss: 0.0013 lr: 0.002\n",
      "iteration: 453060 loss: 0.0012 lr: 0.002\n",
      "iteration: 453070 loss: 0.0012 lr: 0.002\n",
      "iteration: 453080 loss: 0.0009 lr: 0.002\n",
      "iteration: 453090 loss: 0.0013 lr: 0.002\n",
      "iteration: 453100 loss: 0.0010 lr: 0.002\n",
      "iteration: 453110 loss: 0.0014 lr: 0.002\n",
      "iteration: 453120 loss: 0.0010 lr: 0.002\n",
      "iteration: 453130 loss: 0.0014 lr: 0.002\n",
      "iteration: 453140 loss: 0.0009 lr: 0.002\n",
      "iteration: 453150 loss: 0.0015 lr: 0.002\n",
      "iteration: 453160 loss: 0.0018 lr: 0.002\n",
      "iteration: 453170 loss: 0.0012 lr: 0.002\n",
      "iteration: 453180 loss: 0.0011 lr: 0.002\n",
      "iteration: 453190 loss: 0.0010 lr: 0.002\n",
      "iteration: 453200 loss: 0.0010 lr: 0.002\n",
      "iteration: 453210 loss: 0.0011 lr: 0.002\n",
      "iteration: 453220 loss: 0.0009 lr: 0.002\n",
      "iteration: 453230 loss: 0.0008 lr: 0.002\n",
      "iteration: 453240 loss: 0.0021 lr: 0.002\n",
      "iteration: 453250 loss: 0.0008 lr: 0.002\n",
      "iteration: 453260 loss: 0.0011 lr: 0.002\n",
      "iteration: 453270 loss: 0.0009 lr: 0.002\n",
      "iteration: 453280 loss: 0.0012 lr: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 453290 loss: 0.0010 lr: 0.002\n",
      "iteration: 453300 loss: 0.0009 lr: 0.002\n",
      "iteration: 453310 loss: 0.0012 lr: 0.002\n",
      "iteration: 453320 loss: 0.0016 lr: 0.002\n",
      "iteration: 453330 loss: 0.0010 lr: 0.002\n",
      "iteration: 453340 loss: 0.0015 lr: 0.002\n",
      "iteration: 453350 loss: 0.0012 lr: 0.002\n",
      "iteration: 453360 loss: 0.0011 lr: 0.002\n",
      "iteration: 453370 loss: 0.0009 lr: 0.002\n",
      "iteration: 453380 loss: 0.0014 lr: 0.002\n",
      "iteration: 453390 loss: 0.0007 lr: 0.002\n",
      "iteration: 453400 loss: 0.0016 lr: 0.002\n",
      "iteration: 453410 loss: 0.0009 lr: 0.002\n",
      "iteration: 453420 loss: 0.0010 lr: 0.002\n",
      "iteration: 453430 loss: 0.0010 lr: 0.002\n",
      "iteration: 453440 loss: 0.0012 lr: 0.002\n",
      "iteration: 453450 loss: 0.0009 lr: 0.002\n",
      "iteration: 453460 loss: 0.0013 lr: 0.002\n",
      "iteration: 453470 loss: 0.0013 lr: 0.002\n",
      "iteration: 453480 loss: 0.0009 lr: 0.002\n",
      "iteration: 453490 loss: 0.0014 lr: 0.002\n",
      "iteration: 453500 loss: 0.0016 lr: 0.002\n",
      "iteration: 453510 loss: 0.0016 lr: 0.002\n",
      "iteration: 453520 loss: 0.0011 lr: 0.002\n",
      "iteration: 453530 loss: 0.0011 lr: 0.002\n",
      "iteration: 453540 loss: 0.0014 lr: 0.002\n",
      "iteration: 453550 loss: 0.0011 lr: 0.002\n",
      "iteration: 453560 loss: 0.0017 lr: 0.002\n",
      "iteration: 453570 loss: 0.0010 lr: 0.002\n",
      "iteration: 453580 loss: 0.0011 lr: 0.002\n",
      "iteration: 453590 loss: 0.0012 lr: 0.002\n",
      "iteration: 453600 loss: 0.0014 lr: 0.002\n",
      "iteration: 453610 loss: 0.0013 lr: 0.002\n",
      "iteration: 453620 loss: 0.0013 lr: 0.002\n",
      "iteration: 453630 loss: 0.0010 lr: 0.002\n",
      "iteration: 453640 loss: 0.0009 lr: 0.002\n",
      "iteration: 453650 loss: 0.0011 lr: 0.002\n",
      "iteration: 453660 loss: 0.0012 lr: 0.002\n",
      "iteration: 453670 loss: 0.0018 lr: 0.002\n",
      "iteration: 453680 loss: 0.0019 lr: 0.002\n",
      "iteration: 453690 loss: 0.0011 lr: 0.002\n",
      "iteration: 453700 loss: 0.0013 lr: 0.002\n",
      "iteration: 453710 loss: 0.0013 lr: 0.002\n",
      "iteration: 453720 loss: 0.0010 lr: 0.002\n",
      "iteration: 453730 loss: 0.0015 lr: 0.002\n",
      "iteration: 453740 loss: 0.0009 lr: 0.002\n",
      "iteration: 453750 loss: 0.0016 lr: 0.002\n",
      "iteration: 453760 loss: 0.0011 lr: 0.002\n",
      "iteration: 453770 loss: 0.0022 lr: 0.002\n",
      "iteration: 453780 loss: 0.0015 lr: 0.002\n",
      "iteration: 453790 loss: 0.0012 lr: 0.002\n",
      "iteration: 453800 loss: 0.0014 lr: 0.002\n",
      "iteration: 453810 loss: 0.0012 lr: 0.002\n",
      "iteration: 453820 loss: 0.0012 lr: 0.002\n",
      "iteration: 453830 loss: 0.0011 lr: 0.002\n",
      "iteration: 453840 loss: 0.0013 lr: 0.002\n",
      "iteration: 453850 loss: 0.0012 lr: 0.002\n",
      "iteration: 453860 loss: 0.0009 lr: 0.002\n",
      "iteration: 453870 loss: 0.0014 lr: 0.002\n",
      "iteration: 453880 loss: 0.0013 lr: 0.002\n",
      "iteration: 453890 loss: 0.0011 lr: 0.002\n",
      "iteration: 453900 loss: 0.0007 lr: 0.002\n",
      "iteration: 453910 loss: 0.0011 lr: 0.002\n",
      "iteration: 453920 loss: 0.0012 lr: 0.002\n",
      "iteration: 453930 loss: 0.0011 lr: 0.002\n",
      "iteration: 453940 loss: 0.0011 lr: 0.002\n",
      "iteration: 453950 loss: 0.0012 lr: 0.002\n",
      "iteration: 453960 loss: 0.0011 lr: 0.002\n",
      "iteration: 453970 loss: 0.0011 lr: 0.002\n",
      "iteration: 453980 loss: 0.0010 lr: 0.002\n",
      "iteration: 453990 loss: 0.0011 lr: 0.002\n",
      "iteration: 454000 loss: 0.0018 lr: 0.002\n",
      "iteration: 454010 loss: 0.0010 lr: 0.002\n",
      "iteration: 454020 loss: 0.0013 lr: 0.002\n",
      "iteration: 454030 loss: 0.0017 lr: 0.002\n",
      "iteration: 454040 loss: 0.0013 lr: 0.002\n",
      "iteration: 454050 loss: 0.0008 lr: 0.002\n",
      "iteration: 454060 loss: 0.0012 lr: 0.002\n",
      "iteration: 454070 loss: 0.0010 lr: 0.002\n",
      "iteration: 454080 loss: 0.0010 lr: 0.002\n",
      "iteration: 454090 loss: 0.0018 lr: 0.002\n",
      "iteration: 454100 loss: 0.0012 lr: 0.002\n",
      "iteration: 454110 loss: 0.0016 lr: 0.002\n",
      "iteration: 454120 loss: 0.0010 lr: 0.002\n",
      "iteration: 454130 loss: 0.0012 lr: 0.002\n",
      "iteration: 454140 loss: 0.0010 lr: 0.002\n",
      "iteration: 454150 loss: 0.0012 lr: 0.002\n",
      "iteration: 454160 loss: 0.0012 lr: 0.002\n",
      "iteration: 454170 loss: 0.0015 lr: 0.002\n",
      "iteration: 454180 loss: 0.0008 lr: 0.002\n",
      "iteration: 454190 loss: 0.0011 lr: 0.002\n",
      "iteration: 454200 loss: 0.0012 lr: 0.002\n",
      "iteration: 454210 loss: 0.0010 lr: 0.002\n",
      "iteration: 454220 loss: 0.0011 lr: 0.002\n",
      "iteration: 454230 loss: 0.0012 lr: 0.002\n",
      "iteration: 454240 loss: 0.0008 lr: 0.002\n",
      "iteration: 454250 loss: 0.0008 lr: 0.002\n",
      "iteration: 454260 loss: 0.0012 lr: 0.002\n",
      "iteration: 454270 loss: 0.0014 lr: 0.002\n",
      "iteration: 454280 loss: 0.0010 lr: 0.002\n",
      "iteration: 454290 loss: 0.0010 lr: 0.002\n",
      "iteration: 454300 loss: 0.0012 lr: 0.002\n",
      "iteration: 454310 loss: 0.0017 lr: 0.002\n",
      "iteration: 454320 loss: 0.0011 lr: 0.002\n",
      "iteration: 454330 loss: 0.0010 lr: 0.002\n",
      "iteration: 454340 loss: 0.0011 lr: 0.002\n",
      "iteration: 454350 loss: 0.0008 lr: 0.002\n",
      "iteration: 454360 loss: 0.0018 lr: 0.002\n",
      "iteration: 454370 loss: 0.0019 lr: 0.002\n",
      "iteration: 454380 loss: 0.0012 lr: 0.002\n",
      "iteration: 454390 loss: 0.0011 lr: 0.002\n",
      "iteration: 454400 loss: 0.0010 lr: 0.002\n",
      "iteration: 454410 loss: 0.0012 lr: 0.002\n",
      "iteration: 454420 loss: 0.0018 lr: 0.002\n",
      "iteration: 454430 loss: 0.0012 lr: 0.002\n",
      "iteration: 454440 loss: 0.0012 lr: 0.002\n",
      "iteration: 454450 loss: 0.0016 lr: 0.002\n",
      "iteration: 454460 loss: 0.0012 lr: 0.002\n",
      "iteration: 454470 loss: 0.0014 lr: 0.002\n",
      "iteration: 454480 loss: 0.0010 lr: 0.002\n",
      "iteration: 454490 loss: 0.0009 lr: 0.002\n",
      "iteration: 454500 loss: 0.0013 lr: 0.002\n",
      "iteration: 454510 loss: 0.0009 lr: 0.002\n",
      "iteration: 454520 loss: 0.0018 lr: 0.002\n",
      "iteration: 454530 loss: 0.0013 lr: 0.002\n",
      "iteration: 454540 loss: 0.0014 lr: 0.002\n",
      "iteration: 454550 loss: 0.0012 lr: 0.002\n",
      "iteration: 454560 loss: 0.0009 lr: 0.002\n",
      "iteration: 454570 loss: 0.0012 lr: 0.002\n",
      "iteration: 454580 loss: 0.0010 lr: 0.002\n",
      "iteration: 454590 loss: 0.0013 lr: 0.002\n",
      "iteration: 454600 loss: 0.0015 lr: 0.002\n",
      "iteration: 454610 loss: 0.0013 lr: 0.002\n",
      "iteration: 454620 loss: 0.0009 lr: 0.002\n",
      "iteration: 454630 loss: 0.0010 lr: 0.002\n",
      "iteration: 454640 loss: 0.0011 lr: 0.002\n",
      "iteration: 454650 loss: 0.0010 lr: 0.002\n",
      "iteration: 454660 loss: 0.0013 lr: 0.002\n",
      "iteration: 454670 loss: 0.0007 lr: 0.002\n",
      "iteration: 454680 loss: 0.0020 lr: 0.002\n",
      "iteration: 454690 loss: 0.0016 lr: 0.002\n",
      "iteration: 454700 loss: 0.0014 lr: 0.002\n",
      "iteration: 454710 loss: 0.0013 lr: 0.002\n",
      "iteration: 454720 loss: 0.0009 lr: 0.002\n",
      "iteration: 454730 loss: 0.0010 lr: 0.002\n",
      "iteration: 454740 loss: 0.0016 lr: 0.002\n",
      "iteration: 454750 loss: 0.0011 lr: 0.002\n",
      "iteration: 454760 loss: 0.0016 lr: 0.002\n",
      "iteration: 454770 loss: 0.0016 lr: 0.002\n",
      "iteration: 454780 loss: 0.0013 lr: 0.002\n",
      "iteration: 454790 loss: 0.0008 lr: 0.002\n",
      "iteration: 454800 loss: 0.0013 lr: 0.002\n",
      "iteration: 454810 loss: 0.0014 lr: 0.002\n",
      "iteration: 454820 loss: 0.0013 lr: 0.002\n",
      "iteration: 454830 loss: 0.0010 lr: 0.002\n",
      "iteration: 454840 loss: 0.0014 lr: 0.002\n",
      "iteration: 454850 loss: 0.0017 lr: 0.002\n",
      "iteration: 454860 loss: 0.0009 lr: 0.002\n",
      "iteration: 454870 loss: 0.0016 lr: 0.002\n",
      "iteration: 454880 loss: 0.0013 lr: 0.002\n",
      "iteration: 454890 loss: 0.0015 lr: 0.002\n",
      "iteration: 454900 loss: 0.0012 lr: 0.002\n",
      "iteration: 454910 loss: 0.0010 lr: 0.002\n",
      "iteration: 454920 loss: 0.0009 lr: 0.002\n",
      "iteration: 454930 loss: 0.0018 lr: 0.002\n",
      "iteration: 454940 loss: 0.0011 lr: 0.002\n",
      "iteration: 454950 loss: 0.0012 lr: 0.002\n",
      "iteration: 454960 loss: 0.0011 lr: 0.002\n",
      "iteration: 454970 loss: 0.0026 lr: 0.002\n",
      "iteration: 454980 loss: 0.0008 lr: 0.002\n",
      "iteration: 454990 loss: 0.0015 lr: 0.002\n",
      "iteration: 455000 loss: 0.0012 lr: 0.002\n",
      "iteration: 455010 loss: 0.0013 lr: 0.002\n",
      "iteration: 455020 loss: 0.0008 lr: 0.002\n",
      "iteration: 455030 loss: 0.0007 lr: 0.002\n",
      "iteration: 455040 loss: 0.0013 lr: 0.002\n",
      "iteration: 455050 loss: 0.0016 lr: 0.002\n",
      "iteration: 455060 loss: 0.0009 lr: 0.002\n",
      "iteration: 455070 loss: 0.0011 lr: 0.002\n",
      "iteration: 455080 loss: 0.0010 lr: 0.002\n",
      "iteration: 455090 loss: 0.0017 lr: 0.002\n",
      "iteration: 455100 loss: 0.0015 lr: 0.002\n",
      "iteration: 455110 loss: 0.0011 lr: 0.002\n",
      "iteration: 455120 loss: 0.0011 lr: 0.002\n",
      "iteration: 455130 loss: 0.0011 lr: 0.002\n",
      "iteration: 455140 loss: 0.0013 lr: 0.002\n",
      "iteration: 455150 loss: 0.0012 lr: 0.002\n",
      "iteration: 455160 loss: 0.0009 lr: 0.002\n",
      "iteration: 455170 loss: 0.0010 lr: 0.002\n",
      "iteration: 455180 loss: 0.0012 lr: 0.002\n",
      "iteration: 455190 loss: 0.0016 lr: 0.002\n",
      "iteration: 455200 loss: 0.0011 lr: 0.002\n",
      "iteration: 455210 loss: 0.0012 lr: 0.002\n",
      "iteration: 455220 loss: 0.0009 lr: 0.002\n",
      "iteration: 455230 loss: 0.0009 lr: 0.002\n",
      "iteration: 455240 loss: 0.0011 lr: 0.002\n",
      "iteration: 455250 loss: 0.0009 lr: 0.002\n",
      "iteration: 455260 loss: 0.0014 lr: 0.002\n",
      "iteration: 455270 loss: 0.0011 lr: 0.002\n",
      "iteration: 455280 loss: 0.0010 lr: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 455290 loss: 0.0014 lr: 0.002\n",
      "iteration: 455300 loss: 0.0012 lr: 0.002\n",
      "iteration: 455310 loss: 0.0011 lr: 0.002\n",
      "iteration: 455320 loss: 0.0017 lr: 0.002\n",
      "iteration: 455330 loss: 0.0010 lr: 0.002\n",
      "iteration: 455340 loss: 0.0014 lr: 0.002\n",
      "iteration: 455350 loss: 0.0016 lr: 0.002\n",
      "iteration: 455360 loss: 0.0011 lr: 0.002\n",
      "iteration: 455370 loss: 0.0014 lr: 0.002\n",
      "iteration: 455380 loss: 0.0009 lr: 0.002\n",
      "iteration: 455390 loss: 0.0012 lr: 0.002\n",
      "iteration: 455400 loss: 0.0014 lr: 0.002\n",
      "iteration: 455410 loss: 0.0013 lr: 0.002\n",
      "iteration: 455420 loss: 0.0008 lr: 0.002\n",
      "iteration: 455430 loss: 0.0017 lr: 0.002\n",
      "iteration: 455440 loss: 0.0011 lr: 0.002\n",
      "iteration: 455450 loss: 0.0010 lr: 0.002\n",
      "iteration: 455460 loss: 0.0010 lr: 0.002\n",
      "iteration: 455470 loss: 0.0012 lr: 0.002\n",
      "iteration: 455480 loss: 0.0013 lr: 0.002\n",
      "iteration: 455490 loss: 0.0010 lr: 0.002\n",
      "iteration: 455500 loss: 0.0011 lr: 0.002\n",
      "iteration: 455510 loss: 0.0011 lr: 0.002\n",
      "iteration: 455520 loss: 0.0015 lr: 0.002\n",
      "iteration: 455530 loss: 0.0015 lr: 0.002\n",
      "iteration: 455540 loss: 0.0013 lr: 0.002\n",
      "iteration: 455550 loss: 0.0010 lr: 0.002\n",
      "iteration: 455560 loss: 0.0009 lr: 0.002\n",
      "iteration: 455570 loss: 0.0013 lr: 0.002\n",
      "iteration: 455580 loss: 0.0013 lr: 0.002\n",
      "iteration: 455590 loss: 0.0013 lr: 0.002\n",
      "iteration: 455600 loss: 0.0009 lr: 0.002\n",
      "iteration: 455610 loss: 0.0013 lr: 0.002\n",
      "iteration: 455620 loss: 0.0010 lr: 0.002\n",
      "iteration: 455630 loss: 0.0011 lr: 0.002\n",
      "iteration: 455640 loss: 0.0010 lr: 0.002\n",
      "iteration: 455650 loss: 0.0010 lr: 0.002\n",
      "iteration: 455660 loss: 0.0012 lr: 0.002\n",
      "iteration: 455670 loss: 0.0008 lr: 0.002\n",
      "iteration: 455680 loss: 0.0010 lr: 0.002\n",
      "iteration: 455690 loss: 0.0013 lr: 0.002\n",
      "iteration: 455700 loss: 0.0017 lr: 0.002\n",
      "iteration: 455710 loss: 0.0011 lr: 0.002\n",
      "iteration: 455720 loss: 0.0008 lr: 0.002\n",
      "iteration: 455730 loss: 0.0012 lr: 0.002\n",
      "iteration: 455740 loss: 0.0010 lr: 0.002\n",
      "iteration: 455750 loss: 0.0013 lr: 0.002\n",
      "iteration: 455760 loss: 0.0015 lr: 0.002\n",
      "iteration: 455770 loss: 0.0010 lr: 0.002\n",
      "iteration: 455780 loss: 0.0010 lr: 0.002\n",
      "iteration: 455790 loss: 0.0007 lr: 0.002\n",
      "iteration: 455800 loss: 0.0016 lr: 0.002\n",
      "iteration: 455810 loss: 0.0016 lr: 0.002\n",
      "iteration: 455820 loss: 0.0008 lr: 0.002\n",
      "iteration: 455830 loss: 0.0014 lr: 0.002\n",
      "iteration: 455840 loss: 0.0016 lr: 0.002\n",
      "iteration: 455850 loss: 0.0011 lr: 0.002\n",
      "iteration: 455860 loss: 0.0010 lr: 0.002\n",
      "iteration: 455870 loss: 0.0010 lr: 0.002\n",
      "iteration: 455880 loss: 0.0010 lr: 0.002\n",
      "iteration: 455890 loss: 0.0014 lr: 0.002\n",
      "iteration: 455900 loss: 0.0013 lr: 0.002\n",
      "iteration: 455910 loss: 0.0013 lr: 0.002\n",
      "iteration: 455920 loss: 0.0013 lr: 0.002\n",
      "iteration: 455930 loss: 0.0013 lr: 0.002\n",
      "iteration: 455940 loss: 0.0016 lr: 0.002\n",
      "iteration: 455950 loss: 0.0011 lr: 0.002\n",
      "iteration: 455960 loss: 0.0011 lr: 0.002\n",
      "iteration: 455970 loss: 0.0012 lr: 0.002\n",
      "iteration: 455980 loss: 0.0016 lr: 0.002\n",
      "iteration: 455990 loss: 0.0011 lr: 0.002\n",
      "iteration: 456000 loss: 0.0011 lr: 0.002\n",
      "iteration: 456010 loss: 0.0009 lr: 0.002\n",
      "iteration: 456020 loss: 0.0014 lr: 0.002\n",
      "iteration: 456030 loss: 0.0010 lr: 0.002\n",
      "iteration: 456040 loss: 0.0012 lr: 0.002\n",
      "iteration: 456050 loss: 0.0012 lr: 0.002\n",
      "iteration: 456060 loss: 0.0009 lr: 0.002\n",
      "iteration: 456070 loss: 0.0010 lr: 0.002\n",
      "iteration: 456080 loss: 0.0009 lr: 0.002\n",
      "iteration: 456090 loss: 0.0010 lr: 0.002\n",
      "iteration: 456100 loss: 0.0014 lr: 0.002\n",
      "iteration: 456110 loss: 0.0014 lr: 0.002\n",
      "iteration: 456120 loss: 0.0011 lr: 0.002\n",
      "iteration: 456130 loss: 0.0018 lr: 0.002\n",
      "iteration: 456140 loss: 0.0016 lr: 0.002\n",
      "iteration: 456150 loss: 0.0011 lr: 0.002\n",
      "iteration: 456160 loss: 0.0018 lr: 0.002\n",
      "iteration: 456170 loss: 0.0012 lr: 0.002\n",
      "iteration: 456180 loss: 0.0015 lr: 0.002\n",
      "iteration: 456190 loss: 0.0014 lr: 0.002\n",
      "iteration: 456200 loss: 0.0012 lr: 0.002\n",
      "iteration: 456210 loss: 0.0011 lr: 0.002\n",
      "iteration: 456220 loss: 0.0007 lr: 0.002\n",
      "iteration: 456230 loss: 0.0007 lr: 0.002\n",
      "iteration: 456240 loss: 0.0011 lr: 0.002\n",
      "iteration: 456250 loss: 0.0010 lr: 0.002\n",
      "iteration: 456260 loss: 0.0009 lr: 0.002\n",
      "iteration: 456270 loss: 0.0010 lr: 0.002\n",
      "iteration: 456280 loss: 0.0010 lr: 0.002\n",
      "iteration: 456290 loss: 0.0013 lr: 0.002\n",
      "iteration: 456300 loss: 0.0011 lr: 0.002\n",
      "iteration: 456310 loss: 0.0011 lr: 0.002\n",
      "iteration: 456320 loss: 0.0009 lr: 0.002\n",
      "iteration: 456330 loss: 0.0011 lr: 0.002\n",
      "iteration: 456340 loss: 0.0010 lr: 0.002\n",
      "iteration: 456350 loss: 0.0008 lr: 0.002\n",
      "iteration: 456360 loss: 0.0013 lr: 0.002\n",
      "iteration: 456370 loss: 0.0010 lr: 0.002\n",
      "iteration: 456380 loss: 0.0011 lr: 0.002\n",
      "iteration: 456390 loss: 0.0010 lr: 0.002\n",
      "iteration: 456400 loss: 0.0009 lr: 0.002\n",
      "iteration: 456410 loss: 0.0019 lr: 0.002\n",
      "iteration: 456420 loss: 0.0010 lr: 0.002\n",
      "iteration: 456430 loss: 0.0009 lr: 0.002\n",
      "iteration: 456440 loss: 0.0010 lr: 0.002\n",
      "iteration: 456450 loss: 0.0010 lr: 0.002\n",
      "iteration: 456460 loss: 0.0015 lr: 0.002\n",
      "iteration: 456470 loss: 0.0013 lr: 0.002\n",
      "iteration: 456480 loss: 0.0011 lr: 0.002\n",
      "iteration: 456490 loss: 0.0015 lr: 0.002\n",
      "iteration: 456500 loss: 0.0011 lr: 0.002\n",
      "iteration: 456510 loss: 0.0009 lr: 0.002\n",
      "iteration: 456520 loss: 0.0012 lr: 0.002\n",
      "iteration: 456530 loss: 0.0009 lr: 0.002\n",
      "iteration: 456540 loss: 0.0013 lr: 0.002\n",
      "iteration: 456550 loss: 0.0013 lr: 0.002\n",
      "iteration: 456560 loss: 0.0012 lr: 0.002\n",
      "iteration: 456570 loss: 0.0012 lr: 0.002\n",
      "iteration: 456580 loss: 0.0011 lr: 0.002\n",
      "iteration: 456590 loss: 0.0009 lr: 0.002\n",
      "iteration: 456600 loss: 0.0008 lr: 0.002\n",
      "iteration: 456610 loss: 0.0011 lr: 0.002\n",
      "iteration: 456620 loss: 0.0010 lr: 0.002\n",
      "iteration: 456630 loss: 0.0020 lr: 0.002\n",
      "iteration: 456640 loss: 0.0011 lr: 0.002\n",
      "iteration: 456650 loss: 0.0012 lr: 0.002\n",
      "iteration: 456660 loss: 0.0012 lr: 0.002\n",
      "iteration: 456670 loss: 0.0012 lr: 0.002\n",
      "iteration: 456680 loss: 0.0013 lr: 0.002\n",
      "iteration: 456690 loss: 0.0013 lr: 0.002\n",
      "iteration: 456700 loss: 0.0012 lr: 0.002\n",
      "iteration: 456710 loss: 0.0009 lr: 0.002\n",
      "iteration: 456720 loss: 0.0013 lr: 0.002\n",
      "iteration: 456730 loss: 0.0015 lr: 0.002\n",
      "iteration: 456740 loss: 0.0013 lr: 0.002\n",
      "iteration: 456750 loss: 0.0013 lr: 0.002\n",
      "iteration: 456760 loss: 0.0011 lr: 0.002\n",
      "iteration: 456770 loss: 0.0010 lr: 0.002\n",
      "iteration: 456780 loss: 0.0014 lr: 0.002\n",
      "iteration: 456790 loss: 0.0019 lr: 0.002\n",
      "iteration: 456800 loss: 0.0018 lr: 0.002\n",
      "iteration: 456810 loss: 0.0015 lr: 0.002\n",
      "iteration: 456820 loss: 0.0014 lr: 0.002\n",
      "iteration: 456830 loss: 0.0011 lr: 0.002\n",
      "iteration: 456840 loss: 0.0014 lr: 0.002\n",
      "iteration: 456850 loss: 0.0015 lr: 0.002\n",
      "iteration: 456860 loss: 0.0010 lr: 0.002\n",
      "iteration: 456870 loss: 0.0018 lr: 0.002\n",
      "iteration: 456880 loss: 0.0009 lr: 0.002\n",
      "iteration: 456890 loss: 0.0009 lr: 0.002\n",
      "iteration: 456900 loss: 0.0010 lr: 0.002\n",
      "iteration: 456910 loss: 0.0015 lr: 0.002\n",
      "iteration: 456920 loss: 0.0012 lr: 0.002\n",
      "iteration: 456930 loss: 0.0008 lr: 0.002\n",
      "iteration: 456940 loss: 0.0010 lr: 0.002\n",
      "iteration: 456950 loss: 0.0010 lr: 0.002\n",
      "iteration: 456960 loss: 0.0018 lr: 0.002\n",
      "iteration: 456970 loss: 0.0013 lr: 0.002\n",
      "iteration: 456980 loss: 0.0010 lr: 0.002\n",
      "iteration: 456990 loss: 0.0011 lr: 0.002\n",
      "iteration: 457000 loss: 0.0015 lr: 0.002\n",
      "iteration: 457010 loss: 0.0019 lr: 0.002\n",
      "iteration: 457020 loss: 0.0011 lr: 0.002\n",
      "iteration: 457030 loss: 0.0010 lr: 0.002\n",
      "iteration: 457040 loss: 0.0009 lr: 0.002\n",
      "iteration: 457050 loss: 0.0012 lr: 0.002\n",
      "iteration: 457060 loss: 0.0011 lr: 0.002\n",
      "iteration: 457070 loss: 0.0011 lr: 0.002\n",
      "iteration: 457080 loss: 0.0010 lr: 0.002\n",
      "iteration: 457090 loss: 0.0009 lr: 0.002\n",
      "iteration: 457100 loss: 0.0010 lr: 0.002\n",
      "iteration: 457110 loss: 0.0011 lr: 0.002\n",
      "iteration: 457120 loss: 0.0012 lr: 0.002\n",
      "iteration: 457130 loss: 0.0017 lr: 0.002\n",
      "iteration: 457140 loss: 0.0013 lr: 0.002\n",
      "iteration: 457150 loss: 0.0012 lr: 0.002\n",
      "iteration: 457160 loss: 0.0010 lr: 0.002\n",
      "iteration: 457170 loss: 0.0009 lr: 0.002\n",
      "iteration: 457180 loss: 0.0016 lr: 0.002\n",
      "iteration: 457190 loss: 0.0013 lr: 0.002\n",
      "iteration: 457200 loss: 0.0011 lr: 0.002\n",
      "iteration: 457210 loss: 0.0010 lr: 0.002\n",
      "iteration: 457220 loss: 0.0007 lr: 0.002\n",
      "iteration: 457230 loss: 0.0010 lr: 0.002\n",
      "iteration: 457240 loss: 0.0009 lr: 0.002\n",
      "iteration: 457250 loss: 0.0013 lr: 0.002\n",
      "iteration: 457260 loss: 0.0013 lr: 0.002\n",
      "iteration: 457270 loss: 0.0011 lr: 0.002\n",
      "iteration: 457280 loss: 0.0009 lr: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 457290 loss: 0.0011 lr: 0.002\n",
      "iteration: 457300 loss: 0.0013 lr: 0.002\n",
      "iteration: 457310 loss: 0.0011 lr: 0.002\n",
      "iteration: 457320 loss: 0.0011 lr: 0.002\n",
      "iteration: 457330 loss: 0.0010 lr: 0.002\n",
      "iteration: 457340 loss: 0.0012 lr: 0.002\n",
      "iteration: 457350 loss: 0.0011 lr: 0.002\n",
      "iteration: 457360 loss: 0.0016 lr: 0.002\n",
      "iteration: 457370 loss: 0.0009 lr: 0.002\n",
      "iteration: 457380 loss: 0.0008 lr: 0.002\n",
      "iteration: 457390 loss: 0.0012 lr: 0.002\n",
      "iteration: 457400 loss: 0.0014 lr: 0.002\n",
      "iteration: 457410 loss: 0.0011 lr: 0.002\n",
      "iteration: 457420 loss: 0.0008 lr: 0.002\n",
      "iteration: 457430 loss: 0.0014 lr: 0.002\n",
      "iteration: 457440 loss: 0.0012 lr: 0.002\n",
      "iteration: 457450 loss: 0.0011 lr: 0.002\n",
      "iteration: 457460 loss: 0.0011 lr: 0.002\n",
      "iteration: 457470 loss: 0.0023 lr: 0.002\n",
      "iteration: 457480 loss: 0.0013 lr: 0.002\n",
      "iteration: 457490 loss: 0.0018 lr: 0.002\n",
      "iteration: 457500 loss: 0.0019 lr: 0.002\n",
      "iteration: 457510 loss: 0.0010 lr: 0.002\n",
      "iteration: 457520 loss: 0.0014 lr: 0.002\n",
      "iteration: 457530 loss: 0.0011 lr: 0.002\n",
      "iteration: 457540 loss: 0.0013 lr: 0.002\n",
      "iteration: 457550 loss: 0.0014 lr: 0.002\n",
      "iteration: 457560 loss: 0.0013 lr: 0.002\n",
      "iteration: 457570 loss: 0.0013 lr: 0.002\n",
      "iteration: 457580 loss: 0.0010 lr: 0.002\n",
      "iteration: 457590 loss: 0.0011 lr: 0.002\n",
      "iteration: 457600 loss: 0.0012 lr: 0.002\n",
      "iteration: 457610 loss: 0.0010 lr: 0.002\n",
      "iteration: 457620 loss: 0.0013 lr: 0.002\n",
      "iteration: 457630 loss: 0.0020 lr: 0.002\n",
      "iteration: 457640 loss: 0.0008 lr: 0.002\n",
      "iteration: 457650 loss: 0.0008 lr: 0.002\n",
      "iteration: 457660 loss: 0.0020 lr: 0.002\n",
      "iteration: 457670 loss: 0.0011 lr: 0.002\n",
      "iteration: 457680 loss: 0.0011 lr: 0.002\n",
      "iteration: 457690 loss: 0.0018 lr: 0.002\n",
      "iteration: 457700 loss: 0.0017 lr: 0.002\n",
      "iteration: 457710 loss: 0.0016 lr: 0.002\n",
      "iteration: 457720 loss: 0.0015 lr: 0.002\n",
      "iteration: 457730 loss: 0.0013 lr: 0.002\n",
      "iteration: 457740 loss: 0.0013 lr: 0.002\n",
      "iteration: 457750 loss: 0.0010 lr: 0.002\n",
      "iteration: 457760 loss: 0.0010 lr: 0.002\n",
      "iteration: 457770 loss: 0.0010 lr: 0.002\n",
      "iteration: 457780 loss: 0.0013 lr: 0.002\n",
      "iteration: 457790 loss: 0.0011 lr: 0.002\n",
      "iteration: 457800 loss: 0.0010 lr: 0.002\n",
      "iteration: 457810 loss: 0.0013 lr: 0.002\n",
      "iteration: 457820 loss: 0.0010 lr: 0.002\n",
      "iteration: 457830 loss: 0.0020 lr: 0.002\n",
      "iteration: 457840 loss: 0.0010 lr: 0.002\n",
      "iteration: 457850 loss: 0.0009 lr: 0.002\n",
      "iteration: 457860 loss: 0.0015 lr: 0.002\n",
      "iteration: 457870 loss: 0.0013 lr: 0.002\n",
      "iteration: 457880 loss: 0.0014 lr: 0.002\n",
      "iteration: 457890 loss: 0.0016 lr: 0.002\n",
      "iteration: 457900 loss: 0.0015 lr: 0.002\n",
      "iteration: 457910 loss: 0.0013 lr: 0.002\n",
      "iteration: 457920 loss: 0.0012 lr: 0.002\n",
      "iteration: 457930 loss: 0.0013 lr: 0.002\n",
      "iteration: 457940 loss: 0.0014 lr: 0.002\n",
      "iteration: 457950 loss: 0.0011 lr: 0.002\n",
      "iteration: 457960 loss: 0.0019 lr: 0.002\n",
      "iteration: 457970 loss: 0.0007 lr: 0.002\n",
      "iteration: 457980 loss: 0.0013 lr: 0.002\n",
      "iteration: 457990 loss: 0.0018 lr: 0.002\n",
      "iteration: 458000 loss: 0.0011 lr: 0.002\n",
      "iteration: 458010 loss: 0.0012 lr: 0.002\n",
      "iteration: 458020 loss: 0.0009 lr: 0.002\n",
      "iteration: 458030 loss: 0.0011 lr: 0.002\n",
      "iteration: 458040 loss: 0.0010 lr: 0.002\n",
      "iteration: 458050 loss: 0.0015 lr: 0.002\n",
      "iteration: 458060 loss: 0.0011 lr: 0.002\n",
      "iteration: 458070 loss: 0.0014 lr: 0.002\n",
      "iteration: 458080 loss: 0.0013 lr: 0.002\n",
      "iteration: 458090 loss: 0.0008 lr: 0.002\n",
      "iteration: 458100 loss: 0.0012 lr: 0.002\n",
      "iteration: 458110 loss: 0.0010 lr: 0.002\n",
      "iteration: 458120 loss: 0.0008 lr: 0.002\n",
      "iteration: 458130 loss: 0.0009 lr: 0.002\n",
      "iteration: 458140 loss: 0.0013 lr: 0.002\n",
      "iteration: 458150 loss: 0.0010 lr: 0.002\n",
      "iteration: 458160 loss: 0.0010 lr: 0.002\n",
      "iteration: 458170 loss: 0.0012 lr: 0.002\n",
      "iteration: 458180 loss: 0.0010 lr: 0.002\n",
      "iteration: 458190 loss: 0.0008 lr: 0.002\n",
      "iteration: 458200 loss: 0.0008 lr: 0.002\n",
      "iteration: 458210 loss: 0.0013 lr: 0.002\n",
      "iteration: 458220 loss: 0.0009 lr: 0.002\n",
      "iteration: 458230 loss: 0.0011 lr: 0.002\n",
      "iteration: 458240 loss: 0.0013 lr: 0.002\n",
      "iteration: 458250 loss: 0.0016 lr: 0.002\n",
      "iteration: 458260 loss: 0.0014 lr: 0.002\n",
      "iteration: 458270 loss: 0.0015 lr: 0.002\n",
      "iteration: 458280 loss: 0.0009 lr: 0.002\n",
      "iteration: 458290 loss: 0.0014 lr: 0.002\n",
      "iteration: 458300 loss: 0.0008 lr: 0.002\n",
      "iteration: 458310 loss: 0.0022 lr: 0.002\n",
      "iteration: 458320 loss: 0.0012 lr: 0.002\n",
      "iteration: 458330 loss: 0.0010 lr: 0.002\n",
      "iteration: 458340 loss: 0.0012 lr: 0.002\n",
      "iteration: 458350 loss: 0.0010 lr: 0.002\n",
      "iteration: 458360 loss: 0.0010 lr: 0.002\n",
      "iteration: 458370 loss: 0.0010 lr: 0.002\n",
      "iteration: 458380 loss: 0.0007 lr: 0.002\n",
      "iteration: 458390 loss: 0.0013 lr: 0.002\n",
      "iteration: 458400 loss: 0.0012 lr: 0.002\n",
      "iteration: 458410 loss: 0.0010 lr: 0.002\n",
      "iteration: 458420 loss: 0.0027 lr: 0.002\n",
      "iteration: 458430 loss: 0.0010 lr: 0.002\n",
      "iteration: 458440 loss: 0.0018 lr: 0.002\n",
      "iteration: 458450 loss: 0.0014 lr: 0.002\n",
      "iteration: 458460 loss: 0.0014 lr: 0.002\n",
      "iteration: 458470 loss: 0.0018 lr: 0.002\n",
      "iteration: 458480 loss: 0.0012 lr: 0.002\n",
      "iteration: 458490 loss: 0.0012 lr: 0.002\n",
      "iteration: 458500 loss: 0.0012 lr: 0.002\n",
      "iteration: 458510 loss: 0.0012 lr: 0.002\n",
      "iteration: 458520 loss: 0.0012 lr: 0.002\n",
      "iteration: 458530 loss: 0.0010 lr: 0.002\n",
      "iteration: 458540 loss: 0.0009 lr: 0.002\n",
      "iteration: 458550 loss: 0.0009 lr: 0.002\n",
      "iteration: 458560 loss: 0.0012 lr: 0.002\n",
      "iteration: 458570 loss: 0.0011 lr: 0.002\n",
      "iteration: 458580 loss: 0.0014 lr: 0.002\n",
      "iteration: 458590 loss: 0.0014 lr: 0.002\n",
      "iteration: 458600 loss: 0.0009 lr: 0.002\n",
      "iteration: 458610 loss: 0.0011 lr: 0.002\n",
      "iteration: 458620 loss: 0.0015 lr: 0.002\n",
      "iteration: 458630 loss: 0.0010 lr: 0.002\n",
      "iteration: 458640 loss: 0.0011 lr: 0.002\n",
      "iteration: 458650 loss: 0.0020 lr: 0.002\n",
      "iteration: 458660 loss: 0.0015 lr: 0.002\n",
      "iteration: 458670 loss: 0.0013 lr: 0.002\n",
      "iteration: 458680 loss: 0.0021 lr: 0.002\n",
      "iteration: 458690 loss: 0.0013 lr: 0.002\n",
      "iteration: 458700 loss: 0.0010 lr: 0.002\n",
      "iteration: 458710 loss: 0.0009 lr: 0.002\n",
      "iteration: 458720 loss: 0.0009 lr: 0.002\n",
      "iteration: 458730 loss: 0.0014 lr: 0.002\n",
      "iteration: 458740 loss: 0.0019 lr: 0.002\n",
      "iteration: 458750 loss: 0.0012 lr: 0.002\n",
      "iteration: 458760 loss: 0.0013 lr: 0.002\n",
      "iteration: 458770 loss: 0.0010 lr: 0.002\n",
      "iteration: 458780 loss: 0.0013 lr: 0.002\n",
      "iteration: 458790 loss: 0.0013 lr: 0.002\n",
      "iteration: 458800 loss: 0.0011 lr: 0.002\n",
      "iteration: 458810 loss: 0.0011 lr: 0.002\n",
      "iteration: 458820 loss: 0.0011 lr: 0.002\n",
      "iteration: 458830 loss: 0.0008 lr: 0.002\n",
      "iteration: 458840 loss: 0.0010 lr: 0.002\n",
      "iteration: 458850 loss: 0.0016 lr: 0.002\n",
      "iteration: 458860 loss: 0.0010 lr: 0.002\n",
      "iteration: 458870 loss: 0.0009 lr: 0.002\n",
      "iteration: 458880 loss: 0.0012 lr: 0.002\n",
      "iteration: 458890 loss: 0.0012 lr: 0.002\n",
      "iteration: 458900 loss: 0.0009 lr: 0.002\n",
      "iteration: 458910 loss: 0.0018 lr: 0.002\n",
      "iteration: 458920 loss: 0.0015 lr: 0.002\n",
      "iteration: 458930 loss: 0.0014 lr: 0.002\n",
      "iteration: 458940 loss: 0.0010 lr: 0.002\n",
      "iteration: 458950 loss: 0.0017 lr: 0.002\n",
      "iteration: 458960 loss: 0.0014 lr: 0.002\n",
      "iteration: 458970 loss: 0.0009 lr: 0.002\n",
      "iteration: 458980 loss: 0.0012 lr: 0.002\n",
      "iteration: 458990 loss: 0.0012 lr: 0.002\n",
      "iteration: 459000 loss: 0.0014 lr: 0.002\n",
      "iteration: 459010 loss: 0.0014 lr: 0.002\n",
      "iteration: 459020 loss: 0.0010 lr: 0.002\n",
      "iteration: 459030 loss: 0.0015 lr: 0.002\n",
      "iteration: 459040 loss: 0.0009 lr: 0.002\n",
      "iteration: 459050 loss: 0.0012 lr: 0.002\n",
      "iteration: 459060 loss: 0.0008 lr: 0.002\n",
      "iteration: 459070 loss: 0.0009 lr: 0.002\n",
      "iteration: 459080 loss: 0.0011 lr: 0.002\n",
      "iteration: 459090 loss: 0.0009 lr: 0.002\n",
      "iteration: 459100 loss: 0.0012 lr: 0.002\n",
      "iteration: 459110 loss: 0.0011 lr: 0.002\n",
      "iteration: 459120 loss: 0.0010 lr: 0.002\n",
      "iteration: 459130 loss: 0.0017 lr: 0.002\n",
      "iteration: 459140 loss: 0.0024 lr: 0.002\n",
      "iteration: 459150 loss: 0.0012 lr: 0.002\n",
      "iteration: 459160 loss: 0.0013 lr: 0.002\n",
      "iteration: 459170 loss: 0.0012 lr: 0.002\n",
      "iteration: 459180 loss: 0.0012 lr: 0.002\n",
      "iteration: 459190 loss: 0.0009 lr: 0.002\n",
      "iteration: 459200 loss: 0.0011 lr: 0.002\n",
      "iteration: 459210 loss: 0.0009 lr: 0.002\n",
      "iteration: 459220 loss: 0.0010 lr: 0.002\n",
      "iteration: 459230 loss: 0.0011 lr: 0.002\n",
      "iteration: 459240 loss: 0.0011 lr: 0.002\n",
      "iteration: 459250 loss: 0.0012 lr: 0.002\n",
      "iteration: 459260 loss: 0.0008 lr: 0.002\n",
      "iteration: 459270 loss: 0.0010 lr: 0.002\n",
      "iteration: 459280 loss: 0.0016 lr: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 459290 loss: 0.0007 lr: 0.002\n",
      "iteration: 459300 loss: 0.0010 lr: 0.002\n",
      "iteration: 459310 loss: 0.0016 lr: 0.002\n",
      "iteration: 459320 loss: 0.0015 lr: 0.002\n",
      "iteration: 459330 loss: 0.0011 lr: 0.002\n",
      "iteration: 459340 loss: 0.0013 lr: 0.002\n",
      "iteration: 459350 loss: 0.0009 lr: 0.002\n",
      "iteration: 459360 loss: 0.0010 lr: 0.002\n",
      "iteration: 459370 loss: 0.0010 lr: 0.002\n",
      "iteration: 459380 loss: 0.0008 lr: 0.002\n",
      "iteration: 459390 loss: 0.0014 lr: 0.002\n",
      "iteration: 459400 loss: 0.0011 lr: 0.002\n",
      "iteration: 459410 loss: 0.0010 lr: 0.002\n",
      "iteration: 459420 loss: 0.0017 lr: 0.002\n",
      "iteration: 459430 loss: 0.0010 lr: 0.002\n",
      "iteration: 459440 loss: 0.0013 lr: 0.002\n",
      "iteration: 459450 loss: 0.0011 lr: 0.002\n",
      "iteration: 459460 loss: 0.0011 lr: 0.002\n",
      "iteration: 459470 loss: 0.0014 lr: 0.002\n",
      "iteration: 459480 loss: 0.0010 lr: 0.002\n",
      "iteration: 459490 loss: 0.0023 lr: 0.002\n",
      "iteration: 459500 loss: 0.0008 lr: 0.002\n",
      "iteration: 459510 loss: 0.0012 lr: 0.002\n",
      "iteration: 459520 loss: 0.0019 lr: 0.002\n",
      "iteration: 459530 loss: 0.0014 lr: 0.002\n",
      "iteration: 459540 loss: 0.0009 lr: 0.002\n",
      "iteration: 459550 loss: 0.0020 lr: 0.002\n",
      "iteration: 459560 loss: 0.0010 lr: 0.002\n",
      "iteration: 459570 loss: 0.0014 lr: 0.002\n",
      "iteration: 459580 loss: 0.0013 lr: 0.002\n",
      "iteration: 459590 loss: 0.0008 lr: 0.002\n",
      "iteration: 459600 loss: 0.0017 lr: 0.002\n",
      "iteration: 459610 loss: 0.0029 lr: 0.002\n",
      "iteration: 459620 loss: 0.0013 lr: 0.002\n",
      "iteration: 459630 loss: 0.0012 lr: 0.002\n",
      "iteration: 459640 loss: 0.0011 lr: 0.002\n",
      "iteration: 459650 loss: 0.0010 lr: 0.002\n",
      "iteration: 459660 loss: 0.0022 lr: 0.002\n",
      "iteration: 459670 loss: 0.0011 lr: 0.002\n",
      "iteration: 459680 loss: 0.0015 lr: 0.002\n",
      "iteration: 459690 loss: 0.0012 lr: 0.002\n",
      "iteration: 459700 loss: 0.0011 lr: 0.002\n",
      "iteration: 459710 loss: 0.0011 lr: 0.002\n",
      "iteration: 459720 loss: 0.0010 lr: 0.002\n",
      "iteration: 459730 loss: 0.0010 lr: 0.002\n",
      "iteration: 459740 loss: 0.0013 lr: 0.002\n",
      "iteration: 459750 loss: 0.0007 lr: 0.002\n",
      "iteration: 459760 loss: 0.0010 lr: 0.002\n",
      "iteration: 459770 loss: 0.0013 lr: 0.002\n",
      "iteration: 459780 loss: 0.0012 lr: 0.002\n",
      "iteration: 459790 loss: 0.0012 lr: 0.002\n",
      "iteration: 459800 loss: 0.0011 lr: 0.002\n",
      "iteration: 459810 loss: 0.0009 lr: 0.002\n",
      "iteration: 459820 loss: 0.0012 lr: 0.002\n",
      "iteration: 459830 loss: 0.0008 lr: 0.002\n",
      "iteration: 459840 loss: 0.0015 lr: 0.002\n",
      "iteration: 459850 loss: 0.0009 lr: 0.002\n",
      "iteration: 459860 loss: 0.0017 lr: 0.002\n",
      "iteration: 459870 loss: 0.0011 lr: 0.002\n",
      "iteration: 459880 loss: 0.0011 lr: 0.002\n",
      "iteration: 459890 loss: 0.0011 lr: 0.002\n",
      "iteration: 459900 loss: 0.0013 lr: 0.002\n",
      "iteration: 459910 loss: 0.0008 lr: 0.002\n",
      "iteration: 459920 loss: 0.0015 lr: 0.002\n",
      "iteration: 459930 loss: 0.0012 lr: 0.002\n",
      "iteration: 459940 loss: 0.0017 lr: 0.002\n",
      "iteration: 459950 loss: 0.0011 lr: 0.002\n",
      "iteration: 459960 loss: 0.0008 lr: 0.002\n",
      "iteration: 459970 loss: 0.0009 lr: 0.002\n",
      "iteration: 459980 loss: 0.0007 lr: 0.002\n",
      "iteration: 459990 loss: 0.0012 lr: 0.002\n",
      "iteration: 460000 loss: 0.0015 lr: 0.002\n",
      "iteration: 460010 loss: 0.0011 lr: 0.002\n",
      "iteration: 460020 loss: 0.0010 lr: 0.002\n",
      "iteration: 460030 loss: 0.0018 lr: 0.002\n",
      "iteration: 460040 loss: 0.0013 lr: 0.002\n",
      "iteration: 460050 loss: 0.0008 lr: 0.002\n",
      "iteration: 460060 loss: 0.0008 lr: 0.002\n",
      "iteration: 460070 loss: 0.0013 lr: 0.002\n",
      "iteration: 460080 loss: 0.0012 lr: 0.002\n",
      "iteration: 460090 loss: 0.0014 lr: 0.002\n",
      "iteration: 460100 loss: 0.0013 lr: 0.002\n",
      "iteration: 460110 loss: 0.0009 lr: 0.002\n",
      "iteration: 460120 loss: 0.0010 lr: 0.002\n",
      "iteration: 460130 loss: 0.0013 lr: 0.002\n",
      "iteration: 460140 loss: 0.0008 lr: 0.002\n",
      "iteration: 460150 loss: 0.0011 lr: 0.002\n",
      "iteration: 460160 loss: 0.0015 lr: 0.002\n",
      "iteration: 460170 loss: 0.0010 lr: 0.002\n",
      "iteration: 460180 loss: 0.0010 lr: 0.002\n",
      "iteration: 460190 loss: 0.0011 lr: 0.002\n",
      "iteration: 460200 loss: 0.0014 lr: 0.002\n",
      "iteration: 460210 loss: 0.0015 lr: 0.002\n",
      "iteration: 460220 loss: 0.0008 lr: 0.002\n",
      "iteration: 460230 loss: 0.0018 lr: 0.002\n",
      "iteration: 460240 loss: 0.0010 lr: 0.002\n",
      "iteration: 460250 loss: 0.0011 lr: 0.002\n",
      "iteration: 460260 loss: 0.0009 lr: 0.002\n",
      "iteration: 460270 loss: 0.0007 lr: 0.002\n",
      "iteration: 460280 loss: 0.0017 lr: 0.002\n",
      "iteration: 460290 loss: 0.0015 lr: 0.002\n",
      "iteration: 460300 loss: 0.0015 lr: 0.002\n",
      "iteration: 460310 loss: 0.0010 lr: 0.002\n",
      "iteration: 460320 loss: 0.0010 lr: 0.002\n",
      "iteration: 460330 loss: 0.0011 lr: 0.002\n",
      "iteration: 460340 loss: 0.0015 lr: 0.002\n",
      "iteration: 460350 loss: 0.0009 lr: 0.002\n",
      "iteration: 460360 loss: 0.0014 lr: 0.002\n",
      "iteration: 460370 loss: 0.0011 lr: 0.002\n",
      "iteration: 460380 loss: 0.0015 lr: 0.002\n",
      "iteration: 460390 loss: 0.0009 lr: 0.002\n",
      "iteration: 460400 loss: 0.0010 lr: 0.002\n",
      "iteration: 460410 loss: 0.0012 lr: 0.002\n",
      "iteration: 460420 loss: 0.0010 lr: 0.002\n",
      "iteration: 460430 loss: 0.0013 lr: 0.002\n",
      "iteration: 460440 loss: 0.0013 lr: 0.002\n",
      "iteration: 460450 loss: 0.0014 lr: 0.002\n",
      "iteration: 460460 loss: 0.0007 lr: 0.002\n",
      "iteration: 460470 loss: 0.0024 lr: 0.002\n",
      "iteration: 460480 loss: 0.0016 lr: 0.002\n",
      "iteration: 460490 loss: 0.0008 lr: 0.002\n",
      "iteration: 460500 loss: 0.0011 lr: 0.002\n",
      "iteration: 460510 loss: 0.0009 lr: 0.002\n",
      "iteration: 460520 loss: 0.0008 lr: 0.002\n",
      "iteration: 460530 loss: 0.0010 lr: 0.002\n",
      "iteration: 460540 loss: 0.0014 lr: 0.002\n",
      "iteration: 460550 loss: 0.0011 lr: 0.002\n",
      "iteration: 460560 loss: 0.0012 lr: 0.002\n",
      "iteration: 460570 loss: 0.0009 lr: 0.002\n",
      "iteration: 460580 loss: 0.0020 lr: 0.002\n",
      "iteration: 460590 loss: 0.0010 lr: 0.002\n",
      "iteration: 460600 loss: 0.0014 lr: 0.002\n",
      "iteration: 460610 loss: 0.0012 lr: 0.002\n",
      "iteration: 460620 loss: 0.0012 lr: 0.002\n",
      "iteration: 460630 loss: 0.0018 lr: 0.002\n",
      "iteration: 460640 loss: 0.0014 lr: 0.002\n",
      "iteration: 460650 loss: 0.0011 lr: 0.002\n",
      "iteration: 460660 loss: 0.0007 lr: 0.002\n",
      "iteration: 460670 loss: 0.0009 lr: 0.002\n",
      "iteration: 460680 loss: 0.0011 lr: 0.002\n",
      "iteration: 460690 loss: 0.0008 lr: 0.002\n",
      "iteration: 460700 loss: 0.0020 lr: 0.002\n",
      "iteration: 460710 loss: 0.0012 lr: 0.002\n",
      "iteration: 460720 loss: 0.0010 lr: 0.002\n",
      "iteration: 460730 loss: 0.0015 lr: 0.002\n",
      "iteration: 460740 loss: 0.0011 lr: 0.002\n",
      "iteration: 460750 loss: 0.0011 lr: 0.002\n",
      "iteration: 460760 loss: 0.0010 lr: 0.002\n",
      "iteration: 460770 loss: 0.0015 lr: 0.002\n",
      "iteration: 460780 loss: 0.0012 lr: 0.002\n",
      "iteration: 460790 loss: 0.0014 lr: 0.002\n",
      "iteration: 460800 loss: 0.0013 lr: 0.002\n",
      "iteration: 460810 loss: 0.0015 lr: 0.002\n",
      "iteration: 460820 loss: 0.0013 lr: 0.002\n",
      "iteration: 460830 loss: 0.0012 lr: 0.002\n",
      "iteration: 460840 loss: 0.0010 lr: 0.002\n",
      "iteration: 460850 loss: 0.0012 lr: 0.002\n",
      "iteration: 460860 loss: 0.0012 lr: 0.002\n",
      "iteration: 460870 loss: 0.0012 lr: 0.002\n",
      "iteration: 460880 loss: 0.0016 lr: 0.002\n",
      "iteration: 460890 loss: 0.0012 lr: 0.002\n",
      "iteration: 460900 loss: 0.0010 lr: 0.002\n",
      "iteration: 460910 loss: 0.0008 lr: 0.002\n",
      "iteration: 460920 loss: 0.0008 lr: 0.002\n",
      "iteration: 460930 loss: 0.0014 lr: 0.002\n",
      "iteration: 460940 loss: 0.0011 lr: 0.002\n",
      "iteration: 460950 loss: 0.0011 lr: 0.002\n",
      "iteration: 460960 loss: 0.0008 lr: 0.002\n",
      "iteration: 460970 loss: 0.0010 lr: 0.002\n",
      "iteration: 460980 loss: 0.0011 lr: 0.002\n",
      "iteration: 460990 loss: 0.0013 lr: 0.002\n",
      "iteration: 461000 loss: 0.0012 lr: 0.002\n",
      "iteration: 461010 loss: 0.0007 lr: 0.002\n",
      "iteration: 461020 loss: 0.0014 lr: 0.002\n",
      "iteration: 461030 loss: 0.0013 lr: 0.002\n",
      "iteration: 461040 loss: 0.0010 lr: 0.002\n",
      "iteration: 461050 loss: 0.0011 lr: 0.002\n",
      "iteration: 461060 loss: 0.0013 lr: 0.002\n",
      "iteration: 461070 loss: 0.0019 lr: 0.002\n",
      "iteration: 461080 loss: 0.0009 lr: 0.002\n",
      "iteration: 461090 loss: 0.0011 lr: 0.002\n",
      "iteration: 461100 loss: 0.0010 lr: 0.002\n",
      "iteration: 461110 loss: 0.0010 lr: 0.002\n",
      "iteration: 461120 loss: 0.0014 lr: 0.002\n",
      "iteration: 461130 loss: 0.0008 lr: 0.002\n",
      "iteration: 461140 loss: 0.0014 lr: 0.002\n",
      "iteration: 461150 loss: 0.0015 lr: 0.002\n",
      "iteration: 461160 loss: 0.0010 lr: 0.002\n",
      "iteration: 461170 loss: 0.0014 lr: 0.002\n",
      "iteration: 461180 loss: 0.0009 lr: 0.002\n",
      "iteration: 461190 loss: 0.0011 lr: 0.002\n",
      "iteration: 461200 loss: 0.0011 lr: 0.002\n",
      "iteration: 461210 loss: 0.0018 lr: 0.002\n",
      "iteration: 461220 loss: 0.0018 lr: 0.002\n",
      "iteration: 461230 loss: 0.0016 lr: 0.002\n",
      "iteration: 461240 loss: 0.0011 lr: 0.002\n",
      "iteration: 461250 loss: 0.0022 lr: 0.002\n",
      "iteration: 461260 loss: 0.0011 lr: 0.002\n",
      "iteration: 461270 loss: 0.0011 lr: 0.002\n",
      "iteration: 461280 loss: 0.0016 lr: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 461290 loss: 0.0018 lr: 0.002\n",
      "iteration: 461300 loss: 0.0010 lr: 0.002\n",
      "iteration: 461310 loss: 0.0010 lr: 0.002\n",
      "iteration: 461320 loss: 0.0015 lr: 0.002\n",
      "iteration: 461330 loss: 0.0015 lr: 0.002\n",
      "iteration: 461340 loss: 0.0011 lr: 0.002\n",
      "iteration: 461350 loss: 0.0011 lr: 0.002\n",
      "iteration: 461360 loss: 0.0010 lr: 0.002\n",
      "iteration: 461370 loss: 0.0012 lr: 0.002\n",
      "iteration: 461380 loss: 0.0013 lr: 0.002\n",
      "iteration: 461390 loss: 0.0008 lr: 0.002\n",
      "iteration: 461400 loss: 0.0019 lr: 0.002\n",
      "iteration: 461410 loss: 0.0010 lr: 0.002\n",
      "iteration: 461420 loss: 0.0009 lr: 0.002\n",
      "iteration: 461430 loss: 0.0011 lr: 0.002\n",
      "iteration: 461440 loss: 0.0010 lr: 0.002\n",
      "iteration: 461450 loss: 0.0010 lr: 0.002\n",
      "iteration: 461460 loss: 0.0012 lr: 0.002\n",
      "iteration: 461470 loss: 0.0008 lr: 0.002\n",
      "iteration: 461480 loss: 0.0008 lr: 0.002\n",
      "iteration: 461490 loss: 0.0016 lr: 0.002\n",
      "iteration: 461500 loss: 0.0009 lr: 0.002\n",
      "iteration: 461510 loss: 0.0015 lr: 0.002\n",
      "iteration: 461520 loss: 0.0011 lr: 0.002\n",
      "iteration: 461530 loss: 0.0012 lr: 0.002\n",
      "iteration: 461540 loss: 0.0012 lr: 0.002\n",
      "iteration: 461550 loss: 0.0011 lr: 0.002\n",
      "iteration: 461560 loss: 0.0010 lr: 0.002\n",
      "iteration: 461570 loss: 0.0011 lr: 0.002\n",
      "iteration: 461580 loss: 0.0013 lr: 0.002\n",
      "iteration: 461590 loss: 0.0012 lr: 0.002\n",
      "iteration: 461600 loss: 0.0011 lr: 0.002\n",
      "iteration: 461610 loss: 0.0015 lr: 0.002\n",
      "iteration: 461620 loss: 0.0016 lr: 0.002\n",
      "iteration: 461630 loss: 0.0011 lr: 0.002\n",
      "iteration: 461640 loss: 0.0014 lr: 0.002\n",
      "iteration: 461650 loss: 0.0013 lr: 0.002\n",
      "iteration: 461660 loss: 0.0009 lr: 0.002\n",
      "iteration: 461670 loss: 0.0009 lr: 0.002\n",
      "iteration: 461680 loss: 0.0011 lr: 0.002\n",
      "iteration: 461690 loss: 0.0012 lr: 0.002\n",
      "iteration: 461700 loss: 0.0008 lr: 0.002\n",
      "iteration: 461710 loss: 0.0009 lr: 0.002\n",
      "iteration: 461720 loss: 0.0014 lr: 0.002\n",
      "iteration: 461730 loss: 0.0015 lr: 0.002\n",
      "iteration: 461740 loss: 0.0011 lr: 0.002\n",
      "iteration: 461750 loss: 0.0012 lr: 0.002\n",
      "iteration: 461760 loss: 0.0010 lr: 0.002\n",
      "iteration: 461770 loss: 0.0013 lr: 0.002\n",
      "iteration: 461780 loss: 0.0012 lr: 0.002\n",
      "iteration: 461790 loss: 0.0012 lr: 0.002\n",
      "iteration: 461800 loss: 0.0010 lr: 0.002\n",
      "iteration: 461810 loss: 0.0011 lr: 0.002\n",
      "iteration: 461820 loss: 0.0015 lr: 0.002\n",
      "iteration: 461830 loss: 0.0009 lr: 0.002\n",
      "iteration: 461840 loss: 0.0011 lr: 0.002\n",
      "iteration: 461850 loss: 0.0011 lr: 0.002\n",
      "iteration: 461860 loss: 0.0011 lr: 0.002\n",
      "iteration: 461870 loss: 0.0008 lr: 0.002\n",
      "iteration: 461880 loss: 0.0015 lr: 0.002\n",
      "iteration: 461890 loss: 0.0009 lr: 0.002\n",
      "iteration: 461900 loss: 0.0015 lr: 0.002\n",
      "iteration: 461910 loss: 0.0014 lr: 0.002\n",
      "iteration: 461920 loss: 0.0012 lr: 0.002\n",
      "iteration: 461930 loss: 0.0012 lr: 0.002\n",
      "iteration: 461940 loss: 0.0014 lr: 0.002\n",
      "iteration: 461950 loss: 0.0015 lr: 0.002\n",
      "iteration: 461960 loss: 0.0008 lr: 0.002\n",
      "iteration: 461970 loss: 0.0010 lr: 0.002\n",
      "iteration: 461980 loss: 0.0014 lr: 0.002\n",
      "iteration: 461990 loss: 0.0012 lr: 0.002\n",
      "iteration: 462000 loss: 0.0013 lr: 0.002\n",
      "iteration: 462010 loss: 0.0010 lr: 0.002\n",
      "iteration: 462020 loss: 0.0012 lr: 0.002\n",
      "iteration: 462030 loss: 0.0012 lr: 0.002\n",
      "iteration: 462040 loss: 0.0016 lr: 0.002\n",
      "iteration: 462050 loss: 0.0016 lr: 0.002\n",
      "iteration: 462060 loss: 0.0012 lr: 0.002\n",
      "iteration: 462070 loss: 0.0015 lr: 0.002\n",
      "iteration: 462080 loss: 0.0015 lr: 0.002\n",
      "iteration: 462090 loss: 0.0011 lr: 0.002\n",
      "iteration: 462100 loss: 0.0014 lr: 0.002\n",
      "iteration: 462110 loss: 0.0009 lr: 0.002\n",
      "iteration: 462120 loss: 0.0012 lr: 0.002\n",
      "iteration: 462130 loss: 0.0012 lr: 0.002\n",
      "iteration: 462140 loss: 0.0011 lr: 0.002\n",
      "iteration: 462150 loss: 0.0010 lr: 0.002\n",
      "iteration: 462160 loss: 0.0014 lr: 0.002\n",
      "iteration: 462170 loss: 0.0009 lr: 0.002\n",
      "iteration: 462180 loss: 0.0009 lr: 0.002\n",
      "iteration: 462190 loss: 0.0011 lr: 0.002\n",
      "iteration: 462200 loss: 0.0014 lr: 0.002\n",
      "iteration: 462210 loss: 0.0021 lr: 0.002\n",
      "iteration: 462220 loss: 0.0009 lr: 0.002\n",
      "iteration: 462230 loss: 0.0014 lr: 0.002\n",
      "iteration: 462240 loss: 0.0008 lr: 0.002\n",
      "iteration: 462250 loss: 0.0018 lr: 0.002\n",
      "iteration: 462260 loss: 0.0014 lr: 0.002\n",
      "iteration: 462270 loss: 0.0013 lr: 0.002\n",
      "iteration: 462280 loss: 0.0020 lr: 0.002\n",
      "iteration: 462290 loss: 0.0012 lr: 0.002\n",
      "iteration: 462300 loss: 0.0010 lr: 0.002\n",
      "iteration: 462310 loss: 0.0010 lr: 0.002\n",
      "iteration: 462320 loss: 0.0010 lr: 0.002\n",
      "iteration: 462330 loss: 0.0018 lr: 0.002\n",
      "iteration: 462340 loss: 0.0010 lr: 0.002\n",
      "iteration: 462350 loss: 0.0009 lr: 0.002\n",
      "iteration: 462360 loss: 0.0012 lr: 0.002\n",
      "iteration: 462370 loss: 0.0013 lr: 0.002\n",
      "iteration: 462380 loss: 0.0009 lr: 0.002\n",
      "iteration: 462390 loss: 0.0009 lr: 0.002\n",
      "iteration: 462400 loss: 0.0011 lr: 0.002\n",
      "iteration: 462410 loss: 0.0017 lr: 0.002\n",
      "iteration: 462420 loss: 0.0012 lr: 0.002\n",
      "iteration: 462430 loss: 0.0012 lr: 0.002\n",
      "iteration: 462440 loss: 0.0027 lr: 0.002\n",
      "iteration: 462450 loss: 0.0015 lr: 0.002\n",
      "iteration: 462460 loss: 0.0011 lr: 0.002\n",
      "iteration: 462470 loss: 0.0021 lr: 0.002\n",
      "iteration: 462480 loss: 0.0015 lr: 0.002\n",
      "iteration: 462490 loss: 0.0032 lr: 0.002\n",
      "iteration: 462500 loss: 0.0013 lr: 0.002\n",
      "iteration: 462510 loss: 0.0013 lr: 0.002\n",
      "iteration: 462520 loss: 0.0011 lr: 0.002\n",
      "iteration: 462530 loss: 0.0019 lr: 0.002\n",
      "iteration: 462540 loss: 0.0011 lr: 0.002\n",
      "iteration: 462550 loss: 0.0010 lr: 0.002\n",
      "iteration: 462560 loss: 0.0019 lr: 0.002\n",
      "iteration: 462570 loss: 0.0011 lr: 0.002\n",
      "iteration: 462580 loss: 0.0020 lr: 0.002\n",
      "iteration: 462590 loss: 0.0012 lr: 0.002\n",
      "iteration: 462600 loss: 0.0011 lr: 0.002\n",
      "iteration: 462610 loss: 0.0016 lr: 0.002\n",
      "iteration: 462620 loss: 0.0011 lr: 0.002\n",
      "iteration: 462630 loss: 0.0015 lr: 0.002\n",
      "iteration: 462640 loss: 0.0015 lr: 0.002\n",
      "iteration: 462650 loss: 0.0016 lr: 0.002\n",
      "iteration: 462660 loss: 0.0008 lr: 0.002\n",
      "iteration: 462670 loss: 0.0017 lr: 0.002\n",
      "iteration: 462680 loss: 0.0020 lr: 0.002\n",
      "iteration: 462690 loss: 0.0018 lr: 0.002\n",
      "iteration: 462700 loss: 0.0011 lr: 0.002\n",
      "iteration: 462710 loss: 0.0013 lr: 0.002\n",
      "iteration: 462720 loss: 0.0010 lr: 0.002\n",
      "iteration: 462730 loss: 0.0010 lr: 0.002\n",
      "iteration: 462740 loss: 0.0018 lr: 0.002\n",
      "iteration: 462750 loss: 0.0009 lr: 0.002\n",
      "iteration: 462760 loss: 0.0009 lr: 0.002\n",
      "iteration: 462770 loss: 0.0014 lr: 0.002\n",
      "iteration: 462780 loss: 0.0014 lr: 0.002\n",
      "iteration: 462790 loss: 0.0011 lr: 0.002\n",
      "iteration: 462800 loss: 0.0009 lr: 0.002\n",
      "iteration: 462810 loss: 0.0009 lr: 0.002\n",
      "iteration: 462820 loss: 0.0011 lr: 0.002\n",
      "iteration: 462830 loss: 0.0010 lr: 0.002\n",
      "iteration: 462840 loss: 0.0010 lr: 0.002\n",
      "iteration: 462850 loss: 0.0009 lr: 0.002\n",
      "iteration: 462860 loss: 0.0010 lr: 0.002\n",
      "iteration: 462870 loss: 0.0023 lr: 0.002\n",
      "iteration: 462880 loss: 0.0012 lr: 0.002\n",
      "iteration: 462890 loss: 0.0013 lr: 0.002\n",
      "iteration: 462900 loss: 0.0014 lr: 0.002\n",
      "iteration: 462910 loss: 0.0008 lr: 0.002\n",
      "iteration: 462920 loss: 0.0011 lr: 0.002\n",
      "iteration: 462930 loss: 0.0012 lr: 0.002\n",
      "iteration: 462940 loss: 0.0012 lr: 0.002\n",
      "iteration: 462950 loss: 0.0012 lr: 0.002\n",
      "iteration: 462960 loss: 0.0013 lr: 0.002\n",
      "iteration: 462970 loss: 0.0012 lr: 0.002\n",
      "iteration: 462980 loss: 0.0014 lr: 0.002\n",
      "iteration: 462990 loss: 0.0009 lr: 0.002\n",
      "iteration: 463000 loss: 0.0015 lr: 0.002\n",
      "iteration: 463010 loss: 0.0008 lr: 0.002\n",
      "iteration: 463020 loss: 0.0008 lr: 0.002\n",
      "iteration: 463030 loss: 0.0009 lr: 0.002\n",
      "iteration: 463040 loss: 0.0010 lr: 0.002\n",
      "iteration: 463050 loss: 0.0013 lr: 0.002\n",
      "iteration: 463060 loss: 0.0011 lr: 0.002\n",
      "iteration: 463070 loss: 0.0017 lr: 0.002\n",
      "iteration: 463080 loss: 0.0008 lr: 0.002\n",
      "iteration: 463090 loss: 0.0009 lr: 0.002\n",
      "iteration: 463100 loss: 0.0016 lr: 0.002\n",
      "iteration: 463110 loss: 0.0010 lr: 0.002\n",
      "iteration: 463120 loss: 0.0014 lr: 0.002\n",
      "iteration: 463130 loss: 0.0021 lr: 0.002\n",
      "iteration: 463140 loss: 0.0017 lr: 0.002\n",
      "iteration: 463150 loss: 0.0013 lr: 0.002\n",
      "iteration: 463160 loss: 0.0017 lr: 0.002\n",
      "iteration: 463170 loss: 0.0013 lr: 0.002\n",
      "iteration: 463180 loss: 0.0009 lr: 0.002\n",
      "iteration: 463190 loss: 0.0015 lr: 0.002\n",
      "iteration: 463200 loss: 0.0011 lr: 0.002\n",
      "iteration: 463210 loss: 0.0010 lr: 0.002\n",
      "iteration: 463220 loss: 0.0013 lr: 0.002\n",
      "iteration: 463230 loss: 0.0016 lr: 0.002\n",
      "iteration: 463240 loss: 0.0014 lr: 0.002\n",
      "iteration: 463250 loss: 0.0012 lr: 0.002\n",
      "iteration: 463260 loss: 0.0014 lr: 0.002\n",
      "iteration: 463270 loss: 0.0014 lr: 0.002\n",
      "iteration: 463280 loss: 0.0020 lr: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 463290 loss: 0.0007 lr: 0.002\n",
      "iteration: 463300 loss: 0.0011 lr: 0.002\n",
      "iteration: 463310 loss: 0.0014 lr: 0.002\n",
      "iteration: 463320 loss: 0.0007 lr: 0.002\n",
      "iteration: 463330 loss: 0.0010 lr: 0.002\n",
      "iteration: 463340 loss: 0.0012 lr: 0.002\n",
      "iteration: 463350 loss: 0.0011 lr: 0.002\n",
      "iteration: 463360 loss: 0.0014 lr: 0.002\n",
      "iteration: 463370 loss: 0.0014 lr: 0.002\n",
      "iteration: 463380 loss: 0.0013 lr: 0.002\n",
      "iteration: 463390 loss: 0.0011 lr: 0.002\n",
      "iteration: 463400 loss: 0.0014 lr: 0.002\n",
      "iteration: 463410 loss: 0.0008 lr: 0.002\n",
      "iteration: 463420 loss: 0.0025 lr: 0.002\n",
      "iteration: 463430 loss: 0.0011 lr: 0.002\n",
      "iteration: 463440 loss: 0.0011 lr: 0.002\n",
      "iteration: 463450 loss: 0.0015 lr: 0.002\n",
      "iteration: 463460 loss: 0.0009 lr: 0.002\n",
      "iteration: 463470 loss: 0.0020 lr: 0.002\n",
      "iteration: 463480 loss: 0.0010 lr: 0.002\n",
      "iteration: 463490 loss: 0.0013 lr: 0.002\n",
      "iteration: 463500 loss: 0.0010 lr: 0.002\n",
      "iteration: 463510 loss: 0.0015 lr: 0.002\n",
      "iteration: 463520 loss: 0.0012 lr: 0.002\n",
      "iteration: 463530 loss: 0.0009 lr: 0.002\n",
      "iteration: 463540 loss: 0.0009 lr: 0.002\n",
      "iteration: 463550 loss: 0.0016 lr: 0.002\n",
      "iteration: 463560 loss: 0.0009 lr: 0.002\n",
      "iteration: 463570 loss: 0.0013 lr: 0.002\n",
      "iteration: 463580 loss: 0.0014 lr: 0.002\n",
      "iteration: 463590 loss: 0.0009 lr: 0.002\n",
      "iteration: 463600 loss: 0.0010 lr: 0.002\n",
      "iteration: 463610 loss: 0.0016 lr: 0.002\n",
      "iteration: 463620 loss: 0.0014 lr: 0.002\n",
      "iteration: 463630 loss: 0.0010 lr: 0.002\n",
      "iteration: 463640 loss: 0.0014 lr: 0.002\n",
      "iteration: 463650 loss: 0.0009 lr: 0.002\n",
      "iteration: 463660 loss: 0.0010 lr: 0.002\n",
      "iteration: 463670 loss: 0.0008 lr: 0.002\n",
      "iteration: 463680 loss: 0.0015 lr: 0.002\n",
      "iteration: 463690 loss: 0.0007 lr: 0.002\n",
      "iteration: 463700 loss: 0.0011 lr: 0.002\n",
      "iteration: 463710 loss: 0.0008 lr: 0.002\n",
      "iteration: 463720 loss: 0.0012 lr: 0.002\n",
      "iteration: 463730 loss: 0.0018 lr: 0.002\n",
      "iteration: 463740 loss: 0.0011 lr: 0.002\n",
      "iteration: 463750 loss: 0.0014 lr: 0.002\n",
      "iteration: 463760 loss: 0.0011 lr: 0.002\n",
      "iteration: 463770 loss: 0.0009 lr: 0.002\n",
      "iteration: 463780 loss: 0.0010 lr: 0.002\n",
      "iteration: 463790 loss: 0.0009 lr: 0.002\n",
      "iteration: 463800 loss: 0.0014 lr: 0.002\n",
      "iteration: 463810 loss: 0.0014 lr: 0.002\n",
      "iteration: 463820 loss: 0.0015 lr: 0.002\n",
      "iteration: 463830 loss: 0.0011 lr: 0.002\n",
      "iteration: 463840 loss: 0.0011 lr: 0.002\n",
      "iteration: 463850 loss: 0.0013 lr: 0.002\n",
      "iteration: 463860 loss: 0.0013 lr: 0.002\n",
      "iteration: 463870 loss: 0.0016 lr: 0.002\n",
      "iteration: 463880 loss: 0.0015 lr: 0.002\n",
      "iteration: 463890 loss: 0.0011 lr: 0.002\n",
      "iteration: 463900 loss: 0.0017 lr: 0.002\n",
      "iteration: 463910 loss: 0.0010 lr: 0.002\n",
      "iteration: 463920 loss: 0.0008 lr: 0.002\n",
      "iteration: 463930 loss: 0.0009 lr: 0.002\n",
      "iteration: 463940 loss: 0.0011 lr: 0.002\n",
      "iteration: 463950 loss: 0.0009 lr: 0.002\n",
      "iteration: 463960 loss: 0.0010 lr: 0.002\n",
      "iteration: 463970 loss: 0.0012 lr: 0.002\n",
      "iteration: 463980 loss: 0.0011 lr: 0.002\n",
      "iteration: 463990 loss: 0.0014 lr: 0.002\n",
      "iteration: 464000 loss: 0.0017 lr: 0.002\n",
      "iteration: 464010 loss: 0.0010 lr: 0.002\n",
      "iteration: 464020 loss: 0.0010 lr: 0.002\n",
      "iteration: 464030 loss: 0.0010 lr: 0.002\n",
      "iteration: 464040 loss: 0.0009 lr: 0.002\n",
      "iteration: 464050 loss: 0.0011 lr: 0.002\n",
      "iteration: 464060 loss: 0.0013 lr: 0.002\n",
      "iteration: 464070 loss: 0.0011 lr: 0.002\n",
      "iteration: 464080 loss: 0.0013 lr: 0.002\n",
      "iteration: 464090 loss: 0.0012 lr: 0.002\n",
      "iteration: 464100 loss: 0.0013 lr: 0.002\n",
      "iteration: 464110 loss: 0.0013 lr: 0.002\n",
      "iteration: 464120 loss: 0.0009 lr: 0.002\n",
      "iteration: 464130 loss: 0.0013 lr: 0.002\n",
      "iteration: 464140 loss: 0.0017 lr: 0.002\n",
      "iteration: 464150 loss: 0.0014 lr: 0.002\n",
      "iteration: 464160 loss: 0.0011 lr: 0.002\n",
      "iteration: 464170 loss: 0.0012 lr: 0.002\n",
      "iteration: 464180 loss: 0.0015 lr: 0.002\n",
      "iteration: 464190 loss: 0.0012 lr: 0.002\n",
      "iteration: 464200 loss: 0.0009 lr: 0.002\n",
      "iteration: 464210 loss: 0.0012 lr: 0.002\n",
      "iteration: 464220 loss: 0.0007 lr: 0.002\n",
      "iteration: 464230 loss: 0.0011 lr: 0.002\n",
      "iteration: 464240 loss: 0.0012 lr: 0.002\n",
      "iteration: 464250 loss: 0.0008 lr: 0.002\n",
      "iteration: 464260 loss: 0.0012 lr: 0.002\n",
      "iteration: 464270 loss: 0.0012 lr: 0.002\n",
      "iteration: 464280 loss: 0.0018 lr: 0.002\n",
      "iteration: 464290 loss: 0.0008 lr: 0.002\n",
      "iteration: 464300 loss: 0.0010 lr: 0.002\n",
      "iteration: 464310 loss: 0.0017 lr: 0.002\n",
      "iteration: 464320 loss: 0.0015 lr: 0.002\n",
      "iteration: 464330 loss: 0.0011 lr: 0.002\n",
      "iteration: 464340 loss: 0.0012 lr: 0.002\n",
      "iteration: 464350 loss: 0.0014 lr: 0.002\n",
      "iteration: 464360 loss: 0.0015 lr: 0.002\n",
      "iteration: 464370 loss: 0.0018 lr: 0.002\n",
      "iteration: 464380 loss: 0.0012 lr: 0.002\n",
      "iteration: 464390 loss: 0.0013 lr: 0.002\n",
      "iteration: 464400 loss: 0.0014 lr: 0.002\n",
      "iteration: 464410 loss: 0.0008 lr: 0.002\n",
      "iteration: 464420 loss: 0.0010 lr: 0.002\n",
      "iteration: 464430 loss: 0.0011 lr: 0.002\n",
      "iteration: 464440 loss: 0.0008 lr: 0.002\n",
      "iteration: 464450 loss: 0.0013 lr: 0.002\n",
      "iteration: 464460 loss: 0.0025 lr: 0.002\n",
      "iteration: 464470 loss: 0.0010 lr: 0.002\n",
      "iteration: 464480 loss: 0.0012 lr: 0.002\n",
      "iteration: 464490 loss: 0.0012 lr: 0.002\n",
      "iteration: 464500 loss: 0.0011 lr: 0.002\n",
      "iteration: 464510 loss: 0.0015 lr: 0.002\n",
      "iteration: 464520 loss: 0.0012 lr: 0.002\n",
      "iteration: 464530 loss: 0.0013 lr: 0.002\n",
      "iteration: 464540 loss: 0.0012 lr: 0.002\n",
      "iteration: 464550 loss: 0.0013 lr: 0.002\n",
      "iteration: 464560 loss: 0.0015 lr: 0.002\n",
      "iteration: 464570 loss: 0.0012 lr: 0.002\n",
      "iteration: 464580 loss: 0.0009 lr: 0.002\n",
      "iteration: 464590 loss: 0.0009 lr: 0.002\n",
      "iteration: 464600 loss: 0.0010 lr: 0.002\n",
      "iteration: 464610 loss: 0.0014 lr: 0.002\n",
      "iteration: 464620 loss: 0.0012 lr: 0.002\n",
      "iteration: 464630 loss: 0.0019 lr: 0.002\n",
      "iteration: 464640 loss: 0.0013 lr: 0.002\n",
      "iteration: 464650 loss: 0.0008 lr: 0.002\n",
      "iteration: 464660 loss: 0.0010 lr: 0.002\n",
      "iteration: 464670 loss: 0.0011 lr: 0.002\n",
      "iteration: 464680 loss: 0.0011 lr: 0.002\n",
      "iteration: 464690 loss: 0.0018 lr: 0.002\n",
      "iteration: 464700 loss: 0.0011 lr: 0.002\n",
      "iteration: 464710 loss: 0.0013 lr: 0.002\n",
      "iteration: 464720 loss: 0.0012 lr: 0.002\n",
      "iteration: 464730 loss: 0.0013 lr: 0.002\n",
      "iteration: 464740 loss: 0.0009 lr: 0.002\n",
      "iteration: 464750 loss: 0.0014 lr: 0.002\n",
      "iteration: 464760 loss: 0.0011 lr: 0.002\n",
      "iteration: 464770 loss: 0.0015 lr: 0.002\n",
      "iteration: 464780 loss: 0.0011 lr: 0.002\n",
      "iteration: 464790 loss: 0.0014 lr: 0.002\n",
      "iteration: 464800 loss: 0.0015 lr: 0.002\n",
      "iteration: 464810 loss: 0.0007 lr: 0.002\n",
      "iteration: 464820 loss: 0.0009 lr: 0.002\n",
      "iteration: 464830 loss: 0.0015 lr: 0.002\n",
      "iteration: 464840 loss: 0.0014 lr: 0.002\n",
      "iteration: 464850 loss: 0.0009 lr: 0.002\n",
      "iteration: 464860 loss: 0.0011 lr: 0.002\n",
      "iteration: 464870 loss: 0.0016 lr: 0.002\n",
      "iteration: 464880 loss: 0.0013 lr: 0.002\n",
      "iteration: 464890 loss: 0.0009 lr: 0.002\n",
      "iteration: 464900 loss: 0.0010 lr: 0.002\n",
      "iteration: 464910 loss: 0.0007 lr: 0.002\n",
      "iteration: 464920 loss: 0.0011 lr: 0.002\n",
      "iteration: 464930 loss: 0.0012 lr: 0.002\n",
      "iteration: 464940 loss: 0.0010 lr: 0.002\n",
      "iteration: 464950 loss: 0.0012 lr: 0.002\n",
      "iteration: 464960 loss: 0.0012 lr: 0.002\n",
      "iteration: 464970 loss: 0.0012 lr: 0.002\n",
      "iteration: 464980 loss: 0.0013 lr: 0.002\n",
      "iteration: 464990 loss: 0.0011 lr: 0.002\n",
      "iteration: 465000 loss: 0.0009 lr: 0.002\n",
      "iteration: 465010 loss: 0.0015 lr: 0.002\n",
      "iteration: 465020 loss: 0.0012 lr: 0.002\n",
      "iteration: 465030 loss: 0.0011 lr: 0.002\n",
      "iteration: 465040 loss: 0.0009 lr: 0.002\n",
      "iteration: 465050 loss: 0.0014 lr: 0.002\n",
      "iteration: 465060 loss: 0.0010 lr: 0.002\n",
      "iteration: 465070 loss: 0.0012 lr: 0.002\n",
      "iteration: 465080 loss: 0.0009 lr: 0.002\n",
      "iteration: 465090 loss: 0.0011 lr: 0.002\n",
      "iteration: 465100 loss: 0.0014 lr: 0.002\n",
      "iteration: 465110 loss: 0.0009 lr: 0.002\n",
      "iteration: 465120 loss: 0.0012 lr: 0.002\n",
      "iteration: 465130 loss: 0.0010 lr: 0.002\n",
      "iteration: 465140 loss: 0.0011 lr: 0.002\n",
      "iteration: 465150 loss: 0.0012 lr: 0.002\n",
      "iteration: 465160 loss: 0.0008 lr: 0.002\n",
      "iteration: 465170 loss: 0.0008 lr: 0.002\n",
      "iteration: 465180 loss: 0.0010 lr: 0.002\n",
      "iteration: 465190 loss: 0.0011 lr: 0.002\n",
      "iteration: 465200 loss: 0.0010 lr: 0.002\n",
      "iteration: 465210 loss: 0.0016 lr: 0.002\n",
      "iteration: 465220 loss: 0.0011 lr: 0.002\n",
      "iteration: 465230 loss: 0.0011 lr: 0.002\n",
      "iteration: 465240 loss: 0.0010 lr: 0.002\n",
      "iteration: 465250 loss: 0.0011 lr: 0.002\n",
      "iteration: 465260 loss: 0.0008 lr: 0.002\n",
      "iteration: 465270 loss: 0.0008 lr: 0.002\n",
      "iteration: 465280 loss: 0.0013 lr: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 465290 loss: 0.0008 lr: 0.002\n",
      "iteration: 465300 loss: 0.0008 lr: 0.002\n",
      "iteration: 465310 loss: 0.0013 lr: 0.002\n",
      "iteration: 465320 loss: 0.0016 lr: 0.002\n",
      "iteration: 465330 loss: 0.0012 lr: 0.002\n",
      "iteration: 465340 loss: 0.0010 lr: 0.002\n",
      "iteration: 465350 loss: 0.0014 lr: 0.002\n",
      "iteration: 465360 loss: 0.0013 lr: 0.002\n",
      "iteration: 465370 loss: 0.0009 lr: 0.002\n",
      "iteration: 465380 loss: 0.0013 lr: 0.002\n",
      "iteration: 465390 loss: 0.0009 lr: 0.002\n",
      "iteration: 465400 loss: 0.0009 lr: 0.002\n",
      "iteration: 465410 loss: 0.0010 lr: 0.002\n",
      "iteration: 465420 loss: 0.0012 lr: 0.002\n",
      "iteration: 465430 loss: 0.0014 lr: 0.002\n",
      "iteration: 465440 loss: 0.0008 lr: 0.002\n",
      "iteration: 465450 loss: 0.0013 lr: 0.002\n",
      "iteration: 465460 loss: 0.0010 lr: 0.002\n",
      "iteration: 465470 loss: 0.0010 lr: 0.002\n",
      "iteration: 465480 loss: 0.0015 lr: 0.002\n",
      "iteration: 465490 loss: 0.0017 lr: 0.002\n",
      "iteration: 465500 loss: 0.0014 lr: 0.002\n",
      "iteration: 465510 loss: 0.0013 lr: 0.002\n",
      "iteration: 465520 loss: 0.0009 lr: 0.002\n",
      "iteration: 465530 loss: 0.0011 lr: 0.002\n",
      "iteration: 465540 loss: 0.0015 lr: 0.002\n",
      "iteration: 465550 loss: 0.0011 lr: 0.002\n",
      "iteration: 465560 loss: 0.0011 lr: 0.002\n",
      "iteration: 465570 loss: 0.0008 lr: 0.002\n",
      "iteration: 465580 loss: 0.0011 lr: 0.002\n",
      "iteration: 465590 loss: 0.0008 lr: 0.002\n",
      "iteration: 465600 loss: 0.0014 lr: 0.002\n",
      "iteration: 465610 loss: 0.0012 lr: 0.002\n",
      "iteration: 465620 loss: 0.0011 lr: 0.002\n",
      "iteration: 465630 loss: 0.0016 lr: 0.002\n",
      "iteration: 465640 loss: 0.0010 lr: 0.002\n",
      "iteration: 465650 loss: 0.0014 lr: 0.002\n",
      "iteration: 465660 loss: 0.0008 lr: 0.002\n",
      "iteration: 465670 loss: 0.0011 lr: 0.002\n",
      "iteration: 465680 loss: 0.0014 lr: 0.002\n",
      "iteration: 465690 loss: 0.0011 lr: 0.002\n",
      "iteration: 465700 loss: 0.0009 lr: 0.002\n",
      "iteration: 465710 loss: 0.0011 lr: 0.002\n",
      "iteration: 465720 loss: 0.0014 lr: 0.002\n",
      "iteration: 465730 loss: 0.0011 lr: 0.002\n",
      "iteration: 465740 loss: 0.0007 lr: 0.002\n",
      "iteration: 465750 loss: 0.0021 lr: 0.002\n",
      "iteration: 465760 loss: 0.0010 lr: 0.002\n",
      "iteration: 465770 loss: 0.0008 lr: 0.002\n",
      "iteration: 465780 loss: 0.0010 lr: 0.002\n",
      "iteration: 465790 loss: 0.0013 lr: 0.002\n",
      "iteration: 465800 loss: 0.0009 lr: 0.002\n",
      "iteration: 465810 loss: 0.0011 lr: 0.002\n",
      "iteration: 465820 loss: 0.0010 lr: 0.002\n",
      "iteration: 465830 loss: 0.0015 lr: 0.002\n",
      "iteration: 465840 loss: 0.0012 lr: 0.002\n",
      "iteration: 465850 loss: 0.0012 lr: 0.002\n",
      "iteration: 465860 loss: 0.0010 lr: 0.002\n",
      "iteration: 465870 loss: 0.0017 lr: 0.002\n",
      "iteration: 465880 loss: 0.0018 lr: 0.002\n",
      "iteration: 465890 loss: 0.0010 lr: 0.002\n",
      "iteration: 465900 loss: 0.0015 lr: 0.002\n",
      "iteration: 465910 loss: 0.0011 lr: 0.002\n",
      "iteration: 465920 loss: 0.0010 lr: 0.002\n",
      "iteration: 465930 loss: 0.0011 lr: 0.002\n",
      "iteration: 465940 loss: 0.0010 lr: 0.002\n",
      "iteration: 465950 loss: 0.0010 lr: 0.002\n",
      "iteration: 465960 loss: 0.0014 lr: 0.002\n",
      "iteration: 465970 loss: 0.0013 lr: 0.002\n",
      "iteration: 465980 loss: 0.0011 lr: 0.002\n",
      "iteration: 465990 loss: 0.0008 lr: 0.002\n",
      "iteration: 466000 loss: 0.0010 lr: 0.002\n",
      "iteration: 466010 loss: 0.0009 lr: 0.002\n",
      "iteration: 466020 loss: 0.0011 lr: 0.002\n",
      "iteration: 466030 loss: 0.0015 lr: 0.002\n",
      "iteration: 466040 loss: 0.0017 lr: 0.002\n",
      "iteration: 466050 loss: 0.0008 lr: 0.002\n",
      "iteration: 466060 loss: 0.0011 lr: 0.002\n",
      "iteration: 466070 loss: 0.0010 lr: 0.002\n",
      "iteration: 466080 loss: 0.0011 lr: 0.002\n",
      "iteration: 466090 loss: 0.0015 lr: 0.002\n",
      "iteration: 466100 loss: 0.0017 lr: 0.002\n",
      "iteration: 466110 loss: 0.0009 lr: 0.002\n",
      "iteration: 466120 loss: 0.0016 lr: 0.002\n",
      "iteration: 466130 loss: 0.0011 lr: 0.002\n",
      "iteration: 466140 loss: 0.0012 lr: 0.002\n",
      "iteration: 466150 loss: 0.0011 lr: 0.002\n",
      "iteration: 466160 loss: 0.0009 lr: 0.002\n",
      "iteration: 466170 loss: 0.0016 lr: 0.002\n",
      "iteration: 466180 loss: 0.0019 lr: 0.002\n",
      "iteration: 466190 loss: 0.0010 lr: 0.002\n",
      "iteration: 466200 loss: 0.0012 lr: 0.002\n",
      "iteration: 466210 loss: 0.0010 lr: 0.002\n",
      "iteration: 466220 loss: 0.0012 lr: 0.002\n",
      "iteration: 466230 loss: 0.0015 lr: 0.002\n",
      "iteration: 466240 loss: 0.0010 lr: 0.002\n",
      "iteration: 466250 loss: 0.0012 lr: 0.002\n",
      "iteration: 466260 loss: 0.0010 lr: 0.002\n",
      "iteration: 466270 loss: 0.0016 lr: 0.002\n",
      "iteration: 466280 loss: 0.0013 lr: 0.002\n",
      "iteration: 466290 loss: 0.0011 lr: 0.002\n",
      "iteration: 466300 loss: 0.0010 lr: 0.002\n",
      "iteration: 466310 loss: 0.0017 lr: 0.002\n",
      "iteration: 466320 loss: 0.0017 lr: 0.002\n",
      "iteration: 466330 loss: 0.0009 lr: 0.002\n",
      "iteration: 466340 loss: 0.0013 lr: 0.002\n",
      "iteration: 466350 loss: 0.0011 lr: 0.002\n",
      "iteration: 466360 loss: 0.0013 lr: 0.002\n",
      "iteration: 466370 loss: 0.0012 lr: 0.002\n",
      "iteration: 466380 loss: 0.0012 lr: 0.002\n",
      "iteration: 466390 loss: 0.0015 lr: 0.002\n",
      "iteration: 466400 loss: 0.0012 lr: 0.002\n",
      "iteration: 466410 loss: 0.0012 lr: 0.002\n",
      "iteration: 466420 loss: 0.0010 lr: 0.002\n",
      "iteration: 466430 loss: 0.0011 lr: 0.002\n",
      "iteration: 466440 loss: 0.0014 lr: 0.002\n",
      "iteration: 466450 loss: 0.0010 lr: 0.002\n",
      "iteration: 466460 loss: 0.0009 lr: 0.002\n",
      "iteration: 466470 loss: 0.0009 lr: 0.002\n",
      "iteration: 466480 loss: 0.0007 lr: 0.002\n",
      "iteration: 466490 loss: 0.0017 lr: 0.002\n",
      "iteration: 466500 loss: 0.0013 lr: 0.002\n",
      "iteration: 466510 loss: 0.0014 lr: 0.002\n",
      "iteration: 466520 loss: 0.0013 lr: 0.002\n",
      "iteration: 466530 loss: 0.0011 lr: 0.002\n",
      "iteration: 466540 loss: 0.0010 lr: 0.002\n",
      "iteration: 466550 loss: 0.0008 lr: 0.002\n",
      "iteration: 466560 loss: 0.0010 lr: 0.002\n",
      "iteration: 466570 loss: 0.0010 lr: 0.002\n",
      "iteration: 466580 loss: 0.0014 lr: 0.002\n",
      "iteration: 466590 loss: 0.0014 lr: 0.002\n",
      "iteration: 466600 loss: 0.0008 lr: 0.002\n",
      "iteration: 466610 loss: 0.0014 lr: 0.002\n",
      "iteration: 466620 loss: 0.0007 lr: 0.002\n",
      "iteration: 466630 loss: 0.0010 lr: 0.002\n",
      "iteration: 466640 loss: 0.0010 lr: 0.002\n",
      "iteration: 466650 loss: 0.0010 lr: 0.002\n",
      "iteration: 466660 loss: 0.0011 lr: 0.002\n",
      "iteration: 466670 loss: 0.0010 lr: 0.002\n",
      "iteration: 466680 loss: 0.0010 lr: 0.002\n",
      "iteration: 466690 loss: 0.0012 lr: 0.002\n",
      "iteration: 466700 loss: 0.0009 lr: 0.002\n",
      "iteration: 466710 loss: 0.0011 lr: 0.002\n",
      "iteration: 466720 loss: 0.0012 lr: 0.002\n",
      "iteration: 466730 loss: 0.0011 lr: 0.002\n",
      "iteration: 466740 loss: 0.0018 lr: 0.002\n",
      "iteration: 466750 loss: 0.0012 lr: 0.002\n",
      "iteration: 466760 loss: 0.0013 lr: 0.002\n",
      "iteration: 466770 loss: 0.0011 lr: 0.002\n",
      "iteration: 466780 loss: 0.0015 lr: 0.002\n",
      "iteration: 466790 loss: 0.0010 lr: 0.002\n",
      "iteration: 466800 loss: 0.0008 lr: 0.002\n",
      "iteration: 466810 loss: 0.0013 lr: 0.002\n",
      "iteration: 466820 loss: 0.0011 lr: 0.002\n",
      "iteration: 466830 loss: 0.0012 lr: 0.002\n",
      "iteration: 466840 loss: 0.0012 lr: 0.002\n",
      "iteration: 466850 loss: 0.0014 lr: 0.002\n",
      "iteration: 466860 loss: 0.0018 lr: 0.002\n",
      "iteration: 466870 loss: 0.0013 lr: 0.002\n",
      "iteration: 466880 loss: 0.0012 lr: 0.002\n",
      "iteration: 466890 loss: 0.0019 lr: 0.002\n",
      "iteration: 466900 loss: 0.0012 lr: 0.002\n",
      "iteration: 466910 loss: 0.0010 lr: 0.002\n",
      "iteration: 466920 loss: 0.0011 lr: 0.002\n",
      "iteration: 466930 loss: 0.0009 lr: 0.002\n",
      "iteration: 466940 loss: 0.0012 lr: 0.002\n",
      "iteration: 466950 loss: 0.0008 lr: 0.002\n",
      "iteration: 466960 loss: 0.0010 lr: 0.002\n",
      "iteration: 466970 loss: 0.0018 lr: 0.002\n",
      "iteration: 466980 loss: 0.0010 lr: 0.002\n",
      "iteration: 466990 loss: 0.0005 lr: 0.002\n",
      "iteration: 467000 loss: 0.0008 lr: 0.002\n",
      "iteration: 467010 loss: 0.0008 lr: 0.002\n",
      "iteration: 467020 loss: 0.0010 lr: 0.002\n",
      "iteration: 467030 loss: 0.0009 lr: 0.002\n",
      "iteration: 467040 loss: 0.0010 lr: 0.002\n",
      "iteration: 467050 loss: 0.0018 lr: 0.002\n",
      "iteration: 467060 loss: 0.0013 lr: 0.002\n",
      "iteration: 467070 loss: 0.0013 lr: 0.002\n",
      "iteration: 467080 loss: 0.0009 lr: 0.002\n",
      "iteration: 467090 loss: 0.0015 lr: 0.002\n",
      "iteration: 467100 loss: 0.0011 lr: 0.002\n",
      "iteration: 467110 loss: 0.0013 lr: 0.002\n",
      "iteration: 467120 loss: 0.0012 lr: 0.002\n",
      "iteration: 467130 loss: 0.0010 lr: 0.002\n",
      "iteration: 467140 loss: 0.0009 lr: 0.002\n",
      "iteration: 467150 loss: 0.0015 lr: 0.002\n",
      "iteration: 467160 loss: 0.0015 lr: 0.002\n",
      "iteration: 467170 loss: 0.0011 lr: 0.002\n",
      "iteration: 467180 loss: 0.0011 lr: 0.002\n",
      "iteration: 467190 loss: 0.0013 lr: 0.002\n",
      "iteration: 467200 loss: 0.0015 lr: 0.002\n",
      "iteration: 467210 loss: 0.0025 lr: 0.002\n",
      "iteration: 467220 loss: 0.0013 lr: 0.002\n",
      "iteration: 467230 loss: 0.0011 lr: 0.002\n",
      "iteration: 467240 loss: 0.0012 lr: 0.002\n",
      "iteration: 467250 loss: 0.0017 lr: 0.002\n",
      "iteration: 467260 loss: 0.0012 lr: 0.002\n",
      "iteration: 467270 loss: 0.0016 lr: 0.002\n",
      "iteration: 467280 loss: 0.0025 lr: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 467290 loss: 0.0012 lr: 0.002\n",
      "iteration: 467300 loss: 0.0018 lr: 0.002\n",
      "iteration: 467310 loss: 0.0015 lr: 0.002\n",
      "iteration: 467320 loss: 0.0015 lr: 0.002\n",
      "iteration: 467330 loss: 0.0014 lr: 0.002\n",
      "iteration: 467340 loss: 0.0011 lr: 0.002\n",
      "iteration: 467350 loss: 0.0013 lr: 0.002\n",
      "iteration: 467360 loss: 0.0012 lr: 0.002\n",
      "iteration: 467370 loss: 0.0014 lr: 0.002\n",
      "iteration: 467380 loss: 0.0010 lr: 0.002\n",
      "iteration: 467390 loss: 0.0013 lr: 0.002\n",
      "iteration: 467400 loss: 0.0010 lr: 0.002\n",
      "iteration: 467410 loss: 0.0010 lr: 0.002\n",
      "iteration: 467420 loss: 0.0011 lr: 0.002\n",
      "iteration: 467430 loss: 0.0015 lr: 0.002\n",
      "iteration: 467440 loss: 0.0021 lr: 0.002\n",
      "iteration: 467450 loss: 0.0011 lr: 0.002\n",
      "iteration: 467460 loss: 0.0014 lr: 0.002\n",
      "iteration: 467470 loss: 0.0012 lr: 0.002\n",
      "iteration: 467480 loss: 0.0009 lr: 0.002\n",
      "iteration: 467490 loss: 0.0011 lr: 0.002\n",
      "iteration: 467500 loss: 0.0011 lr: 0.002\n",
      "iteration: 467510 loss: 0.0015 lr: 0.002\n",
      "iteration: 467520 loss: 0.0007 lr: 0.002\n",
      "iteration: 467530 loss: 0.0014 lr: 0.002\n",
      "iteration: 467540 loss: 0.0010 lr: 0.002\n",
      "iteration: 467550 loss: 0.0011 lr: 0.002\n",
      "iteration: 467560 loss: 0.0009 lr: 0.002\n",
      "iteration: 467570 loss: 0.0008 lr: 0.002\n",
      "iteration: 467580 loss: 0.0010 lr: 0.002\n",
      "iteration: 467590 loss: 0.0011 lr: 0.002\n",
      "iteration: 467600 loss: 0.0010 lr: 0.002\n",
      "iteration: 467610 loss: 0.0012 lr: 0.002\n",
      "iteration: 467620 loss: 0.0014 lr: 0.002\n",
      "iteration: 467630 loss: 0.0011 lr: 0.002\n",
      "iteration: 467640 loss: 0.0012 lr: 0.002\n",
      "iteration: 467650 loss: 0.0008 lr: 0.002\n",
      "iteration: 467660 loss: 0.0009 lr: 0.002\n",
      "iteration: 467670 loss: 0.0009 lr: 0.002\n",
      "iteration: 467680 loss: 0.0012 lr: 0.002\n",
      "iteration: 467690 loss: 0.0013 lr: 0.002\n",
      "iteration: 467700 loss: 0.0012 lr: 0.002\n",
      "iteration: 467710 loss: 0.0015 lr: 0.002\n",
      "iteration: 467720 loss: 0.0012 lr: 0.002\n",
      "iteration: 467730 loss: 0.0007 lr: 0.002\n",
      "iteration: 467740 loss: 0.0009 lr: 0.002\n",
      "iteration: 467750 loss: 0.0011 lr: 0.002\n",
      "iteration: 467760 loss: 0.0007 lr: 0.002\n",
      "iteration: 467770 loss: 0.0019 lr: 0.002\n",
      "iteration: 467780 loss: 0.0010 lr: 0.002\n",
      "iteration: 467790 loss: 0.0011 lr: 0.002\n",
      "iteration: 467800 loss: 0.0010 lr: 0.002\n",
      "iteration: 467810 loss: 0.0006 lr: 0.002\n",
      "iteration: 467820 loss: 0.0014 lr: 0.002\n",
      "iteration: 467830 loss: 0.0011 lr: 0.002\n",
      "iteration: 467840 loss: 0.0012 lr: 0.002\n",
      "iteration: 467850 loss: 0.0013 lr: 0.002\n",
      "iteration: 467860 loss: 0.0010 lr: 0.002\n",
      "iteration: 467870 loss: 0.0016 lr: 0.002\n",
      "iteration: 467880 loss: 0.0011 lr: 0.002\n",
      "iteration: 467890 loss: 0.0009 lr: 0.002\n",
      "iteration: 467900 loss: 0.0011 lr: 0.002\n",
      "iteration: 467910 loss: 0.0006 lr: 0.002\n",
      "iteration: 467920 loss: 0.0009 lr: 0.002\n",
      "iteration: 467930 loss: 0.0015 lr: 0.002\n",
      "iteration: 467940 loss: 0.0016 lr: 0.002\n",
      "iteration: 467950 loss: 0.0013 lr: 0.002\n",
      "iteration: 467960 loss: 0.0020 lr: 0.002\n",
      "iteration: 467970 loss: 0.0011 lr: 0.002\n",
      "iteration: 467980 loss: 0.0012 lr: 0.002\n",
      "iteration: 467990 loss: 0.0011 lr: 0.002\n",
      "iteration: 468000 loss: 0.0008 lr: 0.002\n",
      "iteration: 468010 loss: 0.0007 lr: 0.002\n",
      "iteration: 468020 loss: 0.0015 lr: 0.002\n",
      "iteration: 468030 loss: 0.0013 lr: 0.002\n",
      "iteration: 468040 loss: 0.0024 lr: 0.002\n",
      "iteration: 468050 loss: 0.0012 lr: 0.002\n",
      "iteration: 468060 loss: 0.0013 lr: 0.002\n",
      "iteration: 468070 loss: 0.0013 lr: 0.002\n",
      "iteration: 468080 loss: 0.0011 lr: 0.002\n",
      "iteration: 468090 loss: 0.0013 lr: 0.002\n",
      "iteration: 468100 loss: 0.0013 lr: 0.002\n",
      "iteration: 468110 loss: 0.0010 lr: 0.002\n",
      "iteration: 468120 loss: 0.0008 lr: 0.002\n",
      "iteration: 468130 loss: 0.0020 lr: 0.002\n",
      "iteration: 468140 loss: 0.0016 lr: 0.002\n",
      "iteration: 468150 loss: 0.0010 lr: 0.002\n",
      "iteration: 468160 loss: 0.0013 lr: 0.002\n",
      "iteration: 468170 loss: 0.0009 lr: 0.002\n",
      "iteration: 468180 loss: 0.0012 lr: 0.002\n",
      "iteration: 468190 loss: 0.0019 lr: 0.002\n",
      "iteration: 468200 loss: 0.0009 lr: 0.002\n",
      "iteration: 468210 loss: 0.0010 lr: 0.002\n",
      "iteration: 468220 loss: 0.0019 lr: 0.002\n",
      "iteration: 468230 loss: 0.0010 lr: 0.002\n",
      "iteration: 468240 loss: 0.0015 lr: 0.002\n",
      "iteration: 468250 loss: 0.0018 lr: 0.002\n",
      "iteration: 468260 loss: 0.0010 lr: 0.002\n",
      "iteration: 468270 loss: 0.0016 lr: 0.002\n",
      "iteration: 468280 loss: 0.0008 lr: 0.002\n",
      "iteration: 468290 loss: 0.0009 lr: 0.002\n",
      "iteration: 468300 loss: 0.0014 lr: 0.002\n",
      "iteration: 468310 loss: 0.0012 lr: 0.002\n",
      "iteration: 468320 loss: 0.0013 lr: 0.002\n",
      "iteration: 468330 loss: 0.0012 lr: 0.002\n",
      "iteration: 468340 loss: 0.0016 lr: 0.002\n",
      "iteration: 468350 loss: 0.0014 lr: 0.002\n",
      "iteration: 468360 loss: 0.0011 lr: 0.002\n",
      "iteration: 468370 loss: 0.0011 lr: 0.002\n",
      "iteration: 468380 loss: 0.0009 lr: 0.002\n",
      "iteration: 468390 loss: 0.0012 lr: 0.002\n",
      "iteration: 468400 loss: 0.0013 lr: 0.002\n",
      "iteration: 468410 loss: 0.0009 lr: 0.002\n",
      "iteration: 468420 loss: 0.0011 lr: 0.002\n",
      "iteration: 468430 loss: 0.0027 lr: 0.002\n",
      "iteration: 468440 loss: 0.0008 lr: 0.002\n",
      "iteration: 468450 loss: 0.0012 lr: 0.002\n",
      "iteration: 468460 loss: 0.0012 lr: 0.002\n",
      "iteration: 468470 loss: 0.0012 lr: 0.002\n",
      "iteration: 468480 loss: 0.0012 lr: 0.002\n",
      "iteration: 468490 loss: 0.0011 lr: 0.002\n",
      "iteration: 468500 loss: 0.0011 lr: 0.002\n",
      "iteration: 468510 loss: 0.0008 lr: 0.002\n",
      "iteration: 468520 loss: 0.0016 lr: 0.002\n",
      "iteration: 468530 loss: 0.0010 lr: 0.002\n",
      "iteration: 468540 loss: 0.0010 lr: 0.002\n",
      "iteration: 468550 loss: 0.0008 lr: 0.002\n",
      "iteration: 468560 loss: 0.0013 lr: 0.002\n",
      "iteration: 468570 loss: 0.0011 lr: 0.002\n",
      "iteration: 468580 loss: 0.0012 lr: 0.002\n",
      "iteration: 468590 loss: 0.0008 lr: 0.002\n",
      "iteration: 468600 loss: 0.0010 lr: 0.002\n",
      "iteration: 468610 loss: 0.0010 lr: 0.002\n",
      "iteration: 468620 loss: 0.0012 lr: 0.002\n",
      "iteration: 468630 loss: 0.0012 lr: 0.002\n",
      "iteration: 468640 loss: 0.0011 lr: 0.002\n",
      "iteration: 468650 loss: 0.0014 lr: 0.002\n",
      "iteration: 468660 loss: 0.0016 lr: 0.002\n",
      "iteration: 468670 loss: 0.0016 lr: 0.002\n",
      "iteration: 468680 loss: 0.0010 lr: 0.002\n",
      "iteration: 468690 loss: 0.0015 lr: 0.002\n",
      "iteration: 468700 loss: 0.0011 lr: 0.002\n",
      "iteration: 468710 loss: 0.0010 lr: 0.002\n",
      "iteration: 468720 loss: 0.0014 lr: 0.002\n",
      "iteration: 468730 loss: 0.0008 lr: 0.002\n",
      "iteration: 468740 loss: 0.0012 lr: 0.002\n",
      "iteration: 468750 loss: 0.0010 lr: 0.002\n",
      "iteration: 468760 loss: 0.0011 lr: 0.002\n",
      "iteration: 468770 loss: 0.0011 lr: 0.002\n",
      "iteration: 468780 loss: 0.0014 lr: 0.002\n",
      "iteration: 468790 loss: 0.0009 lr: 0.002\n",
      "iteration: 468800 loss: 0.0010 lr: 0.002\n",
      "iteration: 468810 loss: 0.0010 lr: 0.002\n",
      "iteration: 468820 loss: 0.0009 lr: 0.002\n",
      "iteration: 468830 loss: 0.0015 lr: 0.002\n",
      "iteration: 468840 loss: 0.0010 lr: 0.002\n",
      "iteration: 468850 loss: 0.0011 lr: 0.002\n",
      "iteration: 468860 loss: 0.0010 lr: 0.002\n",
      "iteration: 468870 loss: 0.0009 lr: 0.002\n",
      "iteration: 468880 loss: 0.0013 lr: 0.002\n",
      "iteration: 468890 loss: 0.0017 lr: 0.002\n",
      "iteration: 468900 loss: 0.0011 lr: 0.002\n",
      "iteration: 468910 loss: 0.0019 lr: 0.002\n",
      "iteration: 468920 loss: 0.0015 lr: 0.002\n",
      "iteration: 468930 loss: 0.0010 lr: 0.002\n",
      "iteration: 468940 loss: 0.0021 lr: 0.002\n",
      "iteration: 468950 loss: 0.0011 lr: 0.002\n",
      "iteration: 468960 loss: 0.0008 lr: 0.002\n",
      "iteration: 468970 loss: 0.0013 lr: 0.002\n",
      "iteration: 468980 loss: 0.0016 lr: 0.002\n",
      "iteration: 468990 loss: 0.0012 lr: 0.002\n",
      "iteration: 469000 loss: 0.0012 lr: 0.002\n",
      "iteration: 469010 loss: 0.0009 lr: 0.002\n",
      "iteration: 469020 loss: 0.0010 lr: 0.002\n",
      "iteration: 469030 loss: 0.0013 lr: 0.002\n",
      "iteration: 469040 loss: 0.0009 lr: 0.002\n",
      "iteration: 469050 loss: 0.0010 lr: 0.002\n",
      "iteration: 469060 loss: 0.0012 lr: 0.002\n",
      "iteration: 469070 loss: 0.0012 lr: 0.002\n",
      "iteration: 469080 loss: 0.0011 lr: 0.002\n",
      "iteration: 469090 loss: 0.0013 lr: 0.002\n",
      "iteration: 469100 loss: 0.0018 lr: 0.002\n",
      "iteration: 469110 loss: 0.0016 lr: 0.002\n",
      "iteration: 469120 loss: 0.0010 lr: 0.002\n",
      "iteration: 469130 loss: 0.0012 lr: 0.002\n",
      "iteration: 469140 loss: 0.0009 lr: 0.002\n",
      "iteration: 469150 loss: 0.0011 lr: 0.002\n",
      "iteration: 469160 loss: 0.0015 lr: 0.002\n",
      "iteration: 469170 loss: 0.0013 lr: 0.002\n",
      "iteration: 469180 loss: 0.0010 lr: 0.002\n",
      "iteration: 469190 loss: 0.0010 lr: 0.002\n",
      "iteration: 469200 loss: 0.0011 lr: 0.002\n",
      "iteration: 469210 loss: 0.0021 lr: 0.002\n",
      "iteration: 469220 loss: 0.0008 lr: 0.002\n",
      "iteration: 469230 loss: 0.0020 lr: 0.002\n",
      "iteration: 469240 loss: 0.0014 lr: 0.002\n",
      "iteration: 469250 loss: 0.0009 lr: 0.002\n",
      "iteration: 469260 loss: 0.0016 lr: 0.002\n",
      "iteration: 469270 loss: 0.0010 lr: 0.002\n",
      "iteration: 469280 loss: 0.0013 lr: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 469290 loss: 0.0010 lr: 0.002\n",
      "iteration: 469300 loss: 0.0019 lr: 0.002\n",
      "iteration: 469310 loss: 0.0012 lr: 0.002\n",
      "iteration: 469320 loss: 0.0011 lr: 0.002\n",
      "iteration: 469330 loss: 0.0017 lr: 0.002\n",
      "iteration: 469340 loss: 0.0009 lr: 0.002\n",
      "iteration: 469350 loss: 0.0008 lr: 0.002\n",
      "iteration: 469360 loss: 0.0015 lr: 0.002\n",
      "iteration: 469370 loss: 0.0009 lr: 0.002\n",
      "iteration: 469380 loss: 0.0012 lr: 0.002\n",
      "iteration: 469390 loss: 0.0008 lr: 0.002\n",
      "iteration: 469400 loss: 0.0009 lr: 0.002\n",
      "iteration: 469410 loss: 0.0014 lr: 0.002\n",
      "iteration: 469420 loss: 0.0015 lr: 0.002\n",
      "iteration: 469430 loss: 0.0011 lr: 0.002\n",
      "iteration: 469440 loss: 0.0009 lr: 0.002\n",
      "iteration: 469450 loss: 0.0010 lr: 0.002\n",
      "iteration: 469460 loss: 0.0012 lr: 0.002\n",
      "iteration: 469470 loss: 0.0012 lr: 0.002\n",
      "iteration: 469480 loss: 0.0012 lr: 0.002\n",
      "iteration: 469490 loss: 0.0011 lr: 0.002\n",
      "iteration: 469500 loss: 0.0013 lr: 0.002\n",
      "iteration: 469510 loss: 0.0008 lr: 0.002\n",
      "iteration: 469520 loss: 0.0011 lr: 0.002\n",
      "iteration: 469530 loss: 0.0012 lr: 0.002\n",
      "iteration: 469540 loss: 0.0017 lr: 0.002\n",
      "iteration: 469550 loss: 0.0014 lr: 0.002\n",
      "iteration: 469560 loss: 0.0011 lr: 0.002\n",
      "iteration: 469570 loss: 0.0013 lr: 0.002\n",
      "iteration: 469580 loss: 0.0010 lr: 0.002\n",
      "iteration: 469590 loss: 0.0013 lr: 0.002\n",
      "iteration: 469600 loss: 0.0016 lr: 0.002\n",
      "iteration: 469610 loss: 0.0012 lr: 0.002\n",
      "iteration: 469620 loss: 0.0012 lr: 0.002\n",
      "iteration: 469630 loss: 0.0011 lr: 0.002\n",
      "iteration: 469640 loss: 0.0009 lr: 0.002\n",
      "iteration: 469650 loss: 0.0013 lr: 0.002\n",
      "iteration: 469660 loss: 0.0008 lr: 0.002\n",
      "iteration: 469670 loss: 0.0010 lr: 0.002\n",
      "iteration: 469680 loss: 0.0014 lr: 0.002\n",
      "iteration: 469690 loss: 0.0012 lr: 0.002\n",
      "iteration: 469700 loss: 0.0014 lr: 0.002\n",
      "iteration: 469710 loss: 0.0012 lr: 0.002\n",
      "iteration: 469720 loss: 0.0009 lr: 0.002\n",
      "iteration: 469730 loss: 0.0013 lr: 0.002\n",
      "iteration: 469740 loss: 0.0009 lr: 0.002\n",
      "iteration: 469750 loss: 0.0011 lr: 0.002\n",
      "iteration: 469760 loss: 0.0012 lr: 0.002\n",
      "iteration: 469770 loss: 0.0017 lr: 0.002\n",
      "iteration: 469780 loss: 0.0009 lr: 0.002\n",
      "iteration: 469790 loss: 0.0008 lr: 0.002\n",
      "iteration: 469800 loss: 0.0013 lr: 0.002\n",
      "iteration: 469810 loss: 0.0012 lr: 0.002\n",
      "iteration: 469820 loss: 0.0009 lr: 0.002\n",
      "iteration: 469830 loss: 0.0010 lr: 0.002\n",
      "iteration: 469840 loss: 0.0025 lr: 0.002\n",
      "iteration: 469850 loss: 0.0012 lr: 0.002\n",
      "iteration: 469860 loss: 0.0011 lr: 0.002\n",
      "iteration: 469870 loss: 0.0008 lr: 0.002\n",
      "iteration: 469880 loss: 0.0012 lr: 0.002\n",
      "iteration: 469890 loss: 0.0012 lr: 0.002\n",
      "iteration: 469900 loss: 0.0010 lr: 0.002\n",
      "iteration: 469910 loss: 0.0013 lr: 0.002\n",
      "iteration: 469920 loss: 0.0014 lr: 0.002\n",
      "iteration: 469930 loss: 0.0010 lr: 0.002\n",
      "iteration: 469940 loss: 0.0010 lr: 0.002\n",
      "iteration: 469950 loss: 0.0010 lr: 0.002\n",
      "iteration: 469960 loss: 0.0009 lr: 0.002\n",
      "iteration: 469970 loss: 0.0010 lr: 0.002\n",
      "iteration: 469980 loss: 0.0022 lr: 0.002\n",
      "iteration: 469990 loss: 0.0013 lr: 0.002\n",
      "iteration: 470000 loss: 0.0007 lr: 0.002\n",
      "iteration: 470010 loss: 0.0008 lr: 0.002\n",
      "iteration: 470020 loss: 0.0010 lr: 0.002\n",
      "iteration: 470030 loss: 0.0015 lr: 0.002\n",
      "iteration: 470040 loss: 0.0009 lr: 0.002\n",
      "iteration: 470050 loss: 0.0009 lr: 0.002\n",
      "iteration: 470060 loss: 0.0009 lr: 0.002\n",
      "iteration: 470070 loss: 0.0011 lr: 0.002\n",
      "iteration: 470080 loss: 0.0012 lr: 0.002\n",
      "iteration: 470090 loss: 0.0012 lr: 0.002\n",
      "iteration: 470100 loss: 0.0010 lr: 0.002\n",
      "iteration: 470110 loss: 0.0013 lr: 0.002\n",
      "iteration: 470120 loss: 0.0009 lr: 0.002\n",
      "iteration: 470130 loss: 0.0012 lr: 0.002\n",
      "iteration: 470140 loss: 0.0016 lr: 0.002\n",
      "iteration: 470150 loss: 0.0011 lr: 0.002\n",
      "iteration: 470160 loss: 0.0011 lr: 0.002\n",
      "iteration: 470170 loss: 0.0016 lr: 0.002\n",
      "iteration: 470180 loss: 0.0013 lr: 0.002\n",
      "iteration: 470190 loss: 0.0010 lr: 0.002\n",
      "iteration: 470200 loss: 0.0013 lr: 0.002\n",
      "iteration: 470210 loss: 0.0012 lr: 0.002\n",
      "iteration: 470220 loss: 0.0012 lr: 0.002\n",
      "iteration: 470230 loss: 0.0011 lr: 0.002\n",
      "iteration: 470240 loss: 0.0010 lr: 0.002\n",
      "iteration: 470250 loss: 0.0007 lr: 0.002\n",
      "iteration: 470260 loss: 0.0013 lr: 0.002\n",
      "iteration: 470270 loss: 0.0021 lr: 0.002\n",
      "iteration: 470280 loss: 0.0008 lr: 0.002\n",
      "iteration: 470290 loss: 0.0011 lr: 0.002\n",
      "iteration: 470300 loss: 0.0011 lr: 0.002\n",
      "iteration: 470310 loss: 0.0009 lr: 0.002\n",
      "iteration: 470320 loss: 0.0012 lr: 0.002\n",
      "iteration: 470330 loss: 0.0018 lr: 0.002\n",
      "iteration: 470340 loss: 0.0014 lr: 0.002\n",
      "iteration: 470350 loss: 0.0013 lr: 0.002\n",
      "iteration: 470360 loss: 0.0013 lr: 0.002\n",
      "iteration: 470370 loss: 0.0016 lr: 0.002\n",
      "iteration: 470380 loss: 0.0007 lr: 0.002\n",
      "iteration: 470390 loss: 0.0007 lr: 0.002\n",
      "iteration: 470400 loss: 0.0007 lr: 0.002\n",
      "iteration: 470410 loss: 0.0009 lr: 0.002\n",
      "iteration: 470420 loss: 0.0010 lr: 0.002\n",
      "iteration: 470430 loss: 0.0009 lr: 0.002\n",
      "iteration: 470440 loss: 0.0009 lr: 0.002\n",
      "iteration: 470450 loss: 0.0010 lr: 0.002\n",
      "iteration: 470460 loss: 0.0017 lr: 0.002\n",
      "iteration: 470470 loss: 0.0013 lr: 0.002\n",
      "iteration: 470480 loss: 0.0008 lr: 0.002\n",
      "iteration: 470490 loss: 0.0012 lr: 0.002\n",
      "iteration: 470500 loss: 0.0019 lr: 0.002\n",
      "iteration: 470510 loss: 0.0009 lr: 0.002\n",
      "iteration: 470520 loss: 0.0014 lr: 0.002\n",
      "iteration: 470530 loss: 0.0013 lr: 0.002\n",
      "iteration: 470540 loss: 0.0012 lr: 0.002\n",
      "iteration: 470550 loss: 0.0018 lr: 0.002\n",
      "iteration: 470560 loss: 0.0013 lr: 0.002\n",
      "iteration: 470570 loss: 0.0009 lr: 0.002\n",
      "iteration: 470580 loss: 0.0012 lr: 0.002\n",
      "iteration: 470590 loss: 0.0016 lr: 0.002\n",
      "iteration: 470600 loss: 0.0010 lr: 0.002\n",
      "iteration: 470610 loss: 0.0014 lr: 0.002\n",
      "iteration: 470620 loss: 0.0007 lr: 0.002\n",
      "iteration: 470630 loss: 0.0009 lr: 0.002\n",
      "iteration: 470640 loss: 0.0010 lr: 0.002\n",
      "iteration: 470650 loss: 0.0014 lr: 0.002\n",
      "iteration: 470660 loss: 0.0010 lr: 0.002\n",
      "iteration: 470670 loss: 0.0009 lr: 0.002\n",
      "iteration: 470680 loss: 0.0016 lr: 0.002\n",
      "iteration: 470690 loss: 0.0013 lr: 0.002\n",
      "iteration: 470700 loss: 0.0009 lr: 0.002\n",
      "iteration: 470710 loss: 0.0010 lr: 0.002\n",
      "iteration: 470720 loss: 0.0021 lr: 0.002\n",
      "iteration: 470730 loss: 0.0009 lr: 0.002\n",
      "iteration: 470740 loss: 0.0014 lr: 0.002\n",
      "iteration: 470750 loss: 0.0018 lr: 0.002\n",
      "iteration: 470760 loss: 0.0015 lr: 0.002\n",
      "iteration: 470770 loss: 0.0012 lr: 0.002\n",
      "iteration: 470780 loss: 0.0014 lr: 0.002\n",
      "iteration: 470790 loss: 0.0008 lr: 0.002\n",
      "iteration: 470800 loss: 0.0010 lr: 0.002\n",
      "iteration: 470810 loss: 0.0008 lr: 0.002\n",
      "iteration: 470820 loss: 0.0013 lr: 0.002\n",
      "iteration: 470830 loss: 0.0008 lr: 0.002\n",
      "iteration: 470840 loss: 0.0013 lr: 0.002\n",
      "iteration: 470850 loss: 0.0011 lr: 0.002\n",
      "iteration: 470860 loss: 0.0016 lr: 0.002\n",
      "iteration: 470870 loss: 0.0012 lr: 0.002\n",
      "iteration: 470880 loss: 0.0015 lr: 0.002\n",
      "iteration: 470890 loss: 0.0014 lr: 0.002\n",
      "iteration: 470900 loss: 0.0013 lr: 0.002\n",
      "iteration: 470910 loss: 0.0016 lr: 0.002\n",
      "iteration: 470920 loss: 0.0014 lr: 0.002\n",
      "iteration: 470930 loss: 0.0013 lr: 0.002\n",
      "iteration: 470940 loss: 0.0009 lr: 0.002\n",
      "iteration: 470950 loss: 0.0012 lr: 0.002\n",
      "iteration: 470960 loss: 0.0020 lr: 0.002\n",
      "iteration: 470970 loss: 0.0010 lr: 0.002\n",
      "iteration: 470980 loss: 0.0015 lr: 0.002\n",
      "iteration: 470990 loss: 0.0012 lr: 0.002\n",
      "iteration: 471000 loss: 0.0011 lr: 0.002\n",
      "iteration: 471010 loss: 0.0010 lr: 0.002\n",
      "iteration: 471020 loss: 0.0015 lr: 0.002\n",
      "iteration: 471030 loss: 0.0011 lr: 0.002\n",
      "iteration: 471040 loss: 0.0016 lr: 0.002\n",
      "iteration: 471050 loss: 0.0010 lr: 0.002\n",
      "iteration: 471060 loss: 0.0013 lr: 0.002\n",
      "iteration: 471070 loss: 0.0008 lr: 0.002\n",
      "iteration: 471080 loss: 0.0013 lr: 0.002\n",
      "iteration: 471090 loss: 0.0017 lr: 0.002\n",
      "iteration: 471100 loss: 0.0013 lr: 0.002\n",
      "iteration: 471110 loss: 0.0014 lr: 0.002\n",
      "iteration: 471120 loss: 0.0018 lr: 0.002\n",
      "iteration: 471130 loss: 0.0016 lr: 0.002\n",
      "iteration: 471140 loss: 0.0010 lr: 0.002\n",
      "iteration: 471150 loss: 0.0016 lr: 0.002\n",
      "iteration: 471160 loss: 0.0010 lr: 0.002\n",
      "iteration: 471170 loss: 0.0017 lr: 0.002\n",
      "iteration: 471180 loss: 0.0013 lr: 0.002\n",
      "iteration: 471190 loss: 0.0012 lr: 0.002\n",
      "iteration: 471200 loss: 0.0014 lr: 0.002\n",
      "iteration: 471210 loss: 0.0014 lr: 0.002\n",
      "iteration: 471220 loss: 0.0007 lr: 0.002\n",
      "iteration: 471230 loss: 0.0013 lr: 0.002\n",
      "iteration: 471240 loss: 0.0017 lr: 0.002\n",
      "iteration: 471250 loss: 0.0009 lr: 0.002\n",
      "iteration: 471260 loss: 0.0011 lr: 0.002\n",
      "iteration: 471270 loss: 0.0013 lr: 0.002\n",
      "iteration: 471280 loss: 0.0011 lr: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 471290 loss: 0.0008 lr: 0.002\n",
      "iteration: 471300 loss: 0.0014 lr: 0.002\n",
      "iteration: 471310 loss: 0.0011 lr: 0.002\n",
      "iteration: 471320 loss: 0.0008 lr: 0.002\n",
      "iteration: 471330 loss: 0.0015 lr: 0.002\n",
      "iteration: 471340 loss: 0.0008 lr: 0.002\n",
      "iteration: 471350 loss: 0.0009 lr: 0.002\n",
      "iteration: 471360 loss: 0.0010 lr: 0.002\n",
      "iteration: 471370 loss: 0.0011 lr: 0.002\n",
      "iteration: 471380 loss: 0.0012 lr: 0.002\n",
      "iteration: 471390 loss: 0.0016 lr: 0.002\n",
      "iteration: 471400 loss: 0.0010 lr: 0.002\n",
      "iteration: 471410 loss: 0.0014 lr: 0.002\n",
      "iteration: 471420 loss: 0.0011 lr: 0.002\n",
      "iteration: 471430 loss: 0.0010 lr: 0.002\n",
      "iteration: 471440 loss: 0.0010 lr: 0.002\n",
      "iteration: 471450 loss: 0.0009 lr: 0.002\n",
      "iteration: 471460 loss: 0.0012 lr: 0.002\n",
      "iteration: 471470 loss: 0.0017 lr: 0.002\n",
      "iteration: 471480 loss: 0.0013 lr: 0.002\n",
      "iteration: 471490 loss: 0.0008 lr: 0.002\n",
      "iteration: 471500 loss: 0.0008 lr: 0.002\n",
      "iteration: 471510 loss: 0.0012 lr: 0.002\n",
      "iteration: 471520 loss: 0.0012 lr: 0.002\n",
      "iteration: 471530 loss: 0.0011 lr: 0.002\n",
      "iteration: 471540 loss: 0.0009 lr: 0.002\n",
      "iteration: 471550 loss: 0.0011 lr: 0.002\n",
      "iteration: 471560 loss: 0.0008 lr: 0.002\n",
      "iteration: 471570 loss: 0.0012 lr: 0.002\n",
      "iteration: 471580 loss: 0.0010 lr: 0.002\n",
      "iteration: 471590 loss: 0.0011 lr: 0.002\n",
      "iteration: 471600 loss: 0.0010 lr: 0.002\n",
      "iteration: 471610 loss: 0.0014 lr: 0.002\n",
      "iteration: 471620 loss: 0.0008 lr: 0.002\n",
      "iteration: 471630 loss: 0.0010 lr: 0.002\n",
      "iteration: 471640 loss: 0.0010 lr: 0.002\n",
      "iteration: 471650 loss: 0.0012 lr: 0.002\n",
      "iteration: 471660 loss: 0.0009 lr: 0.002\n",
      "iteration: 471670 loss: 0.0015 lr: 0.002\n",
      "iteration: 471680 loss: 0.0010 lr: 0.002\n",
      "iteration: 471690 loss: 0.0008 lr: 0.002\n",
      "iteration: 471700 loss: 0.0012 lr: 0.002\n",
      "iteration: 471710 loss: 0.0008 lr: 0.002\n",
      "iteration: 471720 loss: 0.0012 lr: 0.002\n",
      "iteration: 471730 loss: 0.0009 lr: 0.002\n",
      "iteration: 471740 loss: 0.0013 lr: 0.002\n",
      "iteration: 471750 loss: 0.0010 lr: 0.002\n",
      "iteration: 471760 loss: 0.0013 lr: 0.002\n",
      "iteration: 471770 loss: 0.0010 lr: 0.002\n",
      "iteration: 471780 loss: 0.0011 lr: 0.002\n",
      "iteration: 471790 loss: 0.0009 lr: 0.002\n",
      "iteration: 471800 loss: 0.0012 lr: 0.002\n",
      "iteration: 471810 loss: 0.0014 lr: 0.002\n",
      "iteration: 471820 loss: 0.0017 lr: 0.002\n",
      "iteration: 471830 loss: 0.0009 lr: 0.002\n",
      "iteration: 471840 loss: 0.0014 lr: 0.002\n",
      "iteration: 471850 loss: 0.0012 lr: 0.002\n",
      "iteration: 471860 loss: 0.0016 lr: 0.002\n",
      "iteration: 471870 loss: 0.0012 lr: 0.002\n",
      "iteration: 471880 loss: 0.0010 lr: 0.002\n",
      "iteration: 471890 loss: 0.0012 lr: 0.002\n",
      "iteration: 471900 loss: 0.0022 lr: 0.002\n",
      "iteration: 471910 loss: 0.0014 lr: 0.002\n",
      "iteration: 471920 loss: 0.0018 lr: 0.002\n",
      "iteration: 471930 loss: 0.0011 lr: 0.002\n",
      "iteration: 471940 loss: 0.0011 lr: 0.002\n",
      "iteration: 471950 loss: 0.0012 lr: 0.002\n",
      "iteration: 471960 loss: 0.0010 lr: 0.002\n",
      "iteration: 471970 loss: 0.0014 lr: 0.002\n",
      "iteration: 471980 loss: 0.0011 lr: 0.002\n",
      "iteration: 471990 loss: 0.0011 lr: 0.002\n",
      "iteration: 472000 loss: 0.0011 lr: 0.002\n",
      "iteration: 472010 loss: 0.0013 lr: 0.002\n",
      "iteration: 472020 loss: 0.0010 lr: 0.002\n",
      "iteration: 472030 loss: 0.0010 lr: 0.002\n",
      "iteration: 472040 loss: 0.0017 lr: 0.002\n",
      "iteration: 472050 loss: 0.0011 lr: 0.002\n",
      "iteration: 472060 loss: 0.0012 lr: 0.002\n",
      "iteration: 472070 loss: 0.0026 lr: 0.002\n",
      "iteration: 472080 loss: 0.0011 lr: 0.002\n",
      "iteration: 472090 loss: 0.0010 lr: 0.002\n",
      "iteration: 472100 loss: 0.0017 lr: 0.002\n",
      "iteration: 472110 loss: 0.0013 lr: 0.002\n",
      "iteration: 472120 loss: 0.0013 lr: 0.002\n",
      "iteration: 472130 loss: 0.0010 lr: 0.002\n",
      "iteration: 472140 loss: 0.0013 lr: 0.002\n",
      "iteration: 472150 loss: 0.0007 lr: 0.002\n",
      "iteration: 472160 loss: 0.0010 lr: 0.002\n",
      "iteration: 472170 loss: 0.0007 lr: 0.002\n",
      "iteration: 472180 loss: 0.0011 lr: 0.002\n",
      "iteration: 472190 loss: 0.0010 lr: 0.002\n",
      "iteration: 472200 loss: 0.0015 lr: 0.002\n",
      "iteration: 472210 loss: 0.0010 lr: 0.002\n",
      "iteration: 472220 loss: 0.0013 lr: 0.002\n",
      "iteration: 472230 loss: 0.0010 lr: 0.002\n",
      "iteration: 472240 loss: 0.0011 lr: 0.002\n",
      "iteration: 472250 loss: 0.0014 lr: 0.002\n",
      "iteration: 472260 loss: 0.0012 lr: 0.002\n",
      "iteration: 472270 loss: 0.0007 lr: 0.002\n",
      "iteration: 472280 loss: 0.0015 lr: 0.002\n",
      "iteration: 472290 loss: 0.0011 lr: 0.002\n",
      "iteration: 472300 loss: 0.0011 lr: 0.002\n",
      "iteration: 472310 loss: 0.0013 lr: 0.002\n",
      "iteration: 472320 loss: 0.0010 lr: 0.002\n",
      "iteration: 472330 loss: 0.0018 lr: 0.002\n",
      "iteration: 472340 loss: 0.0014 lr: 0.002\n",
      "iteration: 472350 loss: 0.0009 lr: 0.002\n",
      "iteration: 472360 loss: 0.0008 lr: 0.002\n",
      "iteration: 472370 loss: 0.0011 lr: 0.002\n",
      "iteration: 472380 loss: 0.0010 lr: 0.002\n",
      "iteration: 472390 loss: 0.0013 lr: 0.002\n",
      "iteration: 472400 loss: 0.0009 lr: 0.002\n",
      "iteration: 472410 loss: 0.0011 lr: 0.002\n",
      "iteration: 472420 loss: 0.0011 lr: 0.002\n",
      "iteration: 472430 loss: 0.0008 lr: 0.002\n",
      "iteration: 472440 loss: 0.0009 lr: 0.002\n",
      "iteration: 472450 loss: 0.0012 lr: 0.002\n",
      "iteration: 472460 loss: 0.0012 lr: 0.002\n",
      "iteration: 472470 loss: 0.0011 lr: 0.002\n",
      "iteration: 472480 loss: 0.0013 lr: 0.002\n",
      "iteration: 472490 loss: 0.0016 lr: 0.002\n",
      "iteration: 472500 loss: 0.0015 lr: 0.002\n",
      "iteration: 472510 loss: 0.0014 lr: 0.002\n",
      "iteration: 472520 loss: 0.0007 lr: 0.002\n",
      "iteration: 472530 loss: 0.0009 lr: 0.002\n",
      "iteration: 472540 loss: 0.0008 lr: 0.002\n",
      "iteration: 472550 loss: 0.0019 lr: 0.002\n",
      "iteration: 472560 loss: 0.0019 lr: 0.002\n",
      "iteration: 472570 loss: 0.0011 lr: 0.002\n",
      "iteration: 472580 loss: 0.0017 lr: 0.002\n",
      "iteration: 472590 loss: 0.0011 lr: 0.002\n",
      "iteration: 472600 loss: 0.0010 lr: 0.002\n",
      "iteration: 472610 loss: 0.0012 lr: 0.002\n",
      "iteration: 472620 loss: 0.0011 lr: 0.002\n",
      "iteration: 472630 loss: 0.0009 lr: 0.002\n",
      "iteration: 472640 loss: 0.0010 lr: 0.002\n",
      "iteration: 472650 loss: 0.0012 lr: 0.002\n",
      "iteration: 472660 loss: 0.0014 lr: 0.002\n",
      "iteration: 472670 loss: 0.0011 lr: 0.002\n",
      "iteration: 472680 loss: 0.0011 lr: 0.002\n",
      "iteration: 472690 loss: 0.0016 lr: 0.002\n",
      "iteration: 472700 loss: 0.0010 lr: 0.002\n",
      "iteration: 472710 loss: 0.0009 lr: 0.002\n",
      "iteration: 472720 loss: 0.0009 lr: 0.002\n",
      "iteration: 472730 loss: 0.0014 lr: 0.002\n",
      "iteration: 472740 loss: 0.0015 lr: 0.002\n",
      "iteration: 472750 loss: 0.0011 lr: 0.002\n",
      "iteration: 472760 loss: 0.0011 lr: 0.002\n",
      "iteration: 472770 loss: 0.0013 lr: 0.002\n",
      "iteration: 472780 loss: 0.0018 lr: 0.002\n",
      "iteration: 472790 loss: 0.0011 lr: 0.002\n",
      "iteration: 472800 loss: 0.0013 lr: 0.002\n",
      "iteration: 472810 loss: 0.0014 lr: 0.002\n",
      "iteration: 472820 loss: 0.0017 lr: 0.002\n",
      "iteration: 472830 loss: 0.0016 lr: 0.002\n",
      "iteration: 472840 loss: 0.0009 lr: 0.002\n",
      "iteration: 472850 loss: 0.0010 lr: 0.002\n",
      "iteration: 472860 loss: 0.0007 lr: 0.002\n",
      "iteration: 472870 loss: 0.0010 lr: 0.002\n",
      "iteration: 472880 loss: 0.0013 lr: 0.002\n",
      "iteration: 472890 loss: 0.0013 lr: 0.002\n",
      "iteration: 472900 loss: 0.0012 lr: 0.002\n",
      "iteration: 472910 loss: 0.0018 lr: 0.002\n",
      "iteration: 472920 loss: 0.0009 lr: 0.002\n",
      "iteration: 472930 loss: 0.0008 lr: 0.002\n",
      "iteration: 472940 loss: 0.0010 lr: 0.002\n",
      "iteration: 472950 loss: 0.0013 lr: 0.002\n",
      "iteration: 472960 loss: 0.0013 lr: 0.002\n",
      "iteration: 472970 loss: 0.0010 lr: 0.002\n",
      "iteration: 472980 loss: 0.0012 lr: 0.002\n",
      "iteration: 472990 loss: 0.0014 lr: 0.002\n",
      "iteration: 473000 loss: 0.0009 lr: 0.002\n",
      "iteration: 473010 loss: 0.0010 lr: 0.002\n",
      "iteration: 473020 loss: 0.0012 lr: 0.002\n",
      "iteration: 473030 loss: 0.0011 lr: 0.002\n",
      "iteration: 473040 loss: 0.0012 lr: 0.002\n",
      "iteration: 473050 loss: 0.0007 lr: 0.002\n",
      "iteration: 473060 loss: 0.0014 lr: 0.002\n",
      "iteration: 473070 loss: 0.0012 lr: 0.002\n",
      "iteration: 473080 loss: 0.0012 lr: 0.002\n",
      "iteration: 473090 loss: 0.0014 lr: 0.002\n",
      "iteration: 473100 loss: 0.0015 lr: 0.002\n",
      "iteration: 473110 loss: 0.0009 lr: 0.002\n",
      "iteration: 473120 loss: 0.0010 lr: 0.002\n",
      "iteration: 473130 loss: 0.0018 lr: 0.002\n",
      "iteration: 473140 loss: 0.0008 lr: 0.002\n",
      "iteration: 473150 loss: 0.0016 lr: 0.002\n",
      "iteration: 473160 loss: 0.0012 lr: 0.002\n",
      "iteration: 473170 loss: 0.0017 lr: 0.002\n",
      "iteration: 473180 loss: 0.0008 lr: 0.002\n",
      "iteration: 473190 loss: 0.0010 lr: 0.002\n",
      "iteration: 473200 loss: 0.0012 lr: 0.002\n",
      "iteration: 473210 loss: 0.0010 lr: 0.002\n",
      "iteration: 473220 loss: 0.0009 lr: 0.002\n",
      "iteration: 473230 loss: 0.0010 lr: 0.002\n",
      "iteration: 473240 loss: 0.0012 lr: 0.002\n",
      "iteration: 473250 loss: 0.0015 lr: 0.002\n",
      "iteration: 473260 loss: 0.0013 lr: 0.002\n",
      "iteration: 473270 loss: 0.0014 lr: 0.002\n",
      "iteration: 473280 loss: 0.0007 lr: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 473290 loss: 0.0010 lr: 0.002\n",
      "iteration: 473300 loss: 0.0011 lr: 0.002\n",
      "iteration: 473310 loss: 0.0010 lr: 0.002\n",
      "iteration: 473320 loss: 0.0016 lr: 0.002\n",
      "iteration: 473330 loss: 0.0014 lr: 0.002\n",
      "iteration: 473340 loss: 0.0016 lr: 0.002\n",
      "iteration: 473350 loss: 0.0013 lr: 0.002\n",
      "iteration: 473360 loss: 0.0013 lr: 0.002\n",
      "iteration: 473370 loss: 0.0009 lr: 0.002\n",
      "iteration: 473380 loss: 0.0015 lr: 0.002\n",
      "iteration: 473390 loss: 0.0011 lr: 0.002\n",
      "iteration: 473400 loss: 0.0012 lr: 0.002\n",
      "iteration: 473410 loss: 0.0012 lr: 0.002\n",
      "iteration: 473420 loss: 0.0010 lr: 0.002\n",
      "iteration: 473430 loss: 0.0021 lr: 0.002\n",
      "iteration: 473440 loss: 0.0012 lr: 0.002\n",
      "iteration: 473450 loss: 0.0019 lr: 0.002\n",
      "iteration: 473460 loss: 0.0011 lr: 0.002\n",
      "iteration: 473470 loss: 0.0016 lr: 0.002\n",
      "iteration: 473480 loss: 0.0010 lr: 0.002\n",
      "iteration: 473490 loss: 0.0020 lr: 0.002\n",
      "iteration: 473500 loss: 0.0007 lr: 0.002\n",
      "iteration: 473510 loss: 0.0011 lr: 0.002\n",
      "iteration: 473520 loss: 0.0015 lr: 0.002\n",
      "iteration: 473530 loss: 0.0014 lr: 0.002\n",
      "iteration: 473540 loss: 0.0012 lr: 0.002\n",
      "iteration: 473550 loss: 0.0012 lr: 0.002\n",
      "iteration: 473560 loss: 0.0010 lr: 0.002\n",
      "iteration: 473570 loss: 0.0007 lr: 0.002\n",
      "iteration: 473580 loss: 0.0012 lr: 0.002\n",
      "iteration: 473590 loss: 0.0010 lr: 0.002\n",
      "iteration: 473600 loss: 0.0010 lr: 0.002\n",
      "iteration: 473610 loss: 0.0013 lr: 0.002\n",
      "iteration: 473620 loss: 0.0014 lr: 0.002\n",
      "iteration: 473630 loss: 0.0008 lr: 0.002\n",
      "iteration: 473640 loss: 0.0009 lr: 0.002\n",
      "iteration: 473650 loss: 0.0016 lr: 0.002\n",
      "iteration: 473660 loss: 0.0017 lr: 0.002\n",
      "iteration: 473670 loss: 0.0009 lr: 0.002\n",
      "iteration: 473680 loss: 0.0016 lr: 0.002\n",
      "iteration: 473690 loss: 0.0007 lr: 0.002\n",
      "iteration: 473700 loss: 0.0013 lr: 0.002\n",
      "iteration: 473710 loss: 0.0009 lr: 0.002\n",
      "iteration: 473720 loss: 0.0012 lr: 0.002\n",
      "iteration: 473730 loss: 0.0015 lr: 0.002\n",
      "iteration: 473740 loss: 0.0011 lr: 0.002\n",
      "iteration: 473750 loss: 0.0010 lr: 0.002\n",
      "iteration: 473760 loss: 0.0007 lr: 0.002\n",
      "iteration: 473770 loss: 0.0017 lr: 0.002\n",
      "iteration: 473780 loss: 0.0017 lr: 0.002\n",
      "iteration: 473790 loss: 0.0015 lr: 0.002\n",
      "iteration: 473800 loss: 0.0013 lr: 0.002\n",
      "iteration: 473810 loss: 0.0016 lr: 0.002\n",
      "iteration: 473820 loss: 0.0011 lr: 0.002\n",
      "iteration: 473830 loss: 0.0012 lr: 0.002\n",
      "iteration: 473840 loss: 0.0012 lr: 0.002\n",
      "iteration: 473850 loss: 0.0008 lr: 0.002\n",
      "iteration: 473860 loss: 0.0013 lr: 0.002\n",
      "iteration: 473870 loss: 0.0010 lr: 0.002\n",
      "iteration: 473880 loss: 0.0011 lr: 0.002\n",
      "iteration: 473890 loss: 0.0009 lr: 0.002\n",
      "iteration: 473900 loss: 0.0012 lr: 0.002\n",
      "iteration: 473910 loss: 0.0010 lr: 0.002\n",
      "iteration: 473920 loss: 0.0010 lr: 0.002\n",
      "iteration: 473930 loss: 0.0009 lr: 0.002\n",
      "iteration: 473940 loss: 0.0011 lr: 0.002\n",
      "iteration: 473950 loss: 0.0010 lr: 0.002\n",
      "iteration: 473960 loss: 0.0016 lr: 0.002\n",
      "iteration: 473970 loss: 0.0010 lr: 0.002\n",
      "iteration: 473980 loss: 0.0014 lr: 0.002\n",
      "iteration: 473990 loss: 0.0011 lr: 0.002\n",
      "iteration: 474000 loss: 0.0011 lr: 0.002\n",
      "iteration: 474010 loss: 0.0013 lr: 0.002\n",
      "iteration: 474020 loss: 0.0014 lr: 0.002\n",
      "iteration: 474030 loss: 0.0008 lr: 0.002\n",
      "iteration: 474040 loss: 0.0012 lr: 0.002\n",
      "iteration: 474050 loss: 0.0015 lr: 0.002\n",
      "iteration: 474060 loss: 0.0011 lr: 0.002\n",
      "iteration: 474070 loss: 0.0011 lr: 0.002\n",
      "iteration: 474080 loss: 0.0013 lr: 0.002\n",
      "iteration: 474090 loss: 0.0011 lr: 0.002\n",
      "iteration: 474100 loss: 0.0016 lr: 0.002\n",
      "iteration: 474110 loss: 0.0010 lr: 0.002\n",
      "iteration: 474120 loss: 0.0008 lr: 0.002\n",
      "iteration: 474130 loss: 0.0010 lr: 0.002\n",
      "iteration: 474140 loss: 0.0016 lr: 0.002\n",
      "iteration: 474150 loss: 0.0011 lr: 0.002\n",
      "iteration: 474160 loss: 0.0011 lr: 0.002\n",
      "iteration: 474170 loss: 0.0008 lr: 0.002\n",
      "iteration: 474180 loss: 0.0011 lr: 0.002\n",
      "iteration: 474190 loss: 0.0009 lr: 0.002\n",
      "iteration: 474200 loss: 0.0008 lr: 0.002\n",
      "iteration: 474210 loss: 0.0010 lr: 0.002\n",
      "iteration: 474220 loss: 0.0017 lr: 0.002\n",
      "iteration: 474230 loss: 0.0011 lr: 0.002\n",
      "iteration: 474240 loss: 0.0015 lr: 0.002\n",
      "iteration: 474250 loss: 0.0013 lr: 0.002\n",
      "iteration: 474260 loss: 0.0010 lr: 0.002\n",
      "iteration: 474270 loss: 0.0008 lr: 0.002\n",
      "iteration: 474280 loss: 0.0012 lr: 0.002\n",
      "iteration: 474290 loss: 0.0014 lr: 0.002\n",
      "iteration: 474300 loss: 0.0010 lr: 0.002\n",
      "iteration: 474310 loss: 0.0007 lr: 0.002\n",
      "iteration: 474320 loss: 0.0009 lr: 0.002\n",
      "iteration: 474330 loss: 0.0015 lr: 0.002\n",
      "iteration: 474340 loss: 0.0012 lr: 0.002\n",
      "iteration: 474350 loss: 0.0016 lr: 0.002\n",
      "iteration: 474360 loss: 0.0019 lr: 0.002\n",
      "iteration: 474370 loss: 0.0013 lr: 0.002\n",
      "iteration: 474380 loss: 0.0010 lr: 0.002\n",
      "iteration: 474390 loss: 0.0015 lr: 0.002\n",
      "iteration: 474400 loss: 0.0013 lr: 0.002\n",
      "iteration: 474410 loss: 0.0016 lr: 0.002\n",
      "iteration: 474420 loss: 0.0010 lr: 0.002\n",
      "iteration: 474430 loss: 0.0014 lr: 0.002\n",
      "iteration: 474440 loss: 0.0013 lr: 0.002\n",
      "iteration: 474450 loss: 0.0011 lr: 0.002\n",
      "iteration: 474460 loss: 0.0013 lr: 0.002\n",
      "iteration: 474470 loss: 0.0010 lr: 0.002\n",
      "iteration: 474480 loss: 0.0014 lr: 0.002\n",
      "iteration: 474490 loss: 0.0012 lr: 0.002\n",
      "iteration: 474500 loss: 0.0010 lr: 0.002\n",
      "iteration: 474510 loss: 0.0014 lr: 0.002\n",
      "iteration: 474520 loss: 0.0015 lr: 0.002\n",
      "iteration: 474530 loss: 0.0009 lr: 0.002\n",
      "iteration: 474540 loss: 0.0031 lr: 0.002\n",
      "iteration: 474550 loss: 0.0014 lr: 0.002\n",
      "iteration: 474560 loss: 0.0011 lr: 0.002\n",
      "iteration: 474570 loss: 0.0012 lr: 0.002\n",
      "iteration: 474580 loss: 0.0014 lr: 0.002\n",
      "iteration: 474590 loss: 0.0013 lr: 0.002\n",
      "iteration: 474600 loss: 0.0009 lr: 0.002\n",
      "iteration: 474610 loss: 0.0009 lr: 0.002\n",
      "iteration: 474620 loss: 0.0010 lr: 0.002\n",
      "iteration: 474630 loss: 0.0017 lr: 0.002\n",
      "iteration: 474640 loss: 0.0009 lr: 0.002\n",
      "iteration: 474650 loss: 0.0010 lr: 0.002\n",
      "iteration: 474660 loss: 0.0012 lr: 0.002\n",
      "iteration: 474670 loss: 0.0012 lr: 0.002\n",
      "iteration: 474680 loss: 0.0015 lr: 0.002\n",
      "iteration: 474690 loss: 0.0010 lr: 0.002\n",
      "iteration: 474700 loss: 0.0012 lr: 0.002\n",
      "iteration: 474710 loss: 0.0015 lr: 0.002\n",
      "iteration: 474720 loss: 0.0010 lr: 0.002\n",
      "iteration: 474730 loss: 0.0010 lr: 0.002\n",
      "iteration: 474740 loss: 0.0008 lr: 0.002\n",
      "iteration: 474750 loss: 0.0010 lr: 0.002\n",
      "iteration: 474760 loss: 0.0012 lr: 0.002\n",
      "iteration: 474770 loss: 0.0013 lr: 0.002\n",
      "iteration: 474780 loss: 0.0015 lr: 0.002\n",
      "iteration: 474790 loss: 0.0008 lr: 0.002\n",
      "iteration: 474800 loss: 0.0015 lr: 0.002\n",
      "iteration: 474810 loss: 0.0012 lr: 0.002\n",
      "iteration: 474820 loss: 0.0009 lr: 0.002\n",
      "iteration: 474830 loss: 0.0012 lr: 0.002\n",
      "iteration: 474840 loss: 0.0009 lr: 0.002\n",
      "iteration: 474850 loss: 0.0009 lr: 0.002\n",
      "iteration: 474860 loss: 0.0015 lr: 0.002\n",
      "iteration: 474870 loss: 0.0009 lr: 0.002\n",
      "iteration: 474880 loss: 0.0008 lr: 0.002\n",
      "iteration: 474890 loss: 0.0015 lr: 0.002\n",
      "iteration: 474900 loss: 0.0018 lr: 0.002\n",
      "iteration: 474910 loss: 0.0011 lr: 0.002\n",
      "iteration: 474920 loss: 0.0011 lr: 0.002\n",
      "iteration: 474930 loss: 0.0013 lr: 0.002\n",
      "iteration: 474940 loss: 0.0012 lr: 0.002\n",
      "iteration: 474950 loss: 0.0008 lr: 0.002\n",
      "iteration: 474960 loss: 0.0010 lr: 0.002\n",
      "iteration: 474970 loss: 0.0009 lr: 0.002\n",
      "iteration: 474980 loss: 0.0010 lr: 0.002\n",
      "iteration: 474990 loss: 0.0012 lr: 0.002\n",
      "iteration: 475000 loss: 0.0011 lr: 0.002\n",
      "iteration: 475010 loss: 0.0015 lr: 0.002\n",
      "iteration: 475020 loss: 0.0012 lr: 0.002\n",
      "iteration: 475030 loss: 0.0015 lr: 0.002\n",
      "iteration: 475040 loss: 0.0007 lr: 0.002\n",
      "iteration: 475050 loss: 0.0013 lr: 0.002\n",
      "iteration: 475060 loss: 0.0008 lr: 0.002\n",
      "iteration: 475070 loss: 0.0009 lr: 0.002\n",
      "iteration: 475080 loss: 0.0011 lr: 0.002\n",
      "iteration: 475090 loss: 0.0010 lr: 0.002\n",
      "iteration: 475100 loss: 0.0010 lr: 0.002\n",
      "iteration: 475110 loss: 0.0017 lr: 0.002\n",
      "iteration: 475120 loss: 0.0011 lr: 0.002\n",
      "iteration: 475130 loss: 0.0009 lr: 0.002\n",
      "iteration: 475140 loss: 0.0007 lr: 0.002\n",
      "iteration: 475150 loss: 0.0017 lr: 0.002\n",
      "iteration: 475160 loss: 0.0016 lr: 0.002\n",
      "iteration: 475170 loss: 0.0015 lr: 0.002\n",
      "iteration: 475180 loss: 0.0013 lr: 0.002\n",
      "iteration: 475190 loss: 0.0012 lr: 0.002\n",
      "iteration: 475200 loss: 0.0007 lr: 0.002\n",
      "iteration: 475210 loss: 0.0008 lr: 0.002\n",
      "iteration: 475220 loss: 0.0014 lr: 0.002\n",
      "iteration: 475230 loss: 0.0014 lr: 0.002\n",
      "iteration: 475240 loss: 0.0008 lr: 0.002\n",
      "iteration: 475250 loss: 0.0012 lr: 0.002\n",
      "iteration: 475260 loss: 0.0011 lr: 0.002\n",
      "iteration: 475270 loss: 0.0013 lr: 0.002\n",
      "iteration: 475280 loss: 0.0011 lr: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 475290 loss: 0.0011 lr: 0.002\n",
      "iteration: 475300 loss: 0.0017 lr: 0.002\n",
      "iteration: 475310 loss: 0.0013 lr: 0.002\n",
      "iteration: 475320 loss: 0.0011 lr: 0.002\n",
      "iteration: 475330 loss: 0.0017 lr: 0.002\n",
      "iteration: 475340 loss: 0.0018 lr: 0.002\n",
      "iteration: 475350 loss: 0.0011 lr: 0.002\n",
      "iteration: 475360 loss: 0.0013 lr: 0.002\n",
      "iteration: 475370 loss: 0.0021 lr: 0.002\n",
      "iteration: 475380 loss: 0.0012 lr: 0.002\n",
      "iteration: 475390 loss: 0.0011 lr: 0.002\n",
      "iteration: 475400 loss: 0.0017 lr: 0.002\n",
      "iteration: 475410 loss: 0.0013 lr: 0.002\n",
      "iteration: 475420 loss: 0.0013 lr: 0.002\n",
      "iteration: 475430 loss: 0.0012 lr: 0.002\n",
      "iteration: 475440 loss: 0.0011 lr: 0.002\n",
      "iteration: 475450 loss: 0.0013 lr: 0.002\n",
      "iteration: 475460 loss: 0.0010 lr: 0.002\n",
      "iteration: 475470 loss: 0.0012 lr: 0.002\n",
      "iteration: 475480 loss: 0.0013 lr: 0.002\n",
      "iteration: 475490 loss: 0.0010 lr: 0.002\n",
      "iteration: 475500 loss: 0.0010 lr: 0.002\n",
      "iteration: 475510 loss: 0.0009 lr: 0.002\n",
      "iteration: 475520 loss: 0.0010 lr: 0.002\n",
      "iteration: 475530 loss: 0.0013 lr: 0.002\n",
      "iteration: 475540 loss: 0.0010 lr: 0.002\n",
      "iteration: 475550 loss: 0.0012 lr: 0.002\n",
      "iteration: 475560 loss: 0.0012 lr: 0.002\n",
      "iteration: 475570 loss: 0.0019 lr: 0.002\n",
      "iteration: 475580 loss: 0.0007 lr: 0.002\n",
      "iteration: 475590 loss: 0.0011 lr: 0.002\n",
      "iteration: 475600 loss: 0.0011 lr: 0.002\n",
      "iteration: 475610 loss: 0.0010 lr: 0.002\n",
      "iteration: 475620 loss: 0.0012 lr: 0.002\n",
      "iteration: 475630 loss: 0.0010 lr: 0.002\n",
      "iteration: 475640 loss: 0.0011 lr: 0.002\n",
      "iteration: 475650 loss: 0.0014 lr: 0.002\n",
      "iteration: 475660 loss: 0.0013 lr: 0.002\n",
      "iteration: 475670 loss: 0.0012 lr: 0.002\n",
      "iteration: 475680 loss: 0.0010 lr: 0.002\n",
      "iteration: 475690 loss: 0.0009 lr: 0.002\n",
      "iteration: 475700 loss: 0.0009 lr: 0.002\n",
      "iteration: 475710 loss: 0.0014 lr: 0.002\n",
      "iteration: 475720 loss: 0.0012 lr: 0.002\n",
      "iteration: 475730 loss: 0.0011 lr: 0.002\n",
      "iteration: 475740 loss: 0.0020 lr: 0.002\n",
      "iteration: 475750 loss: 0.0009 lr: 0.002\n",
      "iteration: 475760 loss: 0.0011 lr: 0.002\n",
      "iteration: 475770 loss: 0.0020 lr: 0.002\n",
      "iteration: 475780 loss: 0.0007 lr: 0.002\n",
      "iteration: 475790 loss: 0.0017 lr: 0.002\n",
      "iteration: 475800 loss: 0.0014 lr: 0.002\n",
      "iteration: 475810 loss: 0.0012 lr: 0.002\n",
      "iteration: 475820 loss: 0.0009 lr: 0.002\n",
      "iteration: 475830 loss: 0.0013 lr: 0.002\n",
      "iteration: 475840 loss: 0.0013 lr: 0.002\n",
      "iteration: 475850 loss: 0.0010 lr: 0.002\n",
      "iteration: 475860 loss: 0.0007 lr: 0.002\n",
      "iteration: 475870 loss: 0.0012 lr: 0.002\n",
      "iteration: 475880 loss: 0.0012 lr: 0.002\n",
      "iteration: 475890 loss: 0.0016 lr: 0.002\n",
      "iteration: 475900 loss: 0.0010 lr: 0.002\n",
      "iteration: 475910 loss: 0.0009 lr: 0.002\n",
      "iteration: 475920 loss: 0.0014 lr: 0.002\n",
      "iteration: 475930 loss: 0.0015 lr: 0.002\n",
      "iteration: 475940 loss: 0.0011 lr: 0.002\n",
      "iteration: 475950 loss: 0.0013 lr: 0.002\n",
      "iteration: 475960 loss: 0.0008 lr: 0.002\n",
      "iteration: 475970 loss: 0.0013 lr: 0.002\n",
      "iteration: 475980 loss: 0.0016 lr: 0.002\n",
      "iteration: 475990 loss: 0.0019 lr: 0.002\n",
      "iteration: 476000 loss: 0.0009 lr: 0.002\n",
      "iteration: 476010 loss: 0.0009 lr: 0.002\n",
      "iteration: 476020 loss: 0.0017 lr: 0.002\n",
      "iteration: 476030 loss: 0.0010 lr: 0.002\n",
      "iteration: 476040 loss: 0.0010 lr: 0.002\n",
      "iteration: 476050 loss: 0.0013 lr: 0.002\n",
      "iteration: 476060 loss: 0.0011 lr: 0.002\n",
      "iteration: 476070 loss: 0.0010 lr: 0.002\n",
      "iteration: 476080 loss: 0.0017 lr: 0.002\n",
      "iteration: 476090 loss: 0.0014 lr: 0.002\n",
      "iteration: 476100 loss: 0.0011 lr: 0.002\n",
      "iteration: 476110 loss: 0.0013 lr: 0.002\n",
      "iteration: 476120 loss: 0.0011 lr: 0.002\n",
      "iteration: 476130 loss: 0.0010 lr: 0.002\n",
      "iteration: 476140 loss: 0.0012 lr: 0.002\n",
      "iteration: 476150 loss: 0.0011 lr: 0.002\n",
      "iteration: 476160 loss: 0.0009 lr: 0.002\n",
      "iteration: 476170 loss: 0.0015 lr: 0.002\n",
      "iteration: 476180 loss: 0.0010 lr: 0.002\n",
      "iteration: 476190 loss: 0.0012 lr: 0.002\n",
      "iteration: 476200 loss: 0.0010 lr: 0.002\n",
      "iteration: 476210 loss: 0.0010 lr: 0.002\n",
      "iteration: 476220 loss: 0.0010 lr: 0.002\n",
      "iteration: 476230 loss: 0.0014 lr: 0.002\n",
      "iteration: 476240 loss: 0.0008 lr: 0.002\n",
      "iteration: 476250 loss: 0.0012 lr: 0.002\n",
      "iteration: 476260 loss: 0.0016 lr: 0.002\n",
      "iteration: 476270 loss: 0.0011 lr: 0.002\n",
      "iteration: 476280 loss: 0.0010 lr: 0.002\n",
      "iteration: 476290 loss: 0.0012 lr: 0.002\n",
      "iteration: 476300 loss: 0.0015 lr: 0.002\n",
      "iteration: 476310 loss: 0.0010 lr: 0.002\n",
      "iteration: 476320 loss: 0.0011 lr: 0.002\n",
      "iteration: 476330 loss: 0.0008 lr: 0.002\n",
      "iteration: 476340 loss: 0.0013 lr: 0.002\n",
      "iteration: 476350 loss: 0.0009 lr: 0.002\n",
      "iteration: 476360 loss: 0.0008 lr: 0.002\n",
      "iteration: 476370 loss: 0.0013 lr: 0.002\n",
      "iteration: 476380 loss: 0.0019 lr: 0.002\n",
      "iteration: 476390 loss: 0.0016 lr: 0.002\n",
      "iteration: 476400 loss: 0.0011 lr: 0.002\n",
      "iteration: 476410 loss: 0.0011 lr: 0.002\n",
      "iteration: 476420 loss: 0.0008 lr: 0.002\n",
      "iteration: 476430 loss: 0.0009 lr: 0.002\n",
      "iteration: 476440 loss: 0.0007 lr: 0.002\n",
      "iteration: 476450 loss: 0.0008 lr: 0.002\n",
      "iteration: 476460 loss: 0.0011 lr: 0.002\n",
      "iteration: 476470 loss: 0.0010 lr: 0.002\n",
      "iteration: 476480 loss: 0.0009 lr: 0.002\n",
      "iteration: 476490 loss: 0.0012 lr: 0.002\n",
      "iteration: 476500 loss: 0.0012 lr: 0.002\n",
      "iteration: 476510 loss: 0.0015 lr: 0.002\n",
      "iteration: 476520 loss: 0.0018 lr: 0.002\n",
      "iteration: 476530 loss: 0.0010 lr: 0.002\n",
      "iteration: 476540 loss: 0.0009 lr: 0.002\n",
      "iteration: 476550 loss: 0.0009 lr: 0.002\n",
      "iteration: 476560 loss: 0.0012 lr: 0.002\n",
      "iteration: 476570 loss: 0.0011 lr: 0.002\n",
      "iteration: 476580 loss: 0.0011 lr: 0.002\n",
      "iteration: 476590 loss: 0.0011 lr: 0.002\n",
      "iteration: 476600 loss: 0.0008 lr: 0.002\n",
      "iteration: 476610 loss: 0.0014 lr: 0.002\n",
      "iteration: 476620 loss: 0.0019 lr: 0.002\n",
      "iteration: 476630 loss: 0.0006 lr: 0.002\n",
      "iteration: 476640 loss: 0.0013 lr: 0.002\n",
      "iteration: 476650 loss: 0.0007 lr: 0.002\n",
      "iteration: 476660 loss: 0.0013 lr: 0.002\n",
      "iteration: 476670 loss: 0.0011 lr: 0.002\n",
      "iteration: 476680 loss: 0.0012 lr: 0.002\n",
      "iteration: 476690 loss: 0.0023 lr: 0.002\n",
      "iteration: 476700 loss: 0.0009 lr: 0.002\n",
      "iteration: 476710 loss: 0.0013 lr: 0.002\n",
      "iteration: 476720 loss: 0.0006 lr: 0.002\n",
      "iteration: 476730 loss: 0.0010 lr: 0.002\n",
      "iteration: 476740 loss: 0.0011 lr: 0.002\n",
      "iteration: 476750 loss: 0.0018 lr: 0.002\n",
      "iteration: 476760 loss: 0.0011 lr: 0.002\n",
      "iteration: 476770 loss: 0.0009 lr: 0.002\n",
      "iteration: 476780 loss: 0.0009 lr: 0.002\n",
      "iteration: 476790 loss: 0.0012 lr: 0.002\n",
      "iteration: 476800 loss: 0.0010 lr: 0.002\n",
      "iteration: 476810 loss: 0.0014 lr: 0.002\n",
      "iteration: 476820 loss: 0.0013 lr: 0.002\n",
      "iteration: 476830 loss: 0.0010 lr: 0.002\n",
      "iteration: 476840 loss: 0.0009 lr: 0.002\n",
      "iteration: 476850 loss: 0.0010 lr: 0.002\n",
      "iteration: 476860 loss: 0.0015 lr: 0.002\n",
      "iteration: 476870 loss: 0.0009 lr: 0.002\n",
      "iteration: 476880 loss: 0.0011 lr: 0.002\n",
      "iteration: 476890 loss: 0.0016 lr: 0.002\n",
      "iteration: 476900 loss: 0.0019 lr: 0.002\n",
      "iteration: 476910 loss: 0.0008 lr: 0.002\n",
      "iteration: 476920 loss: 0.0015 lr: 0.002\n",
      "iteration: 476930 loss: 0.0013 lr: 0.002\n",
      "iteration: 476940 loss: 0.0010 lr: 0.002\n",
      "iteration: 476950 loss: 0.0011 lr: 0.002\n",
      "iteration: 476960 loss: 0.0012 lr: 0.002\n",
      "iteration: 476970 loss: 0.0009 lr: 0.002\n",
      "iteration: 476980 loss: 0.0017 lr: 0.002\n",
      "iteration: 476990 loss: 0.0010 lr: 0.002\n",
      "iteration: 477000 loss: 0.0009 lr: 0.002\n",
      "iteration: 477010 loss: 0.0012 lr: 0.002\n",
      "iteration: 477020 loss: 0.0011 lr: 0.002\n",
      "iteration: 477030 loss: 0.0012 lr: 0.002\n",
      "iteration: 477040 loss: 0.0016 lr: 0.002\n",
      "iteration: 477050 loss: 0.0015 lr: 0.002\n",
      "iteration: 477060 loss: 0.0012 lr: 0.002\n",
      "iteration: 477070 loss: 0.0010 lr: 0.002\n",
      "iteration: 477080 loss: 0.0012 lr: 0.002\n",
      "iteration: 477090 loss: 0.0014 lr: 0.002\n",
      "iteration: 477100 loss: 0.0013 lr: 0.002\n",
      "iteration: 477110 loss: 0.0024 lr: 0.002\n",
      "iteration: 477120 loss: 0.0012 lr: 0.002\n",
      "iteration: 477130 loss: 0.0013 lr: 0.002\n",
      "iteration: 477140 loss: 0.0015 lr: 0.002\n",
      "iteration: 477150 loss: 0.0013 lr: 0.002\n",
      "iteration: 477160 loss: 0.0021 lr: 0.002\n",
      "iteration: 477170 loss: 0.0013 lr: 0.002\n",
      "iteration: 477180 loss: 0.0016 lr: 0.002\n",
      "iteration: 477190 loss: 0.0013 lr: 0.002\n",
      "iteration: 477200 loss: 0.0010 lr: 0.002\n",
      "iteration: 477210 loss: 0.0011 lr: 0.002\n",
      "iteration: 477220 loss: 0.0012 lr: 0.002\n",
      "iteration: 477230 loss: 0.0016 lr: 0.002\n",
      "iteration: 477240 loss: 0.0013 lr: 0.002\n",
      "iteration: 477250 loss: 0.0009 lr: 0.002\n",
      "iteration: 477260 loss: 0.0014 lr: 0.002\n",
      "iteration: 477270 loss: 0.0015 lr: 0.002\n",
      "iteration: 477280 loss: 0.0008 lr: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 477290 loss: 0.0008 lr: 0.002\n",
      "iteration: 477300 loss: 0.0015 lr: 0.002\n",
      "iteration: 477310 loss: 0.0009 lr: 0.002\n",
      "iteration: 477320 loss: 0.0011 lr: 0.002\n",
      "iteration: 477330 loss: 0.0010 lr: 0.002\n",
      "iteration: 477340 loss: 0.0011 lr: 0.002\n",
      "iteration: 477350 loss: 0.0013 lr: 0.002\n",
      "iteration: 477360 loss: 0.0010 lr: 0.002\n",
      "iteration: 477370 loss: 0.0012 lr: 0.002\n",
      "iteration: 477380 loss: 0.0010 lr: 0.002\n",
      "iteration: 477390 loss: 0.0012 lr: 0.002\n",
      "iteration: 477400 loss: 0.0017 lr: 0.002\n",
      "iteration: 477410 loss: 0.0012 lr: 0.002\n",
      "iteration: 477420 loss: 0.0019 lr: 0.002\n",
      "iteration: 477430 loss: 0.0008 lr: 0.002\n",
      "iteration: 477440 loss: 0.0013 lr: 0.002\n",
      "iteration: 477450 loss: 0.0008 lr: 0.002\n",
      "iteration: 477460 loss: 0.0015 lr: 0.002\n",
      "iteration: 477470 loss: 0.0011 lr: 0.002\n",
      "iteration: 477480 loss: 0.0007 lr: 0.002\n",
      "iteration: 477490 loss: 0.0008 lr: 0.002\n",
      "iteration: 477500 loss: 0.0011 lr: 0.002\n",
      "iteration: 477510 loss: 0.0010 lr: 0.002\n",
      "iteration: 477520 loss: 0.0012 lr: 0.002\n",
      "iteration: 477530 loss: 0.0007 lr: 0.002\n",
      "iteration: 477540 loss: 0.0009 lr: 0.002\n",
      "iteration: 477550 loss: 0.0013 lr: 0.002\n",
      "iteration: 477560 loss: 0.0016 lr: 0.002\n",
      "iteration: 477570 loss: 0.0013 lr: 0.002\n",
      "iteration: 477580 loss: 0.0008 lr: 0.002\n",
      "iteration: 477590 loss: 0.0009 lr: 0.002\n",
      "iteration: 477600 loss: 0.0010 lr: 0.002\n",
      "iteration: 477610 loss: 0.0008 lr: 0.002\n",
      "iteration: 477620 loss: 0.0011 lr: 0.002\n",
      "iteration: 477630 loss: 0.0019 lr: 0.002\n",
      "iteration: 477640 loss: 0.0016 lr: 0.002\n",
      "iteration: 477650 loss: 0.0007 lr: 0.002\n",
      "iteration: 477660 loss: 0.0010 lr: 0.002\n",
      "iteration: 477670 loss: 0.0010 lr: 0.002\n",
      "iteration: 477680 loss: 0.0018 lr: 0.002\n",
      "iteration: 477690 loss: 0.0010 lr: 0.002\n",
      "iteration: 477700 loss: 0.0012 lr: 0.002\n",
      "iteration: 477710 loss: 0.0012 lr: 0.002\n",
      "iteration: 477720 loss: 0.0012 lr: 0.002\n",
      "iteration: 477730 loss: 0.0017 lr: 0.002\n",
      "iteration: 477740 loss: 0.0010 lr: 0.002\n",
      "iteration: 477750 loss: 0.0012 lr: 0.002\n",
      "iteration: 477760 loss: 0.0012 lr: 0.002\n",
      "iteration: 477770 loss: 0.0014 lr: 0.002\n",
      "iteration: 477780 loss: 0.0010 lr: 0.002\n",
      "iteration: 477790 loss: 0.0011 lr: 0.002\n",
      "iteration: 477800 loss: 0.0013 lr: 0.002\n",
      "iteration: 477810 loss: 0.0009 lr: 0.002\n",
      "iteration: 477820 loss: 0.0014 lr: 0.002\n",
      "iteration: 477830 loss: 0.0012 lr: 0.002\n",
      "iteration: 477840 loss: 0.0011 lr: 0.002\n",
      "iteration: 477850 loss: 0.0009 lr: 0.002\n",
      "iteration: 477860 loss: 0.0010 lr: 0.002\n",
      "iteration: 477870 loss: 0.0014 lr: 0.002\n",
      "iteration: 477880 loss: 0.0008 lr: 0.002\n",
      "iteration: 477890 loss: 0.0016 lr: 0.002\n",
      "iteration: 477900 loss: 0.0012 lr: 0.002\n",
      "iteration: 477910 loss: 0.0012 lr: 0.002\n",
      "iteration: 477920 loss: 0.0012 lr: 0.002\n",
      "iteration: 477930 loss: 0.0013 lr: 0.002\n",
      "iteration: 477940 loss: 0.0010 lr: 0.002\n",
      "iteration: 477950 loss: 0.0012 lr: 0.002\n",
      "iteration: 477960 loss: 0.0013 lr: 0.002\n",
      "iteration: 477970 loss: 0.0008 lr: 0.002\n",
      "iteration: 477980 loss: 0.0012 lr: 0.002\n",
      "iteration: 477990 loss: 0.0010 lr: 0.002\n",
      "iteration: 478000 loss: 0.0013 lr: 0.002\n",
      "iteration: 478010 loss: 0.0012 lr: 0.002\n",
      "iteration: 478020 loss: 0.0011 lr: 0.002\n",
      "iteration: 478030 loss: 0.0011 lr: 0.002\n",
      "iteration: 478040 loss: 0.0011 lr: 0.002\n",
      "iteration: 478050 loss: 0.0018 lr: 0.002\n",
      "iteration: 478060 loss: 0.0014 lr: 0.002\n",
      "iteration: 478070 loss: 0.0011 lr: 0.002\n",
      "iteration: 478080 loss: 0.0010 lr: 0.002\n",
      "iteration: 478090 loss: 0.0011 lr: 0.002\n",
      "iteration: 478100 loss: 0.0015 lr: 0.002\n",
      "iteration: 478110 loss: 0.0019 lr: 0.002\n",
      "iteration: 478120 loss: 0.0011 lr: 0.002\n",
      "iteration: 478130 loss: 0.0013 lr: 0.002\n",
      "iteration: 478140 loss: 0.0018 lr: 0.002\n",
      "iteration: 478150 loss: 0.0008 lr: 0.002\n",
      "iteration: 478160 loss: 0.0010 lr: 0.002\n",
      "iteration: 478170 loss: 0.0010 lr: 0.002\n",
      "iteration: 478180 loss: 0.0008 lr: 0.002\n",
      "iteration: 478190 loss: 0.0014 lr: 0.002\n",
      "iteration: 478200 loss: 0.0020 lr: 0.002\n",
      "iteration: 478210 loss: 0.0013 lr: 0.002\n",
      "iteration: 478220 loss: 0.0012 lr: 0.002\n",
      "iteration: 478230 loss: 0.0013 lr: 0.002\n",
      "iteration: 478240 loss: 0.0009 lr: 0.002\n",
      "iteration: 478250 loss: 0.0009 lr: 0.002\n",
      "iteration: 478260 loss: 0.0011 lr: 0.002\n",
      "iteration: 478270 loss: 0.0012 lr: 0.002\n",
      "iteration: 478280 loss: 0.0013 lr: 0.002\n",
      "iteration: 478290 loss: 0.0009 lr: 0.002\n",
      "iteration: 478300 loss: 0.0008 lr: 0.002\n",
      "iteration: 478310 loss: 0.0015 lr: 0.002\n",
      "iteration: 478320 loss: 0.0009 lr: 0.002\n",
      "iteration: 478330 loss: 0.0011 lr: 0.002\n",
      "iteration: 478340 loss: 0.0010 lr: 0.002\n",
      "iteration: 478350 loss: 0.0011 lr: 0.002\n",
      "iteration: 478360 loss: 0.0017 lr: 0.002\n",
      "iteration: 478370 loss: 0.0010 lr: 0.002\n",
      "iteration: 478380 loss: 0.0008 lr: 0.002\n",
      "iteration: 478390 loss: 0.0010 lr: 0.002\n",
      "iteration: 478400 loss: 0.0012 lr: 0.002\n",
      "iteration: 478410 loss: 0.0014 lr: 0.002\n",
      "iteration: 478420 loss: 0.0017 lr: 0.002\n",
      "iteration: 478430 loss: 0.0008 lr: 0.002\n",
      "iteration: 478440 loss: 0.0011 lr: 0.002\n",
      "iteration: 478450 loss: 0.0007 lr: 0.002\n",
      "iteration: 478460 loss: 0.0010 lr: 0.002\n",
      "iteration: 478470 loss: 0.0009 lr: 0.002\n",
      "iteration: 478480 loss: 0.0008 lr: 0.002\n",
      "iteration: 478490 loss: 0.0014 lr: 0.002\n",
      "iteration: 478500 loss: 0.0011 lr: 0.002\n",
      "iteration: 478510 loss: 0.0009 lr: 0.002\n",
      "iteration: 478520 loss: 0.0011 lr: 0.002\n",
      "iteration: 478530 loss: 0.0012 lr: 0.002\n",
      "iteration: 478540 loss: 0.0011 lr: 0.002\n",
      "iteration: 478550 loss: 0.0006 lr: 0.002\n",
      "iteration: 478560 loss: 0.0010 lr: 0.002\n",
      "iteration: 478570 loss: 0.0009 lr: 0.002\n",
      "iteration: 478580 loss: 0.0012 lr: 0.002\n",
      "iteration: 478590 loss: 0.0011 lr: 0.002\n",
      "iteration: 478600 loss: 0.0010 lr: 0.002\n",
      "iteration: 478610 loss: 0.0011 lr: 0.002\n",
      "iteration: 478620 loss: 0.0014 lr: 0.002\n",
      "iteration: 478630 loss: 0.0013 lr: 0.002\n",
      "iteration: 478640 loss: 0.0012 lr: 0.002\n",
      "iteration: 478650 loss: 0.0010 lr: 0.002\n",
      "iteration: 478660 loss: 0.0009 lr: 0.002\n",
      "iteration: 478670 loss: 0.0011 lr: 0.002\n",
      "iteration: 478680 loss: 0.0017 lr: 0.002\n",
      "iteration: 478690 loss: 0.0012 lr: 0.002\n",
      "iteration: 478700 loss: 0.0011 lr: 0.002\n",
      "iteration: 478710 loss: 0.0017 lr: 0.002\n",
      "iteration: 478720 loss: 0.0007 lr: 0.002\n",
      "iteration: 478730 loss: 0.0009 lr: 0.002\n",
      "iteration: 478740 loss: 0.0009 lr: 0.002\n",
      "iteration: 478750 loss: 0.0012 lr: 0.002\n",
      "iteration: 478760 loss: 0.0010 lr: 0.002\n",
      "iteration: 478770 loss: 0.0011 lr: 0.002\n",
      "iteration: 478780 loss: 0.0010 lr: 0.002\n",
      "iteration: 478790 loss: 0.0010 lr: 0.002\n",
      "iteration: 478800 loss: 0.0009 lr: 0.002\n",
      "iteration: 478810 loss: 0.0014 lr: 0.002\n",
      "iteration: 478820 loss: 0.0013 lr: 0.002\n",
      "iteration: 478830 loss: 0.0016 lr: 0.002\n",
      "iteration: 478840 loss: 0.0007 lr: 0.002\n",
      "iteration: 478850 loss: 0.0012 lr: 0.002\n",
      "iteration: 478860 loss: 0.0009 lr: 0.002\n",
      "iteration: 478870 loss: 0.0014 lr: 0.002\n",
      "iteration: 478880 loss: 0.0011 lr: 0.002\n",
      "iteration: 478890 loss: 0.0012 lr: 0.002\n",
      "iteration: 478900 loss: 0.0009 lr: 0.002\n",
      "iteration: 478910 loss: 0.0008 lr: 0.002\n",
      "iteration: 478920 loss: 0.0007 lr: 0.002\n",
      "iteration: 478930 loss: 0.0013 lr: 0.002\n",
      "iteration: 478940 loss: 0.0012 lr: 0.002\n",
      "iteration: 478950 loss: 0.0011 lr: 0.002\n",
      "iteration: 478960 loss: 0.0014 lr: 0.002\n",
      "iteration: 478970 loss: 0.0021 lr: 0.002\n",
      "iteration: 478980 loss: 0.0016 lr: 0.002\n",
      "iteration: 478990 loss: 0.0010 lr: 0.002\n",
      "iteration: 479000 loss: 0.0012 lr: 0.002\n",
      "iteration: 479010 loss: 0.0008 lr: 0.002\n",
      "iteration: 479020 loss: 0.0014 lr: 0.002\n",
      "iteration: 479030 loss: 0.0009 lr: 0.002\n",
      "iteration: 479040 loss: 0.0011 lr: 0.002\n",
      "iteration: 479050 loss: 0.0016 lr: 0.002\n",
      "iteration: 479060 loss: 0.0013 lr: 0.002\n",
      "iteration: 479070 loss: 0.0010 lr: 0.002\n",
      "iteration: 479080 loss: 0.0013 lr: 0.002\n",
      "iteration: 479090 loss: 0.0013 lr: 0.002\n",
      "iteration: 479100 loss: 0.0010 lr: 0.002\n",
      "iteration: 479110 loss: 0.0013 lr: 0.002\n",
      "iteration: 479120 loss: 0.0014 lr: 0.002\n",
      "iteration: 479130 loss: 0.0011 lr: 0.002\n",
      "iteration: 479140 loss: 0.0013 lr: 0.002\n",
      "iteration: 479150 loss: 0.0008 lr: 0.002\n",
      "iteration: 479160 loss: 0.0010 lr: 0.002\n",
      "iteration: 479170 loss: 0.0011 lr: 0.002\n",
      "iteration: 479180 loss: 0.0012 lr: 0.002\n",
      "iteration: 479190 loss: 0.0009 lr: 0.002\n",
      "iteration: 479200 loss: 0.0008 lr: 0.002\n",
      "iteration: 479210 loss: 0.0012 lr: 0.002\n",
      "iteration: 479220 loss: 0.0011 lr: 0.002\n",
      "iteration: 479230 loss: 0.0016 lr: 0.002\n",
      "iteration: 479240 loss: 0.0010 lr: 0.002\n",
      "iteration: 479250 loss: 0.0013 lr: 0.002\n",
      "iteration: 479260 loss: 0.0010 lr: 0.002\n",
      "iteration: 479270 loss: 0.0010 lr: 0.002\n",
      "iteration: 479280 loss: 0.0014 lr: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 479290 loss: 0.0011 lr: 0.002\n",
      "iteration: 479300 loss: 0.0012 lr: 0.002\n",
      "iteration: 479310 loss: 0.0014 lr: 0.002\n",
      "iteration: 479320 loss: 0.0015 lr: 0.002\n",
      "iteration: 479330 loss: 0.0010 lr: 0.002\n",
      "iteration: 479340 loss: 0.0014 lr: 0.002\n",
      "iteration: 479350 loss: 0.0009 lr: 0.002\n",
      "iteration: 479360 loss: 0.0011 lr: 0.002\n",
      "iteration: 479370 loss: 0.0020 lr: 0.002\n",
      "iteration: 479380 loss: 0.0007 lr: 0.002\n",
      "iteration: 479390 loss: 0.0011 lr: 0.002\n",
      "iteration: 479400 loss: 0.0012 lr: 0.002\n",
      "iteration: 479410 loss: 0.0013 lr: 0.002\n",
      "iteration: 479420 loss: 0.0015 lr: 0.002\n",
      "iteration: 479430 loss: 0.0013 lr: 0.002\n",
      "iteration: 479440 loss: 0.0013 lr: 0.002\n",
      "iteration: 479450 loss: 0.0009 lr: 0.002\n",
      "iteration: 479460 loss: 0.0007 lr: 0.002\n",
      "iteration: 479470 loss: 0.0011 lr: 0.002\n",
      "iteration: 479480 loss: 0.0010 lr: 0.002\n",
      "iteration: 479490 loss: 0.0017 lr: 0.002\n",
      "iteration: 479500 loss: 0.0017 lr: 0.002\n",
      "iteration: 479510 loss: 0.0008 lr: 0.002\n",
      "iteration: 479520 loss: 0.0014 lr: 0.002\n",
      "iteration: 479530 loss: 0.0011 lr: 0.002\n",
      "iteration: 479540 loss: 0.0009 lr: 0.002\n",
      "iteration: 479550 loss: 0.0011 lr: 0.002\n",
      "iteration: 479560 loss: 0.0013 lr: 0.002\n",
      "iteration: 479570 loss: 0.0009 lr: 0.002\n",
      "iteration: 479580 loss: 0.0015 lr: 0.002\n",
      "iteration: 479590 loss: 0.0014 lr: 0.002\n",
      "iteration: 479600 loss: 0.0008 lr: 0.002\n",
      "iteration: 479610 loss: 0.0010 lr: 0.002\n",
      "iteration: 479620 loss: 0.0010 lr: 0.002\n",
      "iteration: 479630 loss: 0.0013 lr: 0.002\n",
      "iteration: 479640 loss: 0.0009 lr: 0.002\n",
      "iteration: 479650 loss: 0.0011 lr: 0.002\n",
      "iteration: 479660 loss: 0.0009 lr: 0.002\n",
      "iteration: 479670 loss: 0.0014 lr: 0.002\n",
      "iteration: 479680 loss: 0.0008 lr: 0.002\n",
      "iteration: 479690 loss: 0.0009 lr: 0.002\n",
      "iteration: 479700 loss: 0.0008 lr: 0.002\n",
      "iteration: 479710 loss: 0.0019 lr: 0.002\n",
      "iteration: 479720 loss: 0.0011 lr: 0.002\n",
      "iteration: 479730 loss: 0.0009 lr: 0.002\n",
      "iteration: 479740 loss: 0.0017 lr: 0.002\n",
      "iteration: 479750 loss: 0.0009 lr: 0.002\n",
      "iteration: 479760 loss: 0.0014 lr: 0.002\n",
      "iteration: 479770 loss: 0.0008 lr: 0.002\n",
      "iteration: 479780 loss: 0.0009 lr: 0.002\n",
      "iteration: 479790 loss: 0.0013 lr: 0.002\n",
      "iteration: 479800 loss: 0.0017 lr: 0.002\n",
      "iteration: 479810 loss: 0.0011 lr: 0.002\n",
      "iteration: 479820 loss: 0.0014 lr: 0.002\n",
      "iteration: 479830 loss: 0.0014 lr: 0.002\n",
      "iteration: 479840 loss: 0.0010 lr: 0.002\n",
      "iteration: 479850 loss: 0.0014 lr: 0.002\n",
      "iteration: 479860 loss: 0.0012 lr: 0.002\n",
      "iteration: 479870 loss: 0.0017 lr: 0.002\n",
      "iteration: 479880 loss: 0.0016 lr: 0.002\n",
      "iteration: 479890 loss: 0.0009 lr: 0.002\n",
      "iteration: 479900 loss: 0.0012 lr: 0.002\n",
      "iteration: 479910 loss: 0.0022 lr: 0.002\n",
      "iteration: 479920 loss: 0.0009 lr: 0.002\n",
      "iteration: 479930 loss: 0.0010 lr: 0.002\n",
      "iteration: 479940 loss: 0.0013 lr: 0.002\n",
      "iteration: 479950 loss: 0.0012 lr: 0.002\n",
      "iteration: 479960 loss: 0.0013 lr: 0.002\n",
      "iteration: 479970 loss: 0.0010 lr: 0.002\n",
      "iteration: 479980 loss: 0.0011 lr: 0.002\n",
      "iteration: 479990 loss: 0.0013 lr: 0.002\n",
      "iteration: 480000 loss: 0.0013 lr: 0.002\n",
      "iteration: 480010 loss: 0.0009 lr: 0.002\n",
      "iteration: 480020 loss: 0.0016 lr: 0.002\n",
      "iteration: 480030 loss: 0.0012 lr: 0.002\n",
      "iteration: 480040 loss: 0.0009 lr: 0.002\n",
      "iteration: 480050 loss: 0.0020 lr: 0.002\n",
      "iteration: 480060 loss: 0.0010 lr: 0.002\n",
      "iteration: 480070 loss: 0.0011 lr: 0.002\n",
      "iteration: 480080 loss: 0.0019 lr: 0.002\n",
      "iteration: 480090 loss: 0.0008 lr: 0.002\n",
      "iteration: 480100 loss: 0.0013 lr: 0.002\n",
      "iteration: 480110 loss: 0.0012 lr: 0.002\n",
      "iteration: 480120 loss: 0.0009 lr: 0.002\n",
      "iteration: 480130 loss: 0.0011 lr: 0.002\n",
      "iteration: 480140 loss: 0.0009 lr: 0.002\n",
      "iteration: 480150 loss: 0.0013 lr: 0.002\n",
      "iteration: 480160 loss: 0.0010 lr: 0.002\n",
      "iteration: 480170 loss: 0.0012 lr: 0.002\n",
      "iteration: 480180 loss: 0.0008 lr: 0.002\n",
      "iteration: 480190 loss: 0.0012 lr: 0.002\n",
      "iteration: 480200 loss: 0.0010 lr: 0.002\n",
      "iteration: 480210 loss: 0.0010 lr: 0.002\n",
      "iteration: 480220 loss: 0.0011 lr: 0.002\n",
      "iteration: 480230 loss: 0.0015 lr: 0.002\n",
      "iteration: 480240 loss: 0.0015 lr: 0.002\n",
      "iteration: 480250 loss: 0.0013 lr: 0.002\n",
      "iteration: 480260 loss: 0.0013 lr: 0.002\n",
      "iteration: 480270 loss: 0.0010 lr: 0.002\n",
      "iteration: 480280 loss: 0.0015 lr: 0.002\n",
      "iteration: 480290 loss: 0.0010 lr: 0.002\n",
      "iteration: 480300 loss: 0.0013 lr: 0.002\n",
      "iteration: 480310 loss: 0.0011 lr: 0.002\n",
      "iteration: 480320 loss: 0.0010 lr: 0.002\n",
      "iteration: 480330 loss: 0.0011 lr: 0.002\n",
      "iteration: 480340 loss: 0.0008 lr: 0.002\n",
      "iteration: 480350 loss: 0.0019 lr: 0.002\n",
      "iteration: 480360 loss: 0.0015 lr: 0.002\n",
      "iteration: 480370 loss: 0.0009 lr: 0.002\n",
      "iteration: 480380 loss: 0.0010 lr: 0.002\n",
      "iteration: 480390 loss: 0.0015 lr: 0.002\n",
      "iteration: 480400 loss: 0.0012 lr: 0.002\n",
      "iteration: 480410 loss: 0.0010 lr: 0.002\n",
      "iteration: 480420 loss: 0.0014 lr: 0.002\n",
      "iteration: 480430 loss: 0.0013 lr: 0.002\n",
      "iteration: 480440 loss: 0.0009 lr: 0.002\n",
      "iteration: 480450 loss: 0.0009 lr: 0.002\n",
      "iteration: 480460 loss: 0.0009 lr: 0.002\n",
      "iteration: 480470 loss: 0.0011 lr: 0.002\n",
      "iteration: 480480 loss: 0.0009 lr: 0.002\n",
      "iteration: 480490 loss: 0.0014 lr: 0.002\n",
      "iteration: 480500 loss: 0.0011 lr: 0.002\n",
      "iteration: 480510 loss: 0.0012 lr: 0.002\n",
      "iteration: 480520 loss: 0.0013 lr: 0.002\n",
      "iteration: 480530 loss: 0.0015 lr: 0.002\n",
      "iteration: 480540 loss: 0.0015 lr: 0.002\n",
      "iteration: 480550 loss: 0.0016 lr: 0.002\n",
      "iteration: 480560 loss: 0.0008 lr: 0.002\n",
      "iteration: 480570 loss: 0.0009 lr: 0.002\n",
      "iteration: 480580 loss: 0.0011 lr: 0.002\n",
      "iteration: 480590 loss: 0.0010 lr: 0.002\n",
      "iteration: 480600 loss: 0.0013 lr: 0.002\n",
      "iteration: 480610 loss: 0.0013 lr: 0.002\n",
      "iteration: 480620 loss: 0.0019 lr: 0.002\n",
      "iteration: 480630 loss: 0.0025 lr: 0.002\n",
      "iteration: 480640 loss: 0.0011 lr: 0.002\n",
      "iteration: 480650 loss: 0.0010 lr: 0.002\n",
      "iteration: 480660 loss: 0.0017 lr: 0.002\n",
      "iteration: 480670 loss: 0.0013 lr: 0.002\n",
      "iteration: 480680 loss: 0.0014 lr: 0.002\n",
      "iteration: 480690 loss: 0.0010 lr: 0.002\n",
      "iteration: 480700 loss: 0.0008 lr: 0.002\n",
      "iteration: 480710 loss: 0.0012 lr: 0.002\n",
      "iteration: 480720 loss: 0.0014 lr: 0.002\n",
      "iteration: 480730 loss: 0.0013 lr: 0.002\n",
      "iteration: 480740 loss: 0.0014 lr: 0.002\n",
      "iteration: 480750 loss: 0.0010 lr: 0.002\n",
      "iteration: 480760 loss: 0.0008 lr: 0.002\n",
      "iteration: 480770 loss: 0.0015 lr: 0.002\n",
      "iteration: 480780 loss: 0.0015 lr: 0.002\n",
      "iteration: 480790 loss: 0.0012 lr: 0.002\n",
      "iteration: 480800 loss: 0.0011 lr: 0.002\n",
      "iteration: 480810 loss: 0.0012 lr: 0.002\n",
      "iteration: 480820 loss: 0.0014 lr: 0.002\n",
      "iteration: 480830 loss: 0.0009 lr: 0.002\n",
      "iteration: 480840 loss: 0.0010 lr: 0.002\n",
      "iteration: 480850 loss: 0.0014 lr: 0.002\n",
      "iteration: 480860 loss: 0.0019 lr: 0.002\n",
      "iteration: 480870 loss: 0.0008 lr: 0.002\n",
      "iteration: 480880 loss: 0.0018 lr: 0.002\n",
      "iteration: 480890 loss: 0.0010 lr: 0.002\n",
      "iteration: 480900 loss: 0.0013 lr: 0.002\n",
      "iteration: 480910 loss: 0.0010 lr: 0.002\n",
      "iteration: 480920 loss: 0.0009 lr: 0.002\n",
      "iteration: 480930 loss: 0.0021 lr: 0.002\n",
      "iteration: 480940 loss: 0.0012 lr: 0.002\n",
      "iteration: 480950 loss: 0.0011 lr: 0.002\n",
      "iteration: 480960 loss: 0.0008 lr: 0.002\n",
      "iteration: 480970 loss: 0.0014 lr: 0.002\n",
      "iteration: 480980 loss: 0.0013 lr: 0.002\n",
      "iteration: 480990 loss: 0.0009 lr: 0.002\n",
      "iteration: 481000 loss: 0.0012 lr: 0.002\n",
      "iteration: 481010 loss: 0.0009 lr: 0.002\n",
      "iteration: 481020 loss: 0.0013 lr: 0.002\n",
      "iteration: 481030 loss: 0.0014 lr: 0.002\n",
      "iteration: 481040 loss: 0.0011 lr: 0.002\n",
      "iteration: 481050 loss: 0.0016 lr: 0.002\n",
      "iteration: 481060 loss: 0.0007 lr: 0.002\n",
      "iteration: 481070 loss: 0.0010 lr: 0.002\n",
      "iteration: 481080 loss: 0.0012 lr: 0.002\n",
      "iteration: 481090 loss: 0.0013 lr: 0.002\n",
      "iteration: 481100 loss: 0.0012 lr: 0.002\n",
      "iteration: 481110 loss: 0.0009 lr: 0.002\n",
      "iteration: 481120 loss: 0.0012 lr: 0.002\n",
      "iteration: 481130 loss: 0.0009 lr: 0.002\n",
      "iteration: 481140 loss: 0.0008 lr: 0.002\n",
      "iteration: 481150 loss: 0.0018 lr: 0.002\n",
      "iteration: 481160 loss: 0.0015 lr: 0.002\n",
      "iteration: 481170 loss: 0.0011 lr: 0.002\n",
      "iteration: 481180 loss: 0.0009 lr: 0.002\n",
      "iteration: 481190 loss: 0.0008 lr: 0.002\n",
      "iteration: 481200 loss: 0.0016 lr: 0.002\n",
      "iteration: 481210 loss: 0.0008 lr: 0.002\n",
      "iteration: 481220 loss: 0.0010 lr: 0.002\n",
      "iteration: 481230 loss: 0.0008 lr: 0.002\n",
      "iteration: 481240 loss: 0.0014 lr: 0.002\n",
      "iteration: 481250 loss: 0.0013 lr: 0.002\n",
      "iteration: 481260 loss: 0.0012 lr: 0.002\n",
      "iteration: 481270 loss: 0.0010 lr: 0.002\n",
      "iteration: 481280 loss: 0.0011 lr: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 481290 loss: 0.0009 lr: 0.002\n",
      "iteration: 481300 loss: 0.0013 lr: 0.002\n",
      "iteration: 481310 loss: 0.0012 lr: 0.002\n",
      "iteration: 481320 loss: 0.0023 lr: 0.002\n",
      "iteration: 481330 loss: 0.0010 lr: 0.002\n",
      "iteration: 481340 loss: 0.0012 lr: 0.002\n",
      "iteration: 481350 loss: 0.0010 lr: 0.002\n",
      "iteration: 481360 loss: 0.0012 lr: 0.002\n",
      "iteration: 481370 loss: 0.0012 lr: 0.002\n",
      "iteration: 481380 loss: 0.0010 lr: 0.002\n",
      "iteration: 481390 loss: 0.0009 lr: 0.002\n",
      "iteration: 481400 loss: 0.0015 lr: 0.002\n",
      "iteration: 481410 loss: 0.0013 lr: 0.002\n",
      "iteration: 481420 loss: 0.0008 lr: 0.002\n",
      "iteration: 481430 loss: 0.0016 lr: 0.002\n",
      "iteration: 481440 loss: 0.0007 lr: 0.002\n",
      "iteration: 481450 loss: 0.0012 lr: 0.002\n",
      "iteration: 481460 loss: 0.0011 lr: 0.002\n",
      "iteration: 481470 loss: 0.0012 lr: 0.002\n",
      "iteration: 481480 loss: 0.0008 lr: 0.002\n",
      "iteration: 481490 loss: 0.0014 lr: 0.002\n",
      "iteration: 481500 loss: 0.0011 lr: 0.002\n",
      "iteration: 481510 loss: 0.0012 lr: 0.002\n",
      "iteration: 481520 loss: 0.0016 lr: 0.002\n",
      "iteration: 481530 loss: 0.0010 lr: 0.002\n",
      "iteration: 481540 loss: 0.0012 lr: 0.002\n",
      "iteration: 481550 loss: 0.0009 lr: 0.002\n",
      "iteration: 481560 loss: 0.0013 lr: 0.002\n",
      "iteration: 481570 loss: 0.0013 lr: 0.002\n",
      "iteration: 481580 loss: 0.0012 lr: 0.002\n",
      "iteration: 481590 loss: 0.0010 lr: 0.002\n",
      "iteration: 481600 loss: 0.0012 lr: 0.002\n",
      "iteration: 481610 loss: 0.0012 lr: 0.002\n",
      "iteration: 481620 loss: 0.0013 lr: 0.002\n",
      "iteration: 481630 loss: 0.0013 lr: 0.002\n",
      "iteration: 481640 loss: 0.0012 lr: 0.002\n",
      "iteration: 481650 loss: 0.0010 lr: 0.002\n",
      "iteration: 481660 loss: 0.0012 lr: 0.002\n",
      "iteration: 481670 loss: 0.0014 lr: 0.002\n",
      "iteration: 481680 loss: 0.0008 lr: 0.002\n",
      "iteration: 481690 loss: 0.0018 lr: 0.002\n",
      "iteration: 481700 loss: 0.0010 lr: 0.002\n",
      "iteration: 481710 loss: 0.0010 lr: 0.002\n",
      "iteration: 481720 loss: 0.0011 lr: 0.002\n",
      "iteration: 481730 loss: 0.0010 lr: 0.002\n",
      "iteration: 481740 loss: 0.0012 lr: 0.002\n",
      "iteration: 481750 loss: 0.0010 lr: 0.002\n",
      "iteration: 481760 loss: 0.0018 lr: 0.002\n",
      "iteration: 481770 loss: 0.0010 lr: 0.002\n",
      "iteration: 481780 loss: 0.0018 lr: 0.002\n",
      "iteration: 481790 loss: 0.0009 lr: 0.002\n",
      "iteration: 481800 loss: 0.0017 lr: 0.002\n",
      "iteration: 481810 loss: 0.0016 lr: 0.002\n",
      "iteration: 481820 loss: 0.0015 lr: 0.002\n",
      "iteration: 481830 loss: 0.0009 lr: 0.002\n",
      "iteration: 481840 loss: 0.0009 lr: 0.002\n",
      "iteration: 481850 loss: 0.0007 lr: 0.002\n",
      "iteration: 481860 loss: 0.0010 lr: 0.002\n",
      "iteration: 481870 loss: 0.0013 lr: 0.002\n",
      "iteration: 481880 loss: 0.0010 lr: 0.002\n",
      "iteration: 481890 loss: 0.0008 lr: 0.002\n",
      "iteration: 481900 loss: 0.0016 lr: 0.002\n",
      "iteration: 481910 loss: 0.0017 lr: 0.002\n",
      "iteration: 481920 loss: 0.0008 lr: 0.002\n",
      "iteration: 481930 loss: 0.0010 lr: 0.002\n",
      "iteration: 481940 loss: 0.0012 lr: 0.002\n",
      "iteration: 481950 loss: 0.0010 lr: 0.002\n",
      "iteration: 481960 loss: 0.0011 lr: 0.002\n",
      "iteration: 481970 loss: 0.0008 lr: 0.002\n",
      "iteration: 481980 loss: 0.0010 lr: 0.002\n",
      "iteration: 481990 loss: 0.0009 lr: 0.002\n",
      "iteration: 482000 loss: 0.0007 lr: 0.002\n",
      "iteration: 482010 loss: 0.0011 lr: 0.002\n",
      "iteration: 482020 loss: 0.0015 lr: 0.002\n",
      "iteration: 482030 loss: 0.0014 lr: 0.002\n",
      "iteration: 482040 loss: 0.0015 lr: 0.002\n",
      "iteration: 482050 loss: 0.0010 lr: 0.002\n",
      "iteration: 482060 loss: 0.0010 lr: 0.002\n",
      "iteration: 482070 loss: 0.0008 lr: 0.002\n",
      "iteration: 482080 loss: 0.0009 lr: 0.002\n",
      "iteration: 482090 loss: 0.0010 lr: 0.002\n",
      "iteration: 482100 loss: 0.0018 lr: 0.002\n",
      "iteration: 482110 loss: 0.0010 lr: 0.002\n",
      "iteration: 482120 loss: 0.0009 lr: 0.002\n",
      "iteration: 482130 loss: 0.0007 lr: 0.002\n",
      "iteration: 482140 loss: 0.0010 lr: 0.002\n",
      "iteration: 482150 loss: 0.0011 lr: 0.002\n",
      "iteration: 482160 loss: 0.0013 lr: 0.002\n",
      "iteration: 482170 loss: 0.0010 lr: 0.002\n",
      "iteration: 482180 loss: 0.0015 lr: 0.002\n",
      "iteration: 482190 loss: 0.0012 lr: 0.002\n",
      "iteration: 482200 loss: 0.0008 lr: 0.002\n",
      "iteration: 482210 loss: 0.0009 lr: 0.002\n",
      "iteration: 482220 loss: 0.0017 lr: 0.002\n",
      "iteration: 482230 loss: 0.0010 lr: 0.002\n",
      "iteration: 482240 loss: 0.0009 lr: 0.002\n",
      "iteration: 482250 loss: 0.0015 lr: 0.002\n",
      "iteration: 482260 loss: 0.0010 lr: 0.002\n",
      "iteration: 482270 loss: 0.0009 lr: 0.002\n",
      "iteration: 482280 loss: 0.0019 lr: 0.002\n",
      "iteration: 482290 loss: 0.0008 lr: 0.002\n",
      "iteration: 482300 loss: 0.0011 lr: 0.002\n",
      "iteration: 482310 loss: 0.0012 lr: 0.002\n",
      "iteration: 482320 loss: 0.0008 lr: 0.002\n",
      "iteration: 482330 loss: 0.0015 lr: 0.002\n",
      "iteration: 482340 loss: 0.0010 lr: 0.002\n",
      "iteration: 482350 loss: 0.0009 lr: 0.002\n",
      "iteration: 482360 loss: 0.0011 lr: 0.002\n",
      "iteration: 482370 loss: 0.0010 lr: 0.002\n",
      "iteration: 482380 loss: 0.0012 lr: 0.002\n",
      "iteration: 482390 loss: 0.0014 lr: 0.002\n",
      "iteration: 482400 loss: 0.0011 lr: 0.002\n",
      "iteration: 482410 loss: 0.0013 lr: 0.002\n",
      "iteration: 482420 loss: 0.0009 lr: 0.002\n",
      "iteration: 482430 loss: 0.0012 lr: 0.002\n",
      "iteration: 482440 loss: 0.0014 lr: 0.002\n",
      "iteration: 482450 loss: 0.0019 lr: 0.002\n",
      "iteration: 482460 loss: 0.0018 lr: 0.002\n",
      "iteration: 482470 loss: 0.0010 lr: 0.002\n",
      "iteration: 482480 loss: 0.0009 lr: 0.002\n",
      "iteration: 482490 loss: 0.0012 lr: 0.002\n",
      "iteration: 482500 loss: 0.0008 lr: 0.002\n",
      "iteration: 482510 loss: 0.0013 lr: 0.002\n",
      "iteration: 482520 loss: 0.0010 lr: 0.002\n",
      "iteration: 482530 loss: 0.0015 lr: 0.002\n",
      "iteration: 482540 loss: 0.0006 lr: 0.002\n",
      "iteration: 482550 loss: 0.0015 lr: 0.002\n",
      "iteration: 482560 loss: 0.0013 lr: 0.002\n",
      "iteration: 482570 loss: 0.0008 lr: 0.002\n",
      "iteration: 482580 loss: 0.0011 lr: 0.002\n",
      "iteration: 482590 loss: 0.0010 lr: 0.002\n",
      "iteration: 482600 loss: 0.0009 lr: 0.002\n",
      "iteration: 482610 loss: 0.0013 lr: 0.002\n",
      "iteration: 482620 loss: 0.0018 lr: 0.002\n",
      "iteration: 482630 loss: 0.0011 lr: 0.002\n",
      "iteration: 482640 loss: 0.0011 lr: 0.002\n",
      "iteration: 482650 loss: 0.0010 lr: 0.002\n",
      "iteration: 482660 loss: 0.0009 lr: 0.002\n",
      "iteration: 482670 loss: 0.0013 lr: 0.002\n",
      "iteration: 482680 loss: 0.0008 lr: 0.002\n",
      "iteration: 482690 loss: 0.0010 lr: 0.002\n",
      "iteration: 482700 loss: 0.0012 lr: 0.002\n",
      "iteration: 482710 loss: 0.0010 lr: 0.002\n",
      "iteration: 482720 loss: 0.0008 lr: 0.002\n",
      "iteration: 482730 loss: 0.0015 lr: 0.002\n",
      "iteration: 482740 loss: 0.0014 lr: 0.002\n",
      "iteration: 482750 loss: 0.0009 lr: 0.002\n",
      "iteration: 482760 loss: 0.0016 lr: 0.002\n",
      "iteration: 482770 loss: 0.0011 lr: 0.002\n",
      "iteration: 482780 loss: 0.0014 lr: 0.002\n",
      "iteration: 482790 loss: 0.0011 lr: 0.002\n",
      "iteration: 482800 loss: 0.0008 lr: 0.002\n",
      "iteration: 482810 loss: 0.0012 lr: 0.002\n",
      "iteration: 482820 loss: 0.0022 lr: 0.002\n",
      "iteration: 482830 loss: 0.0014 lr: 0.002\n",
      "iteration: 482840 loss: 0.0012 lr: 0.002\n",
      "iteration: 482850 loss: 0.0016 lr: 0.002\n",
      "iteration: 482860 loss: 0.0015 lr: 0.002\n",
      "iteration: 482870 loss: 0.0013 lr: 0.002\n",
      "iteration: 482880 loss: 0.0011 lr: 0.002\n",
      "iteration: 482890 loss: 0.0009 lr: 0.002\n",
      "iteration: 482900 loss: 0.0009 lr: 0.002\n",
      "iteration: 482910 loss: 0.0012 lr: 0.002\n",
      "iteration: 482920 loss: 0.0011 lr: 0.002\n",
      "iteration: 482930 loss: 0.0010 lr: 0.002\n",
      "iteration: 482940 loss: 0.0009 lr: 0.002\n",
      "iteration: 482950 loss: 0.0011 lr: 0.002\n",
      "iteration: 482960 loss: 0.0011 lr: 0.002\n",
      "iteration: 482970 loss: 0.0010 lr: 0.002\n",
      "iteration: 482980 loss: 0.0010 lr: 0.002\n",
      "iteration: 482990 loss: 0.0016 lr: 0.002\n",
      "iteration: 483000 loss: 0.0010 lr: 0.002\n",
      "iteration: 483010 loss: 0.0009 lr: 0.002\n",
      "iteration: 483020 loss: 0.0011 lr: 0.002\n",
      "iteration: 483030 loss: 0.0013 lr: 0.002\n",
      "iteration: 483040 loss: 0.0007 lr: 0.002\n",
      "iteration: 483050 loss: 0.0010 lr: 0.002\n",
      "iteration: 483060 loss: 0.0016 lr: 0.002\n",
      "iteration: 483070 loss: 0.0010 lr: 0.002\n",
      "iteration: 483080 loss: 0.0008 lr: 0.002\n",
      "iteration: 483090 loss: 0.0011 lr: 0.002\n",
      "iteration: 483100 loss: 0.0011 lr: 0.002\n",
      "iteration: 483110 loss: 0.0008 lr: 0.002\n",
      "iteration: 483120 loss: 0.0010 lr: 0.002\n",
      "iteration: 483130 loss: 0.0010 lr: 0.002\n",
      "iteration: 483140 loss: 0.0006 lr: 0.002\n",
      "iteration: 483150 loss: 0.0010 lr: 0.002\n",
      "iteration: 483160 loss: 0.0010 lr: 0.002\n",
      "iteration: 483170 loss: 0.0009 lr: 0.002\n",
      "iteration: 483180 loss: 0.0012 lr: 0.002\n",
      "iteration: 483190 loss: 0.0011 lr: 0.002\n",
      "iteration: 483200 loss: 0.0013 lr: 0.002\n",
      "iteration: 483210 loss: 0.0010 lr: 0.002\n",
      "iteration: 483220 loss: 0.0021 lr: 0.002\n",
      "iteration: 483230 loss: 0.0023 lr: 0.002\n",
      "iteration: 483240 loss: 0.0018 lr: 0.002\n",
      "iteration: 483250 loss: 0.0011 lr: 0.002\n",
      "iteration: 483260 loss: 0.0011 lr: 0.002\n",
      "iteration: 483270 loss: 0.0010 lr: 0.002\n",
      "iteration: 483280 loss: 0.0010 lr: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 483290 loss: 0.0012 lr: 0.002\n",
      "iteration: 483300 loss: 0.0010 lr: 0.002\n",
      "iteration: 483310 loss: 0.0013 lr: 0.002\n",
      "iteration: 483320 loss: 0.0018 lr: 0.002\n",
      "iteration: 483330 loss: 0.0018 lr: 0.002\n",
      "iteration: 483340 loss: 0.0011 lr: 0.002\n",
      "iteration: 483350 loss: 0.0013 lr: 0.002\n",
      "iteration: 483360 loss: 0.0015 lr: 0.002\n",
      "iteration: 483370 loss: 0.0010 lr: 0.002\n",
      "iteration: 483380 loss: 0.0011 lr: 0.002\n",
      "iteration: 483390 loss: 0.0012 lr: 0.002\n",
      "iteration: 483400 loss: 0.0009 lr: 0.002\n",
      "iteration: 483410 loss: 0.0008 lr: 0.002\n",
      "iteration: 483420 loss: 0.0015 lr: 0.002\n",
      "iteration: 483430 loss: 0.0007 lr: 0.002\n",
      "iteration: 483440 loss: 0.0011 lr: 0.002\n",
      "iteration: 483450 loss: 0.0012 lr: 0.002\n",
      "iteration: 483460 loss: 0.0006 lr: 0.002\n",
      "iteration: 483470 loss: 0.0008 lr: 0.002\n",
      "iteration: 483480 loss: 0.0011 lr: 0.002\n",
      "iteration: 483490 loss: 0.0009 lr: 0.002\n",
      "iteration: 483500 loss: 0.0011 lr: 0.002\n",
      "iteration: 483510 loss: 0.0011 lr: 0.002\n",
      "iteration: 483520 loss: 0.0010 lr: 0.002\n",
      "iteration: 483530 loss: 0.0011 lr: 0.002\n",
      "iteration: 483540 loss: 0.0009 lr: 0.002\n",
      "iteration: 483550 loss: 0.0010 lr: 0.002\n",
      "iteration: 483560 loss: 0.0012 lr: 0.002\n",
      "iteration: 483570 loss: 0.0010 lr: 0.002\n",
      "iteration: 483580 loss: 0.0010 lr: 0.002\n",
      "iteration: 483590 loss: 0.0009 lr: 0.002\n",
      "iteration: 483600 loss: 0.0009 lr: 0.002\n",
      "iteration: 483610 loss: 0.0011 lr: 0.002\n",
      "iteration: 483620 loss: 0.0026 lr: 0.002\n",
      "iteration: 483630 loss: 0.0017 lr: 0.002\n",
      "iteration: 483640 loss: 0.0008 lr: 0.002\n",
      "iteration: 483650 loss: 0.0013 lr: 0.002\n",
      "iteration: 483660 loss: 0.0018 lr: 0.002\n",
      "iteration: 483670 loss: 0.0018 lr: 0.002\n",
      "iteration: 483680 loss: 0.0010 lr: 0.002\n",
      "iteration: 483690 loss: 0.0013 lr: 0.002\n",
      "iteration: 483700 loss: 0.0015 lr: 0.002\n",
      "iteration: 483710 loss: 0.0010 lr: 0.002\n",
      "iteration: 483720 loss: 0.0007 lr: 0.002\n",
      "iteration: 483730 loss: 0.0009 lr: 0.002\n",
      "iteration: 483740 loss: 0.0009 lr: 0.002\n",
      "iteration: 483750 loss: 0.0008 lr: 0.002\n",
      "iteration: 483760 loss: 0.0007 lr: 0.002\n",
      "iteration: 483770 loss: 0.0010 lr: 0.002\n",
      "iteration: 483780 loss: 0.0013 lr: 0.002\n",
      "iteration: 483790 loss: 0.0014 lr: 0.002\n",
      "iteration: 483800 loss: 0.0009 lr: 0.002\n",
      "iteration: 483810 loss: 0.0011 lr: 0.002\n",
      "iteration: 483820 loss: 0.0012 lr: 0.002\n",
      "iteration: 483830 loss: 0.0015 lr: 0.002\n",
      "iteration: 483840 loss: 0.0009 lr: 0.002\n",
      "iteration: 483850 loss: 0.0014 lr: 0.002\n",
      "iteration: 483860 loss: 0.0009 lr: 0.002\n",
      "iteration: 483870 loss: 0.0012 lr: 0.002\n",
      "iteration: 483880 loss: 0.0013 lr: 0.002\n",
      "iteration: 483890 loss: 0.0010 lr: 0.002\n",
      "iteration: 483900 loss: 0.0011 lr: 0.002\n",
      "iteration: 483910 loss: 0.0008 lr: 0.002\n",
      "iteration: 483920 loss: 0.0009 lr: 0.002\n",
      "iteration: 483930 loss: 0.0018 lr: 0.002\n",
      "iteration: 483940 loss: 0.0016 lr: 0.002\n",
      "iteration: 483950 loss: 0.0021 lr: 0.002\n",
      "iteration: 483960 loss: 0.0013 lr: 0.002\n",
      "iteration: 483970 loss: 0.0010 lr: 0.002\n",
      "iteration: 483980 loss: 0.0012 lr: 0.002\n",
      "iteration: 483990 loss: 0.0010 lr: 0.002\n",
      "iteration: 484000 loss: 0.0007 lr: 0.002\n",
      "iteration: 484010 loss: 0.0014 lr: 0.002\n",
      "iteration: 484020 loss: 0.0014 lr: 0.002\n",
      "iteration: 484030 loss: 0.0020 lr: 0.002\n",
      "iteration: 484040 loss: 0.0015 lr: 0.002\n",
      "iteration: 484050 loss: 0.0009 lr: 0.002\n",
      "iteration: 484060 loss: 0.0014 lr: 0.002\n",
      "iteration: 484070 loss: 0.0013 lr: 0.002\n",
      "iteration: 484080 loss: 0.0013 lr: 0.002\n",
      "iteration: 484090 loss: 0.0010 lr: 0.002\n",
      "iteration: 484100 loss: 0.0009 lr: 0.002\n",
      "iteration: 484110 loss: 0.0011 lr: 0.002\n",
      "iteration: 484120 loss: 0.0010 lr: 0.002\n",
      "iteration: 484130 loss: 0.0010 lr: 0.002\n",
      "iteration: 484140 loss: 0.0015 lr: 0.002\n",
      "iteration: 484150 loss: 0.0012 lr: 0.002\n",
      "iteration: 484160 loss: 0.0011 lr: 0.002\n",
      "iteration: 484170 loss: 0.0014 lr: 0.002\n",
      "iteration: 484180 loss: 0.0009 lr: 0.002\n",
      "iteration: 484190 loss: 0.0012 lr: 0.002\n",
      "iteration: 484200 loss: 0.0011 lr: 0.002\n",
      "iteration: 484210 loss: 0.0011 lr: 0.002\n",
      "iteration: 484220 loss: 0.0013 lr: 0.002\n",
      "iteration: 484230 loss: 0.0007 lr: 0.002\n",
      "iteration: 484240 loss: 0.0012 lr: 0.002\n",
      "iteration: 484250 loss: 0.0012 lr: 0.002\n",
      "iteration: 484260 loss: 0.0013 lr: 0.002\n",
      "iteration: 484270 loss: 0.0010 lr: 0.002\n",
      "iteration: 484280 loss: 0.0010 lr: 0.002\n",
      "iteration: 484290 loss: 0.0010 lr: 0.002\n",
      "iteration: 484300 loss: 0.0013 lr: 0.002\n",
      "iteration: 484310 loss: 0.0012 lr: 0.002\n",
      "iteration: 484320 loss: 0.0014 lr: 0.002\n",
      "iteration: 484330 loss: 0.0009 lr: 0.002\n",
      "iteration: 484340 loss: 0.0014 lr: 0.002\n",
      "iteration: 484350 loss: 0.0009 lr: 0.002\n",
      "iteration: 484360 loss: 0.0008 lr: 0.002\n",
      "iteration: 484370 loss: 0.0009 lr: 0.002\n",
      "iteration: 484380 loss: 0.0012 lr: 0.002\n",
      "iteration: 484390 loss: 0.0010 lr: 0.002\n",
      "iteration: 484400 loss: 0.0008 lr: 0.002\n",
      "iteration: 484410 loss: 0.0010 lr: 0.002\n",
      "iteration: 484420 loss: 0.0018 lr: 0.002\n",
      "iteration: 484430 loss: 0.0009 lr: 0.002\n",
      "iteration: 484440 loss: 0.0015 lr: 0.002\n",
      "iteration: 484450 loss: 0.0007 lr: 0.002\n",
      "iteration: 484460 loss: 0.0014 lr: 0.002\n",
      "iteration: 484470 loss: 0.0012 lr: 0.002\n",
      "iteration: 484480 loss: 0.0021 lr: 0.002\n",
      "iteration: 484490 loss: 0.0010 lr: 0.002\n",
      "iteration: 484500 loss: 0.0013 lr: 0.002\n",
      "iteration: 484510 loss: 0.0012 lr: 0.002\n",
      "iteration: 484520 loss: 0.0015 lr: 0.002\n",
      "iteration: 484530 loss: 0.0010 lr: 0.002\n",
      "iteration: 484540 loss: 0.0011 lr: 0.002\n",
      "iteration: 484550 loss: 0.0012 lr: 0.002\n",
      "iteration: 484560 loss: 0.0013 lr: 0.002\n",
      "iteration: 484570 loss: 0.0015 lr: 0.002\n",
      "iteration: 484580 loss: 0.0014 lr: 0.002\n",
      "iteration: 484590 loss: 0.0009 lr: 0.002\n",
      "iteration: 484600 loss: 0.0012 lr: 0.002\n",
      "iteration: 484610 loss: 0.0015 lr: 0.002\n",
      "iteration: 484620 loss: 0.0017 lr: 0.002\n",
      "iteration: 484630 loss: 0.0011 lr: 0.002\n",
      "iteration: 484640 loss: 0.0012 lr: 0.002\n",
      "iteration: 484650 loss: 0.0010 lr: 0.002\n",
      "iteration: 484660 loss: 0.0012 lr: 0.002\n",
      "iteration: 484670 loss: 0.0015 lr: 0.002\n",
      "iteration: 484680 loss: 0.0014 lr: 0.002\n",
      "iteration: 484690 loss: 0.0010 lr: 0.002\n",
      "iteration: 484700 loss: 0.0012 lr: 0.002\n",
      "iteration: 484710 loss: 0.0008 lr: 0.002\n",
      "iteration: 484720 loss: 0.0011 lr: 0.002\n",
      "iteration: 484730 loss: 0.0009 lr: 0.002\n",
      "iteration: 484740 loss: 0.0013 lr: 0.002\n",
      "iteration: 484750 loss: 0.0012 lr: 0.002\n",
      "iteration: 484760 loss: 0.0008 lr: 0.002\n",
      "iteration: 484770 loss: 0.0012 lr: 0.002\n",
      "iteration: 484780 loss: 0.0010 lr: 0.002\n",
      "iteration: 484790 loss: 0.0011 lr: 0.002\n",
      "iteration: 484800 loss: 0.0009 lr: 0.002\n",
      "iteration: 484810 loss: 0.0011 lr: 0.002\n",
      "iteration: 484820 loss: 0.0020 lr: 0.002\n",
      "iteration: 484830 loss: 0.0013 lr: 0.002\n",
      "iteration: 484840 loss: 0.0013 lr: 0.002\n",
      "iteration: 484850 loss: 0.0011 lr: 0.002\n",
      "iteration: 484860 loss: 0.0009 lr: 0.002\n",
      "iteration: 484870 loss: 0.0013 lr: 0.002\n",
      "iteration: 484880 loss: 0.0010 lr: 0.002\n",
      "iteration: 484890 loss: 0.0010 lr: 0.002\n",
      "iteration: 484900 loss: 0.0014 lr: 0.002\n",
      "iteration: 484910 loss: 0.0015 lr: 0.002\n",
      "iteration: 484920 loss: 0.0010 lr: 0.002\n",
      "iteration: 484930 loss: 0.0011 lr: 0.002\n",
      "iteration: 484940 loss: 0.0011 lr: 0.002\n",
      "iteration: 484950 loss: 0.0010 lr: 0.002\n",
      "iteration: 484960 loss: 0.0012 lr: 0.002\n",
      "iteration: 484970 loss: 0.0011 lr: 0.002\n",
      "iteration: 484980 loss: 0.0008 lr: 0.002\n",
      "iteration: 484990 loss: 0.0012 lr: 0.002\n",
      "iteration: 485000 loss: 0.0008 lr: 0.002\n",
      "iteration: 485010 loss: 0.0008 lr: 0.002\n",
      "iteration: 485020 loss: 0.0012 lr: 0.002\n",
      "iteration: 485030 loss: 0.0009 lr: 0.002\n",
      "iteration: 485040 loss: 0.0011 lr: 0.002\n",
      "iteration: 485050 loss: 0.0010 lr: 0.002\n",
      "iteration: 485060 loss: 0.0008 lr: 0.002\n",
      "iteration: 485070 loss: 0.0015 lr: 0.002\n",
      "iteration: 485080 loss: 0.0014 lr: 0.002\n",
      "iteration: 485090 loss: 0.0012 lr: 0.002\n",
      "iteration: 485100 loss: 0.0011 lr: 0.002\n",
      "iteration: 485110 loss: 0.0008 lr: 0.002\n",
      "iteration: 485120 loss: 0.0008 lr: 0.002\n",
      "iteration: 485130 loss: 0.0011 lr: 0.002\n",
      "iteration: 485140 loss: 0.0017 lr: 0.002\n",
      "iteration: 485150 loss: 0.0013 lr: 0.002\n",
      "iteration: 485160 loss: 0.0013 lr: 0.002\n",
      "iteration: 485170 loss: 0.0010 lr: 0.002\n",
      "iteration: 485180 loss: 0.0010 lr: 0.002\n",
      "iteration: 485190 loss: 0.0009 lr: 0.002\n",
      "iteration: 485200 loss: 0.0010 lr: 0.002\n",
      "iteration: 485210 loss: 0.0016 lr: 0.002\n",
      "iteration: 485220 loss: 0.0011 lr: 0.002\n",
      "iteration: 485230 loss: 0.0015 lr: 0.002\n",
      "iteration: 485240 loss: 0.0014 lr: 0.002\n",
      "iteration: 485250 loss: 0.0009 lr: 0.002\n",
      "iteration: 485260 loss: 0.0020 lr: 0.002\n",
      "iteration: 485270 loss: 0.0009 lr: 0.002\n",
      "iteration: 485280 loss: 0.0019 lr: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 485290 loss: 0.0007 lr: 0.002\n",
      "iteration: 485300 loss: 0.0013 lr: 0.002\n",
      "iteration: 485310 loss: 0.0008 lr: 0.002\n",
      "iteration: 485320 loss: 0.0010 lr: 0.002\n",
      "iteration: 485330 loss: 0.0008 lr: 0.002\n",
      "iteration: 485340 loss: 0.0015 lr: 0.002\n",
      "iteration: 485350 loss: 0.0007 lr: 0.002\n",
      "iteration: 485360 loss: 0.0009 lr: 0.002\n",
      "iteration: 485370 loss: 0.0017 lr: 0.002\n",
      "iteration: 485380 loss: 0.0014 lr: 0.002\n",
      "iteration: 485390 loss: 0.0008 lr: 0.002\n",
      "iteration: 485400 loss: 0.0009 lr: 0.002\n",
      "iteration: 485410 loss: 0.0012 lr: 0.002\n",
      "iteration: 485420 loss: 0.0015 lr: 0.002\n",
      "iteration: 485430 loss: 0.0014 lr: 0.002\n",
      "iteration: 485440 loss: 0.0016 lr: 0.002\n",
      "iteration: 485450 loss: 0.0007 lr: 0.002\n",
      "iteration: 485460 loss: 0.0010 lr: 0.002\n",
      "iteration: 485470 loss: 0.0008 lr: 0.002\n",
      "iteration: 485480 loss: 0.0013 lr: 0.002\n",
      "iteration: 485490 loss: 0.0008 lr: 0.002\n",
      "iteration: 485500 loss: 0.0010 lr: 0.002\n",
      "iteration: 485510 loss: 0.0009 lr: 0.002\n",
      "iteration: 485520 loss: 0.0008 lr: 0.002\n",
      "iteration: 485530 loss: 0.0011 lr: 0.002\n",
      "iteration: 485540 loss: 0.0013 lr: 0.002\n",
      "iteration: 485550 loss: 0.0012 lr: 0.002\n",
      "iteration: 485560 loss: 0.0011 lr: 0.002\n",
      "iteration: 485570 loss: 0.0016 lr: 0.002\n",
      "iteration: 485580 loss: 0.0012 lr: 0.002\n",
      "iteration: 485590 loss: 0.0012 lr: 0.002\n",
      "iteration: 485600 loss: 0.0012 lr: 0.002\n",
      "iteration: 485610 loss: 0.0006 lr: 0.002\n",
      "iteration: 485620 loss: 0.0012 lr: 0.002\n",
      "iteration: 485630 loss: 0.0017 lr: 0.002\n",
      "iteration: 485640 loss: 0.0012 lr: 0.002\n",
      "iteration: 485650 loss: 0.0010 lr: 0.002\n",
      "iteration: 485660 loss: 0.0008 lr: 0.002\n",
      "iteration: 485670 loss: 0.0022 lr: 0.002\n",
      "iteration: 485680 loss: 0.0012 lr: 0.002\n",
      "iteration: 485690 loss: 0.0010 lr: 0.002\n",
      "iteration: 485700 loss: 0.0011 lr: 0.002\n",
      "iteration: 485710 loss: 0.0009 lr: 0.002\n",
      "iteration: 485720 loss: 0.0012 lr: 0.002\n",
      "iteration: 485730 loss: 0.0012 lr: 0.002\n",
      "iteration: 485740 loss: 0.0009 lr: 0.002\n",
      "iteration: 485750 loss: 0.0009 lr: 0.002\n",
      "iteration: 485760 loss: 0.0014 lr: 0.002\n",
      "iteration: 485770 loss: 0.0013 lr: 0.002\n",
      "iteration: 485780 loss: 0.0011 lr: 0.002\n",
      "iteration: 485790 loss: 0.0008 lr: 0.002\n",
      "iteration: 485800 loss: 0.0011 lr: 0.002\n",
      "iteration: 485810 loss: 0.0012 lr: 0.002\n",
      "iteration: 485820 loss: 0.0011 lr: 0.002\n",
      "iteration: 485830 loss: 0.0010 lr: 0.002\n",
      "iteration: 485840 loss: 0.0011 lr: 0.002\n",
      "iteration: 485850 loss: 0.0008 lr: 0.002\n",
      "iteration: 485860 loss: 0.0008 lr: 0.002\n",
      "iteration: 485870 loss: 0.0010 lr: 0.002\n",
      "iteration: 485880 loss: 0.0008 lr: 0.002\n",
      "iteration: 485890 loss: 0.0014 lr: 0.002\n",
      "iteration: 485900 loss: 0.0009 lr: 0.002\n",
      "iteration: 485910 loss: 0.0008 lr: 0.002\n",
      "iteration: 485920 loss: 0.0011 lr: 0.002\n",
      "iteration: 485930 loss: 0.0016 lr: 0.002\n",
      "iteration: 485940 loss: 0.0011 lr: 0.002\n",
      "iteration: 485950 loss: 0.0022 lr: 0.002\n",
      "iteration: 485960 loss: 0.0013 lr: 0.002\n",
      "iteration: 485970 loss: 0.0016 lr: 0.002\n",
      "iteration: 485980 loss: 0.0013 lr: 0.002\n",
      "iteration: 485990 loss: 0.0008 lr: 0.002\n",
      "iteration: 486000 loss: 0.0013 lr: 0.002\n",
      "iteration: 486010 loss: 0.0008 lr: 0.002\n",
      "iteration: 486020 loss: 0.0028 lr: 0.002\n",
      "iteration: 486030 loss: 0.0014 lr: 0.002\n",
      "iteration: 486040 loss: 0.0014 lr: 0.002\n",
      "iteration: 486050 loss: 0.0011 lr: 0.002\n",
      "iteration: 486060 loss: 0.0012 lr: 0.002\n",
      "iteration: 486070 loss: 0.0011 lr: 0.002\n",
      "iteration: 486080 loss: 0.0009 lr: 0.002\n",
      "iteration: 486090 loss: 0.0013 lr: 0.002\n",
      "iteration: 486100 loss: 0.0014 lr: 0.002\n",
      "iteration: 486110 loss: 0.0009 lr: 0.002\n",
      "iteration: 486120 loss: 0.0011 lr: 0.002\n",
      "iteration: 486130 loss: 0.0012 lr: 0.002\n",
      "iteration: 486140 loss: 0.0012 lr: 0.002\n",
      "iteration: 486150 loss: 0.0012 lr: 0.002\n",
      "iteration: 486160 loss: 0.0011 lr: 0.002\n",
      "iteration: 486170 loss: 0.0008 lr: 0.002\n",
      "iteration: 486180 loss: 0.0011 lr: 0.002\n",
      "iteration: 486190 loss: 0.0011 lr: 0.002\n",
      "iteration: 486200 loss: 0.0010 lr: 0.002\n",
      "iteration: 486210 loss: 0.0017 lr: 0.002\n",
      "iteration: 486220 loss: 0.0010 lr: 0.002\n",
      "iteration: 486230 loss: 0.0007 lr: 0.002\n",
      "iteration: 486240 loss: 0.0011 lr: 0.002\n",
      "iteration: 486250 loss: 0.0013 lr: 0.002\n",
      "iteration: 486260 loss: 0.0011 lr: 0.002\n",
      "iteration: 486270 loss: 0.0010 lr: 0.002\n",
      "iteration: 486280 loss: 0.0009 lr: 0.002\n",
      "iteration: 486290 loss: 0.0011 lr: 0.002\n",
      "iteration: 486300 loss: 0.0010 lr: 0.002\n",
      "iteration: 486310 loss: 0.0014 lr: 0.002\n",
      "iteration: 486320 loss: 0.0011 lr: 0.002\n",
      "iteration: 486330 loss: 0.0009 lr: 0.002\n",
      "iteration: 486340 loss: 0.0014 lr: 0.002\n",
      "iteration: 486350 loss: 0.0011 lr: 0.002\n",
      "iteration: 486360 loss: 0.0012 lr: 0.002\n",
      "iteration: 486370 loss: 0.0011 lr: 0.002\n",
      "iteration: 486380 loss: 0.0011 lr: 0.002\n",
      "iteration: 486390 loss: 0.0018 lr: 0.002\n",
      "iteration: 486400 loss: 0.0021 lr: 0.002\n",
      "iteration: 486410 loss: 0.0012 lr: 0.002\n",
      "iteration: 486420 loss: 0.0015 lr: 0.002\n",
      "iteration: 486430 loss: 0.0019 lr: 0.002\n",
      "iteration: 486440 loss: 0.0018 lr: 0.002\n",
      "iteration: 486450 loss: 0.0007 lr: 0.002\n",
      "iteration: 486460 loss: 0.0007 lr: 0.002\n",
      "iteration: 486470 loss: 0.0011 lr: 0.002\n",
      "iteration: 486480 loss: 0.0013 lr: 0.002\n",
      "iteration: 486490 loss: 0.0012 lr: 0.002\n",
      "iteration: 486500 loss: 0.0011 lr: 0.002\n",
      "iteration: 486510 loss: 0.0009 lr: 0.002\n",
      "iteration: 486520 loss: 0.0010 lr: 0.002\n",
      "iteration: 486530 loss: 0.0008 lr: 0.002\n",
      "iteration: 486540 loss: 0.0013 lr: 0.002\n",
      "iteration: 486550 loss: 0.0010 lr: 0.002\n",
      "iteration: 486560 loss: 0.0011 lr: 0.002\n",
      "iteration: 486570 loss: 0.0012 lr: 0.002\n",
      "iteration: 486580 loss: 0.0014 lr: 0.002\n",
      "iteration: 486590 loss: 0.0012 lr: 0.002\n",
      "iteration: 486600 loss: 0.0014 lr: 0.002\n",
      "iteration: 486610 loss: 0.0010 lr: 0.002\n",
      "iteration: 486620 loss: 0.0019 lr: 0.002\n",
      "iteration: 486630 loss: 0.0013 lr: 0.002\n",
      "iteration: 486640 loss: 0.0010 lr: 0.002\n",
      "iteration: 486650 loss: 0.0016 lr: 0.002\n",
      "iteration: 486660 loss: 0.0010 lr: 0.002\n",
      "iteration: 486670 loss: 0.0012 lr: 0.002\n",
      "iteration: 486680 loss: 0.0014 lr: 0.002\n",
      "iteration: 486690 loss: 0.0012 lr: 0.002\n",
      "iteration: 486700 loss: 0.0008 lr: 0.002\n",
      "iteration: 486710 loss: 0.0010 lr: 0.002\n",
      "iteration: 486720 loss: 0.0010 lr: 0.002\n",
      "iteration: 486730 loss: 0.0012 lr: 0.002\n",
      "iteration: 486740 loss: 0.0012 lr: 0.002\n",
      "iteration: 486750 loss: 0.0010 lr: 0.002\n",
      "iteration: 486760 loss: 0.0010 lr: 0.002\n",
      "iteration: 486770 loss: 0.0009 lr: 0.002\n",
      "iteration: 486780 loss: 0.0011 lr: 0.002\n",
      "iteration: 486790 loss: 0.0010 lr: 0.002\n",
      "iteration: 486800 loss: 0.0009 lr: 0.002\n",
      "iteration: 486810 loss: 0.0016 lr: 0.002\n",
      "iteration: 486820 loss: 0.0013 lr: 0.002\n",
      "iteration: 486830 loss: 0.0009 lr: 0.002\n",
      "iteration: 486840 loss: 0.0016 lr: 0.002\n",
      "iteration: 486850 loss: 0.0013 lr: 0.002\n",
      "iteration: 486860 loss: 0.0015 lr: 0.002\n",
      "iteration: 486870 loss: 0.0009 lr: 0.002\n",
      "iteration: 486880 loss: 0.0016 lr: 0.002\n",
      "iteration: 486890 loss: 0.0011 lr: 0.002\n",
      "iteration: 486900 loss: 0.0011 lr: 0.002\n",
      "iteration: 486910 loss: 0.0011 lr: 0.002\n",
      "iteration: 486920 loss: 0.0014 lr: 0.002\n",
      "iteration: 486930 loss: 0.0011 lr: 0.002\n",
      "iteration: 486940 loss: 0.0017 lr: 0.002\n",
      "iteration: 486950 loss: 0.0012 lr: 0.002\n",
      "iteration: 486960 loss: 0.0013 lr: 0.002\n",
      "iteration: 486970 loss: 0.0010 lr: 0.002\n",
      "iteration: 486980 loss: 0.0015 lr: 0.002\n",
      "iteration: 486990 loss: 0.0009 lr: 0.002\n",
      "iteration: 487000 loss: 0.0014 lr: 0.002\n",
      "iteration: 487010 loss: 0.0014 lr: 0.002\n",
      "iteration: 487020 loss: 0.0011 lr: 0.002\n",
      "iteration: 487030 loss: 0.0010 lr: 0.002\n",
      "iteration: 487040 loss: 0.0007 lr: 0.002\n",
      "iteration: 487050 loss: 0.0009 lr: 0.002\n",
      "iteration: 487060 loss: 0.0007 lr: 0.002\n",
      "iteration: 487070 loss: 0.0010 lr: 0.002\n",
      "iteration: 487080 loss: 0.0011 lr: 0.002\n",
      "iteration: 487090 loss: 0.0020 lr: 0.002\n",
      "iteration: 487100 loss: 0.0010 lr: 0.002\n",
      "iteration: 487110 loss: 0.0012 lr: 0.002\n",
      "iteration: 487120 loss: 0.0012 lr: 0.002\n",
      "iteration: 487130 loss: 0.0011 lr: 0.002\n",
      "iteration: 487140 loss: 0.0013 lr: 0.002\n",
      "iteration: 487150 loss: 0.0007 lr: 0.002\n",
      "iteration: 487160 loss: 0.0012 lr: 0.002\n",
      "iteration: 487170 loss: 0.0009 lr: 0.002\n",
      "iteration: 487180 loss: 0.0012 lr: 0.002\n",
      "iteration: 487190 loss: 0.0014 lr: 0.002\n",
      "iteration: 487200 loss: 0.0010 lr: 0.002\n",
      "iteration: 487210 loss: 0.0011 lr: 0.002\n",
      "iteration: 487220 loss: 0.0011 lr: 0.002\n",
      "iteration: 487230 loss: 0.0012 lr: 0.002\n",
      "iteration: 487240 loss: 0.0011 lr: 0.002\n",
      "iteration: 487250 loss: 0.0010 lr: 0.002\n",
      "iteration: 487260 loss: 0.0008 lr: 0.002\n",
      "iteration: 487270 loss: 0.0015 lr: 0.002\n",
      "iteration: 487280 loss: 0.0011 lr: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 487290 loss: 0.0011 lr: 0.002\n",
      "iteration: 487300 loss: 0.0018 lr: 0.002\n",
      "iteration: 487310 loss: 0.0012 lr: 0.002\n",
      "iteration: 487320 loss: 0.0020 lr: 0.002\n",
      "iteration: 487330 loss: 0.0011 lr: 0.002\n",
      "iteration: 487340 loss: 0.0018 lr: 0.002\n",
      "iteration: 487350 loss: 0.0016 lr: 0.002\n",
      "iteration: 487360 loss: 0.0016 lr: 0.002\n",
      "iteration: 487370 loss: 0.0012 lr: 0.002\n",
      "iteration: 487380 loss: 0.0017 lr: 0.002\n",
      "iteration: 487390 loss: 0.0013 lr: 0.002\n",
      "iteration: 487400 loss: 0.0012 lr: 0.002\n",
      "iteration: 487410 loss: 0.0010 lr: 0.002\n",
      "iteration: 487420 loss: 0.0013 lr: 0.002\n",
      "iteration: 487430 loss: 0.0011 lr: 0.002\n",
      "iteration: 487440 loss: 0.0014 lr: 0.002\n",
      "iteration: 487450 loss: 0.0016 lr: 0.002\n",
      "iteration: 487460 loss: 0.0012 lr: 0.002\n",
      "iteration: 487470 loss: 0.0011 lr: 0.002\n",
      "iteration: 487480 loss: 0.0013 lr: 0.002\n",
      "iteration: 487490 loss: 0.0013 lr: 0.002\n",
      "iteration: 487500 loss: 0.0006 lr: 0.002\n",
      "iteration: 487510 loss: 0.0014 lr: 0.002\n",
      "iteration: 487520 loss: 0.0015 lr: 0.002\n",
      "iteration: 487530 loss: 0.0012 lr: 0.002\n",
      "iteration: 487540 loss: 0.0008 lr: 0.002\n",
      "iteration: 487550 loss: 0.0013 lr: 0.002\n",
      "iteration: 487560 loss: 0.0025 lr: 0.002\n",
      "iteration: 487570 loss: 0.0010 lr: 0.002\n",
      "iteration: 487580 loss: 0.0011 lr: 0.002\n",
      "iteration: 487590 loss: 0.0011 lr: 0.002\n",
      "iteration: 487600 loss: 0.0009 lr: 0.002\n",
      "iteration: 487610 loss: 0.0013 lr: 0.002\n",
      "iteration: 487620 loss: 0.0008 lr: 0.002\n",
      "iteration: 487630 loss: 0.0010 lr: 0.002\n",
      "iteration: 487640 loss: 0.0008 lr: 0.002\n",
      "iteration: 487650 loss: 0.0009 lr: 0.002\n",
      "iteration: 487660 loss: 0.0012 lr: 0.002\n",
      "iteration: 487670 loss: 0.0007 lr: 0.002\n",
      "iteration: 487680 loss: 0.0016 lr: 0.002\n",
      "iteration: 487690 loss: 0.0015 lr: 0.002\n",
      "iteration: 487700 loss: 0.0010 lr: 0.002\n",
      "iteration: 487710 loss: 0.0012 lr: 0.002\n",
      "iteration: 487720 loss: 0.0014 lr: 0.002\n",
      "iteration: 487730 loss: 0.0011 lr: 0.002\n",
      "iteration: 487740 loss: 0.0009 lr: 0.002\n",
      "iteration: 487750 loss: 0.0010 lr: 0.002\n",
      "iteration: 487760 loss: 0.0009 lr: 0.002\n",
      "iteration: 487770 loss: 0.0014 lr: 0.002\n",
      "iteration: 487780 loss: 0.0009 lr: 0.002\n",
      "iteration: 487790 loss: 0.0010 lr: 0.002\n",
      "iteration: 487800 loss: 0.0014 lr: 0.002\n",
      "iteration: 487810 loss: 0.0011 lr: 0.002\n",
      "iteration: 487820 loss: 0.0008 lr: 0.002\n",
      "iteration: 487830 loss: 0.0008 lr: 0.002\n",
      "iteration: 487840 loss: 0.0011 lr: 0.002\n",
      "iteration: 487850 loss: 0.0013 lr: 0.002\n",
      "iteration: 487860 loss: 0.0012 lr: 0.002\n",
      "iteration: 487870 loss: 0.0010 lr: 0.002\n",
      "iteration: 487880 loss: 0.0014 lr: 0.002\n",
      "iteration: 487890 loss: 0.0011 lr: 0.002\n",
      "iteration: 487900 loss: 0.0011 lr: 0.002\n",
      "iteration: 487910 loss: 0.0013 lr: 0.002\n",
      "iteration: 487920 loss: 0.0010 lr: 0.002\n",
      "iteration: 487930 loss: 0.0014 lr: 0.002\n",
      "iteration: 487940 loss: 0.0008 lr: 0.002\n",
      "iteration: 487950 loss: 0.0010 lr: 0.002\n",
      "iteration: 487960 loss: 0.0010 lr: 0.002\n",
      "iteration: 487970 loss: 0.0014 lr: 0.002\n",
      "iteration: 487980 loss: 0.0011 lr: 0.002\n",
      "iteration: 487990 loss: 0.0012 lr: 0.002\n",
      "iteration: 488000 loss: 0.0013 lr: 0.002\n",
      "iteration: 488010 loss: 0.0012 lr: 0.002\n",
      "iteration: 488020 loss: 0.0013 lr: 0.002\n",
      "iteration: 488030 loss: 0.0012 lr: 0.002\n",
      "iteration: 488040 loss: 0.0011 lr: 0.002\n",
      "iteration: 488050 loss: 0.0009 lr: 0.002\n",
      "iteration: 488060 loss: 0.0011 lr: 0.002\n",
      "iteration: 488070 loss: 0.0012 lr: 0.002\n",
      "iteration: 488080 loss: 0.0016 lr: 0.002\n",
      "iteration: 488090 loss: 0.0011 lr: 0.002\n",
      "iteration: 488100 loss: 0.0009 lr: 0.002\n",
      "iteration: 488110 loss: 0.0009 lr: 0.002\n",
      "iteration: 488120 loss: 0.0012 lr: 0.002\n",
      "iteration: 488130 loss: 0.0012 lr: 0.002\n",
      "iteration: 488140 loss: 0.0011 lr: 0.002\n",
      "iteration: 488150 loss: 0.0015 lr: 0.002\n",
      "iteration: 488160 loss: 0.0010 lr: 0.002\n",
      "iteration: 488170 loss: 0.0015 lr: 0.002\n",
      "iteration: 488180 loss: 0.0008 lr: 0.002\n",
      "iteration: 488190 loss: 0.0014 lr: 0.002\n",
      "iteration: 488200 loss: 0.0012 lr: 0.002\n",
      "iteration: 488210 loss: 0.0016 lr: 0.002\n",
      "iteration: 488220 loss: 0.0008 lr: 0.002\n",
      "iteration: 488230 loss: 0.0013 lr: 0.002\n",
      "iteration: 488240 loss: 0.0010 lr: 0.002\n",
      "iteration: 488250 loss: 0.0009 lr: 0.002\n",
      "iteration: 488260 loss: 0.0010 lr: 0.002\n",
      "iteration: 488270 loss: 0.0012 lr: 0.002\n",
      "iteration: 488280 loss: 0.0009 lr: 0.002\n",
      "iteration: 488290 loss: 0.0011 lr: 0.002\n",
      "iteration: 488300 loss: 0.0010 lr: 0.002\n",
      "iteration: 488310 loss: 0.0016 lr: 0.002\n",
      "iteration: 488320 loss: 0.0014 lr: 0.002\n",
      "iteration: 488330 loss: 0.0008 lr: 0.002\n",
      "iteration: 488340 loss: 0.0028 lr: 0.002\n",
      "iteration: 488350 loss: 0.0011 lr: 0.002\n",
      "iteration: 488360 loss: 0.0011 lr: 0.002\n",
      "iteration: 488370 loss: 0.0011 lr: 0.002\n",
      "iteration: 488380 loss: 0.0011 lr: 0.002\n",
      "iteration: 488390 loss: 0.0035 lr: 0.002\n",
      "iteration: 488400 loss: 0.0016 lr: 0.002\n",
      "iteration: 488410 loss: 0.0013 lr: 0.002\n",
      "iteration: 488420 loss: 0.0011 lr: 0.002\n",
      "iteration: 488430 loss: 0.0008 lr: 0.002\n",
      "iteration: 488440 loss: 0.0013 lr: 0.002\n",
      "iteration: 488450 loss: 0.0010 lr: 0.002\n",
      "iteration: 488460 loss: 0.0010 lr: 0.002\n",
      "iteration: 488470 loss: 0.0013 lr: 0.002\n",
      "iteration: 488480 loss: 0.0011 lr: 0.002\n",
      "iteration: 488490 loss: 0.0012 lr: 0.002\n",
      "iteration: 488500 loss: 0.0007 lr: 0.002\n",
      "iteration: 488510 loss: 0.0019 lr: 0.002\n",
      "iteration: 488520 loss: 0.0010 lr: 0.002\n",
      "iteration: 488530 loss: 0.0015 lr: 0.002\n",
      "iteration: 488540 loss: 0.0012 lr: 0.002\n",
      "iteration: 488550 loss: 0.0010 lr: 0.002\n",
      "iteration: 488560 loss: 0.0014 lr: 0.002\n",
      "iteration: 488570 loss: 0.0012 lr: 0.002\n",
      "iteration: 488580 loss: 0.0008 lr: 0.002\n",
      "iteration: 488590 loss: 0.0014 lr: 0.002\n",
      "iteration: 488600 loss: 0.0010 lr: 0.002\n",
      "iteration: 488610 loss: 0.0010 lr: 0.002\n",
      "iteration: 488620 loss: 0.0011 lr: 0.002\n",
      "iteration: 488630 loss: 0.0010 lr: 0.002\n",
      "iteration: 488640 loss: 0.0012 lr: 0.002\n",
      "iteration: 488650 loss: 0.0013 lr: 0.002\n",
      "iteration: 488660 loss: 0.0008 lr: 0.002\n",
      "iteration: 488670 loss: 0.0009 lr: 0.002\n",
      "iteration: 488680 loss: 0.0012 lr: 0.002\n",
      "iteration: 488690 loss: 0.0013 lr: 0.002\n",
      "iteration: 488700 loss: 0.0011 lr: 0.002\n",
      "iteration: 488710 loss: 0.0013 lr: 0.002\n",
      "iteration: 488720 loss: 0.0016 lr: 0.002\n",
      "iteration: 488730 loss: 0.0011 lr: 0.002\n",
      "iteration: 488740 loss: 0.0009 lr: 0.002\n",
      "iteration: 488750 loss: 0.0010 lr: 0.002\n",
      "iteration: 488760 loss: 0.0008 lr: 0.002\n",
      "iteration: 488770 loss: 0.0010 lr: 0.002\n",
      "iteration: 488780 loss: 0.0013 lr: 0.002\n",
      "iteration: 488790 loss: 0.0015 lr: 0.002\n",
      "iteration: 488800 loss: 0.0014 lr: 0.002\n",
      "iteration: 488810 loss: 0.0009 lr: 0.002\n",
      "iteration: 488820 loss: 0.0011 lr: 0.002\n",
      "iteration: 488830 loss: 0.0015 lr: 0.002\n",
      "iteration: 488840 loss: 0.0011 lr: 0.002\n",
      "iteration: 488850 loss: 0.0016 lr: 0.002\n",
      "iteration: 488860 loss: 0.0013 lr: 0.002\n",
      "iteration: 488870 loss: 0.0013 lr: 0.002\n",
      "iteration: 488880 loss: 0.0007 lr: 0.002\n",
      "iteration: 488890 loss: 0.0010 lr: 0.002\n",
      "iteration: 488900 loss: 0.0008 lr: 0.002\n",
      "iteration: 488910 loss: 0.0010 lr: 0.002\n",
      "iteration: 488920 loss: 0.0014 lr: 0.002\n",
      "iteration: 488930 loss: 0.0014 lr: 0.002\n",
      "iteration: 488940 loss: 0.0011 lr: 0.002\n",
      "iteration: 488950 loss: 0.0012 lr: 0.002\n",
      "iteration: 488960 loss: 0.0015 lr: 0.002\n",
      "iteration: 488970 loss: 0.0019 lr: 0.002\n",
      "iteration: 488980 loss: 0.0012 lr: 0.002\n",
      "iteration: 488990 loss: 0.0009 lr: 0.002\n",
      "iteration: 489000 loss: 0.0009 lr: 0.002\n",
      "iteration: 489010 loss: 0.0011 lr: 0.002\n",
      "iteration: 489020 loss: 0.0013 lr: 0.002\n",
      "iteration: 489030 loss: 0.0010 lr: 0.002\n",
      "iteration: 489040 loss: 0.0015 lr: 0.002\n",
      "iteration: 489050 loss: 0.0011 lr: 0.002\n",
      "iteration: 489060 loss: 0.0020 lr: 0.002\n",
      "iteration: 489070 loss: 0.0013 lr: 0.002\n",
      "iteration: 489080 loss: 0.0011 lr: 0.002\n",
      "iteration: 489090 loss: 0.0012 lr: 0.002\n",
      "iteration: 489100 loss: 0.0009 lr: 0.002\n",
      "iteration: 489110 loss: 0.0011 lr: 0.002\n",
      "iteration: 489120 loss: 0.0012 lr: 0.002\n",
      "iteration: 489130 loss: 0.0013 lr: 0.002\n",
      "iteration: 489140 loss: 0.0013 lr: 0.002\n",
      "iteration: 489150 loss: 0.0014 lr: 0.002\n",
      "iteration: 489160 loss: 0.0013 lr: 0.002\n",
      "iteration: 489170 loss: 0.0016 lr: 0.002\n",
      "iteration: 489180 loss: 0.0015 lr: 0.002\n",
      "iteration: 489190 loss: 0.0014 lr: 0.002\n",
      "iteration: 489200 loss: 0.0010 lr: 0.002\n",
      "iteration: 489210 loss: 0.0013 lr: 0.002\n",
      "iteration: 489220 loss: 0.0028 lr: 0.002\n",
      "iteration: 489230 loss: 0.0011 lr: 0.002\n",
      "iteration: 489240 loss: 0.0012 lr: 0.002\n",
      "iteration: 489250 loss: 0.0015 lr: 0.002\n",
      "iteration: 489260 loss: 0.0012 lr: 0.002\n",
      "iteration: 489270 loss: 0.0009 lr: 0.002\n",
      "iteration: 489280 loss: 0.0012 lr: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 489290 loss: 0.0014 lr: 0.002\n",
      "iteration: 489300 loss: 0.0009 lr: 0.002\n",
      "iteration: 489310 loss: 0.0012 lr: 0.002\n",
      "iteration: 489320 loss: 0.0012 lr: 0.002\n",
      "iteration: 489330 loss: 0.0010 lr: 0.002\n",
      "iteration: 489340 loss: 0.0010 lr: 0.002\n",
      "iteration: 489350 loss: 0.0010 lr: 0.002\n",
      "iteration: 489360 loss: 0.0013 lr: 0.002\n",
      "iteration: 489370 loss: 0.0009 lr: 0.002\n",
      "iteration: 489380 loss: 0.0011 lr: 0.002\n",
      "iteration: 489390 loss: 0.0012 lr: 0.002\n",
      "iteration: 489400 loss: 0.0009 lr: 0.002\n",
      "iteration: 489410 loss: 0.0017 lr: 0.002\n",
      "iteration: 489420 loss: 0.0013 lr: 0.002\n",
      "iteration: 489430 loss: 0.0013 lr: 0.002\n",
      "iteration: 489440 loss: 0.0014 lr: 0.002\n",
      "iteration: 489450 loss: 0.0011 lr: 0.002\n",
      "iteration: 489460 loss: 0.0011 lr: 0.002\n",
      "iteration: 489470 loss: 0.0010 lr: 0.002\n",
      "iteration: 489480 loss: 0.0009 lr: 0.002\n",
      "iteration: 489490 loss: 0.0015 lr: 0.002\n",
      "iteration: 489500 loss: 0.0013 lr: 0.002\n",
      "iteration: 489510 loss: 0.0012 lr: 0.002\n",
      "iteration: 489520 loss: 0.0013 lr: 0.002\n",
      "iteration: 489530 loss: 0.0015 lr: 0.002\n",
      "iteration: 489540 loss: 0.0012 lr: 0.002\n",
      "iteration: 489550 loss: 0.0009 lr: 0.002\n",
      "iteration: 489560 loss: 0.0011 lr: 0.002\n",
      "iteration: 489570 loss: 0.0008 lr: 0.002\n",
      "iteration: 489580 loss: 0.0013 lr: 0.002\n",
      "iteration: 489590 loss: 0.0010 lr: 0.002\n",
      "iteration: 489600 loss: 0.0012 lr: 0.002\n",
      "iteration: 489610 loss: 0.0017 lr: 0.002\n",
      "iteration: 489620 loss: 0.0017 lr: 0.002\n",
      "iteration: 489630 loss: 0.0013 lr: 0.002\n",
      "iteration: 489640 loss: 0.0015 lr: 0.002\n",
      "iteration: 489650 loss: 0.0010 lr: 0.002\n",
      "iteration: 489660 loss: 0.0016 lr: 0.002\n",
      "iteration: 489670 loss: 0.0016 lr: 0.002\n",
      "iteration: 489680 loss: 0.0009 lr: 0.002\n",
      "iteration: 489690 loss: 0.0010 lr: 0.002\n",
      "iteration: 489700 loss: 0.0007 lr: 0.002\n",
      "iteration: 489710 loss: 0.0010 lr: 0.002\n",
      "iteration: 489720 loss: 0.0019 lr: 0.002\n",
      "iteration: 489730 loss: 0.0007 lr: 0.002\n",
      "iteration: 489740 loss: 0.0011 lr: 0.002\n",
      "iteration: 489750 loss: 0.0009 lr: 0.002\n",
      "iteration: 489760 loss: 0.0008 lr: 0.002\n",
      "iteration: 489770 loss: 0.0009 lr: 0.002\n",
      "iteration: 489780 loss: 0.0010 lr: 0.002\n",
      "iteration: 489790 loss: 0.0012 lr: 0.002\n",
      "iteration: 489800 loss: 0.0014 lr: 0.002\n",
      "iteration: 489810 loss: 0.0021 lr: 0.002\n",
      "iteration: 489820 loss: 0.0012 lr: 0.002\n",
      "iteration: 489830 loss: 0.0013 lr: 0.002\n",
      "iteration: 489840 loss: 0.0009 lr: 0.002\n",
      "iteration: 489850 loss: 0.0011 lr: 0.002\n",
      "iteration: 489860 loss: 0.0011 lr: 0.002\n",
      "iteration: 489870 loss: 0.0029 lr: 0.002\n",
      "iteration: 489880 loss: 0.0011 lr: 0.002\n",
      "iteration: 489890 loss: 0.0008 lr: 0.002\n",
      "iteration: 489900 loss: 0.0012 lr: 0.002\n",
      "iteration: 489910 loss: 0.0009 lr: 0.002\n",
      "iteration: 489920 loss: 0.0010 lr: 0.002\n",
      "iteration: 489930 loss: 0.0017 lr: 0.002\n",
      "iteration: 489940 loss: 0.0010 lr: 0.002\n",
      "iteration: 489950 loss: 0.0011 lr: 0.002\n",
      "iteration: 489960 loss: 0.0011 lr: 0.002\n",
      "iteration: 489970 loss: 0.0013 lr: 0.002\n",
      "iteration: 489980 loss: 0.0010 lr: 0.002\n",
      "iteration: 489990 loss: 0.0010 lr: 0.002\n",
      "iteration: 490000 loss: 0.0015 lr: 0.002\n",
      "iteration: 490010 loss: 0.0013 lr: 0.002\n",
      "iteration: 490020 loss: 0.0019 lr: 0.002\n",
      "iteration: 490030 loss: 0.0024 lr: 0.002\n",
      "iteration: 490040 loss: 0.0011 lr: 0.002\n",
      "iteration: 490050 loss: 0.0010 lr: 0.002\n",
      "iteration: 490060 loss: 0.0017 lr: 0.002\n",
      "iteration: 490070 loss: 0.0011 lr: 0.002\n",
      "iteration: 490080 loss: 0.0011 lr: 0.002\n",
      "iteration: 490090 loss: 0.0009 lr: 0.002\n",
      "iteration: 490100 loss: 0.0013 lr: 0.002\n",
      "iteration: 490110 loss: 0.0008 lr: 0.002\n",
      "iteration: 490120 loss: 0.0013 lr: 0.002\n",
      "iteration: 490130 loss: 0.0008 lr: 0.002\n",
      "iteration: 490140 loss: 0.0013 lr: 0.002\n",
      "iteration: 490150 loss: 0.0012 lr: 0.002\n",
      "iteration: 490160 loss: 0.0012 lr: 0.002\n",
      "iteration: 490170 loss: 0.0024 lr: 0.002\n",
      "iteration: 490180 loss: 0.0017 lr: 0.002\n",
      "iteration: 490190 loss: 0.0010 lr: 0.002\n",
      "iteration: 490200 loss: 0.0010 lr: 0.002\n",
      "iteration: 490210 loss: 0.0013 lr: 0.002\n",
      "iteration: 490220 loss: 0.0012 lr: 0.002\n",
      "iteration: 490230 loss: 0.0009 lr: 0.002\n",
      "iteration: 490240 loss: 0.0010 lr: 0.002\n",
      "iteration: 490250 loss: 0.0013 lr: 0.002\n",
      "iteration: 490260 loss: 0.0013 lr: 0.002\n",
      "iteration: 490270 loss: 0.0014 lr: 0.002\n",
      "iteration: 490280 loss: 0.0012 lr: 0.002\n",
      "iteration: 490290 loss: 0.0010 lr: 0.002\n",
      "iteration: 490300 loss: 0.0008 lr: 0.002\n",
      "iteration: 490310 loss: 0.0011 lr: 0.002\n",
      "iteration: 490320 loss: 0.0011 lr: 0.002\n",
      "iteration: 490330 loss: 0.0011 lr: 0.002\n",
      "iteration: 490340 loss: 0.0010 lr: 0.002\n",
      "iteration: 490350 loss: 0.0010 lr: 0.002\n",
      "iteration: 490360 loss: 0.0012 lr: 0.002\n",
      "iteration: 490370 loss: 0.0012 lr: 0.002\n",
      "iteration: 490380 loss: 0.0013 lr: 0.002\n",
      "iteration: 490390 loss: 0.0010 lr: 0.002\n",
      "iteration: 490400 loss: 0.0016 lr: 0.002\n",
      "iteration: 490410 loss: 0.0012 lr: 0.002\n",
      "iteration: 490420 loss: 0.0012 lr: 0.002\n",
      "iteration: 490430 loss: 0.0009 lr: 0.002\n",
      "iteration: 490440 loss: 0.0012 lr: 0.002\n",
      "iteration: 490450 loss: 0.0006 lr: 0.002\n",
      "iteration: 490460 loss: 0.0010 lr: 0.002\n",
      "iteration: 490470 loss: 0.0012 lr: 0.002\n",
      "iteration: 490480 loss: 0.0015 lr: 0.002\n",
      "iteration: 490490 loss: 0.0011 lr: 0.002\n",
      "iteration: 490500 loss: 0.0008 lr: 0.002\n",
      "iteration: 490510 loss: 0.0011 lr: 0.002\n",
      "iteration: 490520 loss: 0.0013 lr: 0.002\n",
      "iteration: 490530 loss: 0.0010 lr: 0.002\n",
      "iteration: 490540 loss: 0.0009 lr: 0.002\n",
      "iteration: 490550 loss: 0.0013 lr: 0.002\n",
      "iteration: 490560 loss: 0.0012 lr: 0.002\n",
      "iteration: 490570 loss: 0.0011 lr: 0.002\n",
      "iteration: 490580 loss: 0.0011 lr: 0.002\n",
      "iteration: 490590 loss: 0.0010 lr: 0.002\n",
      "iteration: 490600 loss: 0.0014 lr: 0.002\n",
      "iteration: 490610 loss: 0.0008 lr: 0.002\n",
      "iteration: 490620 loss: 0.0011 lr: 0.002\n",
      "iteration: 490630 loss: 0.0008 lr: 0.002\n",
      "iteration: 490640 loss: 0.0014 lr: 0.002\n",
      "iteration: 490650 loss: 0.0010 lr: 0.002\n",
      "iteration: 490660 loss: 0.0010 lr: 0.002\n",
      "iteration: 490670 loss: 0.0010 lr: 0.002\n",
      "iteration: 490680 loss: 0.0010 lr: 0.002\n",
      "iteration: 490690 loss: 0.0009 lr: 0.002\n",
      "iteration: 490700 loss: 0.0013 lr: 0.002\n",
      "iteration: 490710 loss: 0.0019 lr: 0.002\n",
      "iteration: 490720 loss: 0.0012 lr: 0.002\n",
      "iteration: 490730 loss: 0.0009 lr: 0.002\n",
      "iteration: 490740 loss: 0.0011 lr: 0.002\n",
      "iteration: 490750 loss: 0.0011 lr: 0.002\n",
      "iteration: 490760 loss: 0.0009 lr: 0.002\n",
      "iteration: 490770 loss: 0.0017 lr: 0.002\n",
      "iteration: 490780 loss: 0.0007 lr: 0.002\n",
      "iteration: 490790 loss: 0.0014 lr: 0.002\n",
      "iteration: 490800 loss: 0.0009 lr: 0.002\n",
      "iteration: 490810 loss: 0.0013 lr: 0.002\n",
      "iteration: 490820 loss: 0.0019 lr: 0.002\n",
      "iteration: 490830 loss: 0.0013 lr: 0.002\n",
      "iteration: 490840 loss: 0.0010 lr: 0.002\n",
      "iteration: 490850 loss: 0.0011 lr: 0.002\n",
      "iteration: 490860 loss: 0.0014 lr: 0.002\n",
      "iteration: 490870 loss: 0.0010 lr: 0.002\n",
      "iteration: 490880 loss: 0.0014 lr: 0.002\n",
      "iteration: 490890 loss: 0.0009 lr: 0.002\n",
      "iteration: 490900 loss: 0.0013 lr: 0.002\n",
      "iteration: 490910 loss: 0.0010 lr: 0.002\n",
      "iteration: 490920 loss: 0.0010 lr: 0.002\n",
      "iteration: 490930 loss: 0.0009 lr: 0.002\n",
      "iteration: 490940 loss: 0.0013 lr: 0.002\n",
      "iteration: 490950 loss: 0.0010 lr: 0.002\n",
      "iteration: 490960 loss: 0.0012 lr: 0.002\n",
      "iteration: 490970 loss: 0.0014 lr: 0.002\n",
      "iteration: 490980 loss: 0.0013 lr: 0.002\n",
      "iteration: 490990 loss: 0.0011 lr: 0.002\n",
      "iteration: 491000 loss: 0.0010 lr: 0.002\n",
      "iteration: 491010 loss: 0.0011 lr: 0.002\n",
      "iteration: 491020 loss: 0.0026 lr: 0.002\n",
      "iteration: 491030 loss: 0.0015 lr: 0.002\n",
      "iteration: 491040 loss: 0.0009 lr: 0.002\n",
      "iteration: 491050 loss: 0.0011 lr: 0.002\n",
      "iteration: 491060 loss: 0.0008 lr: 0.002\n",
      "iteration: 491070 loss: 0.0024 lr: 0.002\n",
      "iteration: 491080 loss: 0.0016 lr: 0.002\n",
      "iteration: 491090 loss: 0.0012 lr: 0.002\n",
      "iteration: 491100 loss: 0.0011 lr: 0.002\n",
      "iteration: 491110 loss: 0.0014 lr: 0.002\n",
      "iteration: 491120 loss: 0.0011 lr: 0.002\n",
      "iteration: 491130 loss: 0.0014 lr: 0.002\n",
      "iteration: 491140 loss: 0.0011 lr: 0.002\n",
      "iteration: 491150 loss: 0.0014 lr: 0.002\n",
      "iteration: 491160 loss: 0.0008 lr: 0.002\n",
      "iteration: 491170 loss: 0.0013 lr: 0.002\n",
      "iteration: 491180 loss: 0.0018 lr: 0.002\n",
      "iteration: 491190 loss: 0.0012 lr: 0.002\n",
      "iteration: 491200 loss: 0.0010 lr: 0.002\n",
      "iteration: 491210 loss: 0.0013 lr: 0.002\n",
      "iteration: 491220 loss: 0.0018 lr: 0.002\n",
      "iteration: 491230 loss: 0.0009 lr: 0.002\n",
      "iteration: 491240 loss: 0.0010 lr: 0.002\n",
      "iteration: 491250 loss: 0.0010 lr: 0.002\n",
      "iteration: 491260 loss: 0.0013 lr: 0.002\n",
      "iteration: 491270 loss: 0.0013 lr: 0.002\n",
      "iteration: 491280 loss: 0.0013 lr: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 491290 loss: 0.0012 lr: 0.002\n",
      "iteration: 491300 loss: 0.0019 lr: 0.002\n",
      "iteration: 491310 loss: 0.0011 lr: 0.002\n",
      "iteration: 491320 loss: 0.0012 lr: 0.002\n",
      "iteration: 491330 loss: 0.0010 lr: 0.002\n",
      "iteration: 491340 loss: 0.0008 lr: 0.002\n",
      "iteration: 491350 loss: 0.0011 lr: 0.002\n",
      "iteration: 491360 loss: 0.0010 lr: 0.002\n",
      "iteration: 491370 loss: 0.0014 lr: 0.002\n",
      "iteration: 491380 loss: 0.0012 lr: 0.002\n",
      "iteration: 491390 loss: 0.0011 lr: 0.002\n",
      "iteration: 491400 loss: 0.0011 lr: 0.002\n",
      "iteration: 491410 loss: 0.0013 lr: 0.002\n",
      "iteration: 491420 loss: 0.0008 lr: 0.002\n",
      "iteration: 491430 loss: 0.0013 lr: 0.002\n",
      "iteration: 491440 loss: 0.0009 lr: 0.002\n",
      "iteration: 491450 loss: 0.0011 lr: 0.002\n",
      "iteration: 491460 loss: 0.0009 lr: 0.002\n",
      "iteration: 491470 loss: 0.0018 lr: 0.002\n",
      "iteration: 491480 loss: 0.0013 lr: 0.002\n",
      "iteration: 491490 loss: 0.0011 lr: 0.002\n",
      "iteration: 491500 loss: 0.0013 lr: 0.002\n",
      "iteration: 491510 loss: 0.0011 lr: 0.002\n",
      "iteration: 491520 loss: 0.0015 lr: 0.002\n",
      "iteration: 491530 loss: 0.0011 lr: 0.002\n",
      "iteration: 491540 loss: 0.0015 lr: 0.002\n",
      "iteration: 491550 loss: 0.0013 lr: 0.002\n",
      "iteration: 491560 loss: 0.0009 lr: 0.002\n",
      "iteration: 491570 loss: 0.0010 lr: 0.002\n",
      "iteration: 491580 loss: 0.0010 lr: 0.002\n",
      "iteration: 491590 loss: 0.0012 lr: 0.002\n",
      "iteration: 491600 loss: 0.0020 lr: 0.002\n",
      "iteration: 491610 loss: 0.0020 lr: 0.002\n",
      "iteration: 491620 loss: 0.0011 lr: 0.002\n",
      "iteration: 491630 loss: 0.0009 lr: 0.002\n",
      "iteration: 491640 loss: 0.0013 lr: 0.002\n",
      "iteration: 491650 loss: 0.0012 lr: 0.002\n",
      "iteration: 491660 loss: 0.0014 lr: 0.002\n",
      "iteration: 491670 loss: 0.0010 lr: 0.002\n",
      "iteration: 491680 loss: 0.0012 lr: 0.002\n",
      "iteration: 491690 loss: 0.0012 lr: 0.002\n",
      "iteration: 491700 loss: 0.0011 lr: 0.002\n",
      "iteration: 491710 loss: 0.0011 lr: 0.002\n",
      "iteration: 491720 loss: 0.0015 lr: 0.002\n",
      "iteration: 491730 loss: 0.0022 lr: 0.002\n",
      "iteration: 491740 loss: 0.0010 lr: 0.002\n",
      "iteration: 491750 loss: 0.0015 lr: 0.002\n",
      "iteration: 491760 loss: 0.0010 lr: 0.002\n",
      "iteration: 491770 loss: 0.0010 lr: 0.002\n",
      "iteration: 491780 loss: 0.0008 lr: 0.002\n",
      "iteration: 491790 loss: 0.0024 lr: 0.002\n",
      "iteration: 491800 loss: 0.0011 lr: 0.002\n",
      "iteration: 491810 loss: 0.0011 lr: 0.002\n",
      "iteration: 491820 loss: 0.0018 lr: 0.002\n",
      "iteration: 491830 loss: 0.0016 lr: 0.002\n",
      "iteration: 491840 loss: 0.0025 lr: 0.002\n",
      "iteration: 491850 loss: 0.0009 lr: 0.002\n",
      "iteration: 491860 loss: 0.0011 lr: 0.002\n",
      "iteration: 491870 loss: 0.0018 lr: 0.002\n",
      "iteration: 491880 loss: 0.0010 lr: 0.002\n",
      "iteration: 491890 loss: 0.0011 lr: 0.002\n",
      "iteration: 491900 loss: 0.0012 lr: 0.002\n",
      "iteration: 491910 loss: 0.0013 lr: 0.002\n",
      "iteration: 491920 loss: 0.0013 lr: 0.002\n",
      "iteration: 491930 loss: 0.0011 lr: 0.002\n",
      "iteration: 491940 loss: 0.0016 lr: 0.002\n",
      "iteration: 491950 loss: 0.0010 lr: 0.002\n",
      "iteration: 491960 loss: 0.0010 lr: 0.002\n",
      "iteration: 491970 loss: 0.0014 lr: 0.002\n",
      "iteration: 491980 loss: 0.0010 lr: 0.002\n",
      "iteration: 491990 loss: 0.0010 lr: 0.002\n",
      "iteration: 492000 loss: 0.0013 lr: 0.002\n",
      "iteration: 492010 loss: 0.0012 lr: 0.002\n",
      "iteration: 492020 loss: 0.0008 lr: 0.002\n",
      "iteration: 492030 loss: 0.0011 lr: 0.002\n",
      "iteration: 492040 loss: 0.0009 lr: 0.002\n",
      "iteration: 492050 loss: 0.0013 lr: 0.002\n",
      "iteration: 492060 loss: 0.0009 lr: 0.002\n",
      "iteration: 492070 loss: 0.0009 lr: 0.002\n",
      "iteration: 492080 loss: 0.0013 lr: 0.002\n",
      "iteration: 492090 loss: 0.0010 lr: 0.002\n",
      "iteration: 492100 loss: 0.0013 lr: 0.002\n",
      "iteration: 492110 loss: 0.0010 lr: 0.002\n",
      "iteration: 492120 loss: 0.0012 lr: 0.002\n",
      "iteration: 492130 loss: 0.0011 lr: 0.002\n",
      "iteration: 492140 loss: 0.0013 lr: 0.002\n",
      "iteration: 492150 loss: 0.0014 lr: 0.002\n",
      "iteration: 492160 loss: 0.0009 lr: 0.002\n",
      "iteration: 492170 loss: 0.0009 lr: 0.002\n",
      "iteration: 492180 loss: 0.0011 lr: 0.002\n",
      "iteration: 492190 loss: 0.0010 lr: 0.002\n",
      "iteration: 492200 loss: 0.0010 lr: 0.002\n",
      "iteration: 492210 loss: 0.0014 lr: 0.002\n",
      "iteration: 492220 loss: 0.0011 lr: 0.002\n",
      "iteration: 492230 loss: 0.0011 lr: 0.002\n",
      "iteration: 492240 loss: 0.0012 lr: 0.002\n",
      "iteration: 492250 loss: 0.0011 lr: 0.002\n",
      "iteration: 492260 loss: 0.0013 lr: 0.002\n",
      "iteration: 492270 loss: 0.0012 lr: 0.002\n",
      "iteration: 492280 loss: 0.0008 lr: 0.002\n",
      "iteration: 492290 loss: 0.0012 lr: 0.002\n",
      "iteration: 492300 loss: 0.0005 lr: 0.002\n",
      "iteration: 492310 loss: 0.0010 lr: 0.002\n",
      "iteration: 492320 loss: 0.0010 lr: 0.002\n",
      "iteration: 492330 loss: 0.0011 lr: 0.002\n",
      "iteration: 492340 loss: 0.0011 lr: 0.002\n",
      "iteration: 492350 loss: 0.0008 lr: 0.002\n",
      "iteration: 492360 loss: 0.0011 lr: 0.002\n",
      "iteration: 492370 loss: 0.0014 lr: 0.002\n",
      "iteration: 492380 loss: 0.0011 lr: 0.002\n",
      "iteration: 492390 loss: 0.0010 lr: 0.002\n",
      "iteration: 492400 loss: 0.0009 lr: 0.002\n",
      "iteration: 492410 loss: 0.0013 lr: 0.002\n",
      "iteration: 492420 loss: 0.0014 lr: 0.002\n",
      "iteration: 492430 loss: 0.0018 lr: 0.002\n",
      "iteration: 492440 loss: 0.0030 lr: 0.002\n",
      "iteration: 492450 loss: 0.0009 lr: 0.002\n",
      "iteration: 492460 loss: 0.0011 lr: 0.002\n",
      "iteration: 492470 loss: 0.0010 lr: 0.002\n",
      "iteration: 492480 loss: 0.0011 lr: 0.002\n",
      "iteration: 492490 loss: 0.0012 lr: 0.002\n",
      "iteration: 492500 loss: 0.0012 lr: 0.002\n",
      "iteration: 492510 loss: 0.0014 lr: 0.002\n",
      "iteration: 492520 loss: 0.0011 lr: 0.002\n",
      "iteration: 492530 loss: 0.0011 lr: 0.002\n",
      "iteration: 492540 loss: 0.0012 lr: 0.002\n",
      "iteration: 492550 loss: 0.0010 lr: 0.002\n",
      "iteration: 492560 loss: 0.0009 lr: 0.002\n",
      "iteration: 492570 loss: 0.0008 lr: 0.002\n",
      "iteration: 492580 loss: 0.0008 lr: 0.002\n",
      "iteration: 492590 loss: 0.0009 lr: 0.002\n",
      "iteration: 492600 loss: 0.0015 lr: 0.002\n",
      "iteration: 492610 loss: 0.0013 lr: 0.002\n",
      "iteration: 492620 loss: 0.0010 lr: 0.002\n",
      "iteration: 492630 loss: 0.0013 lr: 0.002\n",
      "iteration: 492640 loss: 0.0012 lr: 0.002\n",
      "iteration: 492650 loss: 0.0013 lr: 0.002\n",
      "iteration: 492660 loss: 0.0010 lr: 0.002\n",
      "iteration: 492670 loss: 0.0019 lr: 0.002\n",
      "iteration: 492680 loss: 0.0013 lr: 0.002\n",
      "iteration: 492690 loss: 0.0011 lr: 0.002\n",
      "iteration: 492700 loss: 0.0010 lr: 0.002\n",
      "iteration: 492710 loss: 0.0011 lr: 0.002\n",
      "iteration: 492720 loss: 0.0010 lr: 0.002\n",
      "iteration: 492730 loss: 0.0009 lr: 0.002\n",
      "iteration: 492740 loss: 0.0008 lr: 0.002\n",
      "iteration: 492750 loss: 0.0008 lr: 0.002\n",
      "iteration: 492760 loss: 0.0007 lr: 0.002\n",
      "iteration: 492770 loss: 0.0013 lr: 0.002\n",
      "iteration: 492780 loss: 0.0010 lr: 0.002\n",
      "iteration: 492790 loss: 0.0011 lr: 0.002\n",
      "iteration: 492800 loss: 0.0010 lr: 0.002\n",
      "iteration: 492810 loss: 0.0017 lr: 0.002\n",
      "iteration: 492820 loss: 0.0016 lr: 0.002\n",
      "iteration: 492830 loss: 0.0009 lr: 0.002\n",
      "iteration: 492840 loss: 0.0014 lr: 0.002\n",
      "iteration: 492850 loss: 0.0012 lr: 0.002\n",
      "iteration: 492860 loss: 0.0013 lr: 0.002\n",
      "iteration: 492870 loss: 0.0009 lr: 0.002\n",
      "iteration: 492880 loss: 0.0012 lr: 0.002\n",
      "iteration: 492890 loss: 0.0018 lr: 0.002\n",
      "iteration: 492900 loss: 0.0012 lr: 0.002\n",
      "iteration: 492910 loss: 0.0010 lr: 0.002\n",
      "iteration: 492920 loss: 0.0016 lr: 0.002\n",
      "iteration: 492930 loss: 0.0008 lr: 0.002\n",
      "iteration: 492940 loss: 0.0016 lr: 0.002\n",
      "iteration: 492950 loss: 0.0011 lr: 0.002\n",
      "iteration: 492960 loss: 0.0013 lr: 0.002\n",
      "iteration: 492970 loss: 0.0010 lr: 0.002\n",
      "iteration: 492980 loss: 0.0007 lr: 0.002\n",
      "iteration: 492990 loss: 0.0012 lr: 0.002\n",
      "iteration: 493000 loss: 0.0018 lr: 0.002\n",
      "iteration: 493010 loss: 0.0013 lr: 0.002\n",
      "iteration: 493020 loss: 0.0013 lr: 0.002\n",
      "iteration: 493030 loss: 0.0011 lr: 0.002\n",
      "iteration: 493040 loss: 0.0011 lr: 0.002\n",
      "iteration: 493050 loss: 0.0009 lr: 0.002\n",
      "iteration: 493060 loss: 0.0010 lr: 0.002\n",
      "iteration: 493070 loss: 0.0011 lr: 0.002\n",
      "iteration: 493080 loss: 0.0015 lr: 0.002\n",
      "iteration: 493090 loss: 0.0010 lr: 0.002\n",
      "iteration: 493100 loss: 0.0012 lr: 0.002\n",
      "iteration: 493110 loss: 0.0010 lr: 0.002\n",
      "iteration: 493120 loss: 0.0010 lr: 0.002\n",
      "iteration: 493130 loss: 0.0012 lr: 0.002\n",
      "iteration: 493140 loss: 0.0010 lr: 0.002\n",
      "iteration: 493150 loss: 0.0010 lr: 0.002\n",
      "iteration: 493160 loss: 0.0010 lr: 0.002\n",
      "iteration: 493170 loss: 0.0009 lr: 0.002\n",
      "iteration: 493180 loss: 0.0019 lr: 0.002\n",
      "iteration: 493190 loss: 0.0012 lr: 0.002\n",
      "iteration: 493200 loss: 0.0020 lr: 0.002\n",
      "iteration: 493210 loss: 0.0014 lr: 0.002\n",
      "iteration: 493220 loss: 0.0013 lr: 0.002\n",
      "iteration: 493230 loss: 0.0011 lr: 0.002\n",
      "iteration: 493240 loss: 0.0013 lr: 0.002\n",
      "iteration: 493250 loss: 0.0010 lr: 0.002\n",
      "iteration: 493260 loss: 0.0009 lr: 0.002\n",
      "iteration: 493270 loss: 0.0018 lr: 0.002\n",
      "iteration: 493280 loss: 0.0014 lr: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 493290 loss: 0.0030 lr: 0.002\n",
      "iteration: 493300 loss: 0.0013 lr: 0.002\n",
      "iteration: 493310 loss: 0.0010 lr: 0.002\n",
      "iteration: 493320 loss: 0.0011 lr: 0.002\n",
      "iteration: 493330 loss: 0.0014 lr: 0.002\n",
      "iteration: 493340 loss: 0.0007 lr: 0.002\n",
      "iteration: 493350 loss: 0.0010 lr: 0.002\n",
      "iteration: 493360 loss: 0.0008 lr: 0.002\n",
      "iteration: 493370 loss: 0.0009 lr: 0.002\n",
      "iteration: 493380 loss: 0.0012 lr: 0.002\n",
      "iteration: 493390 loss: 0.0008 lr: 0.002\n",
      "iteration: 493400 loss: 0.0012 lr: 0.002\n",
      "iteration: 493410 loss: 0.0024 lr: 0.002\n",
      "iteration: 493420 loss: 0.0012 lr: 0.002\n",
      "iteration: 493430 loss: 0.0013 lr: 0.002\n",
      "iteration: 493440 loss: 0.0010 lr: 0.002\n",
      "iteration: 493450 loss: 0.0012 lr: 0.002\n",
      "iteration: 493460 loss: 0.0016 lr: 0.002\n",
      "iteration: 493470 loss: 0.0010 lr: 0.002\n",
      "iteration: 493480 loss: 0.0007 lr: 0.002\n",
      "iteration: 493490 loss: 0.0010 lr: 0.002\n",
      "iteration: 493500 loss: 0.0012 lr: 0.002\n",
      "iteration: 493510 loss: 0.0008 lr: 0.002\n",
      "iteration: 493520 loss: 0.0011 lr: 0.002\n",
      "iteration: 493530 loss: 0.0009 lr: 0.002\n",
      "iteration: 493540 loss: 0.0009 lr: 0.002\n",
      "iteration: 493550 loss: 0.0014 lr: 0.002\n",
      "iteration: 493560 loss: 0.0008 lr: 0.002\n",
      "iteration: 493570 loss: 0.0015 lr: 0.002\n",
      "iteration: 493580 loss: 0.0011 lr: 0.002\n",
      "iteration: 493590 loss: 0.0009 lr: 0.002\n",
      "iteration: 493600 loss: 0.0021 lr: 0.002\n",
      "iteration: 493610 loss: 0.0011 lr: 0.002\n",
      "iteration: 493620 loss: 0.0011 lr: 0.002\n",
      "iteration: 493630 loss: 0.0010 lr: 0.002\n",
      "iteration: 493640 loss: 0.0024 lr: 0.002\n",
      "iteration: 493650 loss: 0.0013 lr: 0.002\n",
      "iteration: 493660 loss: 0.0016 lr: 0.002\n",
      "iteration: 493670 loss: 0.0014 lr: 0.002\n",
      "iteration: 493680 loss: 0.0013 lr: 0.002\n",
      "iteration: 493690 loss: 0.0010 lr: 0.002\n",
      "iteration: 493700 loss: 0.0011 lr: 0.002\n",
      "iteration: 493710 loss: 0.0016 lr: 0.002\n",
      "iteration: 493720 loss: 0.0012 lr: 0.002\n",
      "iteration: 493730 loss: 0.0012 lr: 0.002\n",
      "iteration: 493740 loss: 0.0013 lr: 0.002\n",
      "iteration: 493750 loss: 0.0012 lr: 0.002\n",
      "iteration: 493760 loss: 0.0014 lr: 0.002\n",
      "iteration: 493770 loss: 0.0007 lr: 0.002\n",
      "iteration: 493780 loss: 0.0013 lr: 0.002\n",
      "iteration: 493790 loss: 0.0009 lr: 0.002\n",
      "iteration: 493800 loss: 0.0012 lr: 0.002\n",
      "iteration: 493810 loss: 0.0013 lr: 0.002\n",
      "iteration: 493820 loss: 0.0011 lr: 0.002\n",
      "iteration: 493830 loss: 0.0012 lr: 0.002\n",
      "iteration: 493840 loss: 0.0009 lr: 0.002\n",
      "iteration: 493850 loss: 0.0013 lr: 0.002\n",
      "iteration: 493860 loss: 0.0013 lr: 0.002\n",
      "iteration: 493870 loss: 0.0010 lr: 0.002\n",
      "iteration: 493880 loss: 0.0013 lr: 0.002\n",
      "iteration: 493890 loss: 0.0008 lr: 0.002\n",
      "iteration: 493900 loss: 0.0012 lr: 0.002\n",
      "iteration: 493910 loss: 0.0008 lr: 0.002\n",
      "iteration: 493920 loss: 0.0010 lr: 0.002\n",
      "iteration: 493930 loss: 0.0013 lr: 0.002\n",
      "iteration: 493940 loss: 0.0012 lr: 0.002\n",
      "iteration: 493950 loss: 0.0012 lr: 0.002\n",
      "iteration: 493960 loss: 0.0009 lr: 0.002\n",
      "iteration: 493970 loss: 0.0012 lr: 0.002\n",
      "iteration: 493980 loss: 0.0009 lr: 0.002\n",
      "iteration: 493990 loss: 0.0016 lr: 0.002\n",
      "iteration: 494000 loss: 0.0012 lr: 0.002\n",
      "iteration: 494010 loss: 0.0010 lr: 0.002\n",
      "iteration: 494020 loss: 0.0010 lr: 0.002\n",
      "iteration: 494030 loss: 0.0012 lr: 0.002\n",
      "iteration: 494040 loss: 0.0012 lr: 0.002\n",
      "iteration: 494050 loss: 0.0007 lr: 0.002\n",
      "iteration: 494060 loss: 0.0009 lr: 0.002\n",
      "iteration: 494070 loss: 0.0011 lr: 0.002\n",
      "iteration: 494080 loss: 0.0013 lr: 0.002\n",
      "iteration: 494090 loss: 0.0007 lr: 0.002\n",
      "iteration: 494100 loss: 0.0008 lr: 0.002\n",
      "iteration: 494110 loss: 0.0010 lr: 0.002\n",
      "iteration: 494120 loss: 0.0016 lr: 0.002\n",
      "iteration: 494130 loss: 0.0012 lr: 0.002\n",
      "iteration: 494140 loss: 0.0010 lr: 0.002\n",
      "iteration: 494150 loss: 0.0013 lr: 0.002\n",
      "iteration: 494160 loss: 0.0011 lr: 0.002\n",
      "iteration: 494170 loss: 0.0011 lr: 0.002\n",
      "iteration: 494180 loss: 0.0011 lr: 0.002\n",
      "iteration: 494190 loss: 0.0013 lr: 0.002\n",
      "iteration: 494200 loss: 0.0013 lr: 0.002\n",
      "iteration: 494210 loss: 0.0010 lr: 0.002\n",
      "iteration: 494220 loss: 0.0007 lr: 0.002\n",
      "iteration: 494230 loss: 0.0011 lr: 0.002\n",
      "iteration: 494240 loss: 0.0012 lr: 0.002\n",
      "iteration: 494250 loss: 0.0009 lr: 0.002\n",
      "iteration: 494260 loss: 0.0008 lr: 0.002\n",
      "iteration: 494270 loss: 0.0013 lr: 0.002\n",
      "iteration: 494280 loss: 0.0009 lr: 0.002\n",
      "iteration: 494290 loss: 0.0014 lr: 0.002\n",
      "iteration: 494300 loss: 0.0009 lr: 0.002\n",
      "iteration: 494310 loss: 0.0012 lr: 0.002\n",
      "iteration: 494320 loss: 0.0021 lr: 0.002\n",
      "iteration: 494330 loss: 0.0012 lr: 0.002\n",
      "iteration: 494340 loss: 0.0010 lr: 0.002\n",
      "iteration: 494350 loss: 0.0010 lr: 0.002\n",
      "iteration: 494360 loss: 0.0017 lr: 0.002\n",
      "iteration: 494370 loss: 0.0017 lr: 0.002\n",
      "iteration: 494380 loss: 0.0010 lr: 0.002\n",
      "iteration: 494390 loss: 0.0014 lr: 0.002\n",
      "iteration: 494400 loss: 0.0008 lr: 0.002\n",
      "iteration: 494410 loss: 0.0013 lr: 0.002\n",
      "iteration: 494420 loss: 0.0008 lr: 0.002\n",
      "iteration: 494430 loss: 0.0012 lr: 0.002\n",
      "iteration: 494440 loss: 0.0011 lr: 0.002\n",
      "iteration: 494450 loss: 0.0009 lr: 0.002\n",
      "iteration: 494460 loss: 0.0008 lr: 0.002\n",
      "iteration: 494470 loss: 0.0014 lr: 0.002\n",
      "iteration: 494480 loss: 0.0009 lr: 0.002\n",
      "iteration: 494490 loss: 0.0013 lr: 0.002\n",
      "iteration: 494500 loss: 0.0013 lr: 0.002\n",
      "iteration: 494510 loss: 0.0015 lr: 0.002\n",
      "iteration: 494520 loss: 0.0007 lr: 0.002\n",
      "iteration: 494530 loss: 0.0014 lr: 0.002\n",
      "iteration: 494540 loss: 0.0010 lr: 0.002\n",
      "iteration: 494550 loss: 0.0009 lr: 0.002\n",
      "iteration: 494560 loss: 0.0012 lr: 0.002\n",
      "iteration: 494570 loss: 0.0012 lr: 0.002\n",
      "iteration: 494580 loss: 0.0023 lr: 0.002\n",
      "iteration: 494590 loss: 0.0013 lr: 0.002\n",
      "iteration: 494600 loss: 0.0009 lr: 0.002\n",
      "iteration: 494610 loss: 0.0010 lr: 0.002\n",
      "iteration: 494620 loss: 0.0012 lr: 0.002\n",
      "iteration: 494630 loss: 0.0014 lr: 0.002\n",
      "iteration: 494640 loss: 0.0024 lr: 0.002\n",
      "iteration: 494650 loss: 0.0014 lr: 0.002\n",
      "iteration: 494660 loss: 0.0022 lr: 0.002\n",
      "iteration: 494670 loss: 0.0013 lr: 0.002\n",
      "iteration: 494680 loss: 0.0010 lr: 0.002\n",
      "iteration: 494690 loss: 0.0010 lr: 0.002\n",
      "iteration: 494700 loss: 0.0014 lr: 0.002\n",
      "iteration: 494710 loss: 0.0010 lr: 0.002\n",
      "iteration: 494720 loss: 0.0015 lr: 0.002\n",
      "iteration: 494730 loss: 0.0010 lr: 0.002\n",
      "iteration: 494740 loss: 0.0015 lr: 0.002\n",
      "iteration: 494750 loss: 0.0009 lr: 0.002\n",
      "iteration: 494760 loss: 0.0013 lr: 0.002\n",
      "iteration: 494770 loss: 0.0007 lr: 0.002\n",
      "iteration: 494780 loss: 0.0008 lr: 0.002\n",
      "iteration: 494790 loss: 0.0008 lr: 0.002\n",
      "iteration: 494800 loss: 0.0013 lr: 0.002\n",
      "iteration: 494810 loss: 0.0008 lr: 0.002\n",
      "iteration: 494820 loss: 0.0010 lr: 0.002\n",
      "iteration: 494830 loss: 0.0009 lr: 0.002\n",
      "iteration: 494840 loss: 0.0008 lr: 0.002\n",
      "iteration: 494850 loss: 0.0008 lr: 0.002\n",
      "iteration: 494860 loss: 0.0012 lr: 0.002\n",
      "iteration: 494870 loss: 0.0011 lr: 0.002\n",
      "iteration: 494880 loss: 0.0016 lr: 0.002\n",
      "iteration: 494890 loss: 0.0011 lr: 0.002\n",
      "iteration: 494900 loss: 0.0022 lr: 0.002\n",
      "iteration: 494910 loss: 0.0009 lr: 0.002\n",
      "iteration: 494920 loss: 0.0017 lr: 0.002\n",
      "iteration: 494930 loss: 0.0018 lr: 0.002\n",
      "iteration: 494940 loss: 0.0009 lr: 0.002\n",
      "iteration: 494950 loss: 0.0006 lr: 0.002\n",
      "iteration: 494960 loss: 0.0008 lr: 0.002\n",
      "iteration: 494970 loss: 0.0014 lr: 0.002\n",
      "iteration: 494980 loss: 0.0009 lr: 0.002\n",
      "iteration: 494990 loss: 0.0015 lr: 0.002\n",
      "iteration: 495000 loss: 0.0011 lr: 0.002\n",
      "iteration: 495010 loss: 0.0013 lr: 0.002\n",
      "iteration: 495020 loss: 0.0011 lr: 0.002\n",
      "iteration: 495030 loss: 0.0016 lr: 0.002\n",
      "iteration: 495040 loss: 0.0011 lr: 0.002\n",
      "iteration: 495050 loss: 0.0011 lr: 0.002\n",
      "iteration: 495060 loss: 0.0009 lr: 0.002\n",
      "iteration: 495070 loss: 0.0010 lr: 0.002\n",
      "iteration: 495080 loss: 0.0015 lr: 0.002\n",
      "iteration: 495090 loss: 0.0009 lr: 0.002\n",
      "iteration: 495100 loss: 0.0007 lr: 0.002\n",
      "iteration: 495110 loss: 0.0011 lr: 0.002\n",
      "iteration: 495120 loss: 0.0016 lr: 0.002\n",
      "iteration: 495130 loss: 0.0012 lr: 0.002\n",
      "iteration: 495140 loss: 0.0013 lr: 0.002\n",
      "iteration: 495150 loss: 0.0008 lr: 0.002\n",
      "iteration: 495160 loss: 0.0011 lr: 0.002\n",
      "iteration: 495170 loss: 0.0010 lr: 0.002\n",
      "iteration: 495180 loss: 0.0007 lr: 0.002\n",
      "iteration: 495190 loss: 0.0010 lr: 0.002\n",
      "iteration: 495200 loss: 0.0015 lr: 0.002\n",
      "iteration: 495210 loss: 0.0015 lr: 0.002\n",
      "iteration: 495220 loss: 0.0009 lr: 0.002\n",
      "iteration: 495230 loss: 0.0012 lr: 0.002\n",
      "iteration: 495240 loss: 0.0007 lr: 0.002\n",
      "iteration: 495250 loss: 0.0012 lr: 0.002\n",
      "iteration: 495260 loss: 0.0011 lr: 0.002\n",
      "iteration: 495270 loss: 0.0009 lr: 0.002\n",
      "iteration: 495280 loss: 0.0013 lr: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 495290 loss: 0.0010 lr: 0.002\n",
      "iteration: 495300 loss: 0.0018 lr: 0.002\n",
      "iteration: 495310 loss: 0.0012 lr: 0.002\n",
      "iteration: 495320 loss: 0.0010 lr: 0.002\n",
      "iteration: 495330 loss: 0.0006 lr: 0.002\n",
      "iteration: 495340 loss: 0.0021 lr: 0.002\n",
      "iteration: 495350 loss: 0.0008 lr: 0.002\n",
      "iteration: 495360 loss: 0.0008 lr: 0.002\n",
      "iteration: 495370 loss: 0.0009 lr: 0.002\n",
      "iteration: 495380 loss: 0.0009 lr: 0.002\n",
      "iteration: 495390 loss: 0.0010 lr: 0.002\n",
      "iteration: 495400 loss: 0.0009 lr: 0.002\n",
      "iteration: 495410 loss: 0.0008 lr: 0.002\n",
      "iteration: 495420 loss: 0.0018 lr: 0.002\n",
      "iteration: 495430 loss: 0.0013 lr: 0.002\n",
      "iteration: 495440 loss: 0.0013 lr: 0.002\n",
      "iteration: 495450 loss: 0.0010 lr: 0.002\n",
      "iteration: 495460 loss: 0.0009 lr: 0.002\n",
      "iteration: 495470 loss: 0.0008 lr: 0.002\n",
      "iteration: 495480 loss: 0.0014 lr: 0.002\n",
      "iteration: 495490 loss: 0.0009 lr: 0.002\n",
      "iteration: 495500 loss: 0.0013 lr: 0.002\n",
      "iteration: 495510 loss: 0.0009 lr: 0.002\n",
      "iteration: 495520 loss: 0.0011 lr: 0.002\n",
      "iteration: 495530 loss: 0.0010 lr: 0.002\n",
      "iteration: 495540 loss: 0.0012 lr: 0.002\n",
      "iteration: 495550 loss: 0.0012 lr: 0.002\n",
      "iteration: 495560 loss: 0.0018 lr: 0.002\n",
      "iteration: 495570 loss: 0.0017 lr: 0.002\n",
      "iteration: 495580 loss: 0.0012 lr: 0.002\n",
      "iteration: 495590 loss: 0.0014 lr: 0.002\n",
      "iteration: 495600 loss: 0.0012 lr: 0.002\n",
      "iteration: 495610 loss: 0.0012 lr: 0.002\n",
      "iteration: 495620 loss: 0.0012 lr: 0.002\n",
      "iteration: 495630 loss: 0.0011 lr: 0.002\n",
      "iteration: 495640 loss: 0.0012 lr: 0.002\n",
      "iteration: 495650 loss: 0.0015 lr: 0.002\n",
      "iteration: 495660 loss: 0.0020 lr: 0.002\n",
      "iteration: 495670 loss: 0.0013 lr: 0.002\n",
      "iteration: 495680 loss: 0.0013 lr: 0.002\n",
      "iteration: 495690 loss: 0.0012 lr: 0.002\n",
      "iteration: 495700 loss: 0.0013 lr: 0.002\n",
      "iteration: 495710 loss: 0.0014 lr: 0.002\n",
      "iteration: 495720 loss: 0.0010 lr: 0.002\n",
      "iteration: 495730 loss: 0.0012 lr: 0.002\n",
      "iteration: 495740 loss: 0.0015 lr: 0.002\n",
      "iteration: 495750 loss: 0.0007 lr: 0.002\n",
      "iteration: 495760 loss: 0.0016 lr: 0.002\n",
      "iteration: 495770 loss: 0.0027 lr: 0.002\n",
      "iteration: 495780 loss: 0.0011 lr: 0.002\n",
      "iteration: 495790 loss: 0.0008 lr: 0.002\n",
      "iteration: 495800 loss: 0.0011 lr: 0.002\n",
      "iteration: 495810 loss: 0.0008 lr: 0.002\n",
      "iteration: 495820 loss: 0.0024 lr: 0.002\n",
      "iteration: 495830 loss: 0.0007 lr: 0.002\n",
      "iteration: 495840 loss: 0.0010 lr: 0.002\n",
      "iteration: 495850 loss: 0.0014 lr: 0.002\n",
      "iteration: 495860 loss: 0.0011 lr: 0.002\n",
      "iteration: 495870 loss: 0.0008 lr: 0.002\n",
      "iteration: 495880 loss: 0.0011 lr: 0.002\n",
      "iteration: 495890 loss: 0.0009 lr: 0.002\n",
      "iteration: 495900 loss: 0.0009 lr: 0.002\n",
      "iteration: 495910 loss: 0.0007 lr: 0.002\n",
      "iteration: 495920 loss: 0.0011 lr: 0.002\n",
      "iteration: 495930 loss: 0.0015 lr: 0.002\n",
      "iteration: 495940 loss: 0.0015 lr: 0.002\n",
      "iteration: 495950 loss: 0.0009 lr: 0.002\n",
      "iteration: 495960 loss: 0.0013 lr: 0.002\n",
      "iteration: 495970 loss: 0.0016 lr: 0.002\n",
      "iteration: 495980 loss: 0.0011 lr: 0.002\n",
      "iteration: 495990 loss: 0.0011 lr: 0.002\n",
      "iteration: 496000 loss: 0.0010 lr: 0.002\n",
      "iteration: 496010 loss: 0.0009 lr: 0.002\n",
      "iteration: 496020 loss: 0.0016 lr: 0.002\n",
      "iteration: 496030 loss: 0.0017 lr: 0.002\n",
      "iteration: 496040 loss: 0.0012 lr: 0.002\n",
      "iteration: 496050 loss: 0.0014 lr: 0.002\n",
      "iteration: 496060 loss: 0.0010 lr: 0.002\n",
      "iteration: 496070 loss: 0.0009 lr: 0.002\n",
      "iteration: 496080 loss: 0.0011 lr: 0.002\n",
      "iteration: 496090 loss: 0.0012 lr: 0.002\n",
      "iteration: 496100 loss: 0.0008 lr: 0.002\n",
      "iteration: 496110 loss: 0.0012 lr: 0.002\n",
      "iteration: 496120 loss: 0.0013 lr: 0.002\n",
      "iteration: 496130 loss: 0.0009 lr: 0.002\n",
      "iteration: 496140 loss: 0.0013 lr: 0.002\n",
      "iteration: 496150 loss: 0.0013 lr: 0.002\n",
      "iteration: 496160 loss: 0.0014 lr: 0.002\n",
      "iteration: 496170 loss: 0.0009 lr: 0.002\n",
      "iteration: 496180 loss: 0.0010 lr: 0.002\n",
      "iteration: 496190 loss: 0.0014 lr: 0.002\n",
      "iteration: 496200 loss: 0.0011 lr: 0.002\n",
      "iteration: 496210 loss: 0.0009 lr: 0.002\n",
      "iteration: 496220 loss: 0.0010 lr: 0.002\n",
      "iteration: 496230 loss: 0.0013 lr: 0.002\n",
      "iteration: 496240 loss: 0.0013 lr: 0.002\n",
      "iteration: 496250 loss: 0.0015 lr: 0.002\n",
      "iteration: 496260 loss: 0.0011 lr: 0.002\n",
      "iteration: 496270 loss: 0.0007 lr: 0.002\n",
      "iteration: 496280 loss: 0.0012 lr: 0.002\n",
      "iteration: 496290 loss: 0.0011 lr: 0.002\n",
      "iteration: 496300 loss: 0.0007 lr: 0.002\n",
      "iteration: 496310 loss: 0.0008 lr: 0.002\n",
      "iteration: 496320 loss: 0.0010 lr: 0.002\n",
      "iteration: 496330 loss: 0.0009 lr: 0.002\n",
      "iteration: 496340 loss: 0.0009 lr: 0.002\n",
      "iteration: 496350 loss: 0.0014 lr: 0.002\n",
      "iteration: 496360 loss: 0.0009 lr: 0.002\n",
      "iteration: 496370 loss: 0.0013 lr: 0.002\n",
      "iteration: 496380 loss: 0.0015 lr: 0.002\n",
      "iteration: 496390 loss: 0.0012 lr: 0.002\n",
      "iteration: 496400 loss: 0.0013 lr: 0.002\n",
      "iteration: 496410 loss: 0.0013 lr: 0.002\n",
      "iteration: 496420 loss: 0.0012 lr: 0.002\n",
      "iteration: 496430 loss: 0.0014 lr: 0.002\n",
      "iteration: 496440 loss: 0.0015 lr: 0.002\n",
      "iteration: 496450 loss: 0.0014 lr: 0.002\n",
      "iteration: 496460 loss: 0.0009 lr: 0.002\n",
      "iteration: 496470 loss: 0.0009 lr: 0.002\n",
      "iteration: 496480 loss: 0.0016 lr: 0.002\n",
      "iteration: 496490 loss: 0.0013 lr: 0.002\n",
      "iteration: 496500 loss: 0.0010 lr: 0.002\n",
      "iteration: 496510 loss: 0.0010 lr: 0.002\n",
      "iteration: 496520 loss: 0.0013 lr: 0.002\n",
      "iteration: 496530 loss: 0.0009 lr: 0.002\n",
      "iteration: 496540 loss: 0.0017 lr: 0.002\n",
      "iteration: 496550 loss: 0.0010 lr: 0.002\n",
      "iteration: 496560 loss: 0.0008 lr: 0.002\n",
      "iteration: 496570 loss: 0.0013 lr: 0.002\n",
      "iteration: 496580 loss: 0.0010 lr: 0.002\n",
      "iteration: 496590 loss: 0.0012 lr: 0.002\n",
      "iteration: 496600 loss: 0.0011 lr: 0.002\n",
      "iteration: 496610 loss: 0.0008 lr: 0.002\n",
      "iteration: 496620 loss: 0.0012 lr: 0.002\n",
      "iteration: 496630 loss: 0.0012 lr: 0.002\n",
      "iteration: 496640 loss: 0.0016 lr: 0.002\n",
      "iteration: 496650 loss: 0.0008 lr: 0.002\n",
      "iteration: 496660 loss: 0.0011 lr: 0.002\n",
      "iteration: 496670 loss: 0.0012 lr: 0.002\n",
      "iteration: 496680 loss: 0.0011 lr: 0.002\n",
      "iteration: 496690 loss: 0.0008 lr: 0.002\n",
      "iteration: 496700 loss: 0.0012 lr: 0.002\n",
      "iteration: 496710 loss: 0.0011 lr: 0.002\n",
      "iteration: 496720 loss: 0.0008 lr: 0.002\n",
      "iteration: 496730 loss: 0.0014 lr: 0.002\n",
      "iteration: 496740 loss: 0.0014 lr: 0.002\n",
      "iteration: 496750 loss: 0.0008 lr: 0.002\n",
      "iteration: 496760 loss: 0.0010 lr: 0.002\n",
      "iteration: 496770 loss: 0.0012 lr: 0.002\n",
      "iteration: 496780 loss: 0.0011 lr: 0.002\n",
      "iteration: 496790 loss: 0.0008 lr: 0.002\n",
      "iteration: 496800 loss: 0.0015 lr: 0.002\n",
      "iteration: 496810 loss: 0.0010 lr: 0.002\n",
      "iteration: 496820 loss: 0.0014 lr: 0.002\n",
      "iteration: 496830 loss: 0.0010 lr: 0.002\n",
      "iteration: 496840 loss: 0.0013 lr: 0.002\n",
      "iteration: 496850 loss: 0.0008 lr: 0.002\n",
      "iteration: 496860 loss: 0.0013 lr: 0.002\n",
      "iteration: 496870 loss: 0.0008 lr: 0.002\n",
      "iteration: 496880 loss: 0.0010 lr: 0.002\n",
      "iteration: 496890 loss: 0.0010 lr: 0.002\n",
      "iteration: 496900 loss: 0.0011 lr: 0.002\n",
      "iteration: 496910 loss: 0.0015 lr: 0.002\n",
      "iteration: 496920 loss: 0.0009 lr: 0.002\n",
      "iteration: 496930 loss: 0.0013 lr: 0.002\n",
      "iteration: 496940 loss: 0.0010 lr: 0.002\n",
      "iteration: 496950 loss: 0.0010 lr: 0.002\n",
      "iteration: 496960 loss: 0.0009 lr: 0.002\n",
      "iteration: 496970 loss: 0.0009 lr: 0.002\n",
      "iteration: 496980 loss: 0.0012 lr: 0.002\n",
      "iteration: 496990 loss: 0.0014 lr: 0.002\n",
      "iteration: 497000 loss: 0.0011 lr: 0.002\n",
      "iteration: 497010 loss: 0.0010 lr: 0.002\n",
      "iteration: 497020 loss: 0.0012 lr: 0.002\n",
      "iteration: 497030 loss: 0.0011 lr: 0.002\n",
      "iteration: 497040 loss: 0.0014 lr: 0.002\n",
      "iteration: 497050 loss: 0.0010 lr: 0.002\n",
      "iteration: 497060 loss: 0.0015 lr: 0.002\n",
      "iteration: 497070 loss: 0.0008 lr: 0.002\n",
      "iteration: 497080 loss: 0.0018 lr: 0.002\n",
      "iteration: 497090 loss: 0.0013 lr: 0.002\n",
      "iteration: 497100 loss: 0.0010 lr: 0.002\n",
      "iteration: 497110 loss: 0.0008 lr: 0.002\n",
      "iteration: 497120 loss: 0.0011 lr: 0.002\n",
      "iteration: 497130 loss: 0.0012 lr: 0.002\n",
      "iteration: 497140 loss: 0.0011 lr: 0.002\n",
      "iteration: 497150 loss: 0.0009 lr: 0.002\n",
      "iteration: 497160 loss: 0.0008 lr: 0.002\n",
      "iteration: 497170 loss: 0.0013 lr: 0.002\n",
      "iteration: 497180 loss: 0.0012 lr: 0.002\n",
      "iteration: 497190 loss: 0.0012 lr: 0.002\n",
      "iteration: 497200 loss: 0.0010 lr: 0.002\n",
      "iteration: 497210 loss: 0.0014 lr: 0.002\n",
      "iteration: 497220 loss: 0.0010 lr: 0.002\n",
      "iteration: 497230 loss: 0.0012 lr: 0.002\n",
      "iteration: 497240 loss: 0.0011 lr: 0.002\n",
      "iteration: 497250 loss: 0.0012 lr: 0.002\n",
      "iteration: 497260 loss: 0.0013 lr: 0.002\n",
      "iteration: 497270 loss: 0.0008 lr: 0.002\n",
      "iteration: 497280 loss: 0.0007 lr: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 497290 loss: 0.0010 lr: 0.002\n",
      "iteration: 497300 loss: 0.0010 lr: 0.002\n",
      "iteration: 497310 loss: 0.0010 lr: 0.002\n",
      "iteration: 497320 loss: 0.0011 lr: 0.002\n",
      "iteration: 497330 loss: 0.0008 lr: 0.002\n",
      "iteration: 497340 loss: 0.0009 lr: 0.002\n",
      "iteration: 497350 loss: 0.0012 lr: 0.002\n",
      "iteration: 497360 loss: 0.0020 lr: 0.002\n",
      "iteration: 497370 loss: 0.0011 lr: 0.002\n",
      "iteration: 497380 loss: 0.0008 lr: 0.002\n",
      "iteration: 497390 loss: 0.0009 lr: 0.002\n",
      "iteration: 497400 loss: 0.0008 lr: 0.002\n",
      "iteration: 497410 loss: 0.0008 lr: 0.002\n",
      "iteration: 497420 loss: 0.0008 lr: 0.002\n",
      "iteration: 497430 loss: 0.0009 lr: 0.002\n",
      "iteration: 497440 loss: 0.0010 lr: 0.002\n",
      "iteration: 497450 loss: 0.0009 lr: 0.002\n",
      "iteration: 497460 loss: 0.0014 lr: 0.002\n",
      "iteration: 497470 loss: 0.0012 lr: 0.002\n",
      "iteration: 497480 loss: 0.0009 lr: 0.002\n",
      "iteration: 497490 loss: 0.0007 lr: 0.002\n",
      "iteration: 497500 loss: 0.0009 lr: 0.002\n",
      "iteration: 497510 loss: 0.0009 lr: 0.002\n",
      "iteration: 497520 loss: 0.0015 lr: 0.002\n",
      "iteration: 497530 loss: 0.0008 lr: 0.002\n",
      "iteration: 497540 loss: 0.0008 lr: 0.002\n",
      "iteration: 497550 loss: 0.0010 lr: 0.002\n",
      "iteration: 497560 loss: 0.0011 lr: 0.002\n",
      "iteration: 497570 loss: 0.0008 lr: 0.002\n",
      "iteration: 497580 loss: 0.0013 lr: 0.002\n",
      "iteration: 497590 loss: 0.0011 lr: 0.002\n",
      "iteration: 497600 loss: 0.0011 lr: 0.002\n",
      "iteration: 497610 loss: 0.0012 lr: 0.002\n",
      "iteration: 497620 loss: 0.0013 lr: 0.002\n",
      "iteration: 497630 loss: 0.0013 lr: 0.002\n",
      "iteration: 497640 loss: 0.0012 lr: 0.002\n",
      "iteration: 497650 loss: 0.0012 lr: 0.002\n",
      "iteration: 497660 loss: 0.0009 lr: 0.002\n",
      "iteration: 497670 loss: 0.0007 lr: 0.002\n",
      "iteration: 497680 loss: 0.0009 lr: 0.002\n",
      "iteration: 497690 loss: 0.0009 lr: 0.002\n",
      "iteration: 497700 loss: 0.0010 lr: 0.002\n",
      "iteration: 497710 loss: 0.0011 lr: 0.002\n",
      "iteration: 497720 loss: 0.0008 lr: 0.002\n",
      "iteration: 497730 loss: 0.0009 lr: 0.002\n",
      "iteration: 497740 loss: 0.0009 lr: 0.002\n",
      "iteration: 497750 loss: 0.0019 lr: 0.002\n",
      "iteration: 497760 loss: 0.0009 lr: 0.002\n",
      "iteration: 497770 loss: 0.0009 lr: 0.002\n",
      "iteration: 497780 loss: 0.0010 lr: 0.002\n",
      "iteration: 497790 loss: 0.0009 lr: 0.002\n",
      "iteration: 497800 loss: 0.0014 lr: 0.002\n",
      "iteration: 497810 loss: 0.0008 lr: 0.002\n",
      "iteration: 497820 loss: 0.0008 lr: 0.002\n",
      "iteration: 497830 loss: 0.0012 lr: 0.002\n",
      "iteration: 497840 loss: 0.0010 lr: 0.002\n",
      "iteration: 497850 loss: 0.0009 lr: 0.002\n",
      "iteration: 497860 loss: 0.0009 lr: 0.002\n",
      "iteration: 497870 loss: 0.0013 lr: 0.002\n",
      "iteration: 497880 loss: 0.0012 lr: 0.002\n",
      "iteration: 497890 loss: 0.0012 lr: 0.002\n",
      "iteration: 497900 loss: 0.0010 lr: 0.002\n",
      "iteration: 497910 loss: 0.0010 lr: 0.002\n",
      "iteration: 497920 loss: 0.0013 lr: 0.002\n",
      "iteration: 497930 loss: 0.0010 lr: 0.002\n",
      "iteration: 497940 loss: 0.0012 lr: 0.002\n",
      "iteration: 497950 loss: 0.0011 lr: 0.002\n",
      "iteration: 497960 loss: 0.0008 lr: 0.002\n",
      "iteration: 497970 loss: 0.0013 lr: 0.002\n",
      "iteration: 497980 loss: 0.0009 lr: 0.002\n",
      "iteration: 497990 loss: 0.0008 lr: 0.002\n",
      "iteration: 498000 loss: 0.0010 lr: 0.002\n",
      "iteration: 498010 loss: 0.0019 lr: 0.002\n",
      "iteration: 498020 loss: 0.0013 lr: 0.002\n",
      "iteration: 498030 loss: 0.0021 lr: 0.002\n",
      "iteration: 498040 loss: 0.0016 lr: 0.002\n",
      "iteration: 498050 loss: 0.0010 lr: 0.002\n",
      "iteration: 498060 loss: 0.0011 lr: 0.002\n",
      "iteration: 498070 loss: 0.0010 lr: 0.002\n",
      "iteration: 498080 loss: 0.0010 lr: 0.002\n",
      "iteration: 498090 loss: 0.0011 lr: 0.002\n",
      "iteration: 498100 loss: 0.0017 lr: 0.002\n",
      "iteration: 498110 loss: 0.0013 lr: 0.002\n",
      "iteration: 498120 loss: 0.0012 lr: 0.002\n",
      "iteration: 498130 loss: 0.0010 lr: 0.002\n",
      "iteration: 498140 loss: 0.0015 lr: 0.002\n",
      "iteration: 498150 loss: 0.0019 lr: 0.002\n",
      "iteration: 498160 loss: 0.0009 lr: 0.002\n",
      "iteration: 498170 loss: 0.0008 lr: 0.002\n",
      "iteration: 498180 loss: 0.0010 lr: 0.002\n",
      "iteration: 498190 loss: 0.0010 lr: 0.002\n",
      "iteration: 498200 loss: 0.0008 lr: 0.002\n",
      "iteration: 498210 loss: 0.0017 lr: 0.002\n",
      "iteration: 498220 loss: 0.0013 lr: 0.002\n",
      "iteration: 498230 loss: 0.0015 lr: 0.002\n",
      "iteration: 498240 loss: 0.0010 lr: 0.002\n",
      "iteration: 498250 loss: 0.0008 lr: 0.002\n",
      "iteration: 498260 loss: 0.0013 lr: 0.002\n",
      "iteration: 498270 loss: 0.0013 lr: 0.002\n",
      "iteration: 498280 loss: 0.0017 lr: 0.002\n",
      "iteration: 498290 loss: 0.0009 lr: 0.002\n",
      "iteration: 498300 loss: 0.0012 lr: 0.002\n",
      "iteration: 498310 loss: 0.0009 lr: 0.002\n",
      "iteration: 498320 loss: 0.0016 lr: 0.002\n",
      "iteration: 498330 loss: 0.0015 lr: 0.002\n",
      "iteration: 498340 loss: 0.0007 lr: 0.002\n",
      "iteration: 498350 loss: 0.0011 lr: 0.002\n",
      "iteration: 498360 loss: 0.0008 lr: 0.002\n",
      "iteration: 498370 loss: 0.0012 lr: 0.002\n",
      "iteration: 498380 loss: 0.0012 lr: 0.002\n",
      "iteration: 498390 loss: 0.0008 lr: 0.002\n",
      "iteration: 498400 loss: 0.0013 lr: 0.002\n",
      "iteration: 498410 loss: 0.0012 lr: 0.002\n",
      "iteration: 498420 loss: 0.0009 lr: 0.002\n",
      "iteration: 498430 loss: 0.0008 lr: 0.002\n",
      "iteration: 498440 loss: 0.0009 lr: 0.002\n",
      "iteration: 498450 loss: 0.0012 lr: 0.002\n",
      "iteration: 498460 loss: 0.0012 lr: 0.002\n",
      "iteration: 498470 loss: 0.0012 lr: 0.002\n",
      "iteration: 498480 loss: 0.0016 lr: 0.002\n",
      "iteration: 498490 loss: 0.0012 lr: 0.002\n",
      "iteration: 498500 loss: 0.0012 lr: 0.002\n",
      "iteration: 498510 loss: 0.0011 lr: 0.002\n",
      "iteration: 498520 loss: 0.0013 lr: 0.002\n",
      "iteration: 498530 loss: 0.0007 lr: 0.002\n",
      "iteration: 498540 loss: 0.0012 lr: 0.002\n",
      "iteration: 498550 loss: 0.0008 lr: 0.002\n",
      "iteration: 498560 loss: 0.0012 lr: 0.002\n",
      "iteration: 498570 loss: 0.0011 lr: 0.002\n",
      "iteration: 498580 loss: 0.0010 lr: 0.002\n",
      "iteration: 498590 loss: 0.0012 lr: 0.002\n",
      "iteration: 498600 loss: 0.0012 lr: 0.002\n",
      "iteration: 498610 loss: 0.0013 lr: 0.002\n",
      "iteration: 498620 loss: 0.0009 lr: 0.002\n",
      "iteration: 498630 loss: 0.0016 lr: 0.002\n",
      "iteration: 498640 loss: 0.0012 lr: 0.002\n",
      "iteration: 498650 loss: 0.0009 lr: 0.002\n",
      "iteration: 498660 loss: 0.0014 lr: 0.002\n",
      "iteration: 498670 loss: 0.0009 lr: 0.002\n",
      "iteration: 498680 loss: 0.0012 lr: 0.002\n",
      "iteration: 498690 loss: 0.0011 lr: 0.002\n",
      "iteration: 498700 loss: 0.0014 lr: 0.002\n",
      "iteration: 498710 loss: 0.0015 lr: 0.002\n",
      "iteration: 498720 loss: 0.0015 lr: 0.002\n",
      "iteration: 498730 loss: 0.0014 lr: 0.002\n",
      "iteration: 498740 loss: 0.0010 lr: 0.002\n",
      "iteration: 498750 loss: 0.0015 lr: 0.002\n",
      "iteration: 498760 loss: 0.0012 lr: 0.002\n",
      "iteration: 498770 loss: 0.0011 lr: 0.002\n",
      "iteration: 498780 loss: 0.0011 lr: 0.002\n",
      "iteration: 498790 loss: 0.0011 lr: 0.002\n",
      "iteration: 498800 loss: 0.0008 lr: 0.002\n",
      "iteration: 498810 loss: 0.0010 lr: 0.002\n",
      "iteration: 498820 loss: 0.0007 lr: 0.002\n",
      "iteration: 498830 loss: 0.0015 lr: 0.002\n",
      "iteration: 498840 loss: 0.0008 lr: 0.002\n",
      "iteration: 498850 loss: 0.0007 lr: 0.002\n",
      "iteration: 498860 loss: 0.0011 lr: 0.002\n",
      "iteration: 498870 loss: 0.0013 lr: 0.002\n",
      "iteration: 498880 loss: 0.0012 lr: 0.002\n",
      "iteration: 498890 loss: 0.0016 lr: 0.002\n",
      "iteration: 498900 loss: 0.0013 lr: 0.002\n",
      "iteration: 498910 loss: 0.0012 lr: 0.002\n",
      "iteration: 498920 loss: 0.0010 lr: 0.002\n",
      "iteration: 498930 loss: 0.0014 lr: 0.002\n",
      "iteration: 498940 loss: 0.0010 lr: 0.002\n",
      "iteration: 498950 loss: 0.0014 lr: 0.002\n",
      "iteration: 498960 loss: 0.0009 lr: 0.002\n",
      "iteration: 498970 loss: 0.0007 lr: 0.002\n",
      "iteration: 498980 loss: 0.0009 lr: 0.002\n",
      "iteration: 498990 loss: 0.0010 lr: 0.002\n",
      "iteration: 499000 loss: 0.0010 lr: 0.002\n",
      "iteration: 499010 loss: 0.0009 lr: 0.002\n",
      "iteration: 499020 loss: 0.0018 lr: 0.002\n",
      "iteration: 499030 loss: 0.0009 lr: 0.002\n",
      "iteration: 499040 loss: 0.0009 lr: 0.002\n",
      "iteration: 499050 loss: 0.0010 lr: 0.002\n",
      "iteration: 499060 loss: 0.0010 lr: 0.002\n",
      "iteration: 499070 loss: 0.0012 lr: 0.002\n",
      "iteration: 499080 loss: 0.0005 lr: 0.002\n",
      "iteration: 499090 loss: 0.0009 lr: 0.002\n",
      "iteration: 499100 loss: 0.0011 lr: 0.002\n",
      "iteration: 499110 loss: 0.0009 lr: 0.002\n",
      "iteration: 499120 loss: 0.0011 lr: 0.002\n",
      "iteration: 499130 loss: 0.0008 lr: 0.002\n",
      "iteration: 499140 loss: 0.0012 lr: 0.002\n",
      "iteration: 499150 loss: 0.0011 lr: 0.002\n",
      "iteration: 499160 loss: 0.0009 lr: 0.002\n",
      "iteration: 499170 loss: 0.0015 lr: 0.002\n",
      "iteration: 499180 loss: 0.0012 lr: 0.002\n",
      "iteration: 499190 loss: 0.0008 lr: 0.002\n",
      "iteration: 499200 loss: 0.0015 lr: 0.002\n",
      "iteration: 499210 loss: 0.0009 lr: 0.002\n",
      "iteration: 499220 loss: 0.0014 lr: 0.002\n",
      "iteration: 499230 loss: 0.0012 lr: 0.002\n",
      "iteration: 499240 loss: 0.0009 lr: 0.002\n",
      "iteration: 499250 loss: 0.0012 lr: 0.002\n",
      "iteration: 499260 loss: 0.0013 lr: 0.002\n",
      "iteration: 499270 loss: 0.0013 lr: 0.002\n",
      "iteration: 499280 loss: 0.0011 lr: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 499290 loss: 0.0010 lr: 0.002\n",
      "iteration: 499300 loss: 0.0015 lr: 0.002\n",
      "iteration: 499310 loss: 0.0009 lr: 0.002\n",
      "iteration: 499320 loss: 0.0007 lr: 0.002\n",
      "iteration: 499330 loss: 0.0008 lr: 0.002\n",
      "iteration: 499340 loss: 0.0011 lr: 0.002\n",
      "iteration: 499350 loss: 0.0016 lr: 0.002\n",
      "iteration: 499360 loss: 0.0010 lr: 0.002\n",
      "iteration: 499370 loss: 0.0009 lr: 0.002\n",
      "iteration: 499380 loss: 0.0009 lr: 0.002\n",
      "iteration: 499390 loss: 0.0009 lr: 0.002\n",
      "iteration: 499400 loss: 0.0009 lr: 0.002\n",
      "iteration: 499410 loss: 0.0015 lr: 0.002\n",
      "iteration: 499420 loss: 0.0012 lr: 0.002\n",
      "iteration: 499430 loss: 0.0009 lr: 0.002\n",
      "iteration: 499440 loss: 0.0013 lr: 0.002\n",
      "iteration: 499450 loss: 0.0009 lr: 0.002\n",
      "iteration: 499460 loss: 0.0011 lr: 0.002\n",
      "iteration: 499470 loss: 0.0012 lr: 0.002\n",
      "iteration: 499480 loss: 0.0013 lr: 0.002\n",
      "iteration: 499490 loss: 0.0011 lr: 0.002\n",
      "iteration: 499500 loss: 0.0015 lr: 0.002\n",
      "iteration: 499510 loss: 0.0016 lr: 0.002\n",
      "iteration: 499520 loss: 0.0012 lr: 0.002\n",
      "iteration: 499530 loss: 0.0008 lr: 0.002\n",
      "iteration: 499540 loss: 0.0009 lr: 0.002\n",
      "iteration: 499550 loss: 0.0013 lr: 0.002\n",
      "iteration: 499560 loss: 0.0016 lr: 0.002\n",
      "iteration: 499570 loss: 0.0010 lr: 0.002\n",
      "iteration: 499580 loss: 0.0012 lr: 0.002\n",
      "iteration: 499590 loss: 0.0009 lr: 0.002\n",
      "iteration: 499600 loss: 0.0013 lr: 0.002\n",
      "iteration: 499610 loss: 0.0015 lr: 0.002\n",
      "iteration: 499620 loss: 0.0009 lr: 0.002\n",
      "iteration: 499630 loss: 0.0014 lr: 0.002\n",
      "iteration: 499640 loss: 0.0009 lr: 0.002\n",
      "iteration: 499650 loss: 0.0019 lr: 0.002\n",
      "iteration: 499660 loss: 0.0014 lr: 0.002\n",
      "iteration: 499670 loss: 0.0013 lr: 0.002\n",
      "iteration: 499680 loss: 0.0018 lr: 0.002\n",
      "iteration: 499690 loss: 0.0017 lr: 0.002\n",
      "iteration: 499700 loss: 0.0009 lr: 0.002\n",
      "iteration: 499710 loss: 0.0016 lr: 0.002\n",
      "iteration: 499720 loss: 0.0008 lr: 0.002\n",
      "iteration: 499730 loss: 0.0013 lr: 0.002\n",
      "iteration: 499740 loss: 0.0011 lr: 0.002\n",
      "iteration: 499750 loss: 0.0014 lr: 0.002\n",
      "iteration: 499760 loss: 0.0013 lr: 0.002\n",
      "iteration: 499770 loss: 0.0012 lr: 0.002\n",
      "iteration: 499780 loss: 0.0012 lr: 0.002\n",
      "iteration: 499790 loss: 0.0008 lr: 0.002\n",
      "iteration: 499800 loss: 0.0008 lr: 0.002\n",
      "iteration: 499810 loss: 0.0010 lr: 0.002\n",
      "iteration: 499820 loss: 0.0010 lr: 0.002\n",
      "iteration: 499830 loss: 0.0010 lr: 0.002\n",
      "iteration: 499840 loss: 0.0011 lr: 0.002\n",
      "iteration: 499850 loss: 0.0013 lr: 0.002\n",
      "iteration: 499860 loss: 0.0013 lr: 0.002\n",
      "iteration: 499870 loss: 0.0009 lr: 0.002\n",
      "iteration: 499880 loss: 0.0019 lr: 0.002\n",
      "iteration: 499890 loss: 0.0009 lr: 0.002\n",
      "iteration: 499900 loss: 0.0010 lr: 0.002\n",
      "iteration: 499910 loss: 0.0012 lr: 0.002\n",
      "iteration: 499920 loss: 0.0009 lr: 0.002\n",
      "iteration: 499930 loss: 0.0019 lr: 0.002\n",
      "iteration: 499940 loss: 0.0015 lr: 0.002\n",
      "iteration: 499950 loss: 0.0014 lr: 0.002\n",
      "iteration: 499960 loss: 0.0008 lr: 0.002\n",
      "iteration: 499970 loss: 0.0009 lr: 0.002\n",
      "iteration: 499980 loss: 0.0012 lr: 0.002\n",
      "iteration: 499990 loss: 0.0020 lr: 0.002\n",
      "iteration: 500000 loss: 0.0010 lr: 0.002\n",
      "iteration: 500010 loss: 0.0010 lr: 0.002\n",
      "iteration: 500020 loss: 0.0012 lr: 0.002\n",
      "iteration: 500030 loss: 0.0010 lr: 0.002\n",
      "iteration: 500040 loss: 0.0010 lr: 0.002\n",
      "iteration: 500050 loss: 0.0015 lr: 0.002\n",
      "iteration: 500060 loss: 0.0016 lr: 0.002\n",
      "iteration: 500070 loss: 0.0010 lr: 0.002\n",
      "iteration: 500080 loss: 0.0008 lr: 0.002\n",
      "iteration: 500090 loss: 0.0010 lr: 0.002\n",
      "iteration: 500100 loss: 0.0009 lr: 0.002\n",
      "iteration: 500110 loss: 0.0013 lr: 0.002\n",
      "iteration: 500120 loss: 0.0013 lr: 0.002\n",
      "iteration: 500130 loss: 0.0011 lr: 0.002\n",
      "iteration: 500140 loss: 0.0012 lr: 0.002\n",
      "iteration: 500150 loss: 0.0010 lr: 0.002\n",
      "iteration: 500160 loss: 0.0009 lr: 0.002\n",
      "iteration: 500170 loss: 0.0014 lr: 0.002\n",
      "iteration: 500180 loss: 0.0009 lr: 0.002\n",
      "iteration: 500190 loss: 0.0008 lr: 0.002\n",
      "iteration: 500200 loss: 0.0011 lr: 0.002\n",
      "iteration: 500210 loss: 0.0010 lr: 0.002\n",
      "iteration: 500220 loss: 0.0009 lr: 0.002\n",
      "iteration: 500230 loss: 0.0010 lr: 0.002\n",
      "iteration: 500240 loss: 0.0011 lr: 0.002\n",
      "iteration: 500250 loss: 0.0013 lr: 0.002\n",
      "iteration: 500260 loss: 0.0011 lr: 0.002\n",
      "iteration: 500270 loss: 0.0006 lr: 0.002\n",
      "iteration: 500280 loss: 0.0015 lr: 0.002\n",
      "iteration: 500290 loss: 0.0010 lr: 0.002\n",
      "iteration: 500300 loss: 0.0011 lr: 0.002\n",
      "iteration: 500310 loss: 0.0007 lr: 0.002\n",
      "iteration: 500320 loss: 0.0010 lr: 0.002\n",
      "iteration: 500330 loss: 0.0015 lr: 0.002\n",
      "iteration: 500340 loss: 0.0013 lr: 0.002\n",
      "iteration: 500350 loss: 0.0014 lr: 0.002\n",
      "iteration: 500360 loss: 0.0009 lr: 0.002\n",
      "iteration: 500370 loss: 0.0021 lr: 0.002\n",
      "iteration: 500380 loss: 0.0013 lr: 0.002\n",
      "iteration: 500390 loss: 0.0008 lr: 0.002\n",
      "iteration: 500400 loss: 0.0011 lr: 0.002\n",
      "iteration: 500410 loss: 0.0015 lr: 0.002\n",
      "iteration: 500420 loss: 0.0012 lr: 0.002\n",
      "iteration: 500430 loss: 0.0010 lr: 0.002\n",
      "iteration: 500440 loss: 0.0010 lr: 0.002\n",
      "iteration: 500450 loss: 0.0010 lr: 0.002\n",
      "iteration: 500460 loss: 0.0010 lr: 0.002\n",
      "iteration: 500470 loss: 0.0011 lr: 0.002\n",
      "iteration: 500480 loss: 0.0011 lr: 0.002\n",
      "iteration: 500490 loss: 0.0008 lr: 0.002\n",
      "iteration: 500500 loss: 0.0016 lr: 0.002\n",
      "iteration: 500510 loss: 0.0015 lr: 0.002\n",
      "iteration: 500520 loss: 0.0015 lr: 0.002\n",
      "iteration: 500530 loss: 0.0019 lr: 0.002\n",
      "iteration: 500540 loss: 0.0011 lr: 0.002\n",
      "iteration: 500550 loss: 0.0010 lr: 0.002\n",
      "iteration: 500560 loss: 0.0010 lr: 0.002\n",
      "iteration: 500570 loss: 0.0009 lr: 0.002\n",
      "iteration: 500580 loss: 0.0009 lr: 0.002\n",
      "iteration: 500590 loss: 0.0014 lr: 0.002\n",
      "iteration: 500600 loss: 0.0017 lr: 0.002\n",
      "iteration: 500610 loss: 0.0008 lr: 0.002\n",
      "iteration: 500620 loss: 0.0014 lr: 0.002\n",
      "iteration: 500630 loss: 0.0009 lr: 0.002\n",
      "iteration: 500640 loss: 0.0022 lr: 0.002\n",
      "iteration: 500650 loss: 0.0010 lr: 0.002\n",
      "iteration: 500660 loss: 0.0013 lr: 0.002\n",
      "iteration: 500670 loss: 0.0010 lr: 0.002\n",
      "iteration: 500680 loss: 0.0011 lr: 0.002\n",
      "iteration: 500690 loss: 0.0014 lr: 0.002\n",
      "iteration: 500700 loss: 0.0009 lr: 0.002\n",
      "iteration: 500710 loss: 0.0008 lr: 0.002\n",
      "iteration: 500720 loss: 0.0011 lr: 0.002\n",
      "iteration: 500730 loss: 0.0008 lr: 0.002\n",
      "iteration: 500740 loss: 0.0008 lr: 0.002\n",
      "iteration: 500750 loss: 0.0009 lr: 0.002\n",
      "iteration: 500760 loss: 0.0011 lr: 0.002\n",
      "iteration: 500770 loss: 0.0010 lr: 0.002\n",
      "iteration: 500780 loss: 0.0019 lr: 0.002\n",
      "iteration: 500790 loss: 0.0007 lr: 0.002\n",
      "iteration: 500800 loss: 0.0013 lr: 0.002\n",
      "iteration: 500810 loss: 0.0010 lr: 0.002\n",
      "iteration: 500820 loss: 0.0011 lr: 0.002\n",
      "iteration: 500830 loss: 0.0016 lr: 0.002\n",
      "iteration: 500840 loss: 0.0014 lr: 0.002\n",
      "iteration: 500850 loss: 0.0013 lr: 0.002\n",
      "iteration: 500860 loss: 0.0015 lr: 0.002\n",
      "iteration: 500870 loss: 0.0010 lr: 0.002\n",
      "iteration: 500880 loss: 0.0009 lr: 0.002\n",
      "iteration: 500890 loss: 0.0008 lr: 0.002\n",
      "iteration: 500900 loss: 0.0010 lr: 0.002\n",
      "iteration: 500910 loss: 0.0007 lr: 0.002\n",
      "iteration: 500920 loss: 0.0013 lr: 0.002\n",
      "iteration: 500930 loss: 0.0009 lr: 0.002\n",
      "iteration: 500940 loss: 0.0009 lr: 0.002\n",
      "iteration: 500950 loss: 0.0010 lr: 0.002\n",
      "iteration: 500960 loss: 0.0015 lr: 0.002\n",
      "iteration: 500970 loss: 0.0013 lr: 0.002\n",
      "iteration: 500980 loss: 0.0009 lr: 0.002\n",
      "iteration: 500990 loss: 0.0015 lr: 0.002\n",
      "iteration: 501000 loss: 0.0011 lr: 0.002\n",
      "iteration: 501010 loss: 0.0008 lr: 0.002\n",
      "iteration: 501020 loss: 0.0016 lr: 0.002\n",
      "iteration: 501030 loss: 0.0017 lr: 0.002\n",
      "iteration: 501040 loss: 0.0010 lr: 0.002\n",
      "iteration: 501050 loss: 0.0008 lr: 0.002\n",
      "iteration: 501060 loss: 0.0016 lr: 0.002\n",
      "iteration: 501070 loss: 0.0013 lr: 0.002\n",
      "iteration: 501080 loss: 0.0009 lr: 0.002\n",
      "iteration: 501090 loss: 0.0008 lr: 0.002\n",
      "iteration: 501100 loss: 0.0022 lr: 0.002\n",
      "iteration: 501110 loss: 0.0010 lr: 0.002\n",
      "iteration: 501120 loss: 0.0012 lr: 0.002\n",
      "iteration: 501130 loss: 0.0008 lr: 0.002\n",
      "iteration: 501140 loss: 0.0014 lr: 0.002\n",
      "iteration: 501150 loss: 0.0014 lr: 0.002\n",
      "iteration: 501160 loss: 0.0013 lr: 0.002\n",
      "iteration: 501170 loss: 0.0008 lr: 0.002\n",
      "iteration: 501180 loss: 0.0012 lr: 0.002\n",
      "iteration: 501190 loss: 0.0022 lr: 0.002\n",
      "iteration: 501200 loss: 0.0012 lr: 0.002\n",
      "iteration: 501210 loss: 0.0010 lr: 0.002\n",
      "iteration: 501220 loss: 0.0009 lr: 0.002\n",
      "iteration: 501230 loss: 0.0014 lr: 0.002\n",
      "iteration: 501240 loss: 0.0012 lr: 0.002\n",
      "iteration: 501250 loss: 0.0015 lr: 0.002\n",
      "iteration: 501260 loss: 0.0018 lr: 0.002\n",
      "iteration: 501270 loss: 0.0012 lr: 0.002\n",
      "iteration: 501280 loss: 0.0007 lr: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 501290 loss: 0.0011 lr: 0.002\n",
      "iteration: 501300 loss: 0.0010 lr: 0.002\n",
      "iteration: 501310 loss: 0.0010 lr: 0.002\n",
      "iteration: 501320 loss: 0.0008 lr: 0.002\n",
      "iteration: 501330 loss: 0.0010 lr: 0.002\n",
      "iteration: 501340 loss: 0.0018 lr: 0.002\n",
      "iteration: 501350 loss: 0.0009 lr: 0.002\n",
      "iteration: 501360 loss: 0.0011 lr: 0.002\n",
      "iteration: 501370 loss: 0.0013 lr: 0.002\n",
      "iteration: 501380 loss: 0.0019 lr: 0.002\n",
      "iteration: 501390 loss: 0.0014 lr: 0.002\n",
      "iteration: 501400 loss: 0.0011 lr: 0.002\n",
      "iteration: 501410 loss: 0.0012 lr: 0.002\n",
      "iteration: 501420 loss: 0.0008 lr: 0.002\n",
      "iteration: 501430 loss: 0.0008 lr: 0.002\n",
      "iteration: 501440 loss: 0.0014 lr: 0.002\n",
      "iteration: 501450 loss: 0.0015 lr: 0.002\n",
      "iteration: 501460 loss: 0.0009 lr: 0.002\n",
      "iteration: 501470 loss: 0.0010 lr: 0.002\n",
      "iteration: 501480 loss: 0.0010 lr: 0.002\n",
      "iteration: 501490 loss: 0.0013 lr: 0.002\n",
      "iteration: 501500 loss: 0.0013 lr: 0.002\n",
      "iteration: 501510 loss: 0.0012 lr: 0.002\n",
      "iteration: 501520 loss: 0.0009 lr: 0.002\n",
      "iteration: 501530 loss: 0.0012 lr: 0.002\n",
      "iteration: 501540 loss: 0.0007 lr: 0.002\n",
      "iteration: 501550 loss: 0.0011 lr: 0.002\n",
      "iteration: 501560 loss: 0.0015 lr: 0.002\n",
      "iteration: 501570 loss: 0.0012 lr: 0.002\n",
      "iteration: 501580 loss: 0.0014 lr: 0.002\n",
      "iteration: 501590 loss: 0.0019 lr: 0.002\n",
      "iteration: 501600 loss: 0.0018 lr: 0.002\n",
      "iteration: 501610 loss: 0.0011 lr: 0.002\n",
      "iteration: 501620 loss: 0.0017 lr: 0.002\n",
      "iteration: 501630 loss: 0.0011 lr: 0.002\n",
      "iteration: 501640 loss: 0.0018 lr: 0.002\n",
      "iteration: 501650 loss: 0.0017 lr: 0.002\n",
      "iteration: 501660 loss: 0.0008 lr: 0.002\n",
      "iteration: 501670 loss: 0.0014 lr: 0.002\n",
      "iteration: 501680 loss: 0.0010 lr: 0.002\n",
      "iteration: 501690 loss: 0.0009 lr: 0.002\n",
      "iteration: 501700 loss: 0.0019 lr: 0.002\n",
      "iteration: 501710 loss: 0.0010 lr: 0.002\n",
      "iteration: 501720 loss: 0.0014 lr: 0.002\n",
      "iteration: 501730 loss: 0.0012 lr: 0.002\n",
      "iteration: 501740 loss: 0.0009 lr: 0.002\n",
      "iteration: 501750 loss: 0.0010 lr: 0.002\n",
      "iteration: 501760 loss: 0.0012 lr: 0.002\n",
      "iteration: 501770 loss: 0.0007 lr: 0.002\n",
      "iteration: 501780 loss: 0.0010 lr: 0.002\n",
      "iteration: 501790 loss: 0.0010 lr: 0.002\n",
      "iteration: 501800 loss: 0.0008 lr: 0.002\n",
      "iteration: 501810 loss: 0.0010 lr: 0.002\n",
      "iteration: 501820 loss: 0.0014 lr: 0.002\n",
      "iteration: 501830 loss: 0.0009 lr: 0.002\n",
      "iteration: 501840 loss: 0.0009 lr: 0.002\n",
      "iteration: 501850 loss: 0.0009 lr: 0.002\n",
      "iteration: 501860 loss: 0.0010 lr: 0.002\n",
      "iteration: 501870 loss: 0.0009 lr: 0.002\n",
      "iteration: 501880 loss: 0.0012 lr: 0.002\n",
      "iteration: 501890 loss: 0.0013 lr: 0.002\n",
      "iteration: 501900 loss: 0.0015 lr: 0.002\n",
      "iteration: 501910 loss: 0.0013 lr: 0.002\n",
      "iteration: 501920 loss: 0.0013 lr: 0.002\n",
      "iteration: 501930 loss: 0.0019 lr: 0.002\n",
      "iteration: 501940 loss: 0.0015 lr: 0.002\n",
      "iteration: 501950 loss: 0.0008 lr: 0.002\n",
      "iteration: 501960 loss: 0.0011 lr: 0.002\n",
      "iteration: 501970 loss: 0.0011 lr: 0.002\n",
      "iteration: 501980 loss: 0.0011 lr: 0.002\n",
      "iteration: 501990 loss: 0.0012 lr: 0.002\n",
      "iteration: 502000 loss: 0.0018 lr: 0.002\n",
      "iteration: 502010 loss: 0.0009 lr: 0.002\n",
      "iteration: 502020 loss: 0.0013 lr: 0.002\n",
      "iteration: 502030 loss: 0.0008 lr: 0.002\n",
      "iteration: 502040 loss: 0.0012 lr: 0.002\n",
      "iteration: 502050 loss: 0.0017 lr: 0.002\n",
      "iteration: 502060 loss: 0.0010 lr: 0.002\n",
      "iteration: 502070 loss: 0.0014 lr: 0.002\n",
      "iteration: 502080 loss: 0.0010 lr: 0.002\n",
      "iteration: 502090 loss: 0.0011 lr: 0.002\n",
      "iteration: 502100 loss: 0.0011 lr: 0.002\n",
      "iteration: 502110 loss: 0.0009 lr: 0.002\n",
      "iteration: 502120 loss: 0.0009 lr: 0.002\n",
      "iteration: 502130 loss: 0.0010 lr: 0.002\n",
      "iteration: 502140 loss: 0.0010 lr: 0.002\n",
      "iteration: 502150 loss: 0.0014 lr: 0.002\n",
      "iteration: 502160 loss: 0.0012 lr: 0.002\n",
      "iteration: 502170 loss: 0.0014 lr: 0.002\n",
      "iteration: 502180 loss: 0.0012 lr: 0.002\n",
      "iteration: 502190 loss: 0.0011 lr: 0.002\n",
      "iteration: 502200 loss: 0.0010 lr: 0.002\n",
      "iteration: 502210 loss: 0.0007 lr: 0.002\n",
      "iteration: 502220 loss: 0.0011 lr: 0.002\n",
      "iteration: 502230 loss: 0.0016 lr: 0.002\n",
      "iteration: 502240 loss: 0.0009 lr: 0.002\n",
      "iteration: 502250 loss: 0.0012 lr: 0.002\n",
      "iteration: 502260 loss: 0.0009 lr: 0.002\n",
      "iteration: 502270 loss: 0.0010 lr: 0.002\n",
      "iteration: 502280 loss: 0.0011 lr: 0.002\n",
      "iteration: 502290 loss: 0.0012 lr: 0.002\n",
      "iteration: 502300 loss: 0.0018 lr: 0.002\n",
      "iteration: 502310 loss: 0.0016 lr: 0.002\n",
      "iteration: 502320 loss: 0.0012 lr: 0.002\n",
      "iteration: 502330 loss: 0.0018 lr: 0.002\n",
      "iteration: 502340 loss: 0.0010 lr: 0.002\n",
      "iteration: 502350 loss: 0.0013 lr: 0.002\n",
      "iteration: 502360 loss: 0.0011 lr: 0.002\n",
      "iteration: 502370 loss: 0.0012 lr: 0.002\n",
      "iteration: 502380 loss: 0.0011 lr: 0.002\n",
      "iteration: 502390 loss: 0.0013 lr: 0.002\n",
      "iteration: 502400 loss: 0.0009 lr: 0.002\n",
      "iteration: 502410 loss: 0.0009 lr: 0.002\n",
      "iteration: 502420 loss: 0.0015 lr: 0.002\n",
      "iteration: 502430 loss: 0.0009 lr: 0.002\n",
      "iteration: 502440 loss: 0.0012 lr: 0.002\n",
      "iteration: 502450 loss: 0.0011 lr: 0.002\n",
      "iteration: 502460 loss: 0.0008 lr: 0.002\n",
      "iteration: 502470 loss: 0.0010 lr: 0.002\n",
      "iteration: 502480 loss: 0.0012 lr: 0.002\n",
      "iteration: 502490 loss: 0.0012 lr: 0.002\n",
      "iteration: 502500 loss: 0.0009 lr: 0.002\n",
      "iteration: 502510 loss: 0.0010 lr: 0.002\n",
      "iteration: 502520 loss: 0.0007 lr: 0.002\n",
      "iteration: 502530 loss: 0.0012 lr: 0.002\n",
      "iteration: 502540 loss: 0.0010 lr: 0.002\n",
      "iteration: 502550 loss: 0.0010 lr: 0.002\n",
      "iteration: 502560 loss: 0.0007 lr: 0.002\n",
      "iteration: 502570 loss: 0.0011 lr: 0.002\n",
      "iteration: 502580 loss: 0.0010 lr: 0.002\n",
      "iteration: 502590 loss: 0.0016 lr: 0.002\n",
      "iteration: 502600 loss: 0.0019 lr: 0.002\n",
      "iteration: 502610 loss: 0.0013 lr: 0.002\n",
      "iteration: 502620 loss: 0.0010 lr: 0.002\n",
      "iteration: 502630 loss: 0.0009 lr: 0.002\n",
      "iteration: 502640 loss: 0.0008 lr: 0.002\n",
      "iteration: 502650 loss: 0.0016 lr: 0.002\n",
      "iteration: 502660 loss: 0.0012 lr: 0.002\n",
      "iteration: 502670 loss: 0.0011 lr: 0.002\n",
      "iteration: 502680 loss: 0.0015 lr: 0.002\n",
      "iteration: 502690 loss: 0.0007 lr: 0.002\n",
      "iteration: 502700 loss: 0.0013 lr: 0.002\n",
      "iteration: 502710 loss: 0.0013 lr: 0.002\n",
      "iteration: 502720 loss: 0.0013 lr: 0.002\n",
      "iteration: 502730 loss: 0.0017 lr: 0.002\n",
      "iteration: 502740 loss: 0.0011 lr: 0.002\n",
      "iteration: 502750 loss: 0.0014 lr: 0.002\n",
      "iteration: 502760 loss: 0.0017 lr: 0.002\n",
      "iteration: 502770 loss: 0.0014 lr: 0.002\n",
      "iteration: 502780 loss: 0.0012 lr: 0.002\n",
      "iteration: 502790 loss: 0.0011 lr: 0.002\n",
      "iteration: 502800 loss: 0.0013 lr: 0.002\n",
      "iteration: 502810 loss: 0.0014 lr: 0.002\n",
      "iteration: 502820 loss: 0.0019 lr: 0.002\n",
      "iteration: 502830 loss: 0.0014 lr: 0.002\n",
      "iteration: 502840 loss: 0.0011 lr: 0.002\n",
      "iteration: 502850 loss: 0.0006 lr: 0.002\n",
      "iteration: 502860 loss: 0.0013 lr: 0.002\n",
      "iteration: 502870 loss: 0.0010 lr: 0.002\n",
      "iteration: 502880 loss: 0.0010 lr: 0.002\n",
      "iteration: 502890 loss: 0.0009 lr: 0.002\n",
      "iteration: 502900 loss: 0.0014 lr: 0.002\n",
      "iteration: 502910 loss: 0.0010 lr: 0.002\n",
      "iteration: 502920 loss: 0.0010 lr: 0.002\n",
      "iteration: 502930 loss: 0.0016 lr: 0.002\n",
      "iteration: 502940 loss: 0.0011 lr: 0.002\n",
      "iteration: 502950 loss: 0.0011 lr: 0.002\n",
      "iteration: 502960 loss: 0.0009 lr: 0.002\n",
      "iteration: 502970 loss: 0.0012 lr: 0.002\n",
      "iteration: 502980 loss: 0.0013 lr: 0.002\n",
      "iteration: 502990 loss: 0.0012 lr: 0.002\n",
      "iteration: 503000 loss: 0.0010 lr: 0.002\n",
      "iteration: 503010 loss: 0.0008 lr: 0.002\n",
      "iteration: 503020 loss: 0.0011 lr: 0.002\n",
      "iteration: 503030 loss: 0.0015 lr: 0.002\n",
      "iteration: 503040 loss: 0.0017 lr: 0.002\n",
      "iteration: 503050 loss: 0.0011 lr: 0.002\n",
      "iteration: 503060 loss: 0.0008 lr: 0.002\n",
      "iteration: 503070 loss: 0.0013 lr: 0.002\n",
      "iteration: 503080 loss: 0.0009 lr: 0.002\n",
      "iteration: 503090 loss: 0.0014 lr: 0.002\n",
      "iteration: 503100 loss: 0.0011 lr: 0.002\n",
      "iteration: 503110 loss: 0.0012 lr: 0.002\n",
      "iteration: 503120 loss: 0.0012 lr: 0.002\n",
      "iteration: 503130 loss: 0.0012 lr: 0.002\n",
      "iteration: 503140 loss: 0.0011 lr: 0.002\n",
      "iteration: 503150 loss: 0.0016 lr: 0.002\n",
      "iteration: 503160 loss: 0.0009 lr: 0.002\n",
      "iteration: 503170 loss: 0.0011 lr: 0.002\n",
      "iteration: 503180 loss: 0.0013 lr: 0.002\n",
      "iteration: 503190 loss: 0.0014 lr: 0.002\n",
      "iteration: 503200 loss: 0.0012 lr: 0.002\n",
      "iteration: 503210 loss: 0.0011 lr: 0.002\n",
      "iteration: 503220 loss: 0.0011 lr: 0.002\n",
      "iteration: 503230 loss: 0.0012 lr: 0.002\n",
      "iteration: 503240 loss: 0.0013 lr: 0.002\n",
      "iteration: 503250 loss: 0.0008 lr: 0.002\n",
      "iteration: 503260 loss: 0.0011 lr: 0.002\n",
      "iteration: 503270 loss: 0.0008 lr: 0.002\n",
      "iteration: 503280 loss: 0.0013 lr: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 503290 loss: 0.0013 lr: 0.002\n",
      "iteration: 503300 loss: 0.0011 lr: 0.002\n",
      "iteration: 503310 loss: 0.0014 lr: 0.002\n",
      "iteration: 503320 loss: 0.0009 lr: 0.002\n",
      "iteration: 503330 loss: 0.0009 lr: 0.002\n",
      "iteration: 503340 loss: 0.0011 lr: 0.002\n",
      "iteration: 503350 loss: 0.0008 lr: 0.002\n",
      "iteration: 503360 loss: 0.0010 lr: 0.002\n",
      "iteration: 503370 loss: 0.0009 lr: 0.002\n",
      "iteration: 503380 loss: 0.0009 lr: 0.002\n",
      "iteration: 503390 loss: 0.0010 lr: 0.002\n",
      "iteration: 503400 loss: 0.0012 lr: 0.002\n",
      "iteration: 503410 loss: 0.0018 lr: 0.002\n",
      "iteration: 503420 loss: 0.0011 lr: 0.002\n",
      "iteration: 503430 loss: 0.0010 lr: 0.002\n",
      "iteration: 503440 loss: 0.0016 lr: 0.002\n",
      "iteration: 503450 loss: 0.0019 lr: 0.002\n",
      "iteration: 503460 loss: 0.0011 lr: 0.002\n",
      "iteration: 503470 loss: 0.0014 lr: 0.002\n",
      "iteration: 503480 loss: 0.0012 lr: 0.002\n",
      "iteration: 503490 loss: 0.0013 lr: 0.002\n",
      "iteration: 503500 loss: 0.0012 lr: 0.002\n",
      "iteration: 503510 loss: 0.0014 lr: 0.002\n",
      "iteration: 503520 loss: 0.0012 lr: 0.002\n",
      "iteration: 503530 loss: 0.0007 lr: 0.002\n",
      "iteration: 503540 loss: 0.0009 lr: 0.002\n",
      "iteration: 503550 loss: 0.0015 lr: 0.002\n",
      "iteration: 503560 loss: 0.0014 lr: 0.002\n",
      "iteration: 503570 loss: 0.0017 lr: 0.002\n",
      "iteration: 503580 loss: 0.0014 lr: 0.002\n",
      "iteration: 503590 loss: 0.0010 lr: 0.002\n",
      "iteration: 503600 loss: 0.0011 lr: 0.002\n",
      "iteration: 503610 loss: 0.0009 lr: 0.002\n",
      "iteration: 503620 loss: 0.0011 lr: 0.002\n",
      "iteration: 503630 loss: 0.0014 lr: 0.002\n",
      "iteration: 503640 loss: 0.0013 lr: 0.002\n",
      "iteration: 503650 loss: 0.0009 lr: 0.002\n",
      "iteration: 503660 loss: 0.0007 lr: 0.002\n",
      "iteration: 503670 loss: 0.0014 lr: 0.002\n",
      "iteration: 503680 loss: 0.0011 lr: 0.002\n",
      "iteration: 503690 loss: 0.0016 lr: 0.002\n",
      "iteration: 503700 loss: 0.0012 lr: 0.002\n",
      "iteration: 503710 loss: 0.0018 lr: 0.002\n",
      "iteration: 503720 loss: 0.0009 lr: 0.002\n",
      "iteration: 503730 loss: 0.0014 lr: 0.002\n",
      "iteration: 503740 loss: 0.0010 lr: 0.002\n",
      "iteration: 503750 loss: 0.0012 lr: 0.002\n",
      "iteration: 503760 loss: 0.0013 lr: 0.002\n",
      "iteration: 503770 loss: 0.0015 lr: 0.002\n",
      "iteration: 503780 loss: 0.0009 lr: 0.002\n",
      "iteration: 503790 loss: 0.0010 lr: 0.002\n",
      "iteration: 503800 loss: 0.0015 lr: 0.002\n",
      "iteration: 503810 loss: 0.0023 lr: 0.002\n",
      "iteration: 503820 loss: 0.0013 lr: 0.002\n",
      "iteration: 503830 loss: 0.0010 lr: 0.002\n",
      "iteration: 503840 loss: 0.0009 lr: 0.002\n",
      "iteration: 503850 loss: 0.0011 lr: 0.002\n",
      "iteration: 503860 loss: 0.0016 lr: 0.002\n",
      "iteration: 503870 loss: 0.0018 lr: 0.002\n",
      "iteration: 503880 loss: 0.0011 lr: 0.002\n",
      "iteration: 503890 loss: 0.0009 lr: 0.002\n",
      "iteration: 503900 loss: 0.0010 lr: 0.002\n",
      "iteration: 503910 loss: 0.0016 lr: 0.002\n",
      "iteration: 503920 loss: 0.0010 lr: 0.002\n",
      "iteration: 503930 loss: 0.0018 lr: 0.002\n",
      "iteration: 503940 loss: 0.0016 lr: 0.002\n",
      "iteration: 503950 loss: 0.0008 lr: 0.002\n",
      "iteration: 503960 loss: 0.0009 lr: 0.002\n",
      "iteration: 503970 loss: 0.0011 lr: 0.002\n",
      "iteration: 503980 loss: 0.0011 lr: 0.002\n",
      "iteration: 503990 loss: 0.0013 lr: 0.002\n",
      "iteration: 504000 loss: 0.0016 lr: 0.002\n",
      "iteration: 504010 loss: 0.0010 lr: 0.002\n",
      "iteration: 504020 loss: 0.0010 lr: 0.002\n",
      "iteration: 504030 loss: 0.0007 lr: 0.002\n",
      "iteration: 504040 loss: 0.0011 lr: 0.002\n",
      "iteration: 504050 loss: 0.0014 lr: 0.002\n",
      "iteration: 504060 loss: 0.0011 lr: 0.002\n",
      "iteration: 504070 loss: 0.0007 lr: 0.002\n",
      "iteration: 504080 loss: 0.0008 lr: 0.002\n",
      "iteration: 504090 loss: 0.0010 lr: 0.002\n",
      "iteration: 504100 loss: 0.0012 lr: 0.002\n",
      "iteration: 504110 loss: 0.0012 lr: 0.002\n",
      "iteration: 504120 loss: 0.0011 lr: 0.002\n",
      "iteration: 504130 loss: 0.0014 lr: 0.002\n",
      "iteration: 504140 loss: 0.0009 lr: 0.002\n",
      "iteration: 504150 loss: 0.0012 lr: 0.002\n",
      "iteration: 504160 loss: 0.0014 lr: 0.002\n",
      "iteration: 504170 loss: 0.0019 lr: 0.002\n",
      "iteration: 504180 loss: 0.0010 lr: 0.002\n",
      "iteration: 504190 loss: 0.0010 lr: 0.002\n",
      "iteration: 504200 loss: 0.0009 lr: 0.002\n",
      "iteration: 504210 loss: 0.0008 lr: 0.002\n",
      "iteration: 504220 loss: 0.0010 lr: 0.002\n",
      "iteration: 504230 loss: 0.0012 lr: 0.002\n",
      "iteration: 504240 loss: 0.0008 lr: 0.002\n",
      "iteration: 504250 loss: 0.0009 lr: 0.002\n",
      "iteration: 504260 loss: 0.0010 lr: 0.002\n",
      "iteration: 504270 loss: 0.0010 lr: 0.002\n",
      "iteration: 504280 loss: 0.0012 lr: 0.002\n",
      "iteration: 504290 loss: 0.0009 lr: 0.002\n",
      "iteration: 504300 loss: 0.0009 lr: 0.002\n",
      "iteration: 504310 loss: 0.0011 lr: 0.002\n",
      "iteration: 504320 loss: 0.0012 lr: 0.002\n",
      "iteration: 504330 loss: 0.0010 lr: 0.002\n",
      "iteration: 504340 loss: 0.0012 lr: 0.002\n",
      "iteration: 504350 loss: 0.0012 lr: 0.002\n",
      "iteration: 504360 loss: 0.0011 lr: 0.002\n",
      "iteration: 504370 loss: 0.0011 lr: 0.002\n",
      "iteration: 504380 loss: 0.0009 lr: 0.002\n",
      "iteration: 504390 loss: 0.0013 lr: 0.002\n",
      "iteration: 504400 loss: 0.0011 lr: 0.002\n",
      "iteration: 504410 loss: 0.0008 lr: 0.002\n",
      "iteration: 504420 loss: 0.0014 lr: 0.002\n",
      "iteration: 504430 loss: 0.0010 lr: 0.002\n",
      "iteration: 504440 loss: 0.0009 lr: 0.002\n",
      "iteration: 504450 loss: 0.0015 lr: 0.002\n",
      "iteration: 504460 loss: 0.0009 lr: 0.002\n",
      "iteration: 504470 loss: 0.0008 lr: 0.002\n",
      "iteration: 504480 loss: 0.0018 lr: 0.002\n",
      "iteration: 504490 loss: 0.0008 lr: 0.002\n",
      "iteration: 504500 loss: 0.0016 lr: 0.002\n",
      "iteration: 504510 loss: 0.0010 lr: 0.002\n",
      "iteration: 504520 loss: 0.0010 lr: 0.002\n",
      "iteration: 504530 loss: 0.0011 lr: 0.002\n",
      "iteration: 504540 loss: 0.0006 lr: 0.002\n",
      "iteration: 504550 loss: 0.0015 lr: 0.002\n",
      "iteration: 504560 loss: 0.0009 lr: 0.002\n",
      "iteration: 504570 loss: 0.0012 lr: 0.002\n",
      "iteration: 504580 loss: 0.0012 lr: 0.002\n",
      "iteration: 504590 loss: 0.0009 lr: 0.002\n",
      "iteration: 504600 loss: 0.0010 lr: 0.002\n",
      "iteration: 504610 loss: 0.0014 lr: 0.002\n",
      "iteration: 504620 loss: 0.0015 lr: 0.002\n",
      "iteration: 504630 loss: 0.0011 lr: 0.002\n",
      "iteration: 504640 loss: 0.0008 lr: 0.002\n",
      "iteration: 504650 loss: 0.0019 lr: 0.002\n",
      "iteration: 504660 loss: 0.0014 lr: 0.002\n",
      "iteration: 504670 loss: 0.0008 lr: 0.002\n",
      "iteration: 504680 loss: 0.0013 lr: 0.002\n",
      "iteration: 504690 loss: 0.0008 lr: 0.002\n",
      "iteration: 504700 loss: 0.0010 lr: 0.002\n",
      "iteration: 504710 loss: 0.0012 lr: 0.002\n",
      "iteration: 504720 loss: 0.0013 lr: 0.002\n",
      "iteration: 504730 loss: 0.0012 lr: 0.002\n",
      "iteration: 504740 loss: 0.0019 lr: 0.002\n",
      "iteration: 504750 loss: 0.0010 lr: 0.002\n",
      "iteration: 504760 loss: 0.0011 lr: 0.002\n",
      "iteration: 504770 loss: 0.0011 lr: 0.002\n",
      "iteration: 504780 loss: 0.0014 lr: 0.002\n",
      "iteration: 504790 loss: 0.0012 lr: 0.002\n",
      "iteration: 504800 loss: 0.0010 lr: 0.002\n",
      "iteration: 504810 loss: 0.0010 lr: 0.002\n",
      "iteration: 504820 loss: 0.0016 lr: 0.002\n",
      "iteration: 504830 loss: 0.0014 lr: 0.002\n",
      "iteration: 504840 loss: 0.0014 lr: 0.002\n",
      "iteration: 504850 loss: 0.0011 lr: 0.002\n",
      "iteration: 504860 loss: 0.0007 lr: 0.002\n",
      "iteration: 504870 loss: 0.0012 lr: 0.002\n",
      "iteration: 504880 loss: 0.0007 lr: 0.002\n",
      "iteration: 504890 loss: 0.0016 lr: 0.002\n",
      "iteration: 504900 loss: 0.0008 lr: 0.002\n",
      "iteration: 504910 loss: 0.0009 lr: 0.002\n",
      "iteration: 504920 loss: 0.0008 lr: 0.002\n",
      "iteration: 504930 loss: 0.0017 lr: 0.002\n",
      "iteration: 504940 loss: 0.0010 lr: 0.002\n",
      "iteration: 504950 loss: 0.0014 lr: 0.002\n",
      "iteration: 504960 loss: 0.0011 lr: 0.002\n",
      "iteration: 504970 loss: 0.0027 lr: 0.002\n",
      "iteration: 504980 loss: 0.0007 lr: 0.002\n",
      "iteration: 504990 loss: 0.0009 lr: 0.002\n",
      "iteration: 505000 loss: 0.0027 lr: 0.002\n",
      "iteration: 505010 loss: 0.0007 lr: 0.002\n",
      "iteration: 505020 loss: 0.0012 lr: 0.002\n",
      "iteration: 505030 loss: 0.0011 lr: 0.002\n",
      "iteration: 505040 loss: 0.0009 lr: 0.002\n",
      "iteration: 505050 loss: 0.0015 lr: 0.002\n",
      "iteration: 505060 loss: 0.0015 lr: 0.002\n",
      "iteration: 505070 loss: 0.0011 lr: 0.002\n",
      "iteration: 505080 loss: 0.0014 lr: 0.002\n",
      "iteration: 505090 loss: 0.0010 lr: 0.002\n",
      "iteration: 505100 loss: 0.0014 lr: 0.002\n",
      "iteration: 505110 loss: 0.0012 lr: 0.002\n",
      "iteration: 505120 loss: 0.0011 lr: 0.002\n",
      "iteration: 505130 loss: 0.0010 lr: 0.002\n",
      "iteration: 505140 loss: 0.0016 lr: 0.002\n",
      "iteration: 505150 loss: 0.0010 lr: 0.002\n",
      "iteration: 505160 loss: 0.0012 lr: 0.002\n",
      "iteration: 505170 loss: 0.0012 lr: 0.002\n",
      "iteration: 505180 loss: 0.0011 lr: 0.002\n",
      "iteration: 505190 loss: 0.0011 lr: 0.002\n",
      "iteration: 505200 loss: 0.0015 lr: 0.002\n",
      "iteration: 505210 loss: 0.0011 lr: 0.002\n",
      "iteration: 505220 loss: 0.0006 lr: 0.002\n",
      "iteration: 505230 loss: 0.0013 lr: 0.002\n",
      "iteration: 505240 loss: 0.0015 lr: 0.002\n",
      "iteration: 505250 loss: 0.0008 lr: 0.002\n",
      "iteration: 505260 loss: 0.0008 lr: 0.002\n",
      "iteration: 505270 loss: 0.0009 lr: 0.002\n",
      "iteration: 505280 loss: 0.0011 lr: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 505290 loss: 0.0022 lr: 0.002\n",
      "iteration: 505300 loss: 0.0013 lr: 0.002\n",
      "iteration: 505310 loss: 0.0024 lr: 0.002\n",
      "iteration: 505320 loss: 0.0010 lr: 0.002\n",
      "iteration: 505330 loss: 0.0006 lr: 0.002\n",
      "iteration: 505340 loss: 0.0012 lr: 0.002\n",
      "iteration: 505350 loss: 0.0010 lr: 0.002\n",
      "iteration: 505360 loss: 0.0015 lr: 0.002\n",
      "iteration: 505370 loss: 0.0009 lr: 0.002\n",
      "iteration: 505380 loss: 0.0008 lr: 0.002\n",
      "iteration: 505390 loss: 0.0008 lr: 0.002\n",
      "iteration: 505400 loss: 0.0012 lr: 0.002\n",
      "iteration: 505410 loss: 0.0013 lr: 0.002\n",
      "iteration: 505420 loss: 0.0015 lr: 0.002\n",
      "iteration: 505430 loss: 0.0012 lr: 0.002\n",
      "iteration: 505440 loss: 0.0010 lr: 0.002\n",
      "iteration: 505450 loss: 0.0010 lr: 0.002\n",
      "iteration: 505460 loss: 0.0008 lr: 0.002\n",
      "iteration: 505470 loss: 0.0012 lr: 0.002\n",
      "iteration: 505480 loss: 0.0009 lr: 0.002\n",
      "iteration: 505490 loss: 0.0013 lr: 0.002\n",
      "iteration: 505500 loss: 0.0011 lr: 0.002\n",
      "iteration: 505510 loss: 0.0012 lr: 0.002\n",
      "iteration: 505520 loss: 0.0016 lr: 0.002\n",
      "iteration: 505530 loss: 0.0018 lr: 0.002\n",
      "iteration: 505540 loss: 0.0013 lr: 0.002\n",
      "iteration: 505550 loss: 0.0015 lr: 0.002\n",
      "iteration: 505560 loss: 0.0010 lr: 0.002\n",
      "iteration: 505570 loss: 0.0011 lr: 0.002\n",
      "iteration: 505580 loss: 0.0009 lr: 0.002\n",
      "iteration: 505590 loss: 0.0009 lr: 0.002\n",
      "iteration: 505600 loss: 0.0010 lr: 0.002\n",
      "iteration: 505610 loss: 0.0009 lr: 0.002\n",
      "iteration: 505620 loss: 0.0013 lr: 0.002\n",
      "iteration: 505630 loss: 0.0011 lr: 0.002\n",
      "iteration: 505640 loss: 0.0009 lr: 0.002\n",
      "iteration: 505650 loss: 0.0007 lr: 0.002\n",
      "iteration: 505660 loss: 0.0010 lr: 0.002\n",
      "iteration: 505670 loss: 0.0012 lr: 0.002\n",
      "iteration: 505680 loss: 0.0012 lr: 0.002\n",
      "iteration: 505690 loss: 0.0009 lr: 0.002\n",
      "iteration: 505700 loss: 0.0009 lr: 0.002\n",
      "iteration: 505710 loss: 0.0013 lr: 0.002\n",
      "iteration: 505720 loss: 0.0011 lr: 0.002\n",
      "iteration: 505730 loss: 0.0011 lr: 0.002\n",
      "iteration: 505740 loss: 0.0009 lr: 0.002\n",
      "iteration: 505750 loss: 0.0014 lr: 0.002\n",
      "iteration: 505760 loss: 0.0009 lr: 0.002\n",
      "iteration: 505770 loss: 0.0008 lr: 0.002\n",
      "iteration: 505780 loss: 0.0014 lr: 0.002\n",
      "iteration: 505790 loss: 0.0009 lr: 0.002\n",
      "iteration: 505800 loss: 0.0009 lr: 0.002\n",
      "iteration: 505810 loss: 0.0013 lr: 0.002\n",
      "iteration: 505820 loss: 0.0014 lr: 0.002\n",
      "iteration: 505830 loss: 0.0010 lr: 0.002\n",
      "iteration: 505840 loss: 0.0010 lr: 0.002\n",
      "iteration: 505850 loss: 0.0010 lr: 0.002\n",
      "iteration: 505860 loss: 0.0016 lr: 0.002\n",
      "iteration: 505870 loss: 0.0015 lr: 0.002\n",
      "iteration: 505880 loss: 0.0013 lr: 0.002\n",
      "iteration: 505890 loss: 0.0010 lr: 0.002\n",
      "iteration: 505900 loss: 0.0013 lr: 0.002\n",
      "iteration: 505910 loss: 0.0011 lr: 0.002\n",
      "iteration: 505920 loss: 0.0007 lr: 0.002\n",
      "iteration: 505930 loss: 0.0008 lr: 0.002\n",
      "iteration: 505940 loss: 0.0011 lr: 0.002\n",
      "iteration: 505950 loss: 0.0008 lr: 0.002\n",
      "iteration: 505960 loss: 0.0013 lr: 0.002\n",
      "iteration: 505970 loss: 0.0012 lr: 0.002\n",
      "iteration: 505980 loss: 0.0010 lr: 0.002\n",
      "iteration: 505990 loss: 0.0008 lr: 0.002\n",
      "iteration: 506000 loss: 0.0016 lr: 0.002\n",
      "iteration: 506010 loss: 0.0008 lr: 0.002\n",
      "iteration: 506020 loss: 0.0008 lr: 0.002\n",
      "iteration: 506030 loss: 0.0010 lr: 0.002\n",
      "iteration: 506040 loss: 0.0013 lr: 0.002\n",
      "iteration: 506050 loss: 0.0010 lr: 0.002\n",
      "iteration: 506060 loss: 0.0010 lr: 0.002\n",
      "iteration: 506070 loss: 0.0015 lr: 0.002\n",
      "iteration: 506080 loss: 0.0008 lr: 0.002\n",
      "iteration: 506090 loss: 0.0010 lr: 0.002\n",
      "iteration: 506100 loss: 0.0020 lr: 0.002\n",
      "iteration: 506110 loss: 0.0012 lr: 0.002\n",
      "iteration: 506120 loss: 0.0015 lr: 0.002\n",
      "iteration: 506130 loss: 0.0010 lr: 0.002\n",
      "iteration: 506140 loss: 0.0011 lr: 0.002\n",
      "iteration: 506150 loss: 0.0018 lr: 0.002\n",
      "iteration: 506160 loss: 0.0010 lr: 0.002\n",
      "iteration: 506170 loss: 0.0011 lr: 0.002\n",
      "iteration: 506180 loss: 0.0008 lr: 0.002\n",
      "iteration: 506190 loss: 0.0013 lr: 0.002\n",
      "iteration: 506200 loss: 0.0012 lr: 0.002\n",
      "iteration: 506210 loss: 0.0008 lr: 0.002\n",
      "iteration: 506220 loss: 0.0013 lr: 0.002\n",
      "iteration: 506230 loss: 0.0017 lr: 0.002\n",
      "iteration: 506240 loss: 0.0008 lr: 0.002\n",
      "iteration: 506250 loss: 0.0011 lr: 0.002\n",
      "iteration: 506260 loss: 0.0015 lr: 0.002\n",
      "iteration: 506270 loss: 0.0011 lr: 0.002\n",
      "iteration: 506280 loss: 0.0019 lr: 0.002\n",
      "iteration: 506290 loss: 0.0011 lr: 0.002\n",
      "iteration: 506300 loss: 0.0013 lr: 0.002\n",
      "iteration: 506310 loss: 0.0008 lr: 0.002\n",
      "iteration: 506320 loss: 0.0014 lr: 0.002\n",
      "iteration: 506330 loss: 0.0009 lr: 0.002\n",
      "iteration: 506340 loss: 0.0012 lr: 0.002\n",
      "iteration: 506350 loss: 0.0016 lr: 0.002\n",
      "iteration: 506360 loss: 0.0011 lr: 0.002\n",
      "iteration: 506370 loss: 0.0008 lr: 0.002\n",
      "iteration: 506380 loss: 0.0016 lr: 0.002\n",
      "iteration: 506390 loss: 0.0013 lr: 0.002\n",
      "iteration: 506400 loss: 0.0014 lr: 0.002\n",
      "iteration: 506410 loss: 0.0013 lr: 0.002\n",
      "iteration: 506420 loss: 0.0009 lr: 0.002\n",
      "iteration: 506430 loss: 0.0012 lr: 0.002\n",
      "iteration: 506440 loss: 0.0010 lr: 0.002\n",
      "iteration: 506450 loss: 0.0007 lr: 0.002\n",
      "iteration: 506460 loss: 0.0015 lr: 0.002\n",
      "iteration: 506470 loss: 0.0016 lr: 0.002\n",
      "iteration: 506480 loss: 0.0009 lr: 0.002\n",
      "iteration: 506490 loss: 0.0012 lr: 0.002\n",
      "iteration: 506500 loss: 0.0010 lr: 0.002\n",
      "iteration: 506510 loss: 0.0010 lr: 0.002\n",
      "iteration: 506520 loss: 0.0013 lr: 0.002\n",
      "iteration: 506530 loss: 0.0012 lr: 0.002\n",
      "iteration: 506540 loss: 0.0014 lr: 0.002\n",
      "iteration: 506550 loss: 0.0015 lr: 0.002\n",
      "iteration: 506560 loss: 0.0007 lr: 0.002\n",
      "iteration: 506570 loss: 0.0014 lr: 0.002\n",
      "iteration: 506580 loss: 0.0010 lr: 0.002\n",
      "iteration: 506590 loss: 0.0013 lr: 0.002\n",
      "iteration: 506600 loss: 0.0008 lr: 0.002\n",
      "iteration: 506610 loss: 0.0012 lr: 0.002\n",
      "iteration: 506620 loss: 0.0015 lr: 0.002\n",
      "iteration: 506630 loss: 0.0010 lr: 0.002\n",
      "iteration: 506640 loss: 0.0007 lr: 0.002\n",
      "iteration: 506650 loss: 0.0013 lr: 0.002\n",
      "iteration: 506660 loss: 0.0008 lr: 0.002\n",
      "iteration: 506670 loss: 0.0006 lr: 0.002\n",
      "iteration: 506680 loss: 0.0008 lr: 0.002\n",
      "iteration: 506690 loss: 0.0013 lr: 0.002\n",
      "iteration: 506700 loss: 0.0012 lr: 0.002\n",
      "iteration: 506710 loss: 0.0012 lr: 0.002\n",
      "iteration: 506720 loss: 0.0020 lr: 0.002\n",
      "iteration: 506730 loss: 0.0012 lr: 0.002\n",
      "iteration: 506740 loss: 0.0010 lr: 0.002\n",
      "iteration: 506750 loss: 0.0009 lr: 0.002\n",
      "iteration: 506760 loss: 0.0008 lr: 0.002\n",
      "iteration: 506770 loss: 0.0008 lr: 0.002\n",
      "iteration: 506780 loss: 0.0012 lr: 0.002\n",
      "iteration: 506790 loss: 0.0007 lr: 0.002\n",
      "iteration: 506800 loss: 0.0017 lr: 0.002\n",
      "iteration: 506810 loss: 0.0011 lr: 0.002\n",
      "iteration: 506820 loss: 0.0007 lr: 0.002\n",
      "iteration: 506830 loss: 0.0009 lr: 0.002\n",
      "iteration: 506840 loss: 0.0007 lr: 0.002\n",
      "iteration: 506850 loss: 0.0008 lr: 0.002\n",
      "iteration: 506860 loss: 0.0017 lr: 0.002\n",
      "iteration: 506870 loss: 0.0013 lr: 0.002\n",
      "iteration: 506880 loss: 0.0010 lr: 0.002\n",
      "iteration: 506890 loss: 0.0012 lr: 0.002\n",
      "iteration: 506900 loss: 0.0010 lr: 0.002\n",
      "iteration: 506910 loss: 0.0015 lr: 0.002\n",
      "iteration: 506920 loss: 0.0009 lr: 0.002\n",
      "iteration: 506930 loss: 0.0011 lr: 0.002\n",
      "iteration: 506940 loss: 0.0012 lr: 0.002\n",
      "iteration: 506950 loss: 0.0006 lr: 0.002\n",
      "iteration: 506960 loss: 0.0011 lr: 0.002\n",
      "iteration: 506970 loss: 0.0007 lr: 0.002\n",
      "iteration: 506980 loss: 0.0009 lr: 0.002\n",
      "iteration: 506990 loss: 0.0010 lr: 0.002\n",
      "iteration: 507000 loss: 0.0010 lr: 0.002\n",
      "iteration: 507010 loss: 0.0009 lr: 0.002\n",
      "iteration: 507020 loss: 0.0006 lr: 0.002\n",
      "iteration: 507030 loss: 0.0009 lr: 0.002\n",
      "iteration: 507040 loss: 0.0010 lr: 0.002\n",
      "iteration: 507050 loss: 0.0011 lr: 0.002\n",
      "iteration: 507060 loss: 0.0008 lr: 0.002\n",
      "iteration: 507070 loss: 0.0008 lr: 0.002\n",
      "iteration: 507080 loss: 0.0010 lr: 0.002\n",
      "iteration: 507090 loss: 0.0007 lr: 0.002\n",
      "iteration: 507100 loss: 0.0015 lr: 0.002\n",
      "iteration: 507110 loss: 0.0013 lr: 0.002\n",
      "iteration: 507120 loss: 0.0009 lr: 0.002\n",
      "iteration: 507130 loss: 0.0012 lr: 0.002\n",
      "iteration: 507140 loss: 0.0009 lr: 0.002\n",
      "iteration: 507150 loss: 0.0016 lr: 0.002\n",
      "iteration: 507160 loss: 0.0013 lr: 0.002\n",
      "iteration: 507170 loss: 0.0008 lr: 0.002\n",
      "iteration: 507180 loss: 0.0019 lr: 0.002\n",
      "iteration: 507190 loss: 0.0014 lr: 0.002\n",
      "iteration: 507200 loss: 0.0009 lr: 0.002\n",
      "iteration: 507210 loss: 0.0010 lr: 0.002\n",
      "iteration: 507220 loss: 0.0016 lr: 0.002\n",
      "iteration: 507230 loss: 0.0011 lr: 0.002\n",
      "iteration: 507240 loss: 0.0010 lr: 0.002\n",
      "iteration: 507250 loss: 0.0012 lr: 0.002\n",
      "iteration: 507260 loss: 0.0011 lr: 0.002\n",
      "iteration: 507270 loss: 0.0010 lr: 0.002\n",
      "iteration: 507280 loss: 0.0012 lr: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 507290 loss: 0.0009 lr: 0.002\n",
      "iteration: 507300 loss: 0.0011 lr: 0.002\n",
      "iteration: 507310 loss: 0.0013 lr: 0.002\n",
      "iteration: 507320 loss: 0.0007 lr: 0.002\n",
      "iteration: 507330 loss: 0.0007 lr: 0.002\n",
      "iteration: 507340 loss: 0.0009 lr: 0.002\n",
      "iteration: 507350 loss: 0.0013 lr: 0.002\n",
      "iteration: 507360 loss: 0.0006 lr: 0.002\n",
      "iteration: 507370 loss: 0.0013 lr: 0.002\n",
      "iteration: 507380 loss: 0.0012 lr: 0.002\n",
      "iteration: 507390 loss: 0.0008 lr: 0.002\n",
      "iteration: 507400 loss: 0.0009 lr: 0.002\n",
      "iteration: 507410 loss: 0.0014 lr: 0.002\n",
      "iteration: 507420 loss: 0.0011 lr: 0.002\n",
      "iteration: 507430 loss: 0.0015 lr: 0.002\n",
      "iteration: 507440 loss: 0.0014 lr: 0.002\n",
      "iteration: 507450 loss: 0.0011 lr: 0.002\n",
      "iteration: 507460 loss: 0.0014 lr: 0.002\n",
      "iteration: 507470 loss: 0.0021 lr: 0.002\n",
      "iteration: 507480 loss: 0.0016 lr: 0.002\n",
      "iteration: 507490 loss: 0.0019 lr: 0.002\n",
      "iteration: 507500 loss: 0.0008 lr: 0.002\n",
      "iteration: 507510 loss: 0.0013 lr: 0.002\n",
      "iteration: 507520 loss: 0.0009 lr: 0.002\n",
      "iteration: 507530 loss: 0.0010 lr: 0.002\n",
      "iteration: 507540 loss: 0.0013 lr: 0.002\n",
      "iteration: 507550 loss: 0.0011 lr: 0.002\n",
      "iteration: 507560 loss: 0.0014 lr: 0.002\n",
      "iteration: 507570 loss: 0.0009 lr: 0.002\n",
      "iteration: 507580 loss: 0.0013 lr: 0.002\n",
      "iteration: 507590 loss: 0.0009 lr: 0.002\n",
      "iteration: 507600 loss: 0.0009 lr: 0.002\n",
      "iteration: 507610 loss: 0.0014 lr: 0.002\n",
      "iteration: 507620 loss: 0.0015 lr: 0.002\n",
      "iteration: 507630 loss: 0.0008 lr: 0.002\n",
      "iteration: 507640 loss: 0.0010 lr: 0.002\n",
      "iteration: 507650 loss: 0.0012 lr: 0.002\n",
      "iteration: 507660 loss: 0.0016 lr: 0.002\n",
      "iteration: 507670 loss: 0.0009 lr: 0.002\n",
      "iteration: 507680 loss: 0.0012 lr: 0.002\n",
      "iteration: 507690 loss: 0.0012 lr: 0.002\n",
      "iteration: 507700 loss: 0.0010 lr: 0.002\n",
      "iteration: 507710 loss: 0.0012 lr: 0.002\n",
      "iteration: 507720 loss: 0.0012 lr: 0.002\n",
      "iteration: 507730 loss: 0.0012 lr: 0.002\n",
      "iteration: 507740 loss: 0.0012 lr: 0.002\n",
      "iteration: 507750 loss: 0.0016 lr: 0.002\n",
      "iteration: 507760 loss: 0.0012 lr: 0.002\n",
      "iteration: 507770 loss: 0.0010 lr: 0.002\n",
      "iteration: 507780 loss: 0.0015 lr: 0.002\n",
      "iteration: 507790 loss: 0.0008 lr: 0.002\n",
      "iteration: 507800 loss: 0.0013 lr: 0.002\n",
      "iteration: 507810 loss: 0.0014 lr: 0.002\n",
      "iteration: 507820 loss: 0.0009 lr: 0.002\n",
      "iteration: 507830 loss: 0.0009 lr: 0.002\n",
      "iteration: 507840 loss: 0.0011 lr: 0.002\n",
      "iteration: 507850 loss: 0.0007 lr: 0.002\n",
      "iteration: 507860 loss: 0.0010 lr: 0.002\n",
      "iteration: 507870 loss: 0.0016 lr: 0.002\n",
      "iteration: 507880 loss: 0.0011 lr: 0.002\n",
      "iteration: 507890 loss: 0.0024 lr: 0.002\n",
      "iteration: 507900 loss: 0.0014 lr: 0.002\n",
      "iteration: 507910 loss: 0.0015 lr: 0.002\n",
      "iteration: 507920 loss: 0.0011 lr: 0.002\n",
      "iteration: 507930 loss: 0.0016 lr: 0.002\n",
      "iteration: 507940 loss: 0.0011 lr: 0.002\n",
      "iteration: 507950 loss: 0.0015 lr: 0.002\n",
      "iteration: 507960 loss: 0.0014 lr: 0.002\n",
      "iteration: 507970 loss: 0.0009 lr: 0.002\n",
      "iteration: 507980 loss: 0.0010 lr: 0.002\n",
      "iteration: 507990 loss: 0.0008 lr: 0.002\n",
      "iteration: 508000 loss: 0.0011 lr: 0.002\n",
      "iteration: 508010 loss: 0.0014 lr: 0.002\n",
      "iteration: 508020 loss: 0.0013 lr: 0.002\n",
      "iteration: 508030 loss: 0.0009 lr: 0.002\n",
      "iteration: 508040 loss: 0.0011 lr: 0.002\n",
      "iteration: 508050 loss: 0.0012 lr: 0.002\n",
      "iteration: 508060 loss: 0.0010 lr: 0.002\n",
      "iteration: 508070 loss: 0.0013 lr: 0.002\n",
      "iteration: 508080 loss: 0.0010 lr: 0.002\n",
      "iteration: 508090 loss: 0.0012 lr: 0.002\n",
      "iteration: 508100 loss: 0.0009 lr: 0.002\n",
      "iteration: 508110 loss: 0.0012 lr: 0.002\n",
      "iteration: 508120 loss: 0.0015 lr: 0.002\n",
      "iteration: 508130 loss: 0.0009 lr: 0.002\n",
      "iteration: 508140 loss: 0.0015 lr: 0.002\n",
      "iteration: 508150 loss: 0.0012 lr: 0.002\n",
      "iteration: 508160 loss: 0.0011 lr: 0.002\n",
      "iteration: 508170 loss: 0.0011 lr: 0.002\n",
      "iteration: 508180 loss: 0.0020 lr: 0.002\n",
      "iteration: 508190 loss: 0.0011 lr: 0.002\n",
      "iteration: 508200 loss: 0.0017 lr: 0.002\n",
      "iteration: 508210 loss: 0.0011 lr: 0.002\n",
      "iteration: 508220 loss: 0.0012 lr: 0.002\n",
      "iteration: 508230 loss: 0.0012 lr: 0.002\n",
      "iteration: 508240 loss: 0.0008 lr: 0.002\n",
      "iteration: 508250 loss: 0.0007 lr: 0.002\n",
      "iteration: 508260 loss: 0.0019 lr: 0.002\n",
      "iteration: 508270 loss: 0.0008 lr: 0.002\n",
      "iteration: 508280 loss: 0.0011 lr: 0.002\n",
      "iteration: 508290 loss: 0.0007 lr: 0.002\n",
      "iteration: 508300 loss: 0.0014 lr: 0.002\n",
      "iteration: 508310 loss: 0.0012 lr: 0.002\n",
      "iteration: 508320 loss: 0.0009 lr: 0.002\n",
      "iteration: 508330 loss: 0.0009 lr: 0.002\n",
      "iteration: 508340 loss: 0.0008 lr: 0.002\n",
      "iteration: 508350 loss: 0.0017 lr: 0.002\n",
      "iteration: 508360 loss: 0.0010 lr: 0.002\n",
      "iteration: 508370 loss: 0.0008 lr: 0.002\n",
      "iteration: 508380 loss: 0.0012 lr: 0.002\n",
      "iteration: 508390 loss: 0.0010 lr: 0.002\n",
      "iteration: 508400 loss: 0.0015 lr: 0.002\n",
      "iteration: 508410 loss: 0.0012 lr: 0.002\n",
      "iteration: 508420 loss: 0.0009 lr: 0.002\n",
      "iteration: 508430 loss: 0.0010 lr: 0.002\n",
      "iteration: 508440 loss: 0.0012 lr: 0.002\n",
      "iteration: 508450 loss: 0.0009 lr: 0.002\n",
      "iteration: 508460 loss: 0.0011 lr: 0.002\n",
      "iteration: 508470 loss: 0.0013 lr: 0.002\n",
      "iteration: 508480 loss: 0.0013 lr: 0.002\n",
      "iteration: 508490 loss: 0.0013 lr: 0.002\n",
      "iteration: 508500 loss: 0.0012 lr: 0.002\n",
      "iteration: 508510 loss: 0.0008 lr: 0.002\n",
      "iteration: 508520 loss: 0.0013 lr: 0.002\n",
      "iteration: 508530 loss: 0.0015 lr: 0.002\n",
      "iteration: 508540 loss: 0.0013 lr: 0.002\n",
      "iteration: 508550 loss: 0.0011 lr: 0.002\n",
      "iteration: 508560 loss: 0.0016 lr: 0.002\n",
      "iteration: 508570 loss: 0.0014 lr: 0.002\n",
      "iteration: 508580 loss: 0.0012 lr: 0.002\n",
      "iteration: 508590 loss: 0.0009 lr: 0.002\n",
      "iteration: 508600 loss: 0.0010 lr: 0.002\n",
      "iteration: 508610 loss: 0.0008 lr: 0.002\n",
      "iteration: 508620 loss: 0.0012 lr: 0.002\n",
      "iteration: 508630 loss: 0.0014 lr: 0.002\n",
      "iteration: 508640 loss: 0.0009 lr: 0.002\n",
      "iteration: 508650 loss: 0.0009 lr: 0.002\n",
      "iteration: 508660 loss: 0.0013 lr: 0.002\n",
      "iteration: 508670 loss: 0.0009 lr: 0.002\n",
      "iteration: 508680 loss: 0.0008 lr: 0.002\n",
      "iteration: 508690 loss: 0.0010 lr: 0.002\n",
      "iteration: 508700 loss: 0.0008 lr: 0.002\n",
      "iteration: 508710 loss: 0.0012 lr: 0.002\n",
      "iteration: 508720 loss: 0.0013 lr: 0.002\n",
      "iteration: 508730 loss: 0.0011 lr: 0.002\n",
      "iteration: 508740 loss: 0.0017 lr: 0.002\n",
      "iteration: 508750 loss: 0.0008 lr: 0.002\n",
      "iteration: 508760 loss: 0.0007 lr: 0.002\n",
      "iteration: 508770 loss: 0.0010 lr: 0.002\n",
      "iteration: 508780 loss: 0.0009 lr: 0.002\n",
      "iteration: 508790 loss: 0.0012 lr: 0.002\n",
      "iteration: 508800 loss: 0.0011 lr: 0.002\n",
      "iteration: 508810 loss: 0.0009 lr: 0.002\n",
      "iteration: 508820 loss: 0.0012 lr: 0.002\n",
      "iteration: 508830 loss: 0.0013 lr: 0.002\n",
      "iteration: 508840 loss: 0.0012 lr: 0.002\n",
      "iteration: 508850 loss: 0.0010 lr: 0.002\n",
      "iteration: 508860 loss: 0.0013 lr: 0.002\n",
      "iteration: 508870 loss: 0.0008 lr: 0.002\n",
      "iteration: 508880 loss: 0.0011 lr: 0.002\n",
      "iteration: 508890 loss: 0.0014 lr: 0.002\n",
      "iteration: 508900 loss: 0.0008 lr: 0.002\n",
      "iteration: 508910 loss: 0.0011 lr: 0.002\n",
      "iteration: 508920 loss: 0.0013 lr: 0.002\n",
      "iteration: 508930 loss: 0.0013 lr: 0.002\n",
      "iteration: 508940 loss: 0.0015 lr: 0.002\n",
      "iteration: 508950 loss: 0.0011 lr: 0.002\n",
      "iteration: 508960 loss: 0.0009 lr: 0.002\n",
      "iteration: 508970 loss: 0.0010 lr: 0.002\n",
      "iteration: 508980 loss: 0.0012 lr: 0.002\n",
      "iteration: 508990 loss: 0.0013 lr: 0.002\n",
      "iteration: 509000 loss: 0.0015 lr: 0.002\n",
      "iteration: 509010 loss: 0.0011 lr: 0.002\n",
      "iteration: 509020 loss: 0.0011 lr: 0.002\n",
      "iteration: 509030 loss: 0.0012 lr: 0.002\n",
      "iteration: 509040 loss: 0.0008 lr: 0.002\n",
      "iteration: 509050 loss: 0.0013 lr: 0.002\n",
      "iteration: 509060 loss: 0.0013 lr: 0.002\n",
      "iteration: 509070 loss: 0.0009 lr: 0.002\n",
      "iteration: 509080 loss: 0.0007 lr: 0.002\n",
      "iteration: 509090 loss: 0.0010 lr: 0.002\n",
      "iteration: 509100 loss: 0.0012 lr: 0.002\n",
      "iteration: 509110 loss: 0.0011 lr: 0.002\n",
      "iteration: 509120 loss: 0.0009 lr: 0.002\n",
      "iteration: 509130 loss: 0.0009 lr: 0.002\n",
      "iteration: 509140 loss: 0.0013 lr: 0.002\n",
      "iteration: 509150 loss: 0.0019 lr: 0.002\n",
      "iteration: 509160 loss: 0.0010 lr: 0.002\n",
      "iteration: 509170 loss: 0.0011 lr: 0.002\n",
      "iteration: 509180 loss: 0.0010 lr: 0.002\n",
      "iteration: 509190 loss: 0.0015 lr: 0.002\n",
      "iteration: 509200 loss: 0.0013 lr: 0.002\n",
      "iteration: 509210 loss: 0.0013 lr: 0.002\n",
      "iteration: 509220 loss: 0.0010 lr: 0.002\n",
      "iteration: 509230 loss: 0.0013 lr: 0.002\n",
      "iteration: 509240 loss: 0.0017 lr: 0.002\n",
      "iteration: 509250 loss: 0.0016 lr: 0.002\n",
      "iteration: 509260 loss: 0.0017 lr: 0.002\n",
      "iteration: 509270 loss: 0.0010 lr: 0.002\n",
      "iteration: 509280 loss: 0.0009 lr: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 509290 loss: 0.0008 lr: 0.002\n",
      "iteration: 509300 loss: 0.0014 lr: 0.002\n",
      "iteration: 509310 loss: 0.0010 lr: 0.002\n",
      "iteration: 509320 loss: 0.0016 lr: 0.002\n",
      "iteration: 509330 loss: 0.0009 lr: 0.002\n",
      "iteration: 509340 loss: 0.0008 lr: 0.002\n",
      "iteration: 509350 loss: 0.0019 lr: 0.002\n",
      "iteration: 509360 loss: 0.0008 lr: 0.002\n",
      "iteration: 509370 loss: 0.0012 lr: 0.002\n",
      "iteration: 509380 loss: 0.0010 lr: 0.002\n",
      "iteration: 509390 loss: 0.0014 lr: 0.002\n",
      "iteration: 509400 loss: 0.0011 lr: 0.002\n",
      "iteration: 509410 loss: 0.0012 lr: 0.002\n",
      "iteration: 509420 loss: 0.0010 lr: 0.002\n",
      "iteration: 509430 loss: 0.0010 lr: 0.002\n",
      "iteration: 509440 loss: 0.0014 lr: 0.002\n",
      "iteration: 509450 loss: 0.0010 lr: 0.002\n",
      "iteration: 509460 loss: 0.0014 lr: 0.002\n",
      "iteration: 509470 loss: 0.0013 lr: 0.002\n",
      "iteration: 509480 loss: 0.0015 lr: 0.002\n",
      "iteration: 509490 loss: 0.0015 lr: 0.002\n",
      "iteration: 509500 loss: 0.0018 lr: 0.002\n",
      "iteration: 509510 loss: 0.0013 lr: 0.002\n",
      "iteration: 509520 loss: 0.0016 lr: 0.002\n",
      "iteration: 509530 loss: 0.0009 lr: 0.002\n",
      "iteration: 509540 loss: 0.0012 lr: 0.002\n",
      "iteration: 509550 loss: 0.0012 lr: 0.002\n",
      "iteration: 509560 loss: 0.0008 lr: 0.002\n",
      "iteration: 509570 loss: 0.0013 lr: 0.002\n",
      "iteration: 509580 loss: 0.0010 lr: 0.002\n",
      "iteration: 509590 loss: 0.0010 lr: 0.002\n",
      "iteration: 509600 loss: 0.0011 lr: 0.002\n",
      "iteration: 509610 loss: 0.0011 lr: 0.002\n",
      "iteration: 509620 loss: 0.0008 lr: 0.002\n",
      "iteration: 509630 loss: 0.0014 lr: 0.002\n",
      "iteration: 509640 loss: 0.0012 lr: 0.002\n",
      "iteration: 509650 loss: 0.0012 lr: 0.002\n",
      "iteration: 509660 loss: 0.0011 lr: 0.002\n",
      "iteration: 509670 loss: 0.0010 lr: 0.002\n",
      "iteration: 509680 loss: 0.0010 lr: 0.002\n",
      "iteration: 509690 loss: 0.0011 lr: 0.002\n",
      "iteration: 509700 loss: 0.0012 lr: 0.002\n",
      "iteration: 509710 loss: 0.0009 lr: 0.002\n",
      "iteration: 509720 loss: 0.0012 lr: 0.002\n",
      "iteration: 509730 loss: 0.0011 lr: 0.002\n",
      "iteration: 509740 loss: 0.0015 lr: 0.002\n",
      "iteration: 509750 loss: 0.0015 lr: 0.002\n",
      "iteration: 509760 loss: 0.0011 lr: 0.002\n",
      "iteration: 509770 loss: 0.0011 lr: 0.002\n",
      "iteration: 509780 loss: 0.0014 lr: 0.002\n",
      "iteration: 509790 loss: 0.0009 lr: 0.002\n",
      "iteration: 509800 loss: 0.0018 lr: 0.002\n",
      "iteration: 509810 loss: 0.0008 lr: 0.002\n",
      "iteration: 509820 loss: 0.0019 lr: 0.002\n",
      "iteration: 509830 loss: 0.0009 lr: 0.002\n",
      "iteration: 509840 loss: 0.0015 lr: 0.002\n",
      "iteration: 509850 loss: 0.0013 lr: 0.002\n",
      "iteration: 509860 loss: 0.0011 lr: 0.002\n",
      "iteration: 509870 loss: 0.0015 lr: 0.002\n",
      "iteration: 509880 loss: 0.0011 lr: 0.002\n",
      "iteration: 509890 loss: 0.0016 lr: 0.002\n",
      "iteration: 509900 loss: 0.0010 lr: 0.002\n",
      "iteration: 509910 loss: 0.0014 lr: 0.002\n",
      "iteration: 509920 loss: 0.0010 lr: 0.002\n",
      "iteration: 509930 loss: 0.0015 lr: 0.002\n",
      "iteration: 509940 loss: 0.0020 lr: 0.002\n",
      "iteration: 509950 loss: 0.0014 lr: 0.002\n",
      "iteration: 509960 loss: 0.0008 lr: 0.002\n",
      "iteration: 509970 loss: 0.0013 lr: 0.002\n",
      "iteration: 509980 loss: 0.0014 lr: 0.002\n",
      "iteration: 509990 loss: 0.0011 lr: 0.002\n",
      "iteration: 510000 loss: 0.0015 lr: 0.002\n",
      "iteration: 510010 loss: 0.0010 lr: 0.002\n",
      "iteration: 510020 loss: 0.0008 lr: 0.002\n",
      "iteration: 510030 loss: 0.0011 lr: 0.002\n",
      "iteration: 510040 loss: 0.0027 lr: 0.002\n",
      "iteration: 510050 loss: 0.0012 lr: 0.002\n",
      "iteration: 510060 loss: 0.0010 lr: 0.002\n",
      "iteration: 510070 loss: 0.0009 lr: 0.002\n",
      "iteration: 510080 loss: 0.0012 lr: 0.002\n",
      "iteration: 510090 loss: 0.0008 lr: 0.002\n",
      "iteration: 510100 loss: 0.0012 lr: 0.002\n",
      "iteration: 510110 loss: 0.0010 lr: 0.002\n",
      "iteration: 510120 loss: 0.0005 lr: 0.002\n",
      "iteration: 510130 loss: 0.0012 lr: 0.002\n",
      "iteration: 510140 loss: 0.0010 lr: 0.002\n",
      "iteration: 510150 loss: 0.0010 lr: 0.002\n",
      "iteration: 510160 loss: 0.0010 lr: 0.002\n",
      "iteration: 510170 loss: 0.0012 lr: 0.002\n",
      "iteration: 510180 loss: 0.0025 lr: 0.002\n",
      "iteration: 510190 loss: 0.0010 lr: 0.002\n",
      "iteration: 510200 loss: 0.0011 lr: 0.002\n",
      "iteration: 510210 loss: 0.0010 lr: 0.002\n",
      "iteration: 510220 loss: 0.0009 lr: 0.002\n",
      "iteration: 510230 loss: 0.0008 lr: 0.002\n",
      "iteration: 510240 loss: 0.0007 lr: 0.002\n",
      "iteration: 510250 loss: 0.0014 lr: 0.002\n",
      "iteration: 510260 loss: 0.0013 lr: 0.002\n",
      "iteration: 510270 loss: 0.0011 lr: 0.002\n",
      "iteration: 510280 loss: 0.0009 lr: 0.002\n",
      "iteration: 510290 loss: 0.0011 lr: 0.002\n",
      "iteration: 510300 loss: 0.0017 lr: 0.002\n",
      "iteration: 510310 loss: 0.0012 lr: 0.002\n",
      "iteration: 510320 loss: 0.0011 lr: 0.002\n",
      "iteration: 510330 loss: 0.0012 lr: 0.002\n",
      "iteration: 510340 loss: 0.0009 lr: 0.002\n",
      "iteration: 510350 loss: 0.0007 lr: 0.002\n",
      "iteration: 510360 loss: 0.0010 lr: 0.002\n",
      "iteration: 510370 loss: 0.0018 lr: 0.002\n",
      "iteration: 510380 loss: 0.0008 lr: 0.002\n",
      "iteration: 510390 loss: 0.0014 lr: 0.002\n",
      "iteration: 510400 loss: 0.0013 lr: 0.002\n",
      "iteration: 510410 loss: 0.0011 lr: 0.002\n",
      "iteration: 510420 loss: 0.0013 lr: 0.002\n",
      "iteration: 510430 loss: 0.0011 lr: 0.002\n",
      "iteration: 510440 loss: 0.0006 lr: 0.002\n",
      "iteration: 510450 loss: 0.0009 lr: 0.002\n",
      "iteration: 510460 loss: 0.0011 lr: 0.002\n",
      "iteration: 510470 loss: 0.0012 lr: 0.002\n",
      "iteration: 510480 loss: 0.0009 lr: 0.002\n",
      "iteration: 510490 loss: 0.0015 lr: 0.002\n",
      "iteration: 510500 loss: 0.0010 lr: 0.002\n",
      "iteration: 510510 loss: 0.0013 lr: 0.002\n",
      "iteration: 510520 loss: 0.0011 lr: 0.002\n",
      "iteration: 510530 loss: 0.0011 lr: 0.002\n",
      "iteration: 510540 loss: 0.0019 lr: 0.002\n",
      "iteration: 510550 loss: 0.0011 lr: 0.002\n",
      "iteration: 510560 loss: 0.0009 lr: 0.002\n",
      "iteration: 510570 loss: 0.0011 lr: 0.002\n",
      "iteration: 510580 loss: 0.0010 lr: 0.002\n",
      "iteration: 510590 loss: 0.0017 lr: 0.002\n",
      "iteration: 510600 loss: 0.0014 lr: 0.002\n",
      "iteration: 510610 loss: 0.0011 lr: 0.002\n",
      "iteration: 510620 loss: 0.0010 lr: 0.002\n",
      "iteration: 510630 loss: 0.0014 lr: 0.002\n",
      "iteration: 510640 loss: 0.0007 lr: 0.002\n",
      "iteration: 510650 loss: 0.0011 lr: 0.002\n",
      "iteration: 510660 loss: 0.0010 lr: 0.002\n",
      "iteration: 510670 loss: 0.0010 lr: 0.002\n",
      "iteration: 510680 loss: 0.0012 lr: 0.002\n",
      "iteration: 510690 loss: 0.0012 lr: 0.002\n",
      "iteration: 510700 loss: 0.0012 lr: 0.002\n",
      "iteration: 510710 loss: 0.0012 lr: 0.002\n",
      "iteration: 510720 loss: 0.0012 lr: 0.002\n",
      "iteration: 510730 loss: 0.0013 lr: 0.002\n",
      "iteration: 510740 loss: 0.0018 lr: 0.002\n",
      "iteration: 510750 loss: 0.0011 lr: 0.002\n",
      "iteration: 510760 loss: 0.0015 lr: 0.002\n",
      "iteration: 510770 loss: 0.0011 lr: 0.002\n",
      "iteration: 510780 loss: 0.0008 lr: 0.002\n",
      "iteration: 510790 loss: 0.0010 lr: 0.002\n",
      "iteration: 510800 loss: 0.0016 lr: 0.002\n",
      "iteration: 510810 loss: 0.0011 lr: 0.002\n",
      "iteration: 510820 loss: 0.0009 lr: 0.002\n",
      "iteration: 510830 loss: 0.0011 lr: 0.002\n",
      "iteration: 510840 loss: 0.0013 lr: 0.002\n",
      "iteration: 510850 loss: 0.0014 lr: 0.002\n",
      "iteration: 510860 loss: 0.0014 lr: 0.002\n",
      "iteration: 510870 loss: 0.0013 lr: 0.002\n",
      "iteration: 510880 loss: 0.0010 lr: 0.002\n",
      "iteration: 510890 loss: 0.0012 lr: 0.002\n",
      "iteration: 510900 loss: 0.0010 lr: 0.002\n",
      "iteration: 510910 loss: 0.0012 lr: 0.002\n",
      "iteration: 510920 loss: 0.0008 lr: 0.002\n",
      "iteration: 510930 loss: 0.0014 lr: 0.002\n",
      "iteration: 510940 loss: 0.0013 lr: 0.002\n",
      "iteration: 510950 loss: 0.0012 lr: 0.002\n",
      "iteration: 510960 loss: 0.0012 lr: 0.002\n",
      "iteration: 510970 loss: 0.0009 lr: 0.002\n",
      "iteration: 510980 loss: 0.0008 lr: 0.002\n",
      "iteration: 510990 loss: 0.0022 lr: 0.002\n",
      "iteration: 511000 loss: 0.0012 lr: 0.002\n",
      "iteration: 511010 loss: 0.0008 lr: 0.002\n",
      "iteration: 511020 loss: 0.0010 lr: 0.002\n",
      "iteration: 511030 loss: 0.0009 lr: 0.002\n",
      "iteration: 511040 loss: 0.0009 lr: 0.002\n",
      "iteration: 511050 loss: 0.0008 lr: 0.002\n",
      "iteration: 511060 loss: 0.0011 lr: 0.002\n",
      "iteration: 511070 loss: 0.0016 lr: 0.002\n",
      "iteration: 511080 loss: 0.0011 lr: 0.002\n",
      "iteration: 511090 loss: 0.0013 lr: 0.002\n",
      "iteration: 511100 loss: 0.0008 lr: 0.002\n",
      "iteration: 511110 loss: 0.0011 lr: 0.002\n",
      "iteration: 511120 loss: 0.0011 lr: 0.002\n",
      "iteration: 511130 loss: 0.0016 lr: 0.002\n",
      "iteration: 511140 loss: 0.0011 lr: 0.002\n",
      "iteration: 511150 loss: 0.0013 lr: 0.002\n",
      "iteration: 511160 loss: 0.0008 lr: 0.002\n",
      "iteration: 511170 loss: 0.0016 lr: 0.002\n",
      "iteration: 511180 loss: 0.0009 lr: 0.002\n",
      "iteration: 511190 loss: 0.0009 lr: 0.002\n",
      "iteration: 511200 loss: 0.0012 lr: 0.002\n",
      "iteration: 511210 loss: 0.0016 lr: 0.002\n",
      "iteration: 511220 loss: 0.0016 lr: 0.002\n",
      "iteration: 511230 loss: 0.0013 lr: 0.002\n",
      "iteration: 511240 loss: 0.0020 lr: 0.002\n",
      "iteration: 511250 loss: 0.0010 lr: 0.002\n",
      "iteration: 511260 loss: 0.0010 lr: 0.002\n",
      "iteration: 511270 loss: 0.0008 lr: 0.002\n",
      "iteration: 511280 loss: 0.0011 lr: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 511290 loss: 0.0010 lr: 0.002\n",
      "iteration: 511300 loss: 0.0018 lr: 0.002\n",
      "iteration: 511310 loss: 0.0010 lr: 0.002\n",
      "iteration: 511320 loss: 0.0019 lr: 0.002\n",
      "iteration: 511330 loss: 0.0013 lr: 0.002\n",
      "iteration: 511340 loss: 0.0017 lr: 0.002\n",
      "iteration: 511350 loss: 0.0018 lr: 0.002\n",
      "iteration: 511360 loss: 0.0012 lr: 0.002\n",
      "iteration: 511370 loss: 0.0012 lr: 0.002\n",
      "iteration: 511380 loss: 0.0009 lr: 0.002\n",
      "iteration: 511390 loss: 0.0012 lr: 0.002\n",
      "iteration: 511400 loss: 0.0012 lr: 0.002\n",
      "iteration: 511410 loss: 0.0009 lr: 0.002\n",
      "iteration: 511420 loss: 0.0011 lr: 0.002\n",
      "iteration: 511430 loss: 0.0012 lr: 0.002\n",
      "iteration: 511440 loss: 0.0012 lr: 0.002\n",
      "iteration: 511450 loss: 0.0011 lr: 0.002\n",
      "iteration: 511460 loss: 0.0013 lr: 0.002\n",
      "iteration: 511470 loss: 0.0013 lr: 0.002\n",
      "iteration: 511480 loss: 0.0014 lr: 0.002\n",
      "iteration: 511490 loss: 0.0009 lr: 0.002\n",
      "iteration: 511500 loss: 0.0009 lr: 0.002\n",
      "iteration: 511510 loss: 0.0009 lr: 0.002\n",
      "iteration: 511520 loss: 0.0011 lr: 0.002\n",
      "iteration: 511530 loss: 0.0011 lr: 0.002\n",
      "iteration: 511540 loss: 0.0009 lr: 0.002\n",
      "iteration: 511550 loss: 0.0014 lr: 0.002\n",
      "iteration: 511560 loss: 0.0009 lr: 0.002\n",
      "iteration: 511570 loss: 0.0012 lr: 0.002\n",
      "iteration: 511580 loss: 0.0010 lr: 0.002\n",
      "iteration: 511590 loss: 0.0010 lr: 0.002\n",
      "iteration: 511600 loss: 0.0014 lr: 0.002\n",
      "iteration: 511610 loss: 0.0008 lr: 0.002\n",
      "iteration: 511620 loss: 0.0021 lr: 0.002\n",
      "iteration: 511630 loss: 0.0013 lr: 0.002\n",
      "iteration: 511640 loss: 0.0014 lr: 0.002\n",
      "iteration: 511650 loss: 0.0015 lr: 0.002\n",
      "iteration: 511660 loss: 0.0010 lr: 0.002\n",
      "iteration: 511670 loss: 0.0012 lr: 0.002\n",
      "iteration: 511680 loss: 0.0010 lr: 0.002\n",
      "iteration: 511690 loss: 0.0009 lr: 0.002\n",
      "iteration: 511700 loss: 0.0007 lr: 0.002\n",
      "iteration: 511710 loss: 0.0011 lr: 0.002\n",
      "iteration: 511720 loss: 0.0012 lr: 0.002\n",
      "iteration: 511730 loss: 0.0013 lr: 0.002\n",
      "iteration: 511740 loss: 0.0017 lr: 0.002\n",
      "iteration: 511750 loss: 0.0016 lr: 0.002\n",
      "iteration: 511760 loss: 0.0015 lr: 0.002\n",
      "iteration: 511770 loss: 0.0012 lr: 0.002\n",
      "iteration: 511780 loss: 0.0012 lr: 0.002\n",
      "iteration: 511790 loss: 0.0011 lr: 0.002\n",
      "iteration: 511800 loss: 0.0019 lr: 0.002\n",
      "iteration: 511810 loss: 0.0012 lr: 0.002\n",
      "iteration: 511820 loss: 0.0009 lr: 0.002\n",
      "iteration: 511830 loss: 0.0014 lr: 0.002\n",
      "iteration: 511840 loss: 0.0011 lr: 0.002\n",
      "iteration: 511850 loss: 0.0016 lr: 0.002\n",
      "iteration: 511860 loss: 0.0015 lr: 0.002\n",
      "iteration: 511870 loss: 0.0009 lr: 0.002\n",
      "iteration: 511880 loss: 0.0010 lr: 0.002\n",
      "iteration: 511890 loss: 0.0010 lr: 0.002\n",
      "iteration: 511900 loss: 0.0008 lr: 0.002\n",
      "iteration: 511910 loss: 0.0014 lr: 0.002\n",
      "iteration: 511920 loss: 0.0016 lr: 0.002\n",
      "iteration: 511930 loss: 0.0013 lr: 0.002\n",
      "iteration: 511940 loss: 0.0008 lr: 0.002\n",
      "iteration: 511950 loss: 0.0011 lr: 0.002\n",
      "iteration: 511960 loss: 0.0024 lr: 0.002\n",
      "iteration: 511970 loss: 0.0007 lr: 0.002\n",
      "iteration: 511980 loss: 0.0011 lr: 0.002\n",
      "iteration: 511990 loss: 0.0010 lr: 0.002\n",
      "iteration: 512000 loss: 0.0009 lr: 0.002\n",
      "iteration: 512010 loss: 0.0008 lr: 0.002\n",
      "iteration: 512020 loss: 0.0015 lr: 0.002\n",
      "iteration: 512030 loss: 0.0018 lr: 0.002\n",
      "iteration: 512040 loss: 0.0009 lr: 0.002\n",
      "iteration: 512050 loss: 0.0012 lr: 0.002\n",
      "iteration: 512060 loss: 0.0008 lr: 0.002\n",
      "iteration: 512070 loss: 0.0009 lr: 0.002\n",
      "iteration: 512080 loss: 0.0013 lr: 0.002\n",
      "iteration: 512090 loss: 0.0015 lr: 0.002\n",
      "iteration: 512100 loss: 0.0011 lr: 0.002\n",
      "iteration: 512110 loss: 0.0012 lr: 0.002\n",
      "iteration: 512120 loss: 0.0007 lr: 0.002\n",
      "iteration: 512130 loss: 0.0013 lr: 0.002\n",
      "iteration: 512140 loss: 0.0011 lr: 0.002\n",
      "iteration: 512150 loss: 0.0011 lr: 0.002\n",
      "iteration: 512160 loss: 0.0010 lr: 0.002\n",
      "iteration: 512170 loss: 0.0010 lr: 0.002\n",
      "iteration: 512180 loss: 0.0009 lr: 0.002\n",
      "iteration: 512190 loss: 0.0009 lr: 0.002\n",
      "iteration: 512200 loss: 0.0012 lr: 0.002\n",
      "iteration: 512210 loss: 0.0018 lr: 0.002\n",
      "iteration: 512220 loss: 0.0010 lr: 0.002\n",
      "iteration: 512230 loss: 0.0009 lr: 0.002\n",
      "iteration: 512240 loss: 0.0007 lr: 0.002\n",
      "iteration: 512250 loss: 0.0019 lr: 0.002\n",
      "iteration: 512260 loss: 0.0009 lr: 0.002\n",
      "iteration: 512270 loss: 0.0010 lr: 0.002\n",
      "iteration: 512280 loss: 0.0014 lr: 0.002\n",
      "iteration: 512290 loss: 0.0011 lr: 0.002\n",
      "iteration: 512300 loss: 0.0009 lr: 0.002\n",
      "iteration: 512310 loss: 0.0011 lr: 0.002\n",
      "iteration: 512320 loss: 0.0008 lr: 0.002\n",
      "iteration: 512330 loss: 0.0014 lr: 0.002\n",
      "iteration: 512340 loss: 0.0009 lr: 0.002\n",
      "iteration: 512350 loss: 0.0010 lr: 0.002\n",
      "iteration: 512360 loss: 0.0015 lr: 0.002\n",
      "iteration: 512370 loss: 0.0016 lr: 0.002\n",
      "iteration: 512380 loss: 0.0016 lr: 0.002\n",
      "iteration: 512390 loss: 0.0008 lr: 0.002\n",
      "iteration: 512400 loss: 0.0010 lr: 0.002\n",
      "iteration: 512410 loss: 0.0010 lr: 0.002\n",
      "iteration: 512420 loss: 0.0009 lr: 0.002\n",
      "iteration: 512430 loss: 0.0011 lr: 0.002\n",
      "iteration: 512440 loss: 0.0013 lr: 0.002\n",
      "iteration: 512450 loss: 0.0010 lr: 0.002\n",
      "iteration: 512460 loss: 0.0013 lr: 0.002\n",
      "iteration: 512470 loss: 0.0016 lr: 0.002\n",
      "iteration: 512480 loss: 0.0012 lr: 0.002\n",
      "iteration: 512490 loss: 0.0010 lr: 0.002\n",
      "iteration: 512500 loss: 0.0008 lr: 0.002\n",
      "iteration: 512510 loss: 0.0008 lr: 0.002\n",
      "iteration: 512520 loss: 0.0013 lr: 0.002\n",
      "iteration: 512530 loss: 0.0013 lr: 0.002\n",
      "iteration: 512540 loss: 0.0012 lr: 0.002\n",
      "iteration: 512550 loss: 0.0011 lr: 0.002\n",
      "iteration: 512560 loss: 0.0008 lr: 0.002\n",
      "iteration: 512570 loss: 0.0010 lr: 0.002\n",
      "iteration: 512580 loss: 0.0012 lr: 0.002\n",
      "iteration: 512590 loss: 0.0009 lr: 0.002\n",
      "iteration: 512600 loss: 0.0024 lr: 0.002\n",
      "iteration: 512610 loss: 0.0009 lr: 0.002\n",
      "iteration: 512620 loss: 0.0012 lr: 0.002\n",
      "iteration: 512630 loss: 0.0013 lr: 0.002\n",
      "iteration: 512640 loss: 0.0014 lr: 0.002\n",
      "iteration: 512650 loss: 0.0008 lr: 0.002\n",
      "iteration: 512660 loss: 0.0013 lr: 0.002\n",
      "iteration: 512670 loss: 0.0013 lr: 0.002\n",
      "iteration: 512680 loss: 0.0009 lr: 0.002\n",
      "iteration: 512690 loss: 0.0012 lr: 0.002\n",
      "iteration: 512700 loss: 0.0009 lr: 0.002\n",
      "iteration: 512710 loss: 0.0009 lr: 0.002\n",
      "iteration: 512720 loss: 0.0007 lr: 0.002\n",
      "iteration: 512730 loss: 0.0013 lr: 0.002\n",
      "iteration: 512740 loss: 0.0011 lr: 0.002\n",
      "iteration: 512750 loss: 0.0013 lr: 0.002\n",
      "iteration: 512760 loss: 0.0008 lr: 0.002\n",
      "iteration: 512770 loss: 0.0014 lr: 0.002\n",
      "iteration: 512780 loss: 0.0011 lr: 0.002\n",
      "iteration: 512790 loss: 0.0010 lr: 0.002\n",
      "iteration: 512800 loss: 0.0009 lr: 0.002\n",
      "iteration: 512810 loss: 0.0010 lr: 0.002\n",
      "iteration: 512820 loss: 0.0011 lr: 0.002\n",
      "iteration: 512830 loss: 0.0009 lr: 0.002\n",
      "iteration: 512840 loss: 0.0012 lr: 0.002\n",
      "iteration: 512850 loss: 0.0010 lr: 0.002\n",
      "iteration: 512860 loss: 0.0008 lr: 0.002\n",
      "iteration: 512870 loss: 0.0014 lr: 0.002\n",
      "iteration: 512880 loss: 0.0016 lr: 0.002\n",
      "iteration: 512890 loss: 0.0009 lr: 0.002\n",
      "iteration: 512900 loss: 0.0012 lr: 0.002\n",
      "iteration: 512910 loss: 0.0011 lr: 0.002\n",
      "iteration: 512920 loss: 0.0009 lr: 0.002\n",
      "iteration: 512930 loss: 0.0011 lr: 0.002\n",
      "iteration: 512940 loss: 0.0009 lr: 0.002\n",
      "iteration: 512950 loss: 0.0010 lr: 0.002\n",
      "iteration: 512960 loss: 0.0009 lr: 0.002\n",
      "iteration: 512970 loss: 0.0016 lr: 0.002\n",
      "iteration: 512980 loss: 0.0014 lr: 0.002\n",
      "iteration: 512990 loss: 0.0010 lr: 0.002\n",
      "iteration: 513000 loss: 0.0015 lr: 0.002\n",
      "iteration: 513010 loss: 0.0008 lr: 0.002\n",
      "iteration: 513020 loss: 0.0015 lr: 0.002\n",
      "iteration: 513030 loss: 0.0014 lr: 0.002\n",
      "iteration: 513040 loss: 0.0009 lr: 0.002\n",
      "iteration: 513050 loss: 0.0011 lr: 0.002\n",
      "iteration: 513060 loss: 0.0008 lr: 0.002\n",
      "iteration: 513070 loss: 0.0014 lr: 0.002\n",
      "iteration: 513080 loss: 0.0014 lr: 0.002\n",
      "iteration: 513090 loss: 0.0010 lr: 0.002\n",
      "iteration: 513100 loss: 0.0009 lr: 0.002\n",
      "iteration: 513110 loss: 0.0010 lr: 0.002\n",
      "iteration: 513120 loss: 0.0011 lr: 0.002\n",
      "iteration: 513130 loss: 0.0011 lr: 0.002\n",
      "iteration: 513140 loss: 0.0008 lr: 0.002\n",
      "iteration: 513150 loss: 0.0014 lr: 0.002\n",
      "iteration: 513160 loss: 0.0010 lr: 0.002\n",
      "iteration: 513170 loss: 0.0010 lr: 0.002\n",
      "iteration: 513180 loss: 0.0013 lr: 0.002\n",
      "iteration: 513190 loss: 0.0010 lr: 0.002\n",
      "iteration: 513200 loss: 0.0009 lr: 0.002\n",
      "iteration: 513210 loss: 0.0010 lr: 0.002\n",
      "iteration: 513220 loss: 0.0013 lr: 0.002\n",
      "iteration: 513230 loss: 0.0010 lr: 0.002\n",
      "iteration: 513240 loss: 0.0019 lr: 0.002\n",
      "iteration: 513250 loss: 0.0010 lr: 0.002\n",
      "iteration: 513260 loss: 0.0008 lr: 0.002\n",
      "iteration: 513270 loss: 0.0017 lr: 0.002\n",
      "iteration: 513280 loss: 0.0009 lr: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 513290 loss: 0.0012 lr: 0.002\n",
      "iteration: 513300 loss: 0.0011 lr: 0.002\n",
      "iteration: 513310 loss: 0.0014 lr: 0.002\n",
      "iteration: 513320 loss: 0.0022 lr: 0.002\n",
      "iteration: 513330 loss: 0.0013 lr: 0.002\n",
      "iteration: 513340 loss: 0.0016 lr: 0.002\n",
      "iteration: 513350 loss: 0.0013 lr: 0.002\n",
      "iteration: 513360 loss: 0.0011 lr: 0.002\n",
      "iteration: 513370 loss: 0.0012 lr: 0.002\n",
      "iteration: 513380 loss: 0.0011 lr: 0.002\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-ea0164fec930>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#tf.reset_default_graph()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdeeplabcut\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_config_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaveiters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisplayiters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgputouse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#this will run until you stop it (CTRL+C), or hit \"STOP\" icon, or when it hits the end (default, 1.3M iterations).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\mysoredata\\nbk\\mousevideoanalysis\\dlc_env_conda\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\training.py\u001b[0m in \u001b[0;36mtrain_network\u001b[1;34m(config, shuffle, trainingsetindex, max_snapshots_to_keep, displayiters, saveiters, maxiters, allow_growth, gputouse, autotune, keepdeconvweights, modelprefix)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\mysoredata\\nbk\\mousevideoanalysis\\dlc_env_conda\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\training.py\u001b[0m in \u001b[0;36mtrain_network\u001b[1;34m(config, shuffle, trainingsetindex, max_snapshots_to_keep, displayiters, saveiters, maxiters, allow_growth, gputouse, autotune, keepdeconvweights, modelprefix)\u001b[0m\n\u001b[0;32m    187\u001b[0m                 \u001b[0mmax_to_keep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_snapshots_to_keep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m                 \u001b[0mkeepdeconvweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeepdeconvweights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m                 \u001b[0mallow_growth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mallow_growth\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m             )  # pass on path and file name for pose_cfg.yaml!\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\mysoredata\\nbk\\mousevideoanalysis\\dlc_env_conda\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\train.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(config_yaml, displayiters, saveiters, maxiters, max_to_keep, keepdeconvweights, allow_growth)\u001b[0m\n\u001b[0;32m    276\u001b[0m         [_, loss_val, summary] = sess.run(\n\u001b[0;32m    277\u001b[0m             \u001b[1;33m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmerged_summaries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 278\u001b[1;33m             \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    279\u001b[0m         )\n\u001b[0;32m    280\u001b[0m         \u001b[0mcum_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\mysoredata\\nbk\\mousevideoanalysis\\dlc_env_conda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\mysoredata\\nbk\\mousevideoanalysis\\dlc_env_conda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1171\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1173\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1174\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\mysoredata\\nbk\\mousevideoanalysis\\dlc_env_conda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1350\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\mysoredata\\nbk\\mousevideoanalysis\\dlc_env_conda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1354\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1355\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1356\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1357\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\mysoredata\\nbk\\mousevideoanalysis\\dlc_env_conda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1341\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\mysoredata\\nbk\\mousevideoanalysis\\dlc_env_conda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1429\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1431\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#reset in case you started a session before...\n",
    "#tf.reset_default_graph()\n",
    "\n",
    "deeplabcut.train_network(path_config_file, shuffle=1, saveiters=1000, displayiters=10, gputouse=0)\n",
    "\n",
    "#this will run until you stop it (CTRL+C), or hit \"STOP\" icon, or when it hits the end (default, 1.3M iterations). \n",
    "#Whichever you chose, you will see what looks like an error message, but it's not an error - don't worry....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A couple tips for possible troubleshooting (1): \n",
    "\n",
    "if you get **permission errors** when you run this step (above), first check if the weights downloaded. As some docker containers might not have privileges for this (it can be user specific). They should be under 'init_weights' (see path in the pose_cfg.yaml file). You can enter the DOCKER in the terminal:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see more here: https://github.com/MMathisLab/Docker4DeepLabCut2.0#using-the-docker-for-training-and-video-analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Troubleshooting (2): \n",
    "if it appears the training does not start (i.e. \"Starting training...\" does not print immediately),\n",
    "then you have another session running on your GPU. Go check \"nvidia-smi\" and look at the process names. You can only have 1 per GPU!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xZygsb2DoEJc"
   },
   "source": [
    "## Start evaluating\n",
    "This funtion evaluates a trained model for a specific shuffle/shuffles at a particular state or all the states on the data set (images)\n",
    "and stores the results as .csv file in a subdirectory under **evaluation-results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nv4zlbrnoEJg",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "{'all_joints': [[0], [1], [2]],\n",
      " 'all_joints_names': ['bodypart1', 'bodypart2', 'bodypart3'],\n",
      " 'batch_size': 1,\n",
      " 'crop_pad': 0,\n",
      " 'dataset': 'training-datasets\\\\iteration-0\\\\UnaugmentedDataSet_box1_cam1Mar18\\\\box1_cam1_spencerloggia95shuffle1.mat',\n",
      " 'dataset_type': 'imgaug',\n",
      " 'deterministic': False,\n",
      " 'fg_fraction': 0.25,\n",
      " 'global_scale': 0.8,\n",
      " 'init_weights': 'f:\\\\mysoredata\\\\nbk\\\\mousevideoanalysis\\\\dlc_env_conda\\\\lib\\\\site-packages\\\\deeplabcut\\\\pose_estimation_tensorflow\\\\models\\\\pretrained\\\\resnet_v1_50.ckpt',\n",
      " 'intermediate_supervision': False,\n",
      " 'intermediate_supervision_layer': 12,\n",
      " 'location_refinement': True,\n",
      " 'locref_huber_loss': True,\n",
      " 'locref_loss_weight': 1.0,\n",
      " 'locref_stdev': 7.2801,\n",
      " 'log_dir': 'log',\n",
      " 'mean_pixel': [123.68, 116.779, 103.939],\n",
      " 'mirror': False,\n",
      " 'net_type': 'resnet_50',\n",
      " 'num_joints': 3,\n",
      " 'optimizer': 'sgd',\n",
      " 'pairwise_huber_loss': True,\n",
      " 'pairwise_predict': False,\n",
      " 'partaffinityfield_predict': False,\n",
      " 'regularize': False,\n",
      " 'scoremap_dir': 'test',\n",
      " 'shuffle': True,\n",
      " 'snapshot_prefix': 'F:\\\\MysoreData\\\\nbk\\\\mouseVideoAnalysis\\\\Box1\\\\cam1\\\\box1_cam1-spencerloggia-2021-03-18\\\\dlc-models\\\\iteration-0\\\\box1_cam1Mar18-trainset95shuffle1\\\\test\\\\snapshot',\n",
      " 'stride': 8.0,\n",
      " 'weigh_negatives': False,\n",
      " 'weigh_only_present_joints': False,\n",
      " 'weigh_part_predictions': False,\n",
      " 'weight_decay': 0.0001}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running  DLC_resnet_50_box1_cam1Mar18shuffle1_513000  with # of trainingiterations: 513000\n",
      "Initializing ResNet\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002123E7940B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002123E7940B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000211E7949128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000211E7949128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x000002123E7947B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x000002123E7947B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002123E845198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002123E845198>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000211E73F3828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000211E73F3828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002123E845E80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002123E845E80>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212541E2C88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212541E2C88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002123E845400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002123E845400>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002123E7CE7F0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002123E7CE7F0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002123E845D68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002123E845D68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021256A0DE80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021256A0DE80>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002123E845D68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002123E845D68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021253B60DD8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021253B60DD8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002123E7E9780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002123E7E9780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212540F3BA8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212540F3BA8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212569D1F98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212569D1F98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002125ED3C7B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002125ED3C7B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x000002123E8453C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x000002123E8453C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002123E845AC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002123E845AC8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002125ED12828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002125ED12828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002125ED3C828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002125ED3C828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002123E794668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002123E794668>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002126645CEF0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002126645CEF0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002123E7945F8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002123E7945F8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002125ED12CF8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002125ED12CF8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021266473E48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021266473E48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212541E2A90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212541E2A90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021256A0DB70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021256A0DB70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212664C46D8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212664C46D8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021256D4EB00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021256D4EB00>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002125ED121D0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002125ED121D0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002125ED3CC18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002125ED3CC18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000021288347F28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000021288347F28>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212886A6F60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212886A6F60>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000021288668AC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000021288668AC8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002126648E080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002126648E080>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002125ED3CCC0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002125ED3CCC0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021288309EF0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021288309EF0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000021256D4EEF0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000021256D4EEF0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021288309358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021288309358>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212886A6438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212886A6438>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002126670CCF8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002126670CCF8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002129EDBBAC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002129EDBBAC8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002126670CA58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002126670CA58>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x00000212AB2A2550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x00000212AB2A2550>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212886A6320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212886A6320>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021288637400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021288637400>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002126670CCC0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002126670CCC0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212886A67B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212886A67B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212886A6D30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212886A6D30>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021289746DD8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021289746DD8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212AB2A22B0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212AB2A22B0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021289746748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000021289746748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002129ED9A6A0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002129ED9A6A0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002128975DB00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002128975DB00>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212BEBEA1D0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212BEBEA1D0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002129EDD24E0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002129EDD24E0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212AB2A20B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212AB2A20B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002129EDD25F8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002129EDD25F8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212BEBEA1D0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212BEBEA1D0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212BEBDFE10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212BEBDFE10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212C3E52F28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212C3E52F28>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212BEBDFD68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212BEBDFD68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212B691A4E0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212B691A4E0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212BEBDF4A8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212BEBDF4A8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212AB2A20B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212AB2A20B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212C3E66358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212C3E66358>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212BEBDFF60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212BEBDFF60>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212C3E66588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212C3E66588>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E0C93E48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E0C93E48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212CA110E48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212CA110E48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213DD55CDD8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213DD55CDD8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212CA110DD8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212CA110DD8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E0DFCB38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E0DFCB38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212CA16D550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212CA16D550>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E0DFCCF8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E0DFCCF8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212CA16DFD0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000212CA16DFD0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212CA110DD8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000212CA110DD8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000213E0CE7080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000213E0CE7080>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E36506A0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E36506A0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000213E0CE7208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000213E0CE7208>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E3650A58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E3650A58>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000213E3606F28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000213E3606F28>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E0DFCB38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E0DFCB38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000213E0D59C50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000213E0D59C50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E0D590B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E0D590B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000213E3523128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000213E3523128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E6D5D2E8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E6D5D2E8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000213E3523898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000213E3523898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E3730780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E3730780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000213E6CFAD68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000213E6CFAD68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E375C128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E375C128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000213E6F65C50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000213E6F65C50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E6CFAD68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E6CFAD68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000213E6E95D30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000213E6E95D30>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E6D007B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E6D007B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000213E6D00EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000213E6D00EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E6D31828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E6D31828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000213E6D00668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000213E6D00668>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E7026CC0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E7026CC0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000213E703E208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000213E703E208>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E6E52438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E6E52438>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000213E7225AC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000213E7225AC8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E6ED9E10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E6ED9E10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000213E6F65B70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000213E6F65B70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E71B92B0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E71B92B0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000213E6D007B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000213E6D007B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E6E52438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x00000213E6E52438>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000213E9B09F60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000213E9B09F60>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x000002125ED8D668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x000002125ED8D668>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x00000211E73F3898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x00000211E73F3898>>: AttributeError: module 'gast' has no attribute 'Index'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "205it [00:22,  9.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done and results stored for snapshot:  snapshot-513000\n",
      "Results for 513000  training iterations: 95 1 train error: 1.63 pixels. Test error: 17.13  pixels.\n",
      "With pcutoff of 0.6  train error: 1.63 pixels. Test error: 8.04 pixels\n",
      "Thereby, the errors are given by the average distances between the labels by DLC and the scorer.\n",
      "The network is evaluated and the results are stored in the subdirectory 'evaluation_results'.\n",
      "If it generalizes well, choose the best model for prediction and update the config file with the appropriate index for the 'snapshotindex'.\n",
      "Use the function 'analyze_video' to make predictions on new videos.\n",
      "Otherwise consider retraining the network (see DeepLabCut workflow Fig 2)\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.evaluate_network(path_config_file)\n",
    "\n",
    "# Here you want to see a low pixel error! Of course, it can only be as good as the labeler, so be sure your labels are good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BaLBl3TQtrfB"
   },
   "source": [
    "## There is an optional refinement step\n",
    "- if your pixel errors are not low enough, use DLC docs on how to refine yur network!\n",
    "- You will need to adjust the labels outside of DOCKER! (you can use the createDLCproject notebook) \n",
    "-  see DLC protocol instructions on how to refine your data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OVFLSKKfoEJk"
   },
   "source": [
    "## Start Analyzing videos\n",
    "This function analyzes the new video. The user can choose the best model from the evaluation results and specify the correct snapshot index for the variable **snapshotindex** in the **config.yaml** file. Otherwise, by default the most recent snapshot is used to analyse the video.\n",
    "\n",
    "The results are stored in hd5 file in the same directory where the video resides. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y_LZiS_0oEJl",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "VIDEO_SOURCE = r\"F:\\MysoreData\\nbk\\mouseVideoAnalysis\\Box1\\cam1\\test_labels\"\n",
    "video_all = os.listdir(VIDEO_SOURCE)\n",
    "video = []\n",
    "for i in range(0,len(video_all)):\n",
    "    video.append(VIDEO_SOURCE + '\\\\' + video_all[i])\n",
    "    \n",
    "deeplabcut.analyze_videos(path_config_file,video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pCrUvQIvoEKD"
   },
   "source": [
    "## Create labeled video\n",
    "This funtion is for visualiztion purpose and can be used to create a video in .mp4 format with labels predicted by the network. This video is saved in the same directory where the original video resides. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6aDF7Q7KoEKE",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deeplabcut.create_labeled_video(path_config_file,video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8GTiuJESoEKH"
   },
   "source": [
    "## Plot the trajectories of the analyzed videos\n",
    "This function plots the trajectories of all the body parts across the entire video. Each body part is identified by a unique color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gX21zZbXoEKJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook \n",
    "#for making interactive plots.\n",
    "#deeplabcut.plot_trajectories(path_config_file,videofile_path, plotting=True)\n",
    "\n",
    "deeplabcut.plot_trajectories(path_config_file,video,showfigures=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Colab_TrainNetwork_VideoAnalysis.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
